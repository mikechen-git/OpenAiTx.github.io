<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ProxyAsLocalModel - Stream29/ProxyAsLocalModel ja</title>
    <meta name="title" content="ProxyAsLocalModel - Stream29/ProxyAsLocalModel ja | ProxyAsLocalModel リモートLLM APIをローカルモデルとしてプロキシします。特にJetBrains AI AssistantでカスタムLLMを利用する際に有効です。 Ktorとkotlinx.serializationによって実現されています。これらのリフレクション非依存の特徴に感謝します。 このプ...">
    <meta name="description" content="Stream29/ProxyAsLocalModel - GitHub repository ja documentation and information | ProxyAsLocalModel リモートLLM APIをローカルモデルとしてプロキシします。特にJetBrains AI AssistantでカスタムLLMを利用する際に有効です。 Ktorとkotlinx.serializationによって実現されています。これらのリフレクション非依存の特徴に感謝します。 このプ...">
    <meta name="keywords" content="Stream29, ProxyAsLocalModel, GitHub, repository, ja documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/Stream29/ProxyAsLocalModel/README-ja.html">
    <meta property="og:title" content="ProxyAsLocalModel - Stream29/ProxyAsLocalModel ja | ProxyAsLocalModel リモートLLM APIをローカルモデルとしてプロキシします。特にJetBrains AI AssistantでカスタムLLMを利用する際に有効です。 Ktorとkotlinx.serializationによって実現されています。これらのリフレクション非依存の特徴に感謝します。 このプ...">
    <meta property="og:description" content="Stream29/ProxyAsLocalModel - GitHub repository ja documentation and information | ProxyAsLocalModel リモートLLM APIをローカルモデルとしてプロキシします。特にJetBrains AI AssistantでカスタムLLMを利用する際に有効です。 Ktorとkotlinx.serializationによって実現されています。これらのリフレクション非依存の特徴に感謝します。 このプ...">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">
    <script src="/js/marked.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        body { display: flex; flex-direction: column; min-height: 100vh; }
        .main-container { margin: 0 auto; width: 100%; max-width: 980px; padding: 0 20px; }
        @media (max-width: 768px) { .main-container { padding: 0 15px; } }
        .markdown-body img { max-width: 100%; height: auto; }
        .header { text-align: center; margin-bottom: 30px; padding: 20px; background-color: #f6f8fa; border-bottom: 1px solid #e1e4e8; position: relative; }
        .header .links { margin-top: 10px; font-size: 16px; }
        .header .links a { color: #0366d6; text-decoration: none; margin-left: 5px; }
        .header .links a:hover { text-decoration: underline; }
        .language-badges { margin-top: 15px; text-align: center; }
        .language-badges a { display: inline-block; margin: 2px; text-decoration: none; }
        .language-badges img { height: 20px; border-radius: 3px; }
        .language-badges a:hover img { opacity: 0.8; }
    </style>
</head>
<body>
    <div style="position: fixed; top: 2px; left: 2px; z-index: 2000; background: rgba(255,255,255,0.95); border-radius: 6px; box-shadow: 0 2px 8px rgba(0,0,0,0.06); padding: 4px 14px; font-size: 15px; color: #222; font-family: 'Segoe UI', Arial, sans-serif; font-weight: 500; letter-spacing: 0.5px;">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" style="color: #0366d6; text-decoration: none; font-weight: 600;">Open AI Tx</a>
    </div>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/Stream29/ProxyAsLocalModel" id="githubRepoLink" target="_blank">Stream29/ProxyAsLocalModel</a>
<h1 style="display: none;">ProxyAsLocalModel リモートLLM APIをローカルモデルとしてプロキシします。特にJetBrains AI AssistantでカスタムLLMを利用する際に有効です。 Ktorとkotlinx.serializationによって実現されています。これらのリフレクション非依存の特徴に感謝します。 このプ...</h1>
        </div>
        <div class="language-badges" id="languageBadges">
            <!-- You can generate language badges here if needed -->
        </div>
    </div>
    <div class="main-container">
        <div class="markdown-body" id="content">
            <h1>ProxyAsLocalModel</h1>
<p>リモートLLM APIをローカルモデルとしてプロキシします。特にJetBrains AI AssistantでカスタムLLMを利用する際に有効です。</p>
<p>Ktorとkotlinx.serializationによって実現されています。これらのリフレクション非依存の特徴に感謝します。</p>
<h2>このプロジェクトの経緯</h2>
<p>現在、JetBrains AI Assistantは非常に制限された無料プランを提供しています。試してみたところ、すぐに上限に達してしまいました。</p>
<p>私はすでにGeminiやQwenなど他のLLM APIトークンを購入済みでした。そのため、これらをAI Assistantで利用できないかと考え始めました。しかし、LM StudioとOllamaのローカルモデルのみがサポートされていました。そこで、サードパーティ製LLM APIをLM StudioやOllama APIとしてプロキシするこのアプリケーションの開発に着手しました。これにより、JetBrains IDE上でこれらを利用できるようになります。</p>
<p>これはシンプルなタスクだったため、公式SDKをクライアントとして利用し、LM StudioやOllamaのエンドポイントを提供するシンプルなKtorサーバーを書き始めました。しかし、GraalVMネイティブイメージとして配布しようとした際に問題が発生しました。公式のJava SDKは動的機能を多用しており、トレースエージェントを用いてもネイティブイメージへのコンパイルが困難でした。そこで、Ktorとkotlinx.serialization（どちらもリフレクション非依存、関数型、DSLスタイル）を使って、ストリーミングチャット補完APIのシンプルなクライアントを自作することにしました。</p>
<p>ご覧の通り、このアプリケーションはファットランナブルjarおよびGraalVMネイティブイメージとして配布されており、クロスプラットフォームかつ高速起動を実現しています。</p>
<p>このアプリケーションの開発により、Kotlin/Ktor/kotlinx.serializationへの信頼が深まりました。Kotlinの世界はより関数型でリフレクションが少ないため、GraalVMネイティブイメージにより適しており、高速な起動と低メモリ使用を実現します。</p>
<h2>現在サポートしているもの</h2>
<p>プロキシ元: OpenAI, Claude, DashScope(Alibaba Qwen), Gemini, Deepseek, Mistral, SiliconFlow.</p>
<p>プロキシ先: LM Studio, Ollama.</p>
<p>ストリーミングチャット補完APIのみ対応。</p>
<h2>使い方</h2>
<p>このアプリケーションはプロキシサーバーであり、ファット実行可能JARとGraalVMネイティブイメージ（Windows x64）として配布されています。</p>
<p>アプリケーションを実行すると、ヘルプメッセージが表示されます。</p>
<pre><code>2025-05-02 10:43:53 INFO  Help - 初めてこのプログラムを起動したようです。
2025-05-02 10:43:53 INFO  Help - デフォルトの設定ファイルが your_path\config.yml にスキーマ注釈付きで作成されました。
2025-05-02 10:43:53 INFO  Config - 設定ファイルウォッチャーが your_path\config.yml で開始されました
2025-05-02 10:43:53 INFO  LM Studio Server - LM Studio Server が 1234 で開始されました
2025-05-02 10:43:53 INFO  Ollama Server - Ollama Server が 11434 で開始されました
2025-05-02 10:43:53 INFO  Model List - モデルリストが読み込まれました: []
</code></pre>
<p>その後、設定ファイルを編集してプロキシサーバーをセットアップできます。</p>
<h2>設定ファイル</h2>
<p>この設定ファイルは、変更時に自動的にホットリロードされます。影響を受けるサーバー部分のみが更新されます。</p>
<p>初めて設定ファイルを生成する際、スキーマ注釈付きで作成されます。これにより、エディタで補完やチェックが利用できます。</p>
<h2>サンプル設定ファイル</h2>
<pre><code class="language-yaml"># $schema: https://github.com/Stream29/ProxyAsLocalModel/raw/master/config_v3.schema.json
lmStudio:
  port: 1234 # これはデフォルト値です
  enabled: true # これはデフォルト値です
  host: 0.0.0.0 # これはデフォルト値です
  path: /your/path # 元のエンドポイントの前に追加されます。デフォルト値は空です
ollama:
  port: 11434 # これはデフォルト値です
  enabled: true # これはデフォルト値です
  host: 0.0.0.0 # これはデフォルト値です
  path: /your/path # 元のエンドポイントの前に追加されます。デフォルト値は空です
client:
  socketTimeout: 1919810 # デフォルト値は Long.MAX_VALUE、ミリ秒単位
  connectionTimeout: 1919810 # デフォルト値は Long.MAX_VALUE、ミリ秒単位
  requestTimeout: 1919810 # デフォルト値は Long.MAX_VALUE、ミリ秒単位
  retry: 3 # これはデフォルト値です
  delayBeforeRetry: 1000 # これはデフォルト値です、ミリ秒単位

apiProviders:
  OpenAI:
    type: OpenAi
    baseUrl: https://api.openai.com/v1
    apiKey: &lt;your_api_key&gt;
    modelList:
      - gpt-4o
  Claude:
    type: Claude
    apiKey: &lt;your_api_key&gt;
    modelList:
      - claude-3-7-sonnet
  Qwen:
    type: DashScope
    apiKey: &lt;your_api_key&gt;
    modelList: # これはデフォルト値です
      - qwen-max
      - qwen-plus
      - qwen-turbo
      - qwen-long
  DeepSeek:
    type: DeepSeek
    apiKey: &lt;your_api_key&gt;
    modelList: # これはデフォルト値です
      - deepseek-chat
      - deepseek-reasoner
  Mistral:
    type: Mistral
    apiKey: &lt;your_api_key&gt;
    modelList: # これはデフォルト値です
      - codestral-latest
      - mistral-large
  SiliconFlow:
    type: SiliconFlow
    apiKey: &lt;your_api_key&gt;
    modelList:
      - Qwen/Qwen3-235B-A22B
      - Pro/deepseek-ai/DeepSeek-V3
      - THUDM/GLM-4-32B-0414
  OpenRouter:
    type: OpenRouter
    apiKey: &lt;your_api_key&gt;
    modelList:
      - openai/gpt-4o
  Gemini:
    type: Gemini
    apiKey: &lt;your_api_key&gt;
    modelList:
      - gemini-2.5-flash-preview-04-17
</code></pre>
<hr />
<p>Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx">Open Ai Tx</a> | Last indexed: 2025-07-10</p>
<hr />

        </div>
    </div>
    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Default Statcounter code for openaitx
https://openaitx.github.io/ -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>
    <!-- End of Statcounter Code -->
</body>
</html>