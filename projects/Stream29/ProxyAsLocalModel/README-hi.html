<!DOCTYPE html>
<html lang="hi">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ProxyAsLocalModel - Stream29/ProxyAsLocalModel hi</title>
    <meta name="title" content="ProxyAsLocalModel - Stream29/ProxyAsLocalModel hi | ProxyAsLocalModel रिमोट LLM API को लोकल मॉडल के रूप में प्रॉक्सी करें। विशेष रूप से JetBrains AI Assistant में कस्टम LLM का उपयोग करने के लिए काम करता है। Ktor ...">
    <meta name="description" content="Stream29/ProxyAsLocalModel - GitHub repository hi documentation and information | ProxyAsLocalModel रिमोट LLM API को लोकल मॉडल के रूप में प्रॉक्सी करें। विशेष रूप से JetBrains AI Assistant में कस्टम LLM का उपयोग करने के लिए काम करता है। Ktor ...">
    <meta name="keywords" content="Stream29, ProxyAsLocalModel, GitHub, repository, hi documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/Stream29/ProxyAsLocalModel/README-hi.html">
    <meta property="og:title" content="ProxyAsLocalModel - Stream29/ProxyAsLocalModel hi | ProxyAsLocalModel रिमोट LLM API को लोकल मॉडल के रूप में प्रॉक्सी करें। विशेष रूप से JetBrains AI Assistant में कस्टम LLM का उपयोग करने के लिए काम करता है। Ktor ...">
    <meta property="og:description" content="Stream29/ProxyAsLocalModel - GitHub repository hi documentation and information | ProxyAsLocalModel रिमोट LLM API को लोकल मॉडल के रूप में प्रॉक्सी करें। विशेष रूप से JetBrains AI Assistant में कस्टम LLM का उपयोग करने के लिए काम करता है। Ktor ...">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">
    <script src="/js/marked.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        body { display: flex; flex-direction: column; min-height: 100vh; }
        .main-container { margin: 0 auto; width: 100%; max-width: 980px; padding: 0 20px; }
        @media (max-width: 768px) { .main-container { padding: 0 15px; } }
        .markdown-body img { max-width: 100%; height: auto; }
        .header { text-align: center; margin-bottom: 30px; padding: 20px; background-color: #f6f8fa; border-bottom: 1px solid #e1e4e8; position: relative; }
        .header .links { margin-top: 10px; font-size: 16px; }
        .header .links a { color: #0366d6; text-decoration: none; margin-left: 5px; }
        .header .links a:hover { text-decoration: underline; }
        .language-badges { margin-top: 15px; text-align: center; }
        .language-badges a { display: inline-block; margin: 2px; text-decoration: none; }
        .language-badges img { height: 20px; border-radius: 3px; }
        .language-badges a:hover img { opacity: 0.8; }
    </style>
</head>
<body>
    <div style="position: fixed; top: 2px; left: 2px; z-index: 2000; background: rgba(255,255,255,0.95); border-radius: 6px; box-shadow: 0 2px 8px rgba(0,0,0,0.06); padding: 4px 14px; font-size: 15px; color: #222; font-family: 'Segoe UI', Arial, sans-serif; font-weight: 500; letter-spacing: 0.5px;">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" style="color: #0366d6; text-decoration: none; font-weight: 600;">Open AI Tx</a>
    </div>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/Stream29/ProxyAsLocalModel" id="githubRepoLink" target="_blank">Stream29/ProxyAsLocalModel</a>
<h1 style="display: none;">ProxyAsLocalModel रिमोट LLM API को लोकल मॉडल के रूप में प्रॉक्सी करें। विशेष रूप से JetBrains AI Assistant में कस्टम LLM का उपयोग करने के लिए काम करता है। Ktor ...</h1>
        </div>
        <div class="language-badges" id="languageBadges">
            <!-- You can generate language badges here if needed -->
        </div>
    </div>
    <div class="main-container">
        <div class="markdown-body" id="content">
            <h1>ProxyAsLocalModel</h1>
<p>रिमोट LLM API को लोकल मॉडल के रूप में प्रॉक्सी करें। विशेष रूप से JetBrains AI Assistant में कस्टम LLM का उपयोग करने के लिए काम करता है।</p>
<p>Ktor और kotlinx.serialization द्वारा संचालित। उनके बिना-रिफ्लेक्स फीचर्स के लिए धन्यवाद।</p>
<h2>इस प्रोजेक्ट की कहानी</h2>
<p>वर्तमान में, JetBrains AI Assistant एक फ्री प्लान प्रदान करता है जिसमें बहुत सीमित कोट्स हैं। मैंने इसे आज़माया और मेरी कोटा जल्दी ही समाप्त हो गई।</p>
<p>मैंने पहले ही अन्य LLM API टोकन खरीद लिए थे, जैसे कि Gemini और Qwen। इसलिए मैंने सोचना शुरू किया कि इन्हें AI Assistant में कैसे उपयोग किया जाए। दुर्भाग्य से, केवल LM Studio और Ollama के लोकल मॉडल ही समर्थित हैं। इसलिए मैंने इस प्रॉक्सी एप्लिकेशन पर काम शुरू किया, जो थर्ड पार्टी LLM API को LM Studio और Ollama API के रूप में प्रॉक्सी करता है ताकि मैं इन्हें अपने JetBrains IDEs में उपयोग कर सकूं।</p>
<p>यह एक सरल कार्य था, इसलिए मैंने आधिकारिक SDKs का क्लाइंट के रूप में उपयोग करना शुरू किया और एक सरल Ktor सर्वर लिखा जो LM Studio और Ollama के रूप में एंडपॉइंट्स प्रदान करता है। समस्या तब आई जब मैंने इसे GraalVM नेटिव इमेज के रूप में वितरित करने की कोशिश की। आधिकारिक Java SDKs बहुत अधिक डायनेमिक फीचर्स का उपयोग करते हैं, जिससे इसे नेटिव इमेज में कंपाइल करना मुश्किल हो जाता है, यहां तक कि ट्रेसिंग एजेंट के साथ भी। इसलिए मैंने Ktor और kotlinx.serialization के साथ खुद ही स्ट्रीमिंग चैट कंप्लीशन API का एक सरल क्लाइंट इम्प्लीमेंट करने का निर्णय लिया, जो दोनों ही बिना-रिफ्लेक्स, फंक्शनल और DSL स्टाइल्ड हैं।</p>
<p>जैसा कि आप देख सकते हैं, यह एप्लिकेशन एक फैट रननेबल जार और एक GraalVM नेटिव इमेज के रूप में वितरित किया जाता है, जिससे यह क्रॉस-प्लेटफॉर्म और जल्दी स्टार्ट होने वाला बन जाता है।</p>
<p>इस एप्लिकेशन के विकास ने मुझे Kotlin/Ktor/kotlinx.serialization में आत्मविश्वास दिया। Kotlin वर्ल्ड अधिक फंक्शनल प्रोग्रामिंग और कम रिफ्लेक्शन का उपयोग करती है, जिससे यह GraalVM नेटिव इमेज के लिए अधिक उपयुक्त बनती है, तेज़ स्टार्टअप और कम मेमोरी उपयोग के साथ।</p>
<h2>वर्तमान में समर्थित</h2>
<p>प्रॉक्सी फ्रॉम: OpenAI, Claude, DashScope(Alibaba Qwen), Gemini, Deepseek, Mistral, SiliconFlow।</p>
<p>प्रॉक्सी ऐज़: LM Studio, Ollama।</p>
<p>सिर्फ स्ट्रीमिंग चैट कंप्लीशन API।</p>
<h2>उपयोग कैसे करें</h2>
<p>यह एप्लिकेशन एक प्रॉक्सी सर्वर है, जो एक फैट रननेबल जार और GraalVM नेटिव इमेज (Windows x64) के रूप में वितरित किया गया है।</p>
<p>एप्लिकेशन चलाएँ, और आपको एक सहायता संदेश दिखाई देगा:</p>
<pre><code>2025-05-02 10:43:53 INFO  Help - ऐसा लगता है कि आप पहली बार यहाँ प्रोग्राम शुरू कर रहे हैं।
2025-05-02 10:43:53 INFO  Help - एक डिफ़ॉल्ट कॉन्फ़िग फाइल आपके_path\config.yml पर स्कीमा एनोटेशन के साथ बनाई गई है।
2025-05-02 10:43:53 INFO  Config - कॉन्फ़िग फाइल वॉचर आपके_path\config.yml पर शुरू हो गया है।
2025-05-02 10:43:53 INFO  LM Studio Server - LM Studio सर्वर 1234 पर शुरू हो गया है।
2025-05-02 10:43:53 INFO  Ollama Server - Ollama सर्वर 11434 पर शुरू हो गया है।
2025-05-02 10:43:53 INFO  Model List - मॉडल सूची लोड की गई: []
</code></pre>
<p>इसके बाद आप अपने प्रॉक्सी सर्वर को सेटअप करने के लिए कॉन्फ़िग फाइल को एडिट कर सकते हैं।</p>
<h2>कॉन्फ़िग फाइल</h2>
<p>यह कॉन्फ़िग फाइल स्वचालित रूप से हॉट-रिलोड होती है जब भी आप इसमें बदलाव करते हैं। सर्वर के केवल प्रभावित हिस्से ही अपडेट होंगे।</p>
<p>जब पहली बार कॉन्फ़िग फाइल जेनरेट होती है, तो यह स्कीमा एनोटेशन के साथ बनाई जाएगी। यह आपके एडिटर में ऑटो-कम्प्लीशन और चेकिंग प्रदान करेगा।</p>
<h2>उदाहरण कॉन्फ़िग फ़ाइल</h2>
<pre><code class="language-yaml"># $schema: https://github.com/Stream29/ProxyAsLocalModel/raw/master/config_v3.schema.json
lmStudio:
  port: 1234 # यह डिफ़ॉल्ट मान है
  enabled: true # यह डिफ़ॉल्ट मान है
  host: 0.0.0.0 # यह डिफ़ॉल्ट मान है
  path: /your/path # मूल एंडपॉइंट्स से पहले जोड़ा जाएगा, डिफ़ॉल्ट मान खाली है
ollama:
  port: 11434 # यह डिफ़ॉल्ट मान है
  enabled: true # यह डिफ़ॉल्ट मान है
  host: 0.0.0.0 # यह डिफ़ॉल्ट मान है
  path: /your/path # मूल एंडपॉइंट्स से पहले जोड़ा जाएगा, डिफ़ॉल्ट मान खाली है
client:
  socketTimeout: 1919810 # Long.MAX_VALUE डिफ़ॉल्ट मान है, मिलीसेकंड में
  connectionTimeout: 1919810 # Long.MAX_VALUE डिफ़ॉल्ट मान है, मिलीसेकंड में
  requestTimeout: 1919810 # Long.MAX_VALUE डिफ़ॉल्ट मान है, मिलीसेकंड में
  retry: 3 # यह डिफ़ॉल्ट मान है
  delayBeforeRetry: 1000 # यह डिफ़ॉल्ट मान है, मिलीसेकंड में

apiProviders:
  OpenAI:
    type: OpenAi
    baseUrl: https://api.openai.com/v1
    apiKey: &lt;your_api_key&gt;
    modelList:
      - gpt-4o
  Claude:
    type: Claude
    apiKey: &lt;your_api_key&gt;
    modelList:
      - claude-3-7-sonnet
  Qwen:
    type: DashScope
    apiKey: &lt;your_api_key&gt;
    modelList: # यह डिफ़ॉल्ट मान है
      - qwen-max
      - qwen-plus
      - qwen-turbo
      - qwen-long
  DeepSeek:
    type: DeepSeek
    apiKey: &lt;your_api_key&gt;
    modelList: # यह डिफ़ॉल्ट मान है
      - deepseek-chat
      - deepseek-reasoner
  Mistral:
    type: Mistral
    apiKey: &lt;your_api_key&gt;
    modelList: # यह डिफ़ॉल्ट मान है
      - codestral-latest
      - mistral-large
  SiliconFlow:
    type: SiliconFlow
    apiKey: &lt;your_api_key&gt;
    modelList:
      - Qwen/Qwen3-235B-A22B
      - Pro/deepseek-ai/DeepSeek-V3
      - THUDM/GLM-4-32B-0414
  OpenRouter:
    type: OpenRouter
    apiKey: &lt;your_api_key&gt;
    modelList:
      - openai/gpt-4o
  Gemini:
    type: Gemini
    apiKey: &lt;your_api_key&gt;
    modelList:
      - gemini-2.5-flash-preview-04-17
</code></pre>
<hr />
<p>Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx">Open Ai Tx</a> | Last indexed: 2025-07-10</p>
<hr />

        </div>
    </div>
    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Default Statcounter code for openaitx
https://openaitx.github.io/ -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>
    <!-- End of Statcounter Code -->
</body>
</html>