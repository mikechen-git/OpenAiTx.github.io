<!DOCTYPE html>
<html lang="it">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ProxyAsLocalModel - Italian Documentation</title>
    <meta name="description" content="Read ProxyAsLocalModel documentation in Italian. This project has 135 stars on GitHub.">
    <meta name="keywords" content="ProxyAsLocalModel, Italian, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "ProxyAsLocalModel",
  "description": "Documentation for ProxyAsLocalModel in Italian",
  "author": {
    "@type": "Person",
    "name": "Stream29"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 135
  },
  "url": "https://OpenAiTx.github.io/projects/Stream29/ProxyAsLocalModel/README-it.html",
  "sameAs": "https://raw.githubusercontent.com/Stream29/ProxyAsLocalModel/master/README.md",
  "datePublished": "2025-07-10",
  "dateModified": "2025-07-10"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            color: #24292e;
            margin-bottom: 8px;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">ProxyAsLocalModel</h1>
            <div class="project-meta">
                <span class="stars">⭐ 135 stars</span>
                <span class="language">Italian</span>
                <span>by Stream29</span>
            </div>
        </div>
        
        <div class="content">
            <h1>ProxyAsLocalModel</h1></p><p>Proxy API LLM remoti come modelli locali. Funziona in particolare per l'utilizzo di LLM personalizzati in JetBrains AI Assistant.</p><p>Basato su Ktor e kotlinx.serialization. Grazie alle loro caratteristiche senza riflessione.</p><h2>Storia di questo progetto</h2></p><p>Attualmente, JetBrains AI Assistant offre un piano gratuito con quote molto limitate. L'ho provato e la mia quota si è esaurita rapidamente.</p><p>Avevo già acquistato altri token API LLM, come Gemini e Qwen. Quindi ho iniziato a pensare di usarli in AI Assistant. Sfortunatamente, sono supportati solo i modelli locali da LM Studio e Ollama. Così ho iniziato a lavorare su questa applicazione proxy che fa da proxy alle API LLM di terze parti come se fossero API di LM Studio e Ollama, così da poterle utilizzare nei miei IDE JetBrains.</p><p>Questo è solo un compito semplice, quindi ho iniziato a utilizzare gli SDK ufficiali come client e a scrivere un semplice server Ktor che fornisce endpoint come LM Studio e Ollama. Il problema è apparso quando ho cercato di distribuirlo come immagine nativa GraalVM. Gli SDK Java ufficiali usano troppe funzionalità dinamiche, rendendo difficile la compilazione in un'immagine nativa, anche con un tracing agent. Quindi ho deciso di implementare io stesso un semplice client per l'API di completamento chat in streaming con Ktor e kotlinx.serialization, che sono entrambi senza riflessione, funzionali e in stile DSL.</p><p>Come puoi vedere, questa applicazione è distribuita come jar eseguibile fat e come immagine nativa GraalVM, il che la rende multipiattaforma e veloce all'avvio.</p><p>Lo sviluppo di questa applicazione mi ha dato fiducia in Kotlin/Ktor/kotlinx.serialization. Il mondo Kotlin utilizza più programmazione funzionale e meno riflessione, il che lo rende più adatto per immagini native GraalVM, con avvio più rapido e minore utilizzo di memoria.</p><h2>Attualmente supportato</h2></p><p>Proxy da: OpenAI, Claude, DashScope (Alibaba Qwen), Gemini, Deepseek, Mistral, SiliconFlow.</p><p>Proxy come: LM Studio, Ollama.</p><p>Solo API di completamento chat in streaming.
<h2>Come usare</h2></p><p>Questa applicazione è un server proxy, distribuito come un jar eseguibile standalone e come un'immagine nativa GraalVM (Windows x64).</p><p>Esegui l'applicazione e vedrai un messaggio di aiuto:</p><pre><code class="language-">2025-05-02 10:43:53 INFO  Help - Sembra che tu stia avviando il programma per la prima volta qui.
2025-05-02 10:43:53 INFO  Help - Un file di configurazione predefinito è stato creato in your_path\config.yml con annotazioni di schema.
2025-05-02 10:43:53 INFO  Config - Il watcher del file di configurazione è stato avviato in your_path\config.yml
2025-05-02 10:43:53 INFO  LM Studio Server - LM Studio Server avviato su 1234
2025-05-02 10:43:53 INFO  Ollama Server - Ollama Server avviato su 11434
2025-05-02 10:43:53 INFO  Model List - Elenco modelli caricato con: []</code></pre></p><p>Successivamente puoi modificare il file di configurazione per impostare il tuo server proxy.</p><h2>File di configurazione</h2></p><p>Questo file di configurazione viene automaticamente ricaricato a caldo quando lo modifichi. Solo le parti interessate del server verranno aggiornate.</p><p>Quando il file di configurazione viene generato per la prima volta, sarà creato con annotazioni di schema. Questo fornirà completamento e verifica nel tuo editor.
<h2>Esempio di file di configurazione</h2></p><pre><code class="language-yaml"># $schema: https://github.com/Stream29/ProxyAsLocalModel/raw/master/config_v3.schema.json
lmStudio:
  port: 1234 # Questo è il valore predefinito
  enabled: true # Questo è il valore predefinito
  host: 0.0.0.0 # Questo è il valore predefinito
  path: /your/path # Verrà aggiunto prima degli endpoint originali, il valore predefinito è vuoto
ollama:
  port: 11434 # Questo è il valore predefinito
  enabled: true # Questo è il valore predefinito
  host: 0.0.0.0 # Questo è il valore predefinito
  path: /your/path # Verrà aggiunto prima degli endpoint originali, il valore predefinito è vuoto
client:
  socketTimeout: 1919810 # Long.MAX_VALUE è il valore predefinito, in millisecondi
  connectionTimeout: 1919810 # Long.MAX_VALUE è il valore predefinito, in millisecondi
  requestTimeout: 1919810 # Long.MAX_VALUE è il valore predefinito, in millisecondi
  retry: 3 # Questo è il valore predefinito
  delayBeforeRetry: 1000 # Questo è il valore predefinito, in millisecondi</p><p>apiProviders:
  OpenAI:
    type: OpenAi
    baseUrl: https://api.openai.com/v1
    apiKey: <your_api_key>
    modelList:
      <ul><li>gpt-4o</li>
  </ul>Claude:
    type: Claude
    apiKey: <your_api_key>
    modelList:
      <ul><li>claude-3-7-sonnet</li>
  </ul>Qwen:
    type: DashScope
    apiKey: <your_api_key>
    modelList: # Questo è il valore predefinito
      <ul><li>qwen-max</li>
      <li>qwen-plus</li>
      <li>qwen-turbo</li>
      <li>qwen-long</li>
  </ul>DeepSeek:
    type: DeepSeek
    apiKey: <your_api_key>
    modelList: # Questo è il valore predefinito
      <ul><li>deepseek-chat</li>
      <li>deepseek-reasoner</li>
  </ul>Mistral:
    type: Mistral
    apiKey: <your_api_key>
    modelList: # Questo è il valore predefinito
      <ul><li>codestral-latest</li>
      <li>mistral-large</li>
  </ul>SiliconFlow:
    type: SiliconFlow
    apiKey: <your_api_key>
    modelList:
      <ul><li>Qwen/Qwen3-235B-A22B</li>
      <li>Pro/deepseek-ai/DeepSeek-V3</li>
      <li>THUDM/GLM-4-32B-0414</li>
  </ul>OpenRouter:
    type: OpenRouter
    apiKey: <your_api_key>
    modelList:
      <ul><li>openai/gpt-4o</li>
  </ul>Gemini:
    type: Gemini
    apiKey: <your_api_key>
    modelList:
      <ul><li>gemini-2.5-flash-preview-04-17</code></pre></li>

</ul>---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-07-10

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/Stream29/ProxyAsLocalModel/master/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-07-10 
    </div>
    
</body>
</html>