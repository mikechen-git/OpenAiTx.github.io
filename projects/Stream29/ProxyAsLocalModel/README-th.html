<!DOCTYPE html>
<html lang="th">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ProxyAsLocalModel - Stream29/ProxyAsLocalModel th</title>
    <meta name="title" content="ProxyAsLocalModel - Stream29/ProxyAsLocalModel th | ProxyAsLocalModel พร็อกซี่ LLM API ระยะไกลเป็นโมเดลโลคัล ใช้งานได้ดีโดยเฉพาะสำหรับการใช้ LLM แบบกำหนดเองใน JetBrains AI Assistant ขับเคลื่อนด้วย Ktor และ kotlin...">
    <meta name="description" content="Stream29/ProxyAsLocalModel - GitHub repository th documentation and information | ProxyAsLocalModel พร็อกซี่ LLM API ระยะไกลเป็นโมเดลโลคัล ใช้งานได้ดีโดยเฉพาะสำหรับการใช้ LLM แบบกำหนดเองใน JetBrains AI Assistant ขับเคลื่อนด้วย Ktor และ kotlin...">
    <meta name="keywords" content="Stream29, ProxyAsLocalModel, GitHub, repository, th documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/Stream29/ProxyAsLocalModel/README-th.html">
    <meta property="og:title" content="ProxyAsLocalModel - Stream29/ProxyAsLocalModel th | ProxyAsLocalModel พร็อกซี่ LLM API ระยะไกลเป็นโมเดลโลคัล ใช้งานได้ดีโดยเฉพาะสำหรับการใช้ LLM แบบกำหนดเองใน JetBrains AI Assistant ขับเคลื่อนด้วย Ktor และ kotlin...">
    <meta property="og:description" content="Stream29/ProxyAsLocalModel - GitHub repository th documentation and information | ProxyAsLocalModel พร็อกซี่ LLM API ระยะไกลเป็นโมเดลโลคัล ใช้งานได้ดีโดยเฉพาะสำหรับการใช้ LLM แบบกำหนดเองใน JetBrains AI Assistant ขับเคลื่อนด้วย Ktor และ kotlin...">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">
    <script src="/js/marked.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        body { display: flex; flex-direction: column; min-height: 100vh; }
        .main-container { margin: 0 auto; width: 100%; max-width: 980px; padding: 0 20px; }
        @media (max-width: 768px) { .main-container { padding: 0 15px; } }
        .markdown-body img { max-width: 100%; height: auto; }
        .header { text-align: center; margin-bottom: 30px; padding: 20px; background-color: #f6f8fa; border-bottom: 1px solid #e1e4e8; position: relative; }
        .header .links { margin-top: 10px; font-size: 16px; }
        .header .links a { color: #0366d6; text-decoration: none; margin-left: 5px; }
        .header .links a:hover { text-decoration: underline; }
        .language-badges { margin-top: 15px; text-align: center; }
        .language-badges a { display: inline-block; margin: 2px; text-decoration: none; }
        .language-badges img { height: 20px; border-radius: 3px; }
        .language-badges a:hover img { opacity: 0.8; }
    </style>
</head>
<body>
    <div style="position: fixed; top: 2px; left: 2px; z-index: 2000; background: rgba(255,255,255,0.95); border-radius: 6px; box-shadow: 0 2px 8px rgba(0,0,0,0.06); padding: 4px 14px; font-size: 15px; color: #222; font-family: 'Segoe UI', Arial, sans-serif; font-weight: 500; letter-spacing: 0.5px;">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" style="color: #0366d6; text-decoration: none; font-weight: 600;">Open AI Tx</a>
    </div>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/Stream29/ProxyAsLocalModel" id="githubRepoLink" target="_blank">Stream29/ProxyAsLocalModel</a>
<h1 style="display: none;">ProxyAsLocalModel พร็อกซี่ LLM API ระยะไกลเป็นโมเดลโลคัล ใช้งานได้ดีโดยเฉพาะสำหรับการใช้ LLM แบบกำหนดเองใน JetBrains AI Assistant ขับเคลื่อนด้วย Ktor และ kotlin...</h1>
        </div>
        <div class="language-badges" id="languageBadges">
            <!-- You can generate language badges here if needed -->
        </div>
    </div>
    <div class="main-container">
        <div class="markdown-body" id="content">
            <h1>ProxyAsLocalModel</h1>
<p>พร็อกซี่ LLM API ระยะไกลเป็นโมเดลโลคัล ใช้งานได้ดีโดยเฉพาะสำหรับการใช้ LLM แบบกำหนดเองใน JetBrains AI Assistant</p>
<p>ขับเคลื่อนด้วย Ktor และ kotlinx.serialization ขอบคุณฟีเจอร์ที่ไม่ใช้รีเฟล็กซ์ของพวกเขา</p>
<h2>เรื่องราวของโปรเจกต์นี้</h2>
<p>ปัจจุบัน JetBrains AI Assistant ให้บริการแผนฟรีแต่มีโควตาจำกัดมาก ฉันลองใช้งานและโควตาของฉันหมดอย่างรวดเร็ว</p>
<p>ฉันซื้อโทเค็น LLM API อื่นๆ ไว้อยู่แล้ว เช่น Gemini และ Qwen ดังนั้นฉันจึงเริ่มคิดที่จะใช้มันใน AI Assistant น่าเสียดายที่รองรับเฉพาะโมเดลโลคัลจาก LM Studio และ Ollama เท่านั้น ฉันจึงเริ่มพัฒนาแอปพลิเคชันพร็อกซี่นี้เพื่อทำหน้าที่เป็นพร็อกซี่ให้กับ LLM API ของบุคคลที่สามให้เหมือนกับ LM Studio และ Ollama API เพื่อให้สามารถใช้งานใน JetBrains IDEs ของฉันได้</p>
<p>นี่เป็นงานที่ง่ายมาก ฉันจึงเริ่มต้นด้วยการใช้ SDK อย่างเป็นทางการเป็นไคลเอนต์และเขียน Ktor เซิร์ฟเวอร์ง่ายๆ ที่ให้บริการ endpoint เหมือนกับ LM Studio และ Ollama ปัญหาเกิดขึ้นเมื่อฉันพยายามแจกจ่ายแอปนี้เป็น GraalVM native image SDK Java อย่างเป็นทางการใช้ฟีเจอร์ไดนามิกมากเกินไป ทำให้คอมไพล์เป็น native image ได้ยาก แม้จะใช้ tracing agent ก็ตาม ดังนั้นฉันจึงตัดสินใจพัฒนาไคลเอนต์แบบง่ายสำหรับ streaming chat completion API ด้วยตัวเองโดยใช้ Ktor และ kotlinx.serialization ซึ่งทั้งสองอย่างนี้ไม่ใช้รีเฟล็กซ์ ใช้งานแบบ functional และ DSL</p>
<p>ดังที่เห็น แอปพลิเคชันนี้แจกจ่ายเป็น fat runnable jar และ GraalVM native image ซึ่งทำให้ข้ามแพลตฟอร์มและเริ่มต้นได้รวดเร็ว</p>
<p>การพัฒนาแอปพลิเคชันนี้ทำให้ฉันมั่นใจใน Kotlin/Ktor/kotlinx.serialization โลกของ Kotlin ใช้ functional programming มากกว่า และใช้รีเฟล็กชันน้อยกว่า ซึ่งทำให้เหมาะกับ GraalVM native image มากกว่า เริ่มต้นได้เร็วและใช้หน่วยความจำน้อยกว่า</p>
<h2>รองรับในปัจจุบัน</h2>
<p>พร็อกซี่จาก: OpenAI, Claude, DashScope(Alibaba Qwen), Gemini, Deepseek, Mistral, SiliconFlow</p>
<p>พร็อกซี่เป็น: LM Studio, Ollama</p>
<p>รองรับเฉพาะ Streaming chat completion API</p>
<h2>วิธีการใช้งาน</h2>
<p>แอปพลิเคชันนี้เป็น proxy server ซึ่งแจกจ่ายในรูปแบบ fat runnable jar และ GraalVM native image (Windows x64)</p>
<p>เมื่อรันแอปพลิเคชัน คุณจะเห็นข้อความช่วยเหลือดังนี้:</p>
<pre><code>2025-05-02 10:43:53 INFO  Help - ดูเหมือนว่าคุณกำลังเริ่มโปรแกรมนี้เป็นครั้งแรกที่นี่
2025-05-02 10:43:53 INFO  Help - ไฟล์ config เริ่มต้นถูกสร้างขึ้นที่ your_path\config.yml พร้อมคำอธิบาย schema
2025-05-02 10:43:53 INFO  Config - ตัวเฝ้าดูไฟล์ config เริ่มทำงานที่ your_path\config.yml
2025-05-02 10:43:53 INFO  LM Studio Server - LM Studio Server เริ่มทำงานที่ 1234
2025-05-02 10:43:53 INFO  Ollama Server - Ollama Server เริ่มทำงานที่ 11434
2025-05-02 10:43:53 INFO  Model List - รายการโมเดลถูกโหลดด้วย: []
</code></pre>
<p>จากนั้นคุณสามารถแก้ไขไฟล์ config เพื่อกำหนดค่า proxy server ของคุณ</p>
<h2>ไฟล์ Config</h2>
<p>ไฟล์ config นี้จะถูก hot-reload โดยอัตโนมัติเมื่อคุณมีการเปลี่ยนแปลง เฉพาะส่วนที่ได้รับผลกระทบของเซิร์ฟเวอร์เท่านั้นที่จะถูกอัปเดต</p>
<p>เมื่อไฟล์ config ถูกสร้างขึ้นครั้งแรก จะมีคำอธิบาย schema อยู่ด้วย ซึ่งจะช่วยให้มีการเติมคำและตรวจสอบใน editor ของคุณ</p>
<h2>ตัวอย่างไฟล์ config</h2>
<pre><code class="language-yaml"># $schema: https://github.com/Stream29/ProxyAsLocalModel/raw/master/config_v3.schema.json
lmStudio:
  port: 1234 # ค่านี้เป็นค่าเริ่มต้น
  enabled: true # ค่านี้เป็นค่าเริ่มต้น
  host: 0.0.0.0 # ค่านี้เป็นค่าเริ่มต้น
  path: /your/path # จะถูกเพิ่มก่อน endpoint เดิม, ค่าเริ่มต้นคือค่าว่าง
ollama:
  port: 11434 # ค่านี้เป็นค่าเริ่มต้น
  enabled: true # ค่านี้เป็นค่าเริ่มต้น
  host: 0.0.0.0 # ค่านี้เป็นค่าเริ่มต้น
  path: /your/path # จะถูกเพิ่มก่อน endpoint เดิม, ค่าเริ่มต้นคือค่าว่าง
client:
  socketTimeout: 1919810 # Long.MAX_VALUE คือค่าเริ่มต้น, หน่วยเป็นมิลลิวินาที
  connectionTimeout: 1919810 # Long.MAX_VALUE คือค่าเริ่มต้น, หน่วยเป็นมิลลิวินาที
  requestTimeout: 1919810 # Long.MAX_VALUE คือค่าเริ่มต้น, หน่วยเป็นมิลลิวินาที
  retry: 3 # ค่านี้เป็นค่าเริ่มต้น
  delayBeforeRetry: 1000 # ค่านี้เป็นค่าเริ่มต้น, หน่วยเป็นมิลลิวินาที

apiProviders:
  OpenAI:
    type: OpenAi
    baseUrl: https://api.openai.com/v1
    apiKey: &lt;your_api_key&gt;
    modelList:
      - gpt-4o
  Claude:
    type: Claude
    apiKey: &lt;your_api_key&gt;
    modelList:
      - claude-3-7-sonnet
  Qwen:
    type: DashScope
    apiKey: &lt;your_api_key&gt;
    modelList: # ค่านี้เป็นค่าเริ่มต้น
      - qwen-max
      - qwen-plus
      - qwen-turbo
      - qwen-long
  DeepSeek:
    type: DeepSeek
    apiKey: &lt;your_api_key&gt;
    modelList: # ค่านี้เป็นค่าเริ่มต้น
      - deepseek-chat
      - deepseek-reasoner
  Mistral:
    type: Mistral
    apiKey: &lt;your_api_key&gt;
    modelList: # ค่านี้เป็นค่าเริ่มต้น
      - codestral-latest
      - mistral-large
  SiliconFlow:
    type: SiliconFlow
    apiKey: &lt;your_api_key&gt;
    modelList:
      - Qwen/Qwen3-235B-A22B
      - Pro/deepseek-ai/DeepSeek-V3
      - THUDM/GLM-4-32B-0414
  OpenRouter:
    type: OpenRouter
    apiKey: &lt;your_api_key&gt;
    modelList:
      - openai/gpt-4o
  Gemini:
    type: Gemini
    apiKey: &lt;your_api_key&gt;
    modelList:
      - gemini-2.5-flash-preview-04-17
</code></pre>
<hr />
<p>Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx">Open Ai Tx</a> | Last indexed: 2025-07-10</p>
<hr />

        </div>
    </div>
    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Default Statcounter code for openaitx
https://openaitx.github.io/ -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>
    <!-- End of Statcounter Code -->
</body>
</html>