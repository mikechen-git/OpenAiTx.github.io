<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ProxyAsLocalModel - French Documentation</title>
    <meta name="description" content="Read ProxyAsLocalModel documentation in French. This project has 135 stars on GitHub.">
    <meta name="keywords" content="ProxyAsLocalModel, French, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "ProxyAsLocalModel",
  "description": "Documentation for ProxyAsLocalModel in French",
  "author": {
    "@type": "Person",
    "name": "Stream29"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 135
  },
  "url": "https://OpenAiTx.github.io/projects/Stream29/ProxyAsLocalModel/README-fr.html",
  "sameAs": "https://raw.githubusercontent.com/Stream29/ProxyAsLocalModel/master/README.md",
  "datePublished": "2025-07-10",
  "dateModified": "2025-07-10"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            color: #24292e;
            margin-bottom: 8px;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">ProxyAsLocalModel</h1>
            <div class="project-meta">
                <span class="stars">⭐ 135 stars</span>
                <span class="language">French</span>
                <span>by Stream29</span>
            </div>
        </div>
        
        <div class="content">
            <h1>ProxyAsLocalModel</h1></p><p>Proxy API LLM distant en tant que modèle local. Fonctionne notamment pour utiliser un LLM personnalisé dans JetBrains AI Assistant.</p><p>Propulsé par Ktor et kotlinx.serialization. Merci à leurs fonctionnalités sans réflexion.</p><h2>Histoire de ce projet</h2></p><p>Actuellement, JetBrains AI Assistant propose un plan gratuit avec des quotas très limités. J'ai essayé et mon quota s'est rapidement épuisé.</p><p>J'avais déjà acheté d'autres jetons d'API LLM, tels que Gemini et Qwen. J'ai donc commencé à réfléchir à la façon de les utiliser dans AI Assistant. Malheureusement, seuls les modèles locaux de LM Studio et Ollama sont pris en charge. J'ai donc commencé à travailler sur cette application proxy qui fait l’intermédiaire entre des API LLM tierces et les API de LM Studio et Ollama afin de pouvoir les utiliser dans mes IDE JetBrains.</p><p>C'est une tâche assez simple, j'ai donc commencé à utiliser les SDK officiels comme clients et à écrire un serveur Ktor simple qui fournit les endpoints de LM Studio et Ollama. Le problème est apparu lorsque j'ai essayé de le distribuer en tant qu'image native GraalVM. Les SDK Java officiels utilisent trop de fonctionnalités dynamiques, ce qui rend la compilation en image native difficile, même avec un agent de traçage. J'ai donc décidé d’implémenter moi-même un client simple pour l’API de complétion de chat en streaming avec Ktor et kotlinx.serialization, qui sont tous deux sans réflexion, fonctionnels et orientés DSL.</p><p>Comme vous pouvez le constater, cette application est distribuée sous forme de jar exécutable autonome et d’image native GraalVM, ce qui la rend multiplateforme et rapide au démarrage.</p><p>Le développement de cette application m'a donné confiance dans Kotlin/Ktor/kotlinx.serialization. Le monde Kotlin utilise plus de programmation fonctionnelle et moins de réflexion, ce qui le rend plus adapté à l'image native GraalVM, avec un démarrage plus rapide et une consommation mémoire réduite.</p><h2>Actuellement pris en charge</h2></p><p>Proxy depuis : OpenAI, Claude, DashScope (Alibaba Qwen), Gemini, Deepseek, Mistral, SiliconFlow.</p><p>Proxy en tant que : LM Studio, Ollama.</p><p>API de complétion de chat en streaming uniquement.
<h2>Comment utiliser</h2></p><p>Cette application est un serveur proxy, distribuée sous forme de jar exécutable complet et d’une image native GraalVM (Windows x64).</p><p>Exécutez l’application, et vous verrez un message d’aide :</p><pre><code class="language-">2025-05-02 10:43:53 INFO  Help - Il semble que vous démarrez le programme pour la première fois ici.
2025-05-02 10:43:53 INFO  Help - Un fichier de configuration par défaut a été créé à votre_chemin\config.yml avec annotation de schéma.
2025-05-02 10:43:53 INFO  Config - Surveillance du fichier de configuration démarrée à votre_chemin\config.yml
2025-05-02 10:43:53 INFO  LM Studio Server - Serveur LM Studio démarré sur 1234
2025-05-02 10:43:53 INFO  Ollama Server - Serveur Ollama démarré sur 11434
2025-05-02 10:43:53 INFO  Model List - Liste des modèles chargée avec : []</code></pre></p><p>Vous pouvez ensuite éditer le fichier de configuration pour configurer votre serveur proxy.</p><h2>Fichier de configuration</h2></p><p>Ce fichier de configuration est automatiquement rechargé à chaud lorsque vous le modifiez. Seules les parties concernées du serveur seront mises à jour.</p><p>Lors de la première génération du fichier de configuration, il sera créé avec des annotations de schéma. Cela apportera la complétion et la vérification dans votre éditeur.
<h2>Exemple de fichier de configuration</h2></p><pre><code class="language-yaml"># $schema: https://github.com/Stream29/ProxyAsLocalModel/raw/master/config_v3.schema.json
lmStudio:
  port: 1234 # Ceci est la valeur par défaut
  enabled: true # Ceci est la valeur par défaut
  host: 0.0.0.0 # Ceci est la valeur par défaut
  path: /your/path # Sera ajouté avant les points de terminaison d'origine, la valeur par défaut est vide
ollama:
  port: 11434 # Ceci est la valeur par défaut
  enabled: true # Ceci est la valeur par défaut
  host: 0.0.0.0 # Ceci est la valeur par défaut
  path: /your/path # Sera ajouté avant les points de terminaison d'origine, la valeur par défaut est vide
client:
  socketTimeout: 1919810 # Long.MAX_VALUE est la valeur par défaut, en millisecondes
  connectionTimeout: 1919810 # Long.MAX_VALUE est la valeur par défaut, en millisecondes
  requestTimeout: 1919810 # Long.MAX_VALUE est la valeur par défaut, en millisecondes
  retry: 3 # Ceci est la valeur par défaut
  delayBeforeRetry: 1000 # Ceci est la valeur par défaut, en millisecondes</p><p>apiProviders:
  OpenAI:
    type: OpenAi
    baseUrl: https://api.openai.com/v1
    apiKey: <votre_clé_api>
    modelList:
      <ul><li>gpt-4o</li>
  </ul>Claude:
    type: Claude
    apiKey: <votre_clé_api>
    modelList:
      <ul><li>claude-3-7-sonnet</li>
  </ul>Qwen:
    type: DashScope
    apiKey: <votre_clé_api>
    modelList: # Ceci est la valeur par défaut
      <ul><li>qwen-max</li>
      <li>qwen-plus</li>
      <li>qwen-turbo</li>
      <li>qwen-long</li>
  </ul>DeepSeek:
    type: DeepSeek
    apiKey: <votre_clé_api>
    modelList: # Ceci est la valeur par défaut
      <ul><li>deepseek-chat</li>
      <li>deepseek-reasoner</li>
  </ul>Mistral:
    type: Mistral
    apiKey: <votre_clé_api>
    modelList: # Ceci est la valeur par défaut
      <ul><li>codestral-latest</li>
      <li>mistral-large</li>
  </ul>SiliconFlow:
    type: SiliconFlow
    apiKey: <votre_clé_api>
    modelList:
      <ul><li>Qwen/Qwen3-235B-A22B</li>
      <li>Pro/deepseek-ai/DeepSeek-V3</li>
      <li>THUDM/GLM-4-32B-0414</li>
  </ul>OpenRouter:
    type: OpenRouter
    apiKey: <votre_clé_api>
    modelList:
      <ul><li>openai/gpt-4o</li>
  </ul>Gemini:
    type: Gemini
    apiKey: <votre_clé_api>
    modelList:
      <ul><li>gemini-2.5-flash-preview-04-17</code></pre></li>

</ul>---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-07-10

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/Stream29/ProxyAsLocalModel/master/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-07-10 
    </div>
    
</body>
</html>