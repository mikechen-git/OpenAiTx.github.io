<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ProxyAsLocalModel - Spanish Documentation</title>
    <meta name="description" content="Read ProxyAsLocalModel documentation in Spanish. This project has 135 stars on GitHub.">
    <meta name="keywords" content="ProxyAsLocalModel, Spanish, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "ProxyAsLocalModel",
  "description": "Documentation for ProxyAsLocalModel in Spanish",
  "author": {
    "@type": "Person",
    "name": "Stream29"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 135
  },
  "url": "https://OpenAiTx.github.io/projects/Stream29/ProxyAsLocalModel/README-es.html",
  "sameAs": "https://raw.githubusercontent.com/Stream29/ProxyAsLocalModel/master/README.md",
  "datePublished": "2025-07-10",
  "dateModified": "2025-07-10"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            color: #24292e;
            margin-bottom: 8px;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">ProxyAsLocalModel</h1>
            <div class="project-meta">
                <span class="stars">⭐ 135 stars</span>
                <span class="language">Spanish</span>
                <span>by Stream29</span>
            </div>
        </div>
        
        <div class="content">
            <h1>ProxyAsLocalModel</h1></p><p>Proxy API de LLM remotos como modelo local. Especialmente útil para usar LLM personalizados en JetBrains AI Assistant.</p><p>Impulsado por Ktor y kotlinx.serialization. Gracias a sus características sin reflexión.</p><h2>Historia de este proyecto</h2></p><p>Actualmente, JetBrains AI Assistant ofrece un plan gratuito con cuotas muy limitadas. Lo probé y mi cuota se agotó rápidamente.</p><p>Ya había comprado otros tokens de API de LLM, como Gemini y Qwen. Así que empecé a pensar en usarlos en AI Assistant. Desafortunadamente, solo se admiten modelos locales de LM Studio y Ollama. Así que comencé a trabajar en esta aplicación proxy que actúa como intermediario de API de LLM de terceros como LM Studio y Ollama API para poder usarlos en mis IDEs de JetBrains.</p><p>Esta es solo una tarea sencilla, así que empecé a usar los SDKs oficiales como clientes y a escribir un servidor Ktor simple que proporcione endpoints como LM Studio y Ollama. El problema apareció cuando intenté distribuirlo como una imagen nativa de GraalVM. Los SDKs oficiales de Java usan demasiadas funciones dinámicas, lo que dificulta la compilación en una imagen nativa, incluso con un agente de rastreo. Así que decidí implementar yo mismo un cliente simple de la API de chat de completado por streaming utilizando Ktor y kotlinx.serialization, que son ambos sin reflexión, funcionales y con estilo DSL.</p><p>Como puedes ver, esta aplicación se distribuye como un jar ejecutable "fat" y una imagen nativa de GraalVM, lo que la hace multiplataforma y rápida de iniciar.</p><p>El desarrollo de esta aplicación me ha dado confianza en Kotlin/Ktor/kotlinx.serialization. El mundo Kotlin usa más programación funcional y menos reflexión, lo que lo hace más adecuado para imágenes nativas de GraalVM, con un arranque más rápido y menor uso de memoria.</p><h2>Actualmente soportado</h2></p><p>Proxy desde: OpenAI, Claude, DashScope (Alibaba Qwen), Gemini, Deepseek, Mistral, SiliconFlow.</p><p>Proxy como: LM Studio, Ollama.</p><p>Solo API de chat de completado por streaming.
<h2>Cómo usar</h2></p><p>Esta aplicación es un servidor proxy, distribuido como un archivo jar ejecutable autónomo y una imagen nativa de GraalVM (Windows x64).</p><p>Ejecute la aplicación y verá un mensaje de ayuda:</p><pre><code class="language-">2025-05-02 10:43:53 INFO  Help - Parece que está iniciando el programa por primera vez aquí.
2025-05-02 10:43:53 INFO  Help - Se ha creado un archivo de configuración predeterminado en your_path\config.yml con anotaciones de esquema.
2025-05-02 10:43:53 INFO  Config - El observador del archivo de configuración se inició en your_path\config.yml
2025-05-02 10:43:53 INFO  LM Studio Server - Servidor LM Studio iniciado en 1234
2025-05-02 10:43:53 INFO  Ollama Server - Servidor Ollama iniciado en 11434
2025-05-02 10:43:53 INFO  Model List - Lista de modelos cargada con: []</code></pre></p><p>Luego puede editar el archivo de configuración para configurar su servidor proxy.</p><h2>Archivo de configuración</h2></p><p>Este archivo de configuración se recarga automáticamente en caliente cuando lo modifica. Solo se actualizarán las partes del servidor que se vean afectadas.</p><p>Cuando se genera por primera vez el archivo de configuración, se creará con anotaciones de esquema. Esto proporcionará autocompletado y verificación en su editor.
<h2>Ejemplo de archivo de configuración</h2></p><pre><code class="language-yaml"># $schema: https://github.com/Stream29/ProxyAsLocalModel/raw/master/config_v3.schema.json
lmStudio:
  port: 1234 # Este es el valor por defecto
  enabled: true # Este es el valor por defecto
  host: 0.0.0.0 # Este es el valor por defecto
  path: /your/path # Se añadirá antes de los endpoints originales, el valor por defecto es vacío
ollama:
  port: 11434 # Este es el valor por defecto
  enabled: true # Este es el valor por defecto
  host: 0.0.0.0 # Este es el valor por defecto
  path: /your/path # Se añadirá antes de los endpoints originales, el valor por defecto es vacío
client:
  socketTimeout: 1919810 # Long.MAX_VALUE es el valor por defecto, en milisegundos
  connectionTimeout: 1919810 # Long.MAX_VALUE es el valor por defecto, en milisegundos
  requestTimeout: 1919810 # Long.MAX_VALUE es el valor por defecto, en milisegundos
  retry: 3 # Este es el valor por defecto
  delayBeforeRetry: 1000 # Este es el valor por defecto, en milisegundos</p><p>apiProviders:
  OpenAI:
    type: OpenAi
    baseUrl: https://api.openai.com/v1
    apiKey: <your_api_key>
    modelList:
      <ul><li>gpt-4o</li>
  </ul>Claude:
    type: Claude
    apiKey: <your_api_key>
    modelList:
      <ul><li>claude-3-7-sonnet</li>
  </ul>Qwen:
    type: DashScope
    apiKey: <your_api_key>
    modelList: # Este es el valor por defecto
      <ul><li>qwen-max</li>
      <li>qwen-plus</li>
      <li>qwen-turbo</li>
      <li>qwen-long</li>
  </ul>DeepSeek:
    type: DeepSeek
    apiKey: <your_api_key>
    modelList: # Este es el valor por defecto
      <ul><li>deepseek-chat</li>
      <li>deepseek-reasoner</li>
  </ul>Mistral:
    type: Mistral
    apiKey: <your_api_key>
    modelList: # Este es el valor por defecto
      <ul><li>codestral-latest</li>
      <li>mistral-large</li>
  </ul>SiliconFlow:
    type: SiliconFlow
    apiKey: <your_api_key>
    modelList:
      <ul><li>Qwen/Qwen3-235B-A22B</li>
      <li>Pro/deepseek-ai/DeepSeek-V3</li>
      <li>THUDM/GLM-4-32B-0414</li>
  </ul>OpenRouter:
    type: OpenRouter
    apiKey: <your_api_key>
    modelList:
      <ul><li>openai/gpt-4o</li>
  </ul>Gemini:
    type: Gemini
    apiKey: <your_api_key>
    modelList:
      <ul><li>gemini-2.5-flash-preview-04-17</code></pre></li>

</ul>---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-07-10

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/Stream29/ProxyAsLocalModel/master/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-07-10 
    </div>
    
</body>
</html>