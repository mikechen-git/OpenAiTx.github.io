<!DOCTYPE html>
<html lang="de">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ProxyAsLocalModel - German Documentation</title>
    <meta name="description" content="Read ProxyAsLocalModel documentation in German. This project has 135 stars on GitHub.">
    <meta name="keywords" content="ProxyAsLocalModel, German, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "ProxyAsLocalModel",
  "description": "Documentation for ProxyAsLocalModel in German",
  "author": {
    "@type": "Person",
    "name": "Stream29"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 135
  },
  "url": "https://OpenAiTx.github.io/projects/Stream29/ProxyAsLocalModel/README-de.html",
  "sameAs": "https://raw.githubusercontent.com/Stream29/ProxyAsLocalModel/master/README.md",
  "datePublished": "2025-07-10",
  "dateModified": "2025-07-10"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            color: #24292e;
            margin-bottom: 8px;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">ProxyAsLocalModel</h1>
            <div class="project-meta">
                <span class="stars">⭐ 135 stars</span>
                <span class="language">German</span>
                <span>by Stream29</span>
            </div>
        </div>
        
        <div class="content">
            <h1>ProxyAsLocalModel</h1></p><p>Proxy-Remote-LLM-API als lokales Modell. Besonders geeignet für die Verwendung von benutzerdefinierten LLMs im JetBrains AI Assistant.</p><p>Angetrieben von Ktor und kotlinx.serialization. Dank ihrer No-Reflection-Eigenschaften.</p><h2>Geschichte dieses Projekts</h2></p><p>Derzeit bietet der JetBrains AI Assistant einen kostenlosen Plan mit sehr begrenzten Kontingenten. Ich habe ihn ausprobiert und mein Kontingent war schnell aufgebraucht.</p><p>Ich habe bereits andere LLM-API-Tokens gekauft, wie zum Beispiel Gemini und Qwen. Daher kam mir die Idee, diese im AI Assistant zu verwenden. Leider werden nur lokale Modelle von LM Studio und Ollama unterstützt. Also begann ich mit der Entwicklung dieser Proxy-Anwendung, die Drittanbieter-LLM-APIs als LM Studio- und Ollama-API weiterleitet, sodass ich sie in meinen JetBrains-IDEs verwenden kann.</p><p>Dies ist nur eine einfache Aufgabe, daher begann ich damit, die offiziellen SDKs als Clients zu verwenden und einen einfachen Ktor-Server zu schreiben, der Endpunkte wie LM Studio und Ollama bereitstellt. Das Problem trat auf, als ich versuchte, es als GraalVM-Native-Image zu verteilen. Die offiziellen Java-SDKs nutzen zu viele dynamische Features, was die Kompilierung in ein Native-Image erschwert, selbst mit einem Tracing-Agent. Also entschied ich mich, einen einfachen Client für die Streaming-Chat-Completion-API selbst zu implementieren, mit Ktor und kotlinx.serialization, die beide ohne Reflection, funktional und DSL-basiert sind.</p><p>Wie Sie sehen können, wird diese Anwendung als Fat-Runnable-Jar und als GraalVM-Native-Image ausgeliefert, was sie plattformübergreifend und schnell startend macht.</p><p>Die Entwicklung dieser Anwendung hat mir Vertrauen in Kotlin/Ktor/kotlinx.serialization gegeben. Die Kotlin-Welt verwendet mehr funktionale Programmierung und weniger Reflection, was sie für GraalVM-Native-Images besser geeignet macht, mit schnellerem Start und geringerem Speicherverbrauch.</p><h2>Derzeit unterstützt</h2></p><p>Proxy von: OpenAI, Claude, DashScope (Alibaba Qwen), Gemini, Deepseek, Mistral, SiliconFlow.</p><p>Proxy als: LM Studio, Ollama.</p><p>Nur Streaming-Chat-Completion-API.
<h2>Verwendung</h2></p><p>Diese Anwendung ist ein Proxy-Server, der als ausführbares Fat-Jar und als GraalVM-Native-Image (Windows x64) verteilt wird.</p><p>Starten Sie die Anwendung, und Sie sehen eine Hilfemeldung:</p><pre><code class="language-">2025-05-02 10:43:53 INFO  Help - Es scheint, dass Sie das Programm hier zum ersten Mal starten.
2025-05-02 10:43:53 INFO  Help - Eine Standard-Konfigurationsdatei wurde unter your_path\config.yml mit Schema-Anmerkungen erstellt.
2025-05-02 10:43:53 INFO  Config - Konfigurationsdatei-Überwachung gestartet unter your_path\config.yml
2025-05-02 10:43:53 INFO  LM Studio Server - LM Studio Server gestartet auf 1234
2025-05-02 10:43:53 INFO  Ollama Server - Ollama Server gestartet auf 11434
2025-05-02 10:43:53 INFO  Model List - Modellliste geladen mit: []</code></pre></p><p>Anschließend können Sie die Konfigurationsdatei bearbeiten, um Ihren Proxy-Server einzurichten.</p><h2>Konfigurationsdatei</h2></p><p>Diese Konfigurationsdatei wird automatisch neu geladen, sobald Sie sie ändern. Nur die betroffenen Teile des Servers werden aktualisiert.</p><p>Bei der ersten Erstellung der Konfigurationsdatei wird diese mit Schema-Anmerkungen erstellt. Dies ermöglicht Vervollständigungen und Überprüfungen in Ihrem Editor.
<h2>Beispiel-Konfigurationsdatei</h2></p><pre><code class="language-yaml"># $schema: https://github.com/Stream29/ProxyAsLocalModel/raw/master/config_v3.schema.json
lmStudio:
  port: 1234 # Dies ist der Standardwert
  enabled: true # Dies ist der Standardwert
  host: 0.0.0.0 # Dies ist der Standardwert
  path: /your/path # Wird vor den ursprünglichen Endpunkten hinzugefügt, Standardwert ist leer
ollama:
  port: 11434 # Dies ist der Standardwert
  enabled: true # Dies ist der Standardwert
  host: 0.0.0.0 # Dies ist der Standardwert
  path: /your/path # Wird vor den ursprünglichen Endpunkten hinzugefügt, Standardwert ist leer
client:
  socketTimeout: 1919810 # Long.MAX_VALUE ist der Standardwert, in Millisekunden
  connectionTimeout: 1919810 # Long.MAX_VALUE ist der Standardwert, in Millisekunden
  requestTimeout: 1919810 # Long.MAX_VALUE ist der Standardwert, in Millisekunden
  retry: 3 # Dies ist der Standardwert
  delayBeforeRetry: 1000 # Dies ist der Standardwert, in Millisekunden</p><p>apiProviders:
  OpenAI:
    type: OpenAi
    baseUrl: https://api.openai.com/v1
    apiKey: <your_api_key>
    modelList:
      <ul><li>gpt-4o</li>
  </ul>Claude:
    type: Claude
    apiKey: <your_api_key>
    modelList:
      <ul><li>claude-3-7-sonnet</li>
  </ul>Qwen:
    type: DashScope
    apiKey: <your_api_key>
    modelList: # Dies ist der Standardwert
      <ul><li>qwen-max</li>
      <li>qwen-plus</li>
      <li>qwen-turbo</li>
      <li>qwen-long</li>
  </ul>DeepSeek:
    type: DeepSeek
    apiKey: <your_api_key>
    modelList: # Dies ist der Standardwert
      <ul><li>deepseek-chat</li>
      <li>deepseek-reasoner</li>
  </ul>Mistral:
    type: Mistral
    apiKey: <your_api_key>
    modelList: # Dies ist der Standardwert
      <ul><li>codestral-latest</li>
      <li>mistral-large</li>
  </ul>SiliconFlow:
    type: SiliconFlow
    apiKey: <your_api_key>
    modelList:
      <ul><li>Qwen/Qwen3-235B-A22B</li>
      <li>Pro/deepseek-ai/DeepSeek-V3</li>
      <li>THUDM/GLM-4-32B-0414</li>
  </ul>OpenRouter:
    type: OpenRouter
    apiKey: <your_api_key>
    modelList:
      <ul><li>openai/gpt-4o</li>
  </ul>Gemini:
    type: Gemini
    apiKey: <your_api_key>
    modelList:
      <ul><li>gemini-2.5-flash-preview-04-17</code></pre></li>

</ul>---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-07-10

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/Stream29/ProxyAsLocalModel/master/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-07-10 
    </div>
    
</body>
</html>