<!DOCTYPE html>
<html lang="de">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ProxyAsLocalModel - Stream29/ProxyAsLocalModel de</title>
    <meta name="title" content="ProxyAsLocalModel - Stream29/ProxyAsLocalModel de | ProxyAsLocalModel Proxy-Remote-LLM-API als lokales Modell. Besonders geeignet für die Verwendung von benutzerdefinierten LLMs im JetBrains AI Assistant. Angetri...">
    <meta name="description" content="Stream29/ProxyAsLocalModel - GitHub repository de documentation and information | ProxyAsLocalModel Proxy-Remote-LLM-API als lokales Modell. Besonders geeignet für die Verwendung von benutzerdefinierten LLMs im JetBrains AI Assistant. Angetri...">
    <meta name="keywords" content="Stream29, ProxyAsLocalModel, GitHub, repository, de documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/Stream29/ProxyAsLocalModel/README-de.html">
    <meta property="og:title" content="ProxyAsLocalModel - Stream29/ProxyAsLocalModel de | ProxyAsLocalModel Proxy-Remote-LLM-API als lokales Modell. Besonders geeignet für die Verwendung von benutzerdefinierten LLMs im JetBrains AI Assistant. Angetri...">
    <meta property="og:description" content="Stream29/ProxyAsLocalModel - GitHub repository de documentation and information | ProxyAsLocalModel Proxy-Remote-LLM-API als lokales Modell. Besonders geeignet für die Verwendung von benutzerdefinierten LLMs im JetBrains AI Assistant. Angetri...">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">
    <script src="/js/marked.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        body { display: flex; flex-direction: column; min-height: 100vh; }
        .main-container { margin: 0 auto; width: 100%; max-width: 980px; padding: 0 20px; }
        @media (max-width: 768px) { .main-container { padding: 0 15px; } }
        .markdown-body img { max-width: 100%; height: auto; }
        .header { text-align: center; margin-bottom: 30px; padding: 20px; background-color: #f6f8fa; border-bottom: 1px solid #e1e4e8; position: relative; }
        .header .links { margin-top: 10px; font-size: 16px; }
        .header .links a { color: #0366d6; text-decoration: none; margin-left: 5px; }
        .header .links a:hover { text-decoration: underline; }
        .language-badges { margin-top: 15px; text-align: center; }
        .language-badges a { display: inline-block; margin: 2px; text-decoration: none; }
        .language-badges img { height: 20px; border-radius: 3px; }
        .language-badges a:hover img { opacity: 0.8; }
    </style>
</head>
<body>
    <div style="position: fixed; top: 2px; left: 2px; z-index: 2000; background: rgba(255,255,255,0.95); border-radius: 6px; box-shadow: 0 2px 8px rgba(0,0,0,0.06); padding: 4px 14px; font-size: 15px; color: #222; font-family: 'Segoe UI', Arial, sans-serif; font-weight: 500; letter-spacing: 0.5px;">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" style="color: #0366d6; text-decoration: none; font-weight: 600;">Open AI Tx</a>
    </div>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/Stream29/ProxyAsLocalModel" id="githubRepoLink" target="_blank">Stream29/ProxyAsLocalModel</a>
<h1 style="display: none;">ProxyAsLocalModel Proxy-Remote-LLM-API als lokales Modell. Besonders geeignet für die Verwendung von benutzerdefinierten LLMs im JetBrains AI Assistant. Angetri...</h1>
        </div>
        <div class="language-badges" id="languageBadges">
            <!-- You can generate language badges here if needed -->
        </div>
    </div>
    <div class="main-container">
        <div class="markdown-body" id="content">
            <h1>ProxyAsLocalModel</h1>
<p>Proxy-Remote-LLM-API als lokales Modell. Besonders geeignet für die Verwendung von benutzerdefinierten LLMs im JetBrains AI Assistant.</p>
<p>Angetrieben von Ktor und kotlinx.serialization. Dank ihrer No-Reflection-Eigenschaften.</p>
<h2>Geschichte dieses Projekts</h2>
<p>Derzeit bietet der JetBrains AI Assistant einen kostenlosen Plan mit sehr begrenzten Kontingenten. Ich habe ihn ausprobiert und mein Kontingent war schnell aufgebraucht.</p>
<p>Ich habe bereits andere LLM-API-Tokens gekauft, wie zum Beispiel Gemini und Qwen. Daher kam mir die Idee, diese im AI Assistant zu verwenden. Leider werden nur lokale Modelle von LM Studio und Ollama unterstützt. Also begann ich mit der Entwicklung dieser Proxy-Anwendung, die Drittanbieter-LLM-APIs als LM Studio- und Ollama-API weiterleitet, sodass ich sie in meinen JetBrains-IDEs verwenden kann.</p>
<p>Dies ist nur eine einfache Aufgabe, daher begann ich damit, die offiziellen SDKs als Clients zu verwenden und einen einfachen Ktor-Server zu schreiben, der Endpunkte wie LM Studio und Ollama bereitstellt. Das Problem trat auf, als ich versuchte, es als GraalVM-Native-Image zu verteilen. Die offiziellen Java-SDKs nutzen zu viele dynamische Features, was die Kompilierung in ein Native-Image erschwert, selbst mit einem Tracing-Agent. Also entschied ich mich, einen einfachen Client für die Streaming-Chat-Completion-API selbst zu implementieren, mit Ktor und kotlinx.serialization, die beide ohne Reflection, funktional und DSL-basiert sind.</p>
<p>Wie Sie sehen können, wird diese Anwendung als Fat-Runnable-Jar und als GraalVM-Native-Image ausgeliefert, was sie plattformübergreifend und schnell startend macht.</p>
<p>Die Entwicklung dieser Anwendung hat mir Vertrauen in Kotlin/Ktor/kotlinx.serialization gegeben. Die Kotlin-Welt verwendet mehr funktionale Programmierung und weniger Reflection, was sie für GraalVM-Native-Images besser geeignet macht, mit schnellerem Start und geringerem Speicherverbrauch.</p>
<h2>Derzeit unterstützt</h2>
<p>Proxy von: OpenAI, Claude, DashScope (Alibaba Qwen), Gemini, Deepseek, Mistral, SiliconFlow.</p>
<p>Proxy als: LM Studio, Ollama.</p>
<p>Nur Streaming-Chat-Completion-API.</p>
<h2>Verwendung</h2>
<p>Diese Anwendung ist ein Proxy-Server, der als ausführbares Fat-Jar und als GraalVM-Native-Image (Windows x64) verteilt wird.</p>
<p>Starten Sie die Anwendung, und Sie sehen eine Hilfemeldung:</p>
<pre><code>2025-05-02 10:43:53 INFO  Help - Es scheint, dass Sie das Programm hier zum ersten Mal starten.
2025-05-02 10:43:53 INFO  Help - Eine Standard-Konfigurationsdatei wurde unter your_path\config.yml mit Schema-Anmerkungen erstellt.
2025-05-02 10:43:53 INFO  Config - Konfigurationsdatei-Überwachung gestartet unter your_path\config.yml
2025-05-02 10:43:53 INFO  LM Studio Server - LM Studio Server gestartet auf 1234
2025-05-02 10:43:53 INFO  Ollama Server - Ollama Server gestartet auf 11434
2025-05-02 10:43:53 INFO  Model List - Modellliste geladen mit: []
</code></pre>
<p>Anschließend können Sie die Konfigurationsdatei bearbeiten, um Ihren Proxy-Server einzurichten.</p>
<h2>Konfigurationsdatei</h2>
<p>Diese Konfigurationsdatei wird automatisch neu geladen, sobald Sie sie ändern. Nur die betroffenen Teile des Servers werden aktualisiert.</p>
<p>Bei der ersten Erstellung der Konfigurationsdatei wird diese mit Schema-Anmerkungen erstellt. Dies ermöglicht Vervollständigungen und Überprüfungen in Ihrem Editor.</p>
<h2>Beispiel-Konfigurationsdatei</h2>
<pre><code class="language-yaml"># $schema: https://github.com/Stream29/ProxyAsLocalModel/raw/master/config_v3.schema.json
lmStudio:
  port: 1234 # Dies ist der Standardwert
  enabled: true # Dies ist der Standardwert
  host: 0.0.0.0 # Dies ist der Standardwert
  path: /your/path # Wird vor den ursprünglichen Endpunkten hinzugefügt, Standardwert ist leer
ollama:
  port: 11434 # Dies ist der Standardwert
  enabled: true # Dies ist der Standardwert
  host: 0.0.0.0 # Dies ist der Standardwert
  path: /your/path # Wird vor den ursprünglichen Endpunkten hinzugefügt, Standardwert ist leer
client:
  socketTimeout: 1919810 # Long.MAX_VALUE ist der Standardwert, in Millisekunden
  connectionTimeout: 1919810 # Long.MAX_VALUE ist der Standardwert, in Millisekunden
  requestTimeout: 1919810 # Long.MAX_VALUE ist der Standardwert, in Millisekunden
  retry: 3 # Dies ist der Standardwert
  delayBeforeRetry: 1000 # Dies ist der Standardwert, in Millisekunden

apiProviders:
  OpenAI:
    type: OpenAi
    baseUrl: https://api.openai.com/v1
    apiKey: &lt;your_api_key&gt;
    modelList:
      - gpt-4o
  Claude:
    type: Claude
    apiKey: &lt;your_api_key&gt;
    modelList:
      - claude-3-7-sonnet
  Qwen:
    type: DashScope
    apiKey: &lt;your_api_key&gt;
    modelList: # Dies ist der Standardwert
      - qwen-max
      - qwen-plus
      - qwen-turbo
      - qwen-long
  DeepSeek:
    type: DeepSeek
    apiKey: &lt;your_api_key&gt;
    modelList: # Dies ist der Standardwert
      - deepseek-chat
      - deepseek-reasoner
  Mistral:
    type: Mistral
    apiKey: &lt;your_api_key&gt;
    modelList: # Dies ist der Standardwert
      - codestral-latest
      - mistral-large
  SiliconFlow:
    type: SiliconFlow
    apiKey: &lt;your_api_key&gt;
    modelList:
      - Qwen/Qwen3-235B-A22B
      - Pro/deepseek-ai/DeepSeek-V3
      - THUDM/GLM-4-32B-0414
  OpenRouter:
    type: OpenRouter
    apiKey: &lt;your_api_key&gt;
    modelList:
      - openai/gpt-4o
  Gemini:
    type: Gemini
    apiKey: &lt;your_api_key&gt;
    modelList:
      - gemini-2.5-flash-preview-04-17
</code></pre>
<hr />
<p>Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx">Open Ai Tx</a> | Last indexed: 2025-07-10</p>
<hr />

        </div>
    </div>
    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Default Statcounter code for openaitx
https://openaitx.github.io/ -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>
    <!-- End of Statcounter Code -->
</body>
</html>