<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>tome - French Documentation</title>
    <meta name="description" content="Read tome documentation in French. This project has 292 stars on GitHub.">
    <meta name="keywords" content="tome, French, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "tome",
  "description": "Documentation for tome in French",
  "author": {
    "@type": "Person",
    "name": "runebookai"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 292
  },
  "url": "https://OpenAiTx.github.io/projects/runebookai/tome/README-fr.html",
  "sameAs": "https://raw.githubusercontent.com/runebookai/tome/main/README.md",
  "datePublished": "2025-07-03",
  "dateModified": "2025-07-03"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            color: #24292e;
            margin-bottom: 8px;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">tome</h1>
            <div class="project-meta">
                <span class="stars">⭐ 292 stars</span>
                <span class="language">French</span>
                <span>by runebookai</span>
            </div>
        </div>
        
        <div class="content">
            <h1>Tome - Grimoire Magique d’IA</h1></p><p><img src="https://raw.githubusercontent.com/runebookai/tome/main/static/images/repo-header.png" alt="Tome" /></p><p><p align="center">
    <code>une application de bureau magique qui met la puissance des LLM et MCP entre les mains de tous</code>
</p></p><p><p align="center">
    <a href="https://discord.gg/9CH6us29YA" target="_blank"><img src="https://img.shields.io/discord/1365100902561742868?logo=discord&logoColor=fff&label=Rejoignez-nous!&color=9D7CD8" alt="Rejoignez-nous sur Discord" /></a>
    <a href="https://opensource.org/licenses/Apache-2.0" target="_blank"><img src="https://img.shields.io/badge/License-Apache_2.0-blue.svg" alt="Licence : Apache 2.0" /></a>
    <a href="https://github.com/runebookai/tome/releases" target="_blank"><img src="https://img.shields.io/github/v/release/runebookai/tome" alt="Version GitHub" /></a>
</p></p><p><p align="center">
    🔮 Téléchargez l’application de bureau Tome : <a href="https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_x64-setup.exe">Windows</a> | <a href="https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_aarch64.dmg">MacOS</a>
</p></p><h1>Tome</h1></p><p>Tome est une application de bureau qui permet à <strong>tout le monde</strong> de maîtriser la magie des LLM et MCP. Téléchargez Tome, connectez n'importe quel LLM local ou distant et reliez-le à des milliers de serveurs MCP pour créer votre propre grimoire magique alimenté par l’IA.</p><p>🫥 Vous voulez que ce soit 100% local, 100% privé ? Utilisez Ollama et Qwen3 avec uniquement des serveurs MCP locaux pour lancer des sorts dans votre propre univers de poche. ⚡ Vous souhaitez des modèles cloud de pointe avec les derniers serveurs MCP distants ? C’est possible aussi. C’est vous qui décidez !</p><p>🏗️ Ceci est une préversion technique, donc gardez à l'esprit que tout n'est pas parfaitement abouti. <a href="https://discord.gg/9CH6us29YA" target="_blank" rel="noopener noreferrer">Rejoignez-nous sur Discord</a> pour partager vos astuces, conseils et signaler les problèmes rencontrés. Ajoutez une étoile à ce dépôt pour suivre les mises à jour et nouveautés !</p><h2>🪄 Fonctionnalités</h2></p><ul><li>🧙 <strong>Expérience fluide et accessible aux débutants</strong></li>
  <li>Téléchargez et installez simplement Tome, puis connectez le LLM de votre choix</li>
  <li>Pas besoin de manipuler JSON, Docker, python ou node</li>
<li>🤖 <strong>Prise en charge des modèles d’IA</strong></li>
  <li><strong>Distant</strong> : Google Gemini, OpenAI, tout point de terminaison compatible API OpenAI</li>
  <li><strong>Local</strong> : Ollama, LM Studio, Cortex, tout point de terminaison compatible API OpenAI</li>
<li>🔮 <strong>Support MCP amélioré</strong></li>
  <li>Interface pour installer, supprimer, activer/désactiver les serveurs MCP</li>
  <li>Serveurs MCP npm, uvx, node, python pris en charge nativement</li>
<li>🏪 <strong>Intégration au registre <a href="https://smithery.ai" target="_blank" rel="noopener noreferrer">Smithery.ai</a></strong></li>
  <li>Des milliers de serveurs MCP disponibles en un clic</li>
<li>✏️ <strong>Personnalisation des fenêtres de contexte et de la température</strong></li>
<li>🧰 <strong>Prise en charge native des appels d’outils et des modèles de raisonnement</strong></li>
  <li>Améliorations de l’interface qui distinguent clairement les appels d’outils et les messages de réflexion</li></p><p></ul><h2>Démo</h2></p><p>https://github.com/user-attachments/assets/0775d100-3eba-4219-9e2f-360a01f28cce</p><h1>Démarrage</h1></p><h2>Prérequis</h2></p><ul><li>MacOS ou Windows (Linux bientôt disponible !)</li>
<li>Fournisseur de LLM au choix : <a href="https://ollama.com/" target="_blank" rel="noopener noreferrer">Ollama</a> ou <a href="https://aistudio.google.com/app/apikey" target="_blank" rel="noopener noreferrer">clé API Gemini</a> sont faciles/gratuits</li>
<li><a href="https://github.com/runebookai/tome/releases" target="_blank" rel="noopener noreferrer">Téléchargez la dernière version de Tome</a></li></p><p></ul><h2>Démarrage rapide</h2></p><ul><li>Installez <a href="https://github.com/runebookai/tome/releases" target="_blank" rel="noopener noreferrer">Tome</a></li>
<li>Connectez votre fournisseur LLM préféré – OpenAI, Ollama et Gemini sont préconfigurés, mais vous pouvez aussi ajouter des fournisseurs comme LM Studio en utilisant http://localhost:1234/v1 comme URL</li>
<li>Ouvrez l’onglet MCP dans Tome et installez votre premier <a href="https://github.com/modelcontextprotocol/servers" target="_blank" rel="noopener noreferrer">serveur MCP</a> (Fetch est un bon point de départ, il suffit de coller <code>uvx mcp-server-fetch</code> dans le champ serveur).</li>
<li>Discutez avec votre modèle propulsé par MCP ! Demandez-lui de récupérer la meilleure actualité sur Hacker News.</li></p><p></ul><h1>Vision</h1></p><p>Nous voulons rendre les LLM locaux et MCP accessibles à tous. Nous construisons un outil qui vous permet d’être créatif avec les LLM, que vous soyez ingénieur, bricoleur, amateur ou toute autre personne.</p><h2>Principes fondamentaux</h2></p><ul><li><strong>Tome est local avant tout :</strong> Vous contrôlez où vont vos données.</li>
<li><strong>Tome est pour tous :</strong> Vous ne devriez pas avoir à gérer des langages de programmation, des gestionnaires de paquets ou des fichiers de configuration json.</li></p><p></ul><h2>Prochaines étapes</h2></p><p>Nous avons reçu de nombreux retours incroyables ces dernières semaines depuis la sortie de Tome, mais nous avons de grands projets pour l’avenir. Nous voulons libérer les LLM de leur boîte de dialogue, et de nombreuses fonctionnalités arrivent pour vous aider à y parvenir.</p><ul><li>Tâches planifiées : les LLM devraient effectuer des tâches utiles même lorsque vous n’êtes pas devant l’ordinateur.</li>
<li>Intégrations natives : les serveurs MCP sont un excellent moyen d’accéder à des outils et informations, mais nous voulons ajouter des intégrations encore plus puissantes pour interagir avec les LLM de manière unique.</li>
<li>Générateur d’applications : à long terme, nous pensons que les meilleures expériences ne seront pas dans une interface de chat. Nous prévoyons d’ajouter des outils supplémentaires pour vous permettre de créer des applications et des workflows puissants.</li>
<li>??? Dites-nous ce que vous aimeriez voir ! Rejoignez notre communauté via les liens ci-dessous, nous serions ravis d’avoir vos retours.</li></p><p></ul><h1>Communauté</h1></p><p><a href="https://discord.gg/9CH6us29YA" target="_blank" rel="noopener noreferrer">Discord</a> <a href="https://blog.runebook.ai" target="_blank" rel="noopener noreferrer">Blog</a> <a href="https://bsky.app/profile/gettome.app" target="_blank" rel="noopener noreferrer">Bluesky</a> <a href="https://twitter.com/get_tome" target="_blank" rel="noopener noreferrer">Twitter</a>

---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-07-03

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/runebookai/tome/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-07-03 
    </div>
    
</body>
</html>