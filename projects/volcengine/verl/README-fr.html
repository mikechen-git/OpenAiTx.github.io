<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>verl - volcengine/verl fr</title>
    <meta name="title" content="verl - volcengine/verl fr | 👋 Bonjour à tous ! verl est une bibliothèque d'entraînement RL initiée par l'équipe ByteDance Seed et maintenue par la communauté verl. verl : Volcano Engine R...">
    <meta name="description" content="volcengine/verl - GitHub repository fr documentation and information | 👋 Bonjour à tous ! verl est une bibliothèque d'entraînement RL initiée par l'équipe ByteDance Seed et maintenue par la communauté verl. verl : Volcano Engine R...">
    <meta name="keywords" content="volcengine, verl, GitHub, repository, fr documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/volcengine/verl/README-fr.html">
    <meta property="og:title" content="verl - volcengine/verl fr | 👋 Bonjour à tous ! verl est une bibliothèque d'entraînement RL initiée par l'équipe ByteDance Seed et maintenue par la communauté verl. verl : Volcano Engine R...">
    <meta property="og:description" content="volcengine/verl - GitHub repository fr documentation and information | 👋 Bonjour à tous ! verl est une bibliothèque d'entraînement RL initiée par l'équipe ByteDance Seed et maintenue par la communauté verl. verl : Volcano Engine R...">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">
    <script src="/js/marked.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        body { display: flex; flex-direction: column; min-height: 100vh; }
        .main-container { margin: 0 auto; width: 100%; max-width: 980px; padding: 0 20px; }
        @media (max-width: 768px) { .main-container { padding: 0 15px; } }
        .markdown-body img { max-width: 100%; height: auto; }
        .header { text-align: center; margin-bottom: 30px; padding: 20px; background-color: #f6f8fa; border-bottom: 1px solid #e1e4e8; position: relative; }
        .header .links { margin-top: 10px; font-size: 16px; }
        .header .links a { color: #0366d6; text-decoration: none; margin-left: 5px; }
        .header .links a:hover { text-decoration: underline; }
        .language-badges { margin-top: 15px; text-align: center; }
        .language-badges a { display: inline-block; margin: 2px; text-decoration: none; }
        .language-badges img { height: 20px; border-radius: 3px; }
        .language-badges a:hover img { opacity: 0.8; }
    </style>
</head>
<body>
    <div style="position: fixed; top: 2px; left: 2px; z-index: 2000; background: rgba(255,255,255,0.95); border-radius: 6px; box-shadow: 0 2px 8px rgba(0,0,0,0.06); padding: 4px 14px; font-size: 15px; color: #222; font-family: 'Segoe UI', Arial, sans-serif; font-weight: 500; letter-spacing: 0.5px;">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" style="color: #0366d6; text-decoration: none; font-weight: 600;">Open AI Tx</a>
    </div>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/volcengine/verl" id="githubRepoLink" target="_blank">volcengine/verl</a>
<h1 style="display: none;">👋 Bonjour à tous ! verl est une bibliothèque d'entraînement RL initiée par l'équipe ByteDance Seed et maintenue par la communauté verl. verl : Volcano Engine R...</h1>
        </div>
        <div class="language-badges" id="languageBadges">
            <!-- You can generate language badges here if needed -->
        </div>
    </div>
    <div class="main-container">
        <div class="markdown-body" id="content">
            <div align="center">
 👋 Bonjour à tous ! 
    verl est une bibliothèque d'entraînement RL initiée par l'<b>équipe ByteDance Seed</b> et maintenue par la communauté verl.
    <br>
    <br>
</div>
<div align="center">
<p><a href="https://deepwiki.com/volcengine/verl"><img src="https://devin.ai/assets/deepwiki-badge.png" alt="Ask DeepWiki.com" height="20"/></a>
<a href="https://github.com/volcengine/verl/stargazers"><img src="https://img.shields.io/github/stars/volcengine/verl" alt="GitHub Repo stars" /></a>
<a href="https://twitter.com/verl_project"><img src="https://img.shields.io/twitter/follow/verl_project" alt="Twitter" /></a>
<a href="https://join.slack.com/t/verlgroup/shared_invite/zt-2w5p9o4c3-yy0x2Q56s_VlGLsJ93A6vA"><img src="https://img.shields.io/badge/Slack-verl-blueviolet?logo=slack&amp"></a>
<a href="https://arxiv.org/pdf/2409.19256"><img src="https://img.shields.io/static/v1?label=EuroSys&message=Paper&color=red"></a>
<a href="https://verl.readthedocs.io/en/latest/"><img src="https://img.shields.io/badge/documentation-blue" alt="Documentation" /></a>
<a href="https://raw.githubusercontent.com/eric-haibin-lin/verl-community/refs/heads/main/WeChat.JPG"><img src="https://img.shields.io/badge/微信-green?logo=wechat&amp"></a></p>
</div>
<p><img src="https://github.com/user-attachments/assets/c42e675e-497c-4508-8bb9-093ad4d1f216" alt="seed logo" /></p>
<h1 style="text-align: center;">verl : Volcano Engine Reinforcement Learning pour LLMs</h1>
<p>verl est une bibliothèque d'entraînement RL flexible, efficace et prête pour la production pour les grands modèles de langage (LLMs).</p>
<p>verl est la version open source de l'article <strong><a href="https://arxiv.org/abs/2409.19256v2">HybridFlow: A Flexible and Efficient RLHF Framework</a></strong>.</p>
<p>verl est flexible et facile à utiliser avec :</p>
<ul>
<li><p><strong>Extension facile de divers algorithmes RL</strong> : Le modèle de programmation hybrid-controller permet une représentation flexible et une exécution efficace de flux de données complexes de post-entraînement. Construisez des flux RL tels que GRPO, PPO en quelques lignes de code.</p>
</li>
<li><p><strong>Intégration transparente de l'infrastructure LLM existante avec des API modulaires</strong> : Découple les dépendances de calcul et de données, permettant une intégration transparente avec les frameworks LLM existants, tels que FSDP, Megatron-LM, vLLM, SGLang, etc.</p>
</li>
<li><p><strong>Mapping flexible des dispositifs</strong> : Prend en charge divers placements de modèles sur différents ensembles de GPU pour une utilisation efficace des ressources et une évolutivité sur différentes tailles de clusters.</p>
</li>
<li><p>Intégration prête à l'emploi avec les modèles populaires HuggingFace</p>
</li>
</ul>
<p>verl est rapide avec :</p>
<ul>
<li><p><strong>Débit à l'état de l'art</strong> : Intégration des moteurs d'entraînement et d'inférence SOTA pour LLM, ainsi qu'un débit RL SOTA.</p>
</li>
<li><p><strong>Resharding efficace des modèles acteurs avec 3D-HybridEngine</strong> : Élimine la redondance mémoire et réduit considérablement les surcoûts de communication lors des transitions entre les phases d'entraînement et de génération.</p>
</li>
</ul>
</p>
<h2>Actualités</h2>
<ul>
<li>[2025/06] verl avec backend Megatron permet de supporter de grands modèles MoE tels que <a href="https://verl.readthedocs.io/en/latest/perf/dpsk.html">DeepSeek-671b et Qwen3-236b</a>.</li>
<li>[2025/06] L'équipe verl présentera les dernières avancées du projet lors du <a href="https://www.lfasiallc.com/pytorch-day-china/">PyTorch Day China</a> le 7 juin. Rencontrez notre équipe de dev à Pékin !</li>
<li>[2025/05] <a href="https://arxiv.org/abs/2409.06957">PF-PPO</a>, accepté à l'ICML 2025, est désormais supporté dans verl ! PF-PPO améliore l'efficacité et la robustesse de l'apprentissage de la politique en filtrant les signaux de récompense potentiellement bruyants et en réutilisant les expériences de haute qualité via un buffer de relecture.</li>
<li>[2025/04] Nous donnerons un tutoriel sur les dernières techniques de post-entraînement et le guide de programmation pour verl lors de <a href="https://iclr.cc/virtual/2025/calendar?filter_events=Expo+Talk+Panel&amp;filter_rooms=">ICLR 2025 Expo</a>, <a href="https://open-foundation-model.github.io/">atelier SCI-FM</a> et <a href="https://lu.ma/d23nyynm">LMSys afterparty</a>. Les supports de présentation sont disponibles <a href="https://github.com/eric-haibin-lin/verl-community/tree/main/iclr25">ici</a>.</li>
<li>[2025/04] Le rapport technique <a href="https://github.com/ByteDance-Seed/Seed-Thinking-v1.5/blob/main/seed-thinking-v1.5.pdf">Seed-Thinking-v1.5</a> est publié ! Entraîné avec verl, Seed-Thinking-v1.5 atteint 86.7 sur AIME 2024, 55.0 sur Codeforces et 77.3 sur GPQA, démontrant d'excellentes capacités de raisonnement en STEM et codage. Au-delà des tâches de raisonnement, la méthode démontre une généralisation notable sur divers domaines.</li>
<li>[2025/04] <a href="https://arxiv.org/pdf/2504.05118">VAPO</a> (value-based augmented PPO) présente notre dernière méthode RL pour les modèles de raisonnement. Entraîné à partir du modèle Qwen-32B-base, VAPO atteint 60.4 sur AIME 2024, surpassant DAPO-32B.</li>
<li>[2025/03] verl v0.3.0.post1 est publié ! Voir la <a href="https://github.com/volcengine/verl/releases/">note de version</a> pour plus de détails. Il atteint <a href="https://tongyx361.github.io/blogs/posts/verl-intro/#/verl-flexible-and-efficient-rl-for-llms">~1.4x d'accélération</a> par rapport aux versions précédentes.</li>
<li>[2025/03] <a href="https://dapo-sia.github.io/">DAPO</a> est l'algorithme RL SOTA open source qui atteint 50 points sur AIME 2024 basé sur le modèle pré-entraîné Qwen2.5-32B, surpassant le précédent SOTA de DeepSeek GRPO (DeepSeek-R1-Zero-Qwen-32B). L'entraînement de DAPO est entièrement propulsé par verl et le code de reproduction est disponible dans <code>recipe/dapo</code> maintenant.</li>
</ul>
<details><summary> plus... </summary>
<ul>
  <li>[2025/05] verl sera présenté à [A2M Shanghai](https://a2m.msup.com.cn/home/?aid=4488&city=shanghai) du 16 au 17 mai.</li>
  <li>[2025/05] verl sera présenté à [GOSIM x PyTorch Day 2025](https://paris2025.gosim.org/). À bientôt à Paris !</li>
  <li>[2025/03] Nous avons présenté le modèle de programmation de verl lors du [vLLM Beijing Meetup](https://mp.weixin.qq.com/s/n77GibL2corAtQHtVEAzfg) et [présentation et mises à jour de verl](https://github.com/eric-haibin-lin/verl-community/blob/main/slides/verl-lmsys-meetup.pdf) lors du [SGLang-LMSYS Org Meetup](https://lu.ma/ntjrr7ig) à Sunnyvale mi-mars.</li>
  <li>[2025/03] Nous présenterons verl (HybridFlow) à EuroSys 2025. Rendez-vous à Rotterdam !</li>
  <li>[2025/02] verl v0.2.0.post2 est publié !</li>
  <li>[2025/02] Nous avons présenté verl lors du <a href="https://lu.ma/ji7atxux">Bytedance/NVIDIA/Anyscale Ray Meetup</a>. Rendez-vous à San Jose !</li>
  <li>[2025/01] [Doubao-1.5-pro](https://team.doubao.com/zh/special/doubao_1_5_pro) est publié avec des performances SOTA sur LLM & VLM. Le modèle d'aperçu RL scaling a été entraîné avec verl, atteignant des performances de niveau OpenAI O1 sur les benchmarks de mathématiques (70.0 pass@1 sur AIME).</li>
  <li>[2024/12] verl a été présenté à Ray Forward 2024. Slides disponibles <a href="https://github.com/eric-haibin-lin/verl-community/blob/main/slides/Ray_Forward_2024_%E5%B7%AB%E9%94%A1%E6%96%8C.pdf">ici</a></li>
  <li>[2024/12] L'équipe a présenté <a href="https://neurips.cc/Expo/Conferences/2024/workshop/100677">Post-training LLMs: From Algorithms to Infrastructure</a> à NeurIPS 2024. <a href="https://github.com/eric-haibin-lin/verl-data/tree/neurips">Slides</a> et <a href="https://neurips.cc/Expo/Conferences/2024/workshop/100677">vidéo</a> disponibles.</li>
  <li>[2024/10] verl a été présenté au Ray Summit. <a href="https://www.youtube.com/watch?v=MrhMcXkXvJU&list=PLzTswPQNepXntmT8jr9WaNfqQ60QwW7-U&index=37">Vidéo Youtube</a> disponible.</li>
  <li>[2024/08] HybridFlow (verl) est accepté à EuroSys 2025.</li>
</ul>   
</details>
<h2>Fonctionnalités clés</h2>
<ul>
<li><strong>FSDP</strong>, <strong>FSDP2</strong> et <strong>Megatron-LM</strong> pour l'entraînement.</li>
<li><strong>vLLM</strong>, <strong>SGLang</strong> et <strong>HF Transformers</strong> pour la génération des rollouts.</li>
<li>Compatible avec Hugging Face Transformers et Modelscope Hub : <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/grpo_trainer/run_qwen3-8b.sh">Qwen-3</a>, Qwen-2.5, Llama3.1, Gemma2, DeepSeek-LLM, etc.</li>
<li>Fine-tuning supervisé.</li>
<li>Apprentissage par renforcement avec <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/ppo_trainer/">PPO</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/grpo_trainer/">GRPO</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/remax_trainer/">ReMax</a>, <a href="https://verl.readthedocs.io/en/latest/examples/config.html#algorithm">REINFORCE++</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/rloo_trainer/">RLOO</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/recipe/prime/">PRIME</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/recipe/dapo/">DAPO</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/recipe/drgrpo">DrGRPO</a>, etc.
<ul>
<li>Supporte la récompense basée sur un modèle et la récompense basée sur une fonction (récompense vérifiable) pour les maths, <a href="https://raw.githubusercontent.com/volcengine/verl/main/recipe/dapo">le codage</a>, etc.</li>
<li>Supporte les modèles vision-langage (VLMs) et <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/grpo_trainer/run_qwen2_5_vl-7b.sh">RL multi-modal</a> avec Qwen2.5-vl, Kimi-VL</li>
<li><a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/sglang_multiturn">Multi-turn avec appel d’outils</a></li>
</ul>
</li>
<li>Recettes d'alignement LLM comme <a href="https://raw.githubusercontent.com/volcengine/verl/main/recipe/sppo">Self-play preference optimization (SPPO)</a></li>
<li>Flash attention 2, <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/ppo_trainer/run_qwen2-7b_seq_balance.sh">sequence packing</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/ppo_trainer/run_deepseek7b_llm_sp2.sh">sequence parallelism</a> via DeepSpeed Ulysses, <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/sft/gsm8k/run_qwen_05_peft.sh">LoRA</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/sft/gsm8k/run_qwen_05_sp2_liger.sh">Liger-kernel</a>.</li>
<li>Évolue jusqu’à 671B de paramètres et des centaines de GPU avec <a href="https://github.com/volcengine/verl/pull/1467">expert parallelism</a></li>
<li>Support RL LoRA multi-gpu <a href="https://verl.readthedocs.io/en/latest/advance/ppo_lora.html">LoRA RL</a> pour économiser de la mémoire.</li>
<li>Suivi d'expérience avec wandb, swanlab, mlflow et tensorboard.</li>
</ul>
<h2>Fonctionnalités et changements à venir</h2>
<ul>
<li>Roadmap https://github.com/volcengine/verl/issues/710</li>
<li>Optimisations DeepSeek 671b avec Megatron v0.11 https://github.com/volcengine/verl/issues/708</li>
<li>Multi-turn rollout et outils utilisant des optimisations https://github.com/volcengine/verl/issues/1882</li>
<li>Interactions avec l'environnement https://github.com/volcengine/verl/issues/1172</li>
<li>Liste des changements majeurs depuis v0.3 https://github.com/volcengine/verl/discussions/943, entropy_coeff par défaut à 0</li>
<li>Lora pour RL https://github.com/volcengine/verl/pull/1127</li>
</ul>
<h2>Démarrage rapide</h2>
<p><a href="https://verl.readthedocs.io/en/latest/index.html"><b>Documentation</b></a></p>
<p><strong>Démarrage rapide :</strong></p>
<ul>
<li><a href="https://verl.readthedocs.io/en/latest/start/install.html">Installation</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/start/quickstart.html">Quickstart</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/hybrid_flow.html">Guide de programmation</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/algo/ppo.html">PPO dans verl</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/algo/grpo.html">GRPO dans verl</a></li>
</ul>
<p><strong>Exécution pas à pas d'un exemple PPO :</strong></p>
<ul>
<li><a href="https://verl.readthedocs.io/en/latest/preparation/prepare_data.html">Préparer les données pour le post-entraînement</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/preparation/reward_function.html">Implémenter une fonction de récompense pour le dataset</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/examples/ppo_code_architecture.html">Architecture d’un exemple PPO</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/examples/config.html">Explication des configs</a></li>
</ul>
<p><strong>Algorithmes de référence reproductibles :</strong></p>
<ul>
<li><a href="https://verl.readthedocs.io/en/latest/algo/baseline.html">Performances RL sur le codage, les maths</a></li>
</ul>
<p><strong>Pour l'explication du code et l'utilisation avancée (extension) :</strong></p>
<ul>
<li><p>PPO Trainer et Workers</p>
<ul>
<li><a href="https://verl.readthedocs.io/en/latest/workers/ray_trainer.html">PPO Ray Trainer</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/workers/fsdp_workers.html">Backend PyTorch FSDP</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/index.html">Backend Megatron-LM</a></li>
</ul>
</li>
<li><p>Utilisation avancée et extension</p>
<ul>
<li><a href="https://verl.readthedocs.io/en/latest/advance/fsdp_extension.html">Ajouter des modèles avec le backend FSDP</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/advance/megatron_extension.html">Ajouter des modèles avec le backend Megatron-LM</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/sglang_multiturn/multiturn.html">Support multi-turn rollout</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/sglang_multiturn/search_tool_example.html">Intégration d’outils de recherche</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/examples/sandbox_fusion_example.html">Intégration Sandbox Fusion</a></li>
<li><a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/split_placement">Déploiement avec répartition GPU séparée</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/advance/dpo_extension.html">Extension à d’autres algorithmes RL(HF)</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/advance/placement.html">Tutoriel sur la conception d’API Ray</a></li>
</ul>
</li>
</ul>
<p><strong>Blogs de la communauté</strong></p>
<ul>
<li><a href="https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/verl/multi-turn/verl-multiturn-rollout-Release.md">SGLang, verl, OpenBMB et Université Tsinghua : Pionniers du RLHF multi-turn de bout en bout</a></li>
<li><a href="https://rocm.blogs.amd.com/artificial-intelligence/verl-large-scale/README.html">Reinforcement Learning from Human Feedback sur GPU AMD avec intégration verl et ROCm</a></li>
<li><a href="https://mp.weixin.qq.com/s/7nbqxk4knMGd-hQE9ls2tA">veMLP x verl : Jouer avec l’entraînement RL</a></li>
<li><a href="https://www.volcengine.com/docs/6459/1463942">Meilleures pratiques pour l'entraînement RL distribué GRPO avec verl</a></li>
<li><a href="https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/verl/readme.md">Analyse du texte original HybridFlow verl</a></li>
<li><a href="https://team.doubao.com/en/blog/%E6%9C%80%E9%AB%98%E6%8F%90%E5%8D%8720%E5%80%8D%E5%90%9E%E5%90%90%E9%87%8F-%E8%B1%86%E5%8C%85%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9B%A2%E9%98%9F%E5%8F%91%E5%B8%83%E5%85%A8%E6%96%B0-rlhf-%E6%A1%86%E6%9E%B6-%E7%8E%B0%E5%B7%B2%E5%BC%80%E6%BA%90">Jusqu’à 20 fois plus de débit ! L’équipe Doubao publie un nouveau framework RLHF, désormais open source !</a></li>
</ul>
<h2>Guide d'optimisation des performances</h2>
<p>La performance est essentielle pour les algorithmes RL on-policy. Nous avons rédigé un <a href="https://verl.readthedocs.io/en/latest/perf/perf_tuning.html">guide détaillé d’optimisation des performances</a> pour vous aider à optimiser les performances.</p>
<h2>Mise à niveau vers vLLM &gt;= v0.8.2</h2>
<p>verl supporte désormais vLLM&gt;=0.8.2 lorsque FSDP est utilisé comme backend d'entraînement. Veuillez consulter <a href="https://raw.githubusercontent.com/volcengine/verl/main/docs/README_vllm0.8.md">ce document</a> pour le guide d'installation et plus d'informations. Évitez vllm 0.7.x, qui contient des bugs pouvant entraîner des OOM et des erreurs inattendues.</p>
<h2>Utilisez la dernière version de SGLang</h2>
<p>SGLang est entièrement pris en charge avec verl, et le groupe SGLang RL travaille activement sur le développement de fonctionnalités uniques, y compris RL agentique multi-turn, VLM RLHF, RL basé serveur et rollout partiel. Veuillez consulter <a href="https://verl.readthedocs.io/en/latest/workers/sglang_worker.html">ce document</a> pour le guide d'installation et plus d'informations.</p>
<h2>Mise à niveau vers FSDP2</h2>
<p>verl adopte pleinement FSDP2 ! FSDP2 est recommandé par l’équipe torch distributed, offrant un meilleur débit et une meilleure gestion de la mémoire, et il est composable avec d’autres fonctionnalités (par exemple torch.compile). Pour activer FSDP2, utilisez simplement verl main et définissez les options suivantes :</p>
<pre><code>actor_rollout_ref.ref.strategy=fsdp2
actor_rollout_ref.actor.strategy=fsdp2
critic.strategy=fsdp2 
reward_model.strategy=fsdp2 
</code></pre>
<p>De plus, l’offloading CPU FSDP2 est compatible avec l’accumulation de gradients. Vous pouvez l’activer pour économiser de la mémoire avec <code>actor_rollout_ref.actor.offload_policy=True</code>. Pour plus de détails, voir https://github.com/volcengine/verl/pull/1026</p>
<h2>Support AMD (ROCm Kernel)</h2>
<p>verl prend maintenant en charge FSDP comme moteur d’entraînement (Megatron sera bientôt supporté) et s’intègre à la fois à vLLM et SGLang comme moteurs d’inférence. Veuillez consulter <a href="https://raw.githubusercontent.com/volcengine/verl/main/docs/amd_tutorial/amd_build_dockerfile_page.rst">ce document</a> pour le guide d'installation et plus d'informations, et <a href="https://raw.githubusercontent.com/volcengine/verl/main/docs/amd_tutorial/amd_vllm_page.rst">ce document</a> pour l’optimisation des performances vLLM pour ROCm.</p>
<h2>Citation et remerciements</h2>
<p>Si ce projet vous a été utile, merci de citer :</p>
<ul>
<li><a href="https://arxiv.org/abs/2409.19256v2">HybridFlow: A Flexible and Efficient RLHF Framework</a></li>
<li><a href="https://i.cs.hku.hk/%7Ecwu/papers/gmsheng-NL2Code24.pdf">A Framework for Training Large Language Models for Code Generation via Proximal Policy Optimization</a></li>
</ul>
<pre><code class="language-bibtex">@article{sheng2024hybridflow,
  title   = {HybridFlow: A Flexible and Efficient RLHF Framework},
  author  = {Guangming Sheng and Chi Zhang and Zilingfeng Ye and Xibin Wu and Wang Zhang and Ru Zhang and Yanghua Peng and Haibin Lin and Chuan Wu},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2409.19256}
}
</code></pre>
<p>verl s’inspire du design de Nemo-Aligner, Deepspeed-chat et OpenRLHF. Le projet est adopté et contribué par Bytedance, Anyscale, LMSys.org, <a href="https://github.com/QwenLM/">équipe Alibaba Qwen</a>, Shanghai AI Lab, Université Tsinghua, UC Berkeley, UCLA, UIUC, Université de Hong Kong, ke.com, <a href="https://www.all-hands.dev/">All Hands AI</a>, <a href="http://modelbest.cn/">ModelBest</a>, OpenPipe, JD AI Lab, Microsoft Research, <a href="https://www.stepfun.com/">StepFun</a>, Amazon, Linkedin, Meituan, <a href="https://www.camel-ai.org/">Camel-AI</a>, <a href="https://github.com/OpenManus">OpenManus</a>, Xiaomi, Prime Intellect, NVIDIA research, <a href="https://www.baichuan-ai.com/home">Baichuan</a>, <a href="https://www.xiaohongshu.com/">RedNote</a>, <a href="https://www.swiss-ai.org/">SwissAI</a>, <a href="https://www.moonshot-ai.com/">Moonshot AI (Kimi)</a>, Baidu, Snowflake, et bien d’autres.</p>
<h2>Projets remarquables utilisant verl</h2>
<ul>
<li><a href="https://github.com/Jiayi-Pan/TinyZero">TinyZero</a> : une reproduction de la recette <strong>DeepSeek R1 Zero</strong> pour les tâches de raisonnement <img src="https://img.shields.io/github/stars/Jiayi-Pan/TinyZero" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/NovaSky-AI/SkyThought">SkyThought</a> : entraînement RL pour Sky-T1-7B par l’équipe NovaSky AI. <img src="https://img.shields.io/github/stars/NovaSky-AI/SkyThought" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/hkust-nlp/simpleRL-reason">simpleRL-reason</a> : SimpleRL-Zoo : étude et adaptation du Zero RL pour les modèles open base dans la nature <img src="https://img.shields.io/github/stars/hkust-nlp/simpleRL-reason" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/hiyouga/EasyR1">Easy-R1</a> : framework d’entraînement RL <strong>multi-modal</strong> <img src="https://img.shields.io/github/stars/hiyouga/EasyR1" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/OpenManus/OpenManus-RL">OpenManus-RL</a> : framework RL tuning d’agents LLM pour environnements multi-agents. <img src="https://img.shields.io/github/stars/OpenManus/OpenManus-RL" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/agentica-project/rllm">rllm</a> : entraînement RL asynchrone avec <a href="https://github.com/agentica-project/verl-pipeline">verl-pipeline</a> <img src="https://img.shields.io/github/stars/agentica-project/rllm" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/PRIME-RL/PRIME">PRIME</a> : renforcement de processus via récompenses implicites <img src="https://img.shields.io/github/stars/PRIME-RL/PRIME" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/ZihanWang314/ragen">RAGEN</a> : framework d’entraînement d’<strong>agent</strong> de raisonnement polyvalent <img src="https://img.shields.io/github/stars/ZihanWang314/ragen" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/PeterGriffinJin/Search-R1">Search-R1</a> : RL avec raisonnement et <strong>recherche (tool-call)</strong> intercalés dans les LLMs <img src="https://img.shields.io/github/stars/PeterGriffinJin/Search-R1" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/pat-jj/DeepRetrieval">DeepRetrieval</a> : entraînement RL d’<strong>agents de recherche</strong> avec <strong>résultats de recherche/retrieval</strong> <img src="https://img.shields.io/github/stars/pat-jj/DeepRetrieval" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/Agent-RL/ReSearch">ReSearch</a> : apprentissage du raisonnement avec recherche pour LLMs via RL <img src="https://img.shields.io/github/stars/Agent-RL/ReSearch" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/ganler/code-r1">Code-R1</a> : reproduction de R1 pour le <strong>code</strong> avec récompenses fiables <img src="https://img.shields.io/github/stars/ganler/code-r1" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/SkyworkAI/Skywork-OR1">Skywork-OR1</a> : série open reasoner Skywork <img src="https://img.shields.io/github/stars/SkyworkAI/Skywork-OR1" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/GAIR-NLP/ToRL">ToRL</a> : scaling RL intégré aux outils <img src="https://img.shields.io/github/stars/GAIR-NLP/ToRL" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/langfengQ/verl-agent">verl-agent</a> : framework d’entraînement évolutif pour <strong>agents LLM/VLM longue-horizon</strong>, avec un nouvel algorithme <strong>GiGPO</strong> <img src="https://img.shields.io/github/stars/langfengQ/verl-agent" alt="GitHub Repo stars" /></li>
<li><a href="https://arxiv.org/abs/2409.06957">PF-PPO</a> : Policy Filtration pour PPO basée sur la fiabilité des signaux de récompense pour un RLHF plus efficace et robuste.</li>
<li><a href="https://github.com/ritzz-ai/GUI-R1">GUI-R1</a> : <strong>GUI-R1</strong> : un modèle action Vision-Language R1 généraliste pour <strong>agents GUI</strong> <img src="https://img.shields.io/github/stars/ritzz-ai/GUI-R1" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/GAIR-NLP/DeepResearcher">DeepResearcher</a> : scaling deep research via RL dans des environnements réels <img src="https://img.shields.io/github/stars/GAIR-NLP/DeepResearcher" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/RAGEN-AI/VAGEN">VAGEN</a> : entraînement d’agents VLM avec RL multi-turn <img src="https://img.shields.io/github/stars/RAGEN-AI/VAGEN" alt="GitHub Repo stars" /></li>
<li><a href="https://retool-rl.github.io/">ReTool</a> : ReTool : RL pour l’utilisation stratégique d’outils dans les LLMs. Publication du code en cours...</li>
<li><a href="https://arxiv.org/abs/2505.02387">RM-R1</a> : entraînement RL de modèles de récompense de raisonnement <img src="https://img.shields.io/github/stars/RM-R1-UIUC/RM-R1" alt="GitHub Repo stars" /></li>
<li><a href="https://arxiv.org/abs/2505.03335">Absolute Zero Reasoner</a> : framework self-play sans données humaines pour le raisonnement <img src="https://img.shields.io/github/stars/LeapLabTHU/Absolute-Zero-Reasoner" alt="GitHub Repo stars" /></li>
<li><a href="https://arxiv.org/pdf/2504.14945">LUFFY</a> : apprendre à raisonner sous guidance off-policy <img src="https://img.shields.io/github/stars/ElliottYan/LUFFY" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/TIGER-AI-Lab/verl-tool">verl-tool</a> : un framework unifié et facile à étendre pour l’entraînement d’agents-outils basé sur verl <img src="https://img.shields.io/github/stars/TIGER-AI-Lab/verl-tool" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/zwhe99/DeepMath">DeepMath</a> : données DeepMath-103K et modèles de la série pour le raisonnement mathématique <img src="https://img.shields.io/github/stars/zwhe99/DeepMath" alt="GitHub Repo stars" /></li>
</ul>
<p>et bien d’autres projets remarquables listés dans <a href="https://raw.githubusercontent.com/volcengine/verl/main/recipe/README.md">recipe</a>.</p>
<h2>Guide de contribution</h2>
<p>Les contributions de la communauté sont les bienvenues ! Consultez notre <a href="https://github.com/volcengine/verl/issues/710">roadmap du projet</a> et les <a href="https://github.com/volcengine/verl/issues?q=is%3Aissue%20state%3Aopen%20label%3A%22good%20first%20issue%22">good first issues</a> pour savoir où contribuer.</p>
<h3>Linting et formatage du code</h3>
<p>Nous utilisons pre-commit pour améliorer la qualité du code. Pour initialiser pre-commit, exécutez :</p>
<pre><code class="language-bash">pip install pre-commit
pre-commit install
</code></pre>
<p>Pour résoudre les erreurs CI localement, vous pouvez lancer pre-commit manuellement :</p>
<pre><code class="language-bash">pre-commit run
</code></pre>
<h3>Ajout de tests CI</h3>
<p>Si possible, merci d’ajouter un ou plusieurs tests CI pour votre nouvelle fonctionnalité :</p>
<ol>
<li>Trouvez le fichier workflow yml le plus pertinent, généralement associé à une config hydra par défaut (ex : <code>ppo_trainer</code>, <code>ppo_megatron_trainer</code>, <code>sft_trainer</code>, etc).</li>
<li>Ajoutez les patterns de chemin liés dans la section <code>paths</code> si ce n’est pas déjà fait.</li>
<li>Minimisez la charge de travail des scripts de test (voir les scripts existants pour des exemples).</li>
</ol>
<h2>À propos de l’<a href="https://team.doubao.com/">équipe ByteDance Seed</a></h2>
<p>Fondée en 2023, l’équipe ByteDance Seed s’engage à concevoir les modèles d’IA fondamentaux les plus avancés de l’industrie. L’équipe aspire à devenir une équipe de recherche de classe mondiale et à contribuer de manière significative à l’avancement de la science et de la société. Vous pouvez découvrir l’équipe ByteDance Seed via les canaux suivants👇</p>
<div>
  <a href="https://team.doubao.com/">
    <img src="https://img.shields.io/badge/Website-%231e37ff?style=for-the-badge&logo=bytedance&logoColor=white"></a>
  <a href="https://github.com/user-attachments/assets/469535a8-42f2-4797-acdf-4f7a1d4a0c3e">
    <img src="https://img.shields.io/badge/WeChat-07C160?style=for-the-badge&logo=wechat&logoColor=white"></a>
 <a href="https://www.xiaohongshu.com/user/profile/668e7e15000000000303157d?xsec_token=ABl2-aqekpytY6A8TuxjrwnZskU-6BsMRE_ufQQaSAvjc%3D&xsec_source=pc_search">
    <img src="https://img.shields.io/badge/Xiaohongshu-%23FF2442?style=for-the-badge&logo=xiaohongshu&logoColor=white"></a>
  <a href="https://www.zhihu.com/org/dou-bao-da-mo-xing-tuan-dui/">
    <img src="https://img.shields.io/badge/zhihu-%230084FF?style=for-the-badge&logo=zhihu&logoColor=white"></a>
</div>
---
<p>Nous RECRUTONS ! Envoyez-nous un <a href="mailto:haibin.lin@bytedance.com">email</a> si vous êtes intéressé(e) par un stage ou un poste FTE en RL pour agents.</p>
<hr />
<p>Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx">Open Ai Tx</a> | Last indexed: 2025-06-07</p>
<hr />

        </div>
    </div>
    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Default Statcounter code for openaitx
https://openaitx.github.io/ -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>
    <!-- End of Statcounter Code -->
</body>
</html>