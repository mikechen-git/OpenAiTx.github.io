<!DOCTYPE html>
<html lang="th">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>verl - volcengine/verl th</title>
    <meta name="title" content="verl - volcengine/verl th | 👋 สวัสดีทุกคน! verl คือไลบรารีฝึก RL ที่ริเริ่มโดย ทีม ByteDance Seed และดูแลโดยชุมชน verl verl: Volcano Engine Reinforcement Learning สำหรับ LLMs verl คือไลบร...">
    <meta name="description" content="volcengine/verl - GitHub repository th documentation and information | 👋 สวัสดีทุกคน! verl คือไลบรารีฝึก RL ที่ริเริ่มโดย ทีม ByteDance Seed และดูแลโดยชุมชน verl verl: Volcano Engine Reinforcement Learning สำหรับ LLMs verl คือไลบร...">
    <meta name="keywords" content="volcengine, verl, GitHub, repository, th documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/volcengine/verl/README-th.html">
    <meta property="og:title" content="verl - volcengine/verl th | 👋 สวัสดีทุกคน! verl คือไลบรารีฝึก RL ที่ริเริ่มโดย ทีม ByteDance Seed และดูแลโดยชุมชน verl verl: Volcano Engine Reinforcement Learning สำหรับ LLMs verl คือไลบร...">
    <meta property="og:description" content="volcengine/verl - GitHub repository th documentation and information | 👋 สวัสดีทุกคน! verl คือไลบรารีฝึก RL ที่ริเริ่มโดย ทีม ByteDance Seed และดูแลโดยชุมชน verl verl: Volcano Engine Reinforcement Learning สำหรับ LLMs verl คือไลบร...">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">
    <script src="/js/marked.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        body { display: flex; flex-direction: column; min-height: 100vh; }
        .main-container { margin: 0 auto; width: 100%; max-width: 980px; padding: 0 20px; }
        @media (max-width: 768px) { .main-container { padding: 0 15px; } }
        .markdown-body img { max-width: 100%; height: auto; }
        .header { text-align: center; margin-bottom: 30px; padding: 20px; background-color: #f6f8fa; border-bottom: 1px solid #e1e4e8; position: relative; }
        .header .links { margin-top: 10px; font-size: 16px; }
        .header .links a { color: #0366d6; text-decoration: none; margin-left: 5px; }
        .header .links a:hover { text-decoration: underline; }
        .language-badges { margin-top: 15px; text-align: center; }
        .language-badges a { display: inline-block; margin: 2px; text-decoration: none; }
        .language-badges img { height: 20px; border-radius: 3px; }
        .language-badges a:hover img { opacity: 0.8; }
    </style>
</head>
<body>
    <div style="position: fixed; top: 2px; left: 2px; z-index: 2000; background: rgba(255,255,255,0.95); border-radius: 6px; box-shadow: 0 2px 8px rgba(0,0,0,0.06); padding: 4px 14px; font-size: 15px; color: #222; font-family: 'Segoe UI', Arial, sans-serif; font-weight: 500; letter-spacing: 0.5px;">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" style="color: #0366d6; text-decoration: none; font-weight: 600;">Open AI Tx</a>
    </div>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/volcengine/verl" id="githubRepoLink" target="_blank">volcengine/verl</a>
<h1 style="display: none;">👋 สวัสดีทุกคน! verl คือไลบรารีฝึก RL ที่ริเริ่มโดย ทีม ByteDance Seed และดูแลโดยชุมชน verl verl: Volcano Engine Reinforcement Learning สำหรับ LLMs verl คือไลบร...</h1>
        </div>
        <div class="language-badges" id="languageBadges">
            <!-- You can generate language badges here if needed -->
        </div>
    </div>
    <div class="main-container">
        <div class="markdown-body" id="content">
            <div align="center">
 👋 สวัสดีทุกคน! 
    verl คือไลบรารีฝึก RL ที่ริเริ่มโดย <b>ทีม ByteDance Seed</b> และดูแลโดยชุมชน verl
    <br>
    <br>
</div>
<div align="center">
<p><a href="https://deepwiki.com/volcengine/verl"><img src="https://devin.ai/assets/deepwiki-badge.png" alt="Ask DeepWiki.com" height="20"/></a>
<a href="https://github.com/volcengine/verl/stargazers"><img src="https://img.shields.io/github/stars/volcengine/verl" alt="GitHub Repo stars" /></a>
<a href="https://twitter.com/verl_project"><img src="https://img.shields.io/twitter/follow/verl_project" alt="Twitter" /></a>
<a href="https://join.slack.com/t/verlgroup/shared_invite/zt-2w5p9o4c3-yy0x2Q56s_VlGLsJ93A6vA"><img src="https://img.shields.io/badge/Slack-verl-blueviolet?logo=slack&amp"></a>
<a href="https://arxiv.org/pdf/2409.19256"><img src="https://img.shields.io/static/v1?label=EuroSys&message=Paper&color=red"></a>
<a href="https://verl.readthedocs.io/en/latest/"><img src="https://img.shields.io/badge/documentation-blue" alt="Documentation" /></a>
<a href="https://raw.githubusercontent.com/eric-haibin-lin/verl-community/refs/heads/main/WeChat.JPG"><img src="https://img.shields.io/badge/微信-green?logo=wechat&amp"></a></p>
</div>
<p><img src="https://github.com/user-attachments/assets/c42e675e-497c-4508-8bb9-093ad4d1f216" alt="seed logo" /></p>
<h1 style="text-align: center;">verl: Volcano Engine Reinforcement Learning สำหรับ LLMs</h1>
<p>verl คือไลบรารีฝึก RL ที่ยืดหยุ่น มีประสิทธิภาพ และพร้อมใช้งานสำหรับโมเดลภาษาใหญ่ (LLMs)</p>
<p>verl คือเวอร์ชันโอเพ่นซอร์สของผลงานวิจัย <strong><a href="https://arxiv.org/abs/2409.19256v2">HybridFlow: A Flexible and Efficient RLHF Framework</a></strong></p>
<p>verl ยืดหยุ่นและใช้งานง่ายด้วย:</p>
<ul>
<li><p><strong>ขยายอัลกอริทึม RL ได้ง่าย</strong>: โมเดล hybrid-controller ช่วยให้สามารถแสดงและประมวลผล dataflow หลังการฝึกที่ซับซ้อนได้อย่างยืดหยุ่นและมีประสิทธิภาพ สร้าง RL dataflow เช่น GRPO, PPO ได้ในไม่กี่บรรทัดโค้ด</p>
</li>
<li><p><strong>รวมโครงสร้าง LLM ที่มีอยู่เดิมเข้าด้วย API แบบโมดูลาร์</strong>: แยก computation และ data dependency ออกจากกัน ทำให้รวมกับเฟรมเวิร์ก LLM ที่มีอยู่เช่น FSDP, Megatron-LM, vLLM, SGLang ฯลฯ ได้อย่างไร้รอยต่อ</p>
</li>
<li><p><strong>Flexible device mapping</strong>: รองรับการจัดวางโมเดลบน GPU หลายชุด เพื่อการใช้ทรัพยากรที่มีประสิทธิภาพและรองรับการขยายกับคลัสเตอร์ขนาดต่างๆ</p>
</li>
<li><p>พร้อมใช้กับโมเดลยอดนิยมของ HuggingFace</p>
</li>
</ul>
<p>verl เร็วและมีประสิทธิภาพด้วย:</p>
<ul>
<li><p><strong>Throughput ระดับ SOTA</strong>: ผสานรวม engine ฝึกและ inference ของ LLM ระดับ SOTA และ throughput ของ RL ระดับ SOTA</p>
</li>
<li><p><strong>Resharding actor model อย่างมีประสิทธิภาพด้วย 3D-HybridEngine</strong>: ลดการใช้หน่วยความจำซ้ำซ้อนและลด overhead ในการสื่อสารระหว่างเฟสฝึกและเฟสสร้าง</p>
</li>
</ul>
</p>
<h2>ข่าวสาร</h2>
<ul>
<li>[2025/06] verl พร้อม backend Megatron รองรับโมเดล MoE ใหญ่เช่น <a href="https://verl.readthedocs.io/en/latest/perf/dpsk.html">DeepSeek-671b และ Qwen3-236b</a></li>
<li>[2025/06] ทีม verl จะอัปเดตโปรเจกต์ล่าสุดที่ <a href="https://www.lfasiallc.com/pytorch-day-china/">PyTorch Day China</a> วันที่ 7 มิ.ย. พบทีม dev ของเราได้ที่ปักกิ่ง!</li>
<li>[2025/05] <a href="https://arxiv.org/abs/2409.06957">PF-PPO</a> ที่ได้รับการยอมรับในงาน ICML 2025 ได้รับการรองรับใน verl แล้ว! PF-PPO เพิ่มประสิทธิภาพและความเสถียรในการเรียนรู้ policy ด้วยการกรอง reward ที่อาจมี noise และนำประสบการณ์คุณภาพสูงกลับมาใช้ใหม่ผ่าน replay buffer</li>
<li>[2025/04] เราจะให้ tutorial เรื่องเทคนิค post-training ล่าสุดและแนวทางโปรแกรมสำหรับ verl ที่ <a href="https://iclr.cc/virtual/2025/calendar?filter_events=Expo+Talk+Panel&amp;filter_rooms=">ICLR 2025 Expo</a>, <a href="https://open-foundation-model.github.io/">SCI-FM workshop</a> และ <a href="https://lu.ma/d23nyynm">LMSys afterparty</a> เอกสารประกอบการบรรยาย <a href="https://github.com/eric-haibin-lin/verl-community/tree/main/iclr25">ที่นี่</a></li>
<li>[2025/04] รายงานเทคนิค <a href="https://github.com/ByteDance-Seed/Seed-Thinking-v1.5/blob/main/seed-thinking-v1.5.pdf">Seed-Thinking-v1.5</a> เปิดตัวแล้ว! ฝึกด้วย verl, Seed-Thinking-v1.5 ได้คะแนน 86.7 บน AIME 2024, 55.0 บน Codeforces และ 77.3 บน GPQA แสดงศักยภาพ reasoning ที่ยอดเยี่ยมในสาย STEM และโค้ดดิ้ง นอกจากนี้ยังแสดงการ generalize ได้ดีในงานหลากหลาย</li>
<li>[2025/04] <a href="https://arxiv.org/pdf/2504.05118">VAPO</a> (value-based augmented PPO) นำเสนอวิธี RL ล่าสุดสำหรับ reasoning models ฝึกจาก Qwen-32B-base VAPO ได้ 60.4 บน AIME 2024 ดีกว่า DAPO-32B</li>
<li>[2025/03] verl v0.3.0.post1 เปิดตัว! ดู <a href="https://github.com/volcengine/verl/releases/">release note</a> รายละเอียด เทียบกับเวอร์ชันก่อน เร็วขึ้น <a href="https://tongyx361.github.io/blogs/posts/verl-intro/#/verl-flexible-and-efficient-rl-for-llms">~1.4x</a></li>
<li>[2025/03] <a href="https://dapo-sia.github.io/">DAPO</a> คืออัลกอริทึม RL SOTA แบบโอเพ่นซอร์สที่ได้ 50 คะแนนบน AIME 2024 ด้วย Qwen2.5-32B pre-trained model ดีกว่า SOTA ก่อนหน้าของ DeepSeek’s GRPO (DeepSeek-R1-Zero-Qwen-32B) ฝึกโดยใช้ verl ทั้งหมด โค้ด reproduction มีใน <code>recipe/dapo</code> แล้ว</li>
</ul>
<details><summary> เพิ่มเติม... </summary>
<ul>
  <li>[2025/05] verl จะถูกนำเสนอที่ [A2M Shanghai](https://a2m.msup.com.cn/home/?aid=4488&city=shanghai) วันที่ 16-17 พ.ค.</li>
  <li>[2025/05] verl จะถูกนำเสนอที่ [GOSIM x PyTorch Day 2025](https://paris2025.gosim.org/) พบกันที่ปารีส!</li>
  <li>[2025/03] เราแนะนำ programming model ของ verl ที่ [vLLM Beijing Meetup](https://mp.weixin.qq.com/s/n77GibL2corAtQHtVEAzfg) และ [verl intro and updates](https://github.com/eric-haibin-lin/verl-community/blob/main/slides/verl-lmsys-meetup.pdf) ที่ [SGLang-LMSYS Org Meetup](https://lu.ma/ntjrr7ig) ที่ Sunnyvale กลางเดือนมีนาคม</li>
  <li>[2025/03] เราจะนำเสนอ verl(HybridFlow) ที่ EuroSys 2025 เจอกันที่ Rotterdam!</li>
  <li>[2025/02] verl v0.2.0.post2 เปิดตัวแล้ว!</li>
  <li>[2025/02] เรานำเสนอ verl ใน <a href="https://lu.ma/ji7atxux">Bytedance/NVIDIA/Anyscale Ray Meetup</a> เจอกันที่ San Jose!</li>
  <li>[2025/01] [Doubao-1.5-pro](https://team.doubao.com/zh/special/doubao_1_5_pro) เปิดตัวพร้อมประสิทธิภาพระดับ SOTA บน LLM & VLM โมเดล RL scaling preview ฝึกด้วย verl ได้ระดับ OpenAI O1 บนเบนช์มาร์คคณิตศาสตร์ (70.0 pass@1 บน AIME)</li>
  <li>[2024/12] verl นำเสนอที่ Ray Forward 2024. สไลด์ <a href="https://github.com/eric-haibin-lin/verl-community/blob/main/slides/Ray_Forward_2024_%E5%B7%AB%E9%94%A1%E6%96%8C.pdf">ที่นี่</a></li>
  <li>[2024/12] ทีมงานนำเสนอ <a href="https://neurips.cc/Expo/Conferences/2024/workshop/100677">Post-training LLMs: From Algorithms to Infrastructure</a> ที่ NeurIPS 2024 <a href="https://github.com/eric-haibin-lin/verl-data/tree/neurips">สไลด์</a> และ <a href="https://neurips.cc/Expo/Conferences/2024/workshop/100677">วิดีโอ</a> มีให้ดู</li>
  <li>[2024/10] verl นำเสนอที่ Ray Summit <a href="https://www.youtube.com/watch?v=MrhMcXkXvJU&list=PLzTswPQNepXntmT8jr9WaNfqQ60QwW7-U&index=37">วิดีโอ Youtube</a></li>
  <li>[2024/08] HybridFlow (verl) ได้รับการตอบรับใน EuroSys 2025</li>
</ul>   
</details>
<h2>คุณสมบัติเด่น</h2>
<ul>
<li>รองรับ <strong>FSDP</strong>, <strong>FSDP2</strong> และ <strong>Megatron-LM</strong> สำหรับการฝึก</li>
<li>รองรับ <strong>vLLM</strong>, <strong>SGLang</strong> และ <strong>HF Transformers</strong> สำหรับ rollout generation</li>
<li>เข้ากันได้กับ Hugging Face Transformers และ Modelscope Hub: <a href="https://github.com/volcengine/verl/blob/main/examples/grpo_trainer/run_qwen3-8b.sh">Qwen-3</a>, Qwen-2.5, Llama3.1, Gemma2, DeepSeek-LLM เป็นต้น</li>
<li>Supervised fine-tuning</li>
<li>Reinforcement learning ด้วย <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/ppo_trainer/">PPO</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/grpo_trainer/">GRPO</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/remax_trainer/">ReMax</a>, <a href="https://verl.readthedocs.io/en/latest/examples/config.html#algorithm">REINFORCE++</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/rloo_trainer/">RLOO</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/recipe/prime/">PRIME</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/recipe/dapo/">DAPO</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/recipe/drgrpo">DrGRPO</a> ฯลฯ
<ul>
<li>รองรับ reward จาก model และ reward จากฟังก์ชัน (verifiable reward) สำหรับคณิตศาสตร์, <a href="https://github.com/volcengine/verl/tree/main/recipe/dapo">coding</a> ฯลฯ</li>
<li>รองรับ vision-language models (VLMs) และ <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/grpo_trainer/run_qwen2_5_vl-7b.sh">multi-modal RL</a> ด้วย Qwen2.5-vl, Kimi-VL</li>
<li><a href="https://github.com/volcengine/verl/tree/main/examples/sglang_multiturn">Multi-turn พร้อม tool calling</a></li>
</ul>
</li>
<li>สูตร alignment LLM เช่น <a href="https://github.com/volcengine/verl/tree/main/recipe/sppo">Self-play preference optimization (SPPO)</a></li>
<li>Flash attention 2, <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/ppo_trainer/run_qwen2-7b_seq_balance.sh">sequence packing</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/ppo_trainer/run_deepseek7b_llm_sp2.sh">sequence parallelism</a> ผ่าน DeepSpeed Ulysses, <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/sft/gsm8k/run_qwen_05_peft.sh">LoRA</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/sft/gsm8k/run_qwen_05_sp2_liger.sh">Liger-kernel</a></li>
<li>ขยายได้ถึง 671B models และ GPU หลายร้อยตัวด้วย <a href="https://github.com/volcengine/verl/pull/1467">expert parallelism</a></li>
<li>Multi-gpu <a href="https://verl.readthedocs.io/en/latest/advance/ppo_lora.html">LoRA RL</a> เพื่อประหยัดหน่วยความจำ</li>
<li>ติดตามผลการทดลองด้วย wandb, swanlab, mlflow และ tensorboard</li>
</ul>
<h2>ฟีเจอร์และการเปลี่ยนแปลงในอนาคต</h2>
<ul>
<li>Roadmap https://github.com/volcengine/verl/issues/710</li>
<li>DeepSeek 671b ปรับแต่งด้วย Megatron v0.11 https://github.com/volcengine/verl/issues/708</li>
<li>Multi-turn rollout และ tools ด้วย optimization https://github.com/volcengine/verl/issues/1882</li>
<li>การโต้ตอบกับ environment https://github.com/volcengine/verl/issues/1172</li>
<li>รายการ breaking changes ตั้งแต่ v0.3 https://github.com/volcengine/verl/discussions/943, entropy_coeff ค่า default เป็น 0</li>
<li>Lora สำหรับ RL https://github.com/volcengine/verl/pull/1127</li>
</ul>
<h2>เริ่มต้นใช้งาน</h2>
<p><a href="https://verl.readthedocs.io/en/latest/index.html"><b>เอกสาร</b></a></p>
<p><strong>เริ่มต้นอย่างรวดเร็ว:</strong></p>
<ul>
<li><a href="https://verl.readthedocs.io/en/latest/start/install.html">ติดตั้ง</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/start/quickstart.html">Quickstart</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/hybrid_flow.html">Programming Guide</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/algo/ppo.html">PPO ใน verl</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/algo/grpo.html">GRPO ใน verl</a></li>
</ul>
<p><strong>สอนใช้งาน PPO อย่างละเอียด:</strong></p>
<ul>
<li><a href="https://verl.readthedocs.io/en/latest/preparation/prepare_data.html">เตรียมข้อมูลสำหรับ Post-Training</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/preparation/reward_function.html">สร้าง Reward Function สำหรับ Dataset</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/examples/ppo_code_architecture.html">สถาปัตยกรรมตัวอย่าง PPO</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/examples/config.html">คำอธิบาย config</a></li>
</ul>
<p><strong>อัลกอริทึม baseline ที่ reproducible:</strong></p>
<ul>
<li><a href="https://verl.readthedocs.io/en/latest/algo/baseline.html">ประสิทธิภาพ RL ใน coding, math</a></li>
</ul>
<p><strong>สำหรับอธิบายโค้ดและการใช้งานขั้นสูง (extension):</strong></p>
<ul>
<li><p>PPO Trainer และ Workers</p>
<ul>
<li><a href="https://verl.readthedocs.io/en/latest/workers/ray_trainer.html">PPO Ray Trainer</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/workers/fsdp_workers.html">PyTorch FSDP Backend</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/index.html">Megatron-LM Backend</a></li>
</ul>
</li>
<li><p>การใช้งานขั้นสูงและ extension</p>
<ul>
<li><a href="https://verl.readthedocs.io/en/latest/advance/fsdp_extension.html">เพิ่มโมเดลด้วย FSDP Backend</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/advance/megatron_extension.html">เพิ่มโมเดลด้วย Megatron-LM Backend</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/sglang_multiturn/multiturn.html">รองรับ Multi-turn Rollout</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/sglang_multiturn/search_tool_example.html">การรวม Search Tool</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/examples/sandbox_fusion_example.html">การรวม Sandbox Fusion</a></li>
<li><a href="https://github.com/volcengine/verl/tree/main/examples/split_placement">Deployment แยกใช้ GPU</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/advance/dpo_extension.html">ขยายสู่อัลกอริทึม RL(HF) อื่นๆ</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/advance/placement.html">Ray API design tutorial</a></li>
</ul>
</li>
</ul>
<p><strong>บล็อกจากชุมชน</strong></p>
<ul>
<li><a href="https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/verl/multi-turn/verl-multiturn-rollout-Release.md">SGLang, verl, OpenBMB และ Tsinghua: พัฒนา End-to-End Multi-Turn RLHF</a></li>
<li><a href="https://rocm.blogs.amd.com/artificial-intelligence/verl-large-scale/README.html">Reinforcement Learning from Human Feedback บน AMD GPU ด้วย verl และ ROCm Integration</a></li>
<li><a href="https://mp.weixin.qq.com/s/7nbqxk4knMGd-hQE9ls2tA">veMLP x verl ：เล่นกับการฝึก RL</a></li>
<li><a href="https://www.volcengine.com/docs/6459/1463942">ฝึก GRPO แบบ distributed RL ด้วย verl อย่างไรให้ดีที่สุด</a></li>
<li><a href="https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/verl/readme.md">วิเคราะห์ต้นฉบับ HybridFlow verl</a></li>
<li><a href="https://team.doubao.com/en/blog/%E6%9C%80%E9%AB%98%E6%8F%90%E5%8D%8720%E5%80%8D%E5%90%9E%E5%90%90%E9%87%8F-%E8%B1%86%E5%8C%85%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9B%A2%E9%98%9F%E5%8F%91%E5%B8%83%E5%85%A8%E6%96%B0-rlhf-%E6%A1%86%E6%9E%B6-%E7%8E%B0%E5%B7%B2%E5%BC%80%E6%BA%90">เพิ่ม throughput สูงสุด 20 เท่า! ทีม Doubao เปิดตัว RLHF Framework ใหม่ล่าสุด เปิดซอร์สแล้ว!</a></li>
</ul>
<h2>Performance Tuning Guide</h2>
<p>ประสิทธิภาพสำคัญสำหรับอัลกอริทึม RL แบบ on-policy เราเขียน <a href="https://verl.readthedocs.io/en/latest/perf/perf_tuning.html">คู่มือปรับจูนประสิทธิภาพ</a> อย่างละเอียดเพื่อช่วยให้คุณ optimize ได้</p>
<h2>อัปเกรดเป็น vLLM &gt;= v0.8.2</h2>
<p>verl รองรับ vLLM&gt;=0.8.2 เมื่อใช้ FSDP เป็น backend ฝึก กรุณาดู <a href="https://github.com/volcengine/verl/blob/main/docs/README_vllm0.8.md">เอกสารนี้</a> สำหรับวิธีติดตั้งและข้อมูลเพิ่มเติม ควรหลีกเลี่ยง vllm 0.7.x ที่มี bug อาจทำให้ OOM และ error ที่ไม่คาดคิด</p>
<h2>ใช้ SGLang เวอร์ชันล่าสุด</h2>
<p>SGLang รองรับเต็มรูปแบบกับ verl และกลุ่ม SGLang RL กำลังพัฒนาฟีเจอร์ใหม่ๆ เช่น multi-turn agentic RL, VLM RLHF, server-based RL, partial rollout กรุณาดู <a href="https://verl.readthedocs.io/en/latest/workers/sglang_worker.html">เอกสารนี้</a> สำหรับวิธีติดตั้งและข้อมูลเพิ่มเติม</p>
<h2>อัปเกรดเป็น FSDP2</h2>
<p>verl รองรับ FSDP2 เต็มที่! FSDP2 แนะนำโดยทีม torch distributed ให้ throughput ดีกว่า ใช้ memory น้อยกว่า และรวมกับฟีเจอร์อื่นๆ ได้ (เช่น torch.compile) แค่ใช้ verl main และตั้ง options ต่อไปนี้:</p>
<pre><code>actor_rollout_ref.ref.strategy=fsdp2
actor_rollout_ref.actor.strategy=fsdp2
critic.strategy=fsdp2 
reward_model.strategy=fsdp2 
</code></pre>
<p>นอกจากนี้ FSDP2 cpu offload เข้ากันได้กับ gradient accumulation เปิดใช้งานประหยัดหน่วยความจำได้ด้วย <code>actor_rollout_ref.actor.offload_policy=True</code> ดูรายละเอียดเพิ่มเติมที่ https://github.com/volcengine/verl/pull/1026</p>
<h2>รองรับ AMD (ROCm Kernel)</h2>
<p>verl รองรับ FSDP เป็น engine ฝึก (Megatron จะตามมาเร็วๆ นี้) และรวมกับ vLLM และ SGLang เป็น inference engine ดู <a href="https://github.com/volcengine/verl/blob/main/docs/amd_tutorial/amd_build_dockerfile_page.rst">เอกสารนี้</a> สำหรับวิธีติดตั้งและข้อมูลเพิ่มเติม และ <a href="https://github.com/volcengine/verl/blob/main/docs/amd_tutorial/amd_vllm_page.rst">เอกสารนี้</a> สำหรับปรับจูน vLLM สำหรับ ROCm</p>
<h2>การอ้างอิงและขอบคุณ</h2>
<p>หากคุณพบว่าโปรเจกต์นี้มีประโยชน์ โปรดอ้างอิง:</p>
<ul>
<li><a href="https://arxiv.org/abs/2409.19256v2">HybridFlow: A Flexible and Efficient RLHF Framework</a></li>
<li><a href="https://i.cs.hku.hk/%7Ecwu/papers/gmsheng-NL2Code24.pdf">A Framework for Training Large Language Models for Code Generation via Proximal Policy Optimization</a></li>
</ul>
<pre><code class="language-bibtex">@article{sheng2024hybridflow,
  title   = {HybridFlow: A Flexible and Efficient RLHF Framework},
  author  = {Guangming Sheng and Chi Zhang and Zilingfeng Ye and Xibin Wu and Wang Zhang and Ru Zhang and Yanghua Peng and Haibin Lin and Chuan Wu},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2409.19256}
}
</code></pre>
<p>verl ได้แรงบันดาลใจจาก Nemo-Aligner, Deepspeed-chat และ OpenRLHF โปรเจกต์นี้ได้รับการรับและมีส่วนร่วมจาก Bytedance, Anyscale, LMSys.org, <a href="https://github.com/QwenLM/">Alibaba Qwen team</a>, Shanghai AI Lab, Tsinghua University, UC Berkeley, UCLA, UIUC, University of Hong Kong, ke.com, <a href="https://www.all-hands.dev/">All Hands AI</a>, <a href="http://modelbest.cn/">ModelBest</a>, OpenPipe, JD AI Lab, Microsoft Research, <a href="https://www.stepfun.com/">StepFun</a>, Amazon, Linkedin, Meituan, <a href="https://www.camel-ai.org/">Camel-AI</a>, <a href="https://github.com/OpenManus">OpenManus</a>, Xiaomi, Prime Intellect, NVIDIA research, <a href="https://www.baichuan-ai.com/home">Baichuan</a>, <a href="https://www.xiaohongshu.com/">RedNote</a>, <a href="https://www.swiss-ai.org/">SwissAI</a>, <a href="https://www.moonshot-ai.com/">Moonshot AI (Kimi)</a>, Baidu, Snowflake และอื่นๆ อีกมากมาย</p>
<h2>ผลงานเด่นที่ใช้ verl</h2>
<ul>
<li><a href="https://github.com/Jiayi-Pan/TinyZero">TinyZero</a>: รีโปรดิวซ์ <strong>DeepSeek R1 Zero</strong> สำหรับ reasoning tasks <img src="https://img.shields.io/github/stars/Jiayi-Pan/TinyZero" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/NovaSky-AI/SkyThought">SkyThought</a>: RL training สำหรับ Sky-T1-7B โดยทีม NovaSky AI <img src="https://img.shields.io/github/stars/NovaSky-AI/SkyThought" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/hkust-nlp/simpleRL-reason">simpleRL-reason</a>: SimpleRL-Zoo: สำรวจและควบคุม Zero RL สำหรับ Open Base Models <img src="https://img.shields.io/github/stars/hkust-nlp/simpleRL-reason" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/hiyouga/EasyR1">Easy-R1</a>: เฟรมเวิร์ค RL <strong>มัลติโหมด</strong> <img src="https://img.shields.io/github/stars/hiyouga/EasyR1" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/OpenManus/OpenManus-RL">OpenManus-RL</a>: เฟรมเวิร์ค RL สำหรับ LLM Agents หลาย environment <img src="https://img.shields.io/github/stars/OpenManus/OpenManus-RL" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/agentica-project/rllm">rllm</a>: RL แบบ async ด้วย <a href="https://github.com/agentica-project/verl-pipeline">verl-pipeline</a> <img src="https://img.shields.io/github/stars/agentica-project/rllm" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/PRIME-RL/PRIME">PRIME</a>: Process reinforcement ผ่าน implicit rewards <img src="https://img.shields.io/github/stars/PRIME-RL/PRIME" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/ZihanWang314/ragen">RAGEN</a>: เฟรมเวิร์คฝึก <strong>agent</strong> reasoning ทั่วไป <img src="https://img.shields.io/github/stars/ZihanWang314/ragen" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/PeterGriffinJin/Search-R1">Search-R1</a>: RL ที่ผสม reasoning กับ <strong>searching (tool-call)</strong> <img src="https://img.shields.io/github/stars/PeterGriffinJin/Search-R1" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/pat-jj/DeepRetrieval">DeepRetrieval</a>: RL Training <strong>Search Agent</strong> ด้วย <strong>Search/Retrieval Outcome</strong> <img src="https://img.shields.io/github/stars/pat-jj/DeepRetrieval" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/Agent-RL/ReSearch">ReSearch</a>: สอน <strong>Reason</strong> ด้วย <strong>Search</strong> บน LLMs ผ่าน RL <img src="https://img.shields.io/github/stars/Agent-RL/ReSearch" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/ganler/code-r1">Code-R1</a>: รีโปรดิวซ์ R1 สำหรับ <strong>โค้ด</strong> ด้วยรางวัลที่เชื่อถือได้ <img src="https://img.shields.io/github/stars/ganler/code-r1" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/SkyworkAI/Skywork-OR1">Skywork-OR1</a>: Skywork open reaonser series <img src="https://img.shields.io/github/stars/SkyworkAI/Skywork-OR1" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/GAIR-NLP/ToRL">ToRL</a>: RL แบบ tool-integrated ที่ขยายได้ <img src="https://img.shields.io/github/stars/GAIR-NLP/ToRL" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/langfengQ/verl-agent">verl-agent</a>: เฟรมเวิร์คฝึก <strong>agent LLM/VLM ระยะยาว</strong> ที่ขยายขนาดได้ พร้อมอัลกอริทึมใหม่ <strong>GiGPO</strong> <img src="https://img.shields.io/github/stars/langfengQ/verl-agent" alt="GitHub Repo stars" /></li>
<li><a href="https://arxiv.org/abs/2409.06957">PF-PPO</a>: Policy Filtration for PPO สำหรับความน่าเชื่อถือของ reward เพิ่มประสิทธิภาพและความเสถียรของ RLHF</li>
<li><a href="https://github.com/ritzz-ai/GUI-R1">GUI-R1</a>: <strong>GUI-R1</strong>: Vision-Language Action Model สำหรับ <strong>GUI Agents</strong> <img src="https://img.shields.io/github/stars/ritzz-ai/GUI-R1" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/GAIR-NLP/DeepResearcher">DeepResearcher</a>: RL ขยายขีดความสามารถด้าน research ใน environment จริง <img src="https://img.shields.io/github/stars/GAIR-NLP/DeepResearcher" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/RAGEN-AI/VAGEN">VAGEN</a>: ฝึก VLM agents ด้วย multi-turn RL <img src="https://img.shields.io/github/stars/RAGEN-AI/VAGEN" alt="GitHub Repo stars" /></li>
<li><a href="https://retool-rl.github.io/">ReTool</a>: ReTool: RL สำหรับ LLMs ที่ใช้เครื่องมือเชิงกลยุทธ์ รอโค้ดเร็วๆ นี้...</li>
<li><a href="https://arxiv.org/abs/2505.02387">RM-R1</a>: RL training ของ reasoning reward models <img src="https://img.shields.io/github/stars/RM-R1-UIUC/RM-R1" alt="GitHub Repo stars" /></li>
<li><a href="https://arxiv.org/abs/2505.03335">Absolute Zero Reasoner</a>: เฟรมเวิร์ค self-play reasoning ที่ไม่ต้องใช้ human curated data <img src="https://img.shields.io/github/stars/LeapLabTHU/Absolute-Zero-Reasoner" alt="GitHub Repo stars" /></li>
<li><a href="https://arxiv.org/pdf/2504.14945">LUFFY</a>: Learning to Reason under Off-Policy Guidance<img src="https://img.shields.io/github/stars/ElliottYan/LUFFY" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/TIGER-AI-Lab/verl-tool">verl-tool</a>: เฟรมเวิร์ค train tool-agent ที่ unified และขยายง่ายบน verl<img src="https://img.shields.io/github/stars/TIGER-AI-Lab/verl-tool" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/zwhe99/DeepMath">DeepMath</a>: ข้อมูล DeepMath-103K และโมเดล reasoning ทางคณิตศาสตร์<img src="https://img.shields.io/github/stars/zwhe99/DeepMath" alt="GitHub Repo stars" /></li>
</ul>
<p>และผลงานอื่นๆ อีกมากมายใน <a href="https://raw.githubusercontent.com/volcengine/verl/main/recipe/README.md">recipe</a></p>
<h2>แนวทางการมีส่วนร่วม</h2>
<p>ชุมชนสามารถมีส่วนร่วมได้! ดู <a href="https://github.com/volcengine/verl/issues/710">roadmap โปรเจกต์</a> และ <a href="https://github.com/volcengine/verl/issues?q=is%3Aissue%20state%3Aopen%20label%3A%22good%20first%20issue%22">good first issues</a> เพื่อดูว่าคุณจะช่วยอะไรได้บ้าง</p>
<h3>การตรวจสอบรูปแบบโค้ด</h3>
<p>เราใช้ pre-commit เพื่อช่วยปรับปรุงคุณภาพโค้ด เริ่มต้น pre-commit ด้วย:</p>
<pre><code class="language-bash">pip install pre-commit
pre-commit install
</code></pre>
<p>หาก CI error สามารถแก้ไขเองโดยรัน:</p>
<pre><code class="language-bash">pre-commit run
</code></pre>
<h3>การเพิ่ม CI tests</h3>
<p>ถ้าเป็นไปได้ เพิ่ม CI test สำหรับฟีเจอร์ใหม่ของคุณ:</p>
<ol>
<li>หา workflow yml ที่เกี่ยวข้องที่สุด ซึ่งมักจะตรงกับ config <code>hydra</code> (เช่น <code>ppo_trainer</code>, <code>ppo_megatron_trainer</code>, <code>sft_trainer</code> ฯลฯ)</li>
<li>เพิ่ม path pattern ที่เกี่ยวข้องใน <code>paths</code> ถ้ายังไม่มี</li>
<li>ลด workload ของ test script ให้มากที่สุด (ดูตัวอย่างจากสคริปต์ที่มีอยู่)</li>
</ol>
<h2>เกี่ยวกับ <a href="https://team.doubao.com/">ByteDance Seed Team</a></h2>
<p>ก่อตั้งในปี 2023 ByteDance Seed Team มุ่งเน้นพัฒนาโมเดล AI พื้นฐานที่ล้ำหน้าที่สุดในอุตสาหกรรม ทีมตั้งเป้าสู่ทีมวิจัยระดับโลกและสร้างประโยชน์อย่างมีนัยสำคัญต่อวิทยาศาสตร์และสังคม รู้จักทีม ByteDance Seed เพิ่มเติมได้ที่ช่องทางต่อไปนี้👇</p>
<div>
  <a href="https://team.doubao.com/">
    <img src="https://img.shields.io/badge/Website-%231e37ff?style=for-the-badge&logo=bytedance&logoColor=white"></a>
  <a href="https://github.com/user-attachments/assets/469535a8-42f2-4797-acdf-4f7a1d4a0c3e">
    <img src="https://img.shields.io/badge/WeChat-07C160?style=for-the-badge&logo=wechat&logoColor=white"></a>
 <a href="https://www.xiaohongshu.com/user/profile/668e7e15000000000303157d?xsec_token=ABl2-aqekpytY6A8TuxjrwnZskU-6BsMRE_ufQQaSAvjc%3D&xsec_source=pc_search">
    <img src="https://img.shields.io/badge/Xiaohongshu-%23FF2442?style=for-the-badge&logo=xiaohongshu&logoColor=white"></a>
  <a href="https://www.zhihu.com/org/dou-bao-da-mo-xing-tuan-dui/">
    <img src="https://img.shields.io/badge/zhihu-%230084FF?style=for-the-badge&logo=zhihu&logoColor=white"></a>
</div>
---
<p>เรารับสมัครงาน! สนใจฝึกงาน/ร่วมงานแบบประจำในสาย RL สำหรับเอเจนต์ ส่ง <a href="mailto:haibin.lin@bytedance.com">email</a> มาหาเราได้เลย</p>
<hr />
<p>Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx">Open Ai Tx</a> | Last indexed: 2025-06-07</p>
<hr />

        </div>
    </div>
    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Default Statcounter code for openaitx
https://openaitx.github.io/ -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>
    <!-- End of Statcounter Code -->
</body>
</html>