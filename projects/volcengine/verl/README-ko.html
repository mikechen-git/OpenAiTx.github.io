<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>verl - volcengine/verl ko</title>
    <meta name="title" content="verl - volcengine/verl ko | 👋 안녕하세요, 여러분! verl은 바이트댄스 Seed 팀이 주도하고 verl 커뮤니티가 유지 관리하는 RL(강화학습) 트레이닝 라이브러리입니다. verl: LLM을 위한 Volcano Engine 강화학습 verl은 대규모 언어 모델(LLM)을 위한 유연하고 효율적이며 프로덕션 레디...">
    <meta name="description" content="volcengine/verl - GitHub repository ko documentation and information | 👋 안녕하세요, 여러분! verl은 바이트댄스 Seed 팀이 주도하고 verl 커뮤니티가 유지 관리하는 RL(강화학습) 트레이닝 라이브러리입니다. verl: LLM을 위한 Volcano Engine 강화학습 verl은 대규모 언어 모델(LLM)을 위한 유연하고 효율적이며 프로덕션 레디...">
    <meta name="keywords" content="volcengine, verl, GitHub, repository, ko documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/volcengine/verl/README-ko.html">
    <meta property="og:title" content="verl - volcengine/verl ko | 👋 안녕하세요, 여러분! verl은 바이트댄스 Seed 팀이 주도하고 verl 커뮤니티가 유지 관리하는 RL(강화학습) 트레이닝 라이브러리입니다. verl: LLM을 위한 Volcano Engine 강화학습 verl은 대규모 언어 모델(LLM)을 위한 유연하고 효율적이며 프로덕션 레디...">
    <meta property="og:description" content="volcengine/verl - GitHub repository ko documentation and information | 👋 안녕하세요, 여러분! verl은 바이트댄스 Seed 팀이 주도하고 verl 커뮤니티가 유지 관리하는 RL(강화학습) 트레이닝 라이브러리입니다. verl: LLM을 위한 Volcano Engine 강화학습 verl은 대규모 언어 모델(LLM)을 위한 유연하고 효율적이며 프로덕션 레디...">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">
    <script src="/js/marked.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        body { display: flex; flex-direction: column; min-height: 100vh; }
        .main-container { margin: 0 auto; width: 100%; max-width: 980px; padding: 0 20px; }
        @media (max-width: 768px) { .main-container { padding: 0 15px; } }
        .markdown-body img { max-width: 100%; height: auto; }
        .header { text-align: center; margin-bottom: 30px; padding: 20px; background-color: #f6f8fa; border-bottom: 1px solid #e1e4e8; position: relative; }
        .header .links { margin-top: 10px; font-size: 16px; }
        .header .links a { color: #0366d6; text-decoration: none; margin-left: 5px; }
        .header .links a:hover { text-decoration: underline; }
        .language-badges { margin-top: 15px; text-align: center; }
        .language-badges a { display: inline-block; margin: 2px; text-decoration: none; }
        .language-badges img { height: 20px; border-radius: 3px; }
        .language-badges a:hover img { opacity: 0.8; }
    </style>
</head>
<body>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/volcengine/verl" id="githubRepoLink" target="_blank">volcengine/verl</a>
<h1 style="display: none;">👋 안녕하세요, 여러분! verl은 바이트댄스 Seed 팀이 주도하고 verl 커뮤니티가 유지 관리하는 RL(강화학습) 트레이닝 라이브러리입니다. verl: LLM을 위한 Volcano Engine 강화학습 verl은 대규모 언어 모델(LLM)을 위한 유연하고 효율적이며 프로덕션 레디...</h1>
        </div>
        <div class="language-badges" id="languageBadges">
            <!-- You can generate language badges here if needed -->
        </div>
    </div>
    <div class="main-container">
        <div class="markdown-body" id="content">
            <div align="center">
 👋 안녕하세요, 여러분! 
    verl은 <b>바이트댄스 Seed 팀</b>이 주도하고 verl 커뮤니티가 유지 관리하는 RL(강화학습) 트레이닝 라이브러리입니다.
    <br>
    <br>
</div>
<div align="center">
<p><a href="https://deepwiki.com/volcengine/verl"><img src="https://devin.ai/assets/deepwiki-badge.png" alt="Ask DeepWiki.com" height="20"/></a>
<a href="https://github.com/volcengine/verl/stargazers"><img src="https://img.shields.io/github/stars/volcengine/verl" alt="GitHub Repo stars" /></a>
<a href="https://twitter.com/verl_project"><img src="https://img.shields.io/twitter/follow/verl_project" alt="Twitter" /></a>
<a href="https://join.slack.com/t/verlgroup/shared_invite/zt-2w5p9o4c3-yy0x2Q56s_VlGLsJ93A6vA"><img src="https://img.shields.io/badge/Slack-verl-blueviolet?logo=slack&amp"></a>
<a href="https://arxiv.org/pdf/2409.19256"><img src="https://img.shields.io/static/v1?label=EuroSys&message=Paper&color=red"></a>
<a href="https://verl.readthedocs.io/en/latest/"><img src="https://img.shields.io/badge/documentation-blue" alt="Documentation" /></a>
<a href="https://raw.githubusercontent.com/eric-haibin-lin/verl-community/refs/heads/main/WeChat.JPG"><img src="https://img.shields.io/badge/微信-green?logo=wechat&amp"></a></p>
</div>
<p><img src="https://github.com/user-attachments/assets/c42e675e-497c-4508-8bb9-093ad4d1f216" alt="seed logo" /></p>
<h1 style="text-align: center;">verl: LLM을 위한 Volcano Engine 강화학습</h1>
<p>verl은 대규모 언어 모델(LLM)을 위한 유연하고 효율적이며 프로덕션 레디(Production-ready) RL(강화학습) 트레이닝 라이브러리입니다.</p>
<p>verl은 <strong><a href="https://arxiv.org/abs/2409.19256v2">HybridFlow: A Flexible and Efficient RLHF Framework</a></strong> 논문의 오픈소스 버전입니다.</p>
<p>verl은 다음과 같은 특징으로 유연하고 사용하기 쉽습니다:</p>
<ul>
<li><p><strong>다양한 RL 알고리즘의 손쉬운 확장</strong>: 하이브리드 컨트롤러 프로그래밍 모델을 통해 복잡한 사후 트레이닝 데이터플로우를 유연하게 표현하고 효율적으로 실행할 수 있습니다. GRPO, PPO 등 RL 데이터플로우를 몇 줄의 코드로 구축할 수 있습니다.</p>
</li>
<li><p><strong>모듈형 API로 기존 LLM 인프라와의 완벽한 통합</strong>: 연산과 데이터 종속성을 분리하여 FSDP, Megatron-LM, vLLM, SGLang 등 기존 LLM 프레임워크와 원활하게 통합할 수 있습니다.</p>
</li>
<li><p><strong>유연한 디바이스 매핑</strong>: 다양한 GPU 세트에 모델을 배치하는 것을 지원하여, 다양한 클러스터 크기에서 효율적인 자원 활용 및 확장이 가능합니다.</p>
</li>
<li><p>인기 있는 HuggingFace 모델과의 즉시 통합 지원</p>
</li>
</ul>
<p>verl은 다음과 같이 빠릅니다:</p>
<ul>
<li><p><strong>최첨단(SOTA) 처리량</strong>: SOTA LLM 트레이닝 및 추론 엔진 통합, 그리고 SOTA RL 처리량</p>
</li>
<li><p><strong>3D-HybridEngine 기반 효율적인 액터 모델 리샤딩</strong>: 트레이닝과 생성 단계 전환 시 메모리 중복을 없애고 통신 오버헤드를 크게 줄입니다.</p>
</li>
</ul>
</p>
<h2>소식</h2>
<ul>
<li>[2025/06] Megatron 백엔드와 함께 verl이 <a href="https://verl.readthedocs.io/en/latest/perf/dpsk.html">DeepSeek-671b 및 Qwen3-236b</a>와 같은 대규모 MoE 모델을 지원합니다.</li>
<li>[2025/06] verl 팀이 6월 7일 <a href="https://www.lfasiallc.com/pytorch-day-china/">PyTorch Day China</a>에서 최신 프로젝트 업데이트를 제공합니다. 베이징에서 개발팀을 만나보세요!</li>
<li>[2025/05] <a href="https://arxiv.org/abs/2409.06957">PF-PPO</a>가 ICML 2025에 채택되어 verl에서 지원됩니다! PF-PPO는 잠재적으로 노이즈가 많은 보상 신호를 필터링하고, 리플레이 버퍼를 통해 고품질 경험을 재사용함으로써 정책 학습의 효율성과 견고성을 향상시킵니다.</li>
<li>[2025/04] <a href="https://iclr.cc/virtual/2025/calendar?filter_events=Expo+Talk+Panel&amp;filter_rooms=">ICLR 2025 Expo</a>, <a href="https://open-foundation-model.github.io/">SCI-FM 워크샵</a>, <a href="https://lu.ma/d23nyynm">LMSys afterparty</a>에서 verl의 최신 사후 트레이닝 기법과 프로그래밍 가이드에 대한 튜토리얼을 제공합니다. 발표 자료는 <a href="https://github.com/eric-haibin-lin/verl-community/tree/main/iclr25">여기</a>에서 확인하세요.</li>
<li>[2025/04] <a href="https://github.com/ByteDance-Seed/Seed-Thinking-v1.5/blob/main/seed-thinking-v1.5.pdf">Seed-Thinking-v1.5</a> 기술 보고서가 공개되었습니다! verl로 트레이닝된 Seed-Thinking-v1.5는 AIME 2024에서 86.7, Codeforces에서 55.0, GPQA에서 77.3을 달성하여 STEM 및 코딩 분야에서 탁월한 추론 능력을 보였습니다. 추론 태스크를 넘어 다양한 도메인에서 뛰어난 일반화 능력도 확인되었습니다.</li>
<li>[2025/04] <a href="https://arxiv.org/pdf/2504.05118">VAPO</a> (value-based augmented PPO) 논문은 추론 모델을 위한 최신 RL 방법을 다룹니다. Qwen-32B-base 모델로 트레이닝된 VAPO는 AIME 2024에서 60.4를 기록하여 DAPO-32B를 능가했습니다.</li>
<li>[2025/03] verl v0.3.0.post1 출시! 자세한 내용은 <a href="https://github.com/volcengine/verl/releases/">릴리즈 노트</a> 참조. 이전 버전 대비 <a href="https://tongyx361.github.io/blogs/posts/verl-intro/#/verl-flexible-and-efficient-rl-for-llms">약 1.4배 속도 향상</a>을 달성했습니다.</li>
<li>[2025/03] <a href="https://dapo-sia.github.io/">DAPO</a>는 Qwen2.5-32B 프리트레인 모델을 기반으로 AIME 2024에서 50점을 기록한 오픈소스 SOTA RL 알고리즘으로, DeepSeek의 GRPO(DeepSeek-R1-Zero-Qwen-32B)가 달성한 이전 SOTA를 능가합니다. DAPO 트레이닝은 모두 verl을 기반으로 하며, 재현 코드는 <code>recipe/dapo</code>에 공개되어 있습니다.</li>
</ul>
<details><summary> 더 보기... </summary>
<ul>
  <li>[2025/05] [A2M Shanghai](https://a2m.msup.com.cn/home/?aid=4488&city=shanghai)에서 5/16 - 5/17에 verl이 소개됩니다.</li>
  <li>[2025/05] [GOSIM x PyTorch Day 2025](https://paris2025.gosim.org/)에서 verl이 발표됩니다. 파리에서 만나요! </li>
  <li>[2025/03] [vLLM Beijing Meetup](https://mp.weixin.qq.com/s/n77GibL2corAtQHtVEAzfg)과 [SGLang-LMSYS Org Meetup](https://lu.ma/ntjrr7ig)에서 verl의 프로그래밍 모델과 최신 업데이트를 소개했습니다. [발표자료](https://github.com/eric-haibin-lin/verl-community/blob/main/slides/verl-lmsys-meetup.pdf) 참고.</li>
  <li>[2025/03] EuroSys 2025에서 verl(HybridFlow)이 발표됩니다. 로테르담에서 만나요!</li>
  <li>[2025/02] verl v0.2.0.post2 출시!</li>
  <li>[2025/02] <a href="https://lu.ma/ji7atxux">Bytedance/NVIDIA/Anyscale Ray Meetup</a>에서 verl을 발표했습니다. 산호세에서 만나요!</li>
  <li>[2025/01] [Doubao-1.5-pro](https://team.doubao.com/zh/special/doubao_1_5_pro)가 LLM & VLM에서 SOTA 수준의 성능으로 출시되었습니다. RL 스케일링 프리뷰 모델은 verl로 트레이닝되었으며, 수학 벤치마크에서 OpenAI O1 수준(70.0 pass@1 on AIME)을 달성했습니다.</li>
  <li>[2024/12] Ray Forward 2024에서 verl이 발표되었습니다. 슬라이드는 <a href="https://github.com/eric-haibin-lin/verl-community/blob/main/slides/Ray_Forward_2024_%E5%B7%AB%E9%94%A1%E6%96%8C.pdf">여기</a>에서 확인하세요.</li>
  <li>[2024/12] NeurIPS 2024에서 <a href="https://neurips.cc/Expo/Conferences/2024/workshop/100677">Post-training LLMs: From Algorithms to Infrastructure</a>를 발표했습니다. <a href="https://github.com/eric-haibin-lin/verl-data/tree/neurips">슬라이드</a> 및 <a href="https://neurips.cc/Expo/Conferences/2024/workshop/100677">영상</a> 제공.</li>
  <li>[2024/10] Ray Summit에서 verl이 발표되었습니다. <a href="https://www.youtube.com/watch?v=MrhMcXkXvJU&list=PLzTswPQNepXntmT8jr9WaNfqQ60QwW7-U&index=37">Youtube 영상</a> 참조.</li>
  <li>[2024/08] HybridFlow (verl)이 EuroSys 2025에 채택되었습니다.</li>
</ul>   
</details>
<h2>주요 특징</h2>
<ul>
<li><strong>FSDP</strong>, <strong>FSDP2</strong>, <strong>Megatron-LM</strong>을 이용한 트레이닝 지원</li>
<li><strong>vLLM</strong>, <strong>SGLang</strong>, <strong>HF Transformers</strong>를 이용한 rollout 생성 지원</li>
<li>Hugging Face Transformers 및 Modelscope Hub와 호환: <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/grpo_trainer/run_qwen3-8b.sh">Qwen-3</a>, Qwen-2.5, Llama3.1, Gemma2, DeepSeek-LLM 등</li>
<li>지도학습(SFT) 파인튜닝 지원</li>
<li>강화학습: <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/ppo_trainer/">PPO</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/grpo_trainer/">GRPO</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/remax_trainer/">ReMax</a>, <a href="https://verl.readthedocs.io/en/latest/examples/config.html#algorithm">REINFORCE++</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/rloo_trainer/">RLOO</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/recipe/prime/">PRIME</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/recipe/dapo/">DAPO</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/recipe/drgrpo">DrGRPO</a> 등
<ul>
<li>수학, <a href="https://raw.githubusercontent.com/volcengine/verl/main/recipe/dapo">코딩</a> 등에서 모델 기반 보상 및 함수 기반(검증가능한) 보상 지원</li>
<li>비전-언어 모델(VLM) 및 <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/grpo_trainer/run_qwen2_5_vl-7b.sh">멀티모달 RL</a> 지원 (Qwen2.5-vl, Kimi-VL 등)</li>
<li><a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/sglang_multiturn">툴 콜을 활용한 멀티턴</a> 지원</li>
</ul>
</li>
<li><a href="https://raw.githubusercontent.com/volcengine/verl/main/recipe/sppo">Self-play preference optimization (SPPO)</a> 등 LLM 정렬 레시피</li>
<li>Flash attention 2, <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/ppo_trainer/run_qwen2-7b_seq_balance.sh">시퀀스 패킹</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/ppo_trainer/run_deepseek7b_llm_sp2.sh">시퀀스 병렬처리</a> (DeepSpeed Ulysses 기반), <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/sft/gsm8k/run_qwen_05_peft.sh">LoRA</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/sft/gsm8k/run_qwen_05_sp2_liger.sh">Liger-kernel</a> 지원</li>
<li><a href="https://raw.githubusercontent.com/volcengine/verl/main/pull/1467">전문가 병렬화</a>로 671B 모델 및 수백 개 GPU까지 확장</li>
<li>멀티 GPU <a href="https://verl.readthedocs.io/en/latest/advance/ppo_lora.html">LoRA RL</a> 지원으로 메모리 절약</li>
<li>wandb, swanlab, mlflow, tensorboard를 통한 실험 추적 지원</li>
</ul>
<h2>예정된 기능 및 변경 사항</h2>
<ul>
<li>로드맵 https://github.com/volcengine/verl/issues/710</li>
<li>Megatron v0.11 기반 DeepSeek 671b 최적화 https://github.com/volcengine/verl/issues/708</li>
<li>멀티턴 rollout 및 툴 최적화 https://github.com/volcengine/verl/issues/1882</li>
<li>환경 상호작용 https://github.com/volcengine/verl/issues/1172</li>
<li>v0.3 이후 주요 변경사항 목록 https://github.com/volcengine/verl/discussions/943, entropy_coeff 기본값 0</li>
<li>RL용 Lora https://github.com/volcengine/verl/pull/1127</li>
</ul>
<h2>시작하기</h2>
<p><a href="https://verl.readthedocs.io/en/latest/index.html"><b>문서 바로가기</b></a></p>
<p><strong>퀵스타트:</strong></p>
<ul>
<li><a href="https://verl.readthedocs.io/en/latest/start/install.html">설치</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/start/quickstart.html">퀵스타트</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/hybrid_flow.html">프로그래밍 가이드</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/algo/ppo.html">verl에서 PPO 사용법</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/algo/grpo.html">verl에서 GRPO 사용법</a></li>
</ul>
<p><strong>PPO 예제 단계별 실행법:</strong></p>
<ul>
<li><a href="https://verl.readthedocs.io/en/latest/preparation/prepare_data.html">사후 트레이닝 데이터 준비</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/preparation/reward_function.html">데이터셋 보상 함수 구현</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/examples/ppo_code_architecture.html">PPO 예제 구조</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/examples/config.html">설정(Config) 설명</a></li>
</ul>
<p><strong>재현 가능한 알고리즘 벤치마크:</strong></p>
<ul>
<li><a href="https://verl.readthedocs.io/en/latest/algo/baseline.html">코딩, 수학 RL 성능</a></li>
</ul>
<p><strong>코드 설명 및 고급 활용(확장):</strong></p>
<ul>
<li><p>PPO 트레이너와 워커</p>
<ul>
<li><a href="https://verl.readthedocs.io/en/latest/workers/ray_trainer.html">PPO Ray 트레이너</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/workers/fsdp_workers.html">PyTorch FSDP 백엔드</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/index.html">Megatron-LM 백엔드</a></li>
</ul>
</li>
<li><p>고급 활용 및 확장</p>
<ul>
<li><a href="https://verl.readthedocs.io/en/latest/advance/fsdp_extension.html">FSDP 백엔드로 모델 추가</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/advance/megatron_extension.html">Megatron-LM 백엔드로 모델 추가</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/sglang_multiturn/multiturn.html">멀티턴 rollout 지원</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/sglang_multiturn/search_tool_example.html">검색 툴 통합</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/examples/sandbox_fusion_example.html">Sandbox Fusion 통합</a></li>
<li><a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/split_placement">별도 GPU 자원으로 배포</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/advance/dpo_extension.html">기타 RL(HF) 알고리즘 확장</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/advance/placement.html">Ray API 디자인 튜토리얼</a></li>
</ul>
</li>
</ul>
<p><strong>커뮤니티 블로그</strong></p>
<ul>
<li><a href="https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/verl/multi-turn/verl-multiturn-rollout-Release.md">SGLang, verl, OpenBMB 및 칭화대: 엔드-투-엔드 멀티턴 RLHF 선구자</a></li>
<li><a href="https://rocm.blogs.amd.com/artificial-intelligence/verl-large-scale/README.html">AMD GPU와 verl, ROCm 통합을 통한 Human Feedback 기반 강화학습</a></li>
<li><a href="https://mp.weixin.qq.com/s/7nbqxk4knMGd-hQE9ls2tA">veMLP x verl ：강화학습 트레이닝 마스터하기</a></li>
<li><a href="https://www.volcengine.com/docs/6459/1463942">verl을 활용한 GRPO 분산 강화학습 트레이닝 모범 사례</a></li>
<li><a href="https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/verl/readme.md">HybridFlow verl 원문 분석</a></li>
<li><a href="https://team.doubao.com/en/blog/%E6%9C%80%E9%AB%98%E6%8F%90%E5%8D%8720%E5%80%8D%E5%90%9E%E5%90%90%E9%87%8F-%E8%B1%86%E5%8C%85%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9B%A2%E9%98%9F%E5%8F%91%E5%B8%83%E5%85%A8%E6%96%B0-rlhf-%E6%A1%86%E6%9E%B6-%E7%8E%B0%E5%B7%B2%E5%BC%80%E6%BA%90">최대 20배 향상된 처리량! Doubao 대모델팀 RLHF 프레임워크 오픈소스 공개!</a></li>
</ul>
<h2>성능 튜닝 가이드</h2>
<p>온폴리시 RL 알고리즘에서 성능은 매우 중요합니다. 자세한 <a href="https://verl.readthedocs.io/en/latest/perf/perf_tuning.html">성능 튜닝 가이드</a>를 참고하여 성능을 최적화하세요.</p>
<h2>vLLM &gt;= v0.8.2로 업그레이드</h2>
<p>verl은 FSDP를 트레이닝 백엔드로 사용할 때 vLLM&gt;=0.8.2를 지원합니다. 설치 가이드 및 자세한 내용은 <a href="https://raw.githubusercontent.com/volcengine/verl/main/docs/README_vllm0.8.md">이 문서</a>를 참고하세요. OOM 및 예기치 않은 오류가 발생할 수 있는 vllm 0.7.x 버전은 피해주세요.</p>
<h2>최신 SGLang 사용하기</h2>
<p>SGLang은 verl과 완전히 호환되며, SGLang RL 그룹은 멀티턴 에이전트 RL, VLM RLHF, 서버 기반 RL, 부분 rollout 등 고유 기능 개발에 주력하고 있습니다. 설치 가이드와 자세한 내용은 <a href="https://verl.readthedocs.io/en/latest/workers/sglang_worker.html">이 문서</a>를 참고하세요.</p>
<h2>FSDP2 업그레이드</h2>
<p>verl은 FSDP2를 완전히 지원합니다! FSDP2는 torch distributed 팀이 권장하며, 더 나은 처리량과 메모리 사용성을 제공하며 torch.compile 등 다른 기능과 조합 사용이 가능합니다. FSDP2 활성화는 verl main에서 다음 옵션을 설정하면 됩니다:</p>
<pre><code>actor_rollout_ref.ref.strategy=fsdp2
actor_rollout_ref.actor.strategy=fsdp2
critic.strategy=fsdp2 
reward_model.strategy=fsdp2 
</code></pre>
<p>또한, FSDP2 CPU 오프로드는 그래디언트 누적과 호환됩니다. 메모리를 아끼려면 <code>actor_rollout_ref.actor.offload_policy=True</code>로 설정하세요. 자세한 내용은 https://github.com/volcengine/verl/pull/1026 참고.</p>
<h2>AMD 지원(ROCm 커널)</h2>
<p>verl은 FSDP를 트레이닝 엔진으로 지원(곧 Megatron 지원 예정)하며, vLLM 및 SGLang과 추론 엔진으로 통합됩니다. 설치 가이드는 <a href="https://raw.githubusercontent.com/volcengine/verl/main/docs/amd_tutorial/amd_build_dockerfile_page.rst">이 문서</a>와 vLLM ROCm 성능 튜닝은 <a href="https://raw.githubusercontent.com/volcengine/verl/main/docs/amd_tutorial/amd_vllm_page.rst">이 문서</a>를 참고하세요.</p>
<h2>인용 및 감사</h2>
<p>프로젝트가 도움이 되었다면 아래 논문을 인용해 주세요:</p>
<ul>
<li><a href="https://arxiv.org/abs/2409.19256v2">HybridFlow: A Flexible and Efficient RLHF Framework</a></li>
<li><a href="https://i.cs.hku.hk/%7Ecwu/papers/gmsheng-NL2Code24.pdf">A Framework for Training Large Language Models for Code Generation via Proximal Policy Optimization</a></li>
</ul>
<pre><code class="language-bibtex">@article{sheng2024hybridflow,
  title   = {HybridFlow: A Flexible and Efficient RLHF Framework},
  author  = {Guangming Sheng and Chi Zhang and Zilingfeng Ye and Xibin Wu and Wang Zhang and Ru Zhang and Yanghua Peng and Haibin Lin and Chuan Wu},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2409.19256}
}
</code></pre>
<p>verl은 Nemo-Aligner, Deepspeed-chat, OpenRLHF 설계에서 영감을 받았습니다. Bytedance, Anyscale, LMSys.org, <a href="https://github.com/QwenLM/">Alibaba Qwen team</a>, Shanghai AI Lab, 칭화대, UC 버클리, UCLA, UIUC, 홍콩대, ke.com, <a href="https://www.all-hands.dev/">All Hands AI</a>, <a href="http://modelbest.cn/">ModelBest</a>, OpenPipe, JD AI Lab, Microsoft Research, <a href="https://www.stepfun.com/">StepFun</a>, Amazon, Linkedin, Meituan, <a href="https://www.camel-ai.org/">Camel-AI</a>, <a href="https://github.com/OpenManus">OpenManus</a>, Xiaomi, Prime Intellect, NVIDIA research, <a href="https://www.baichuan-ai.com/home">Baichuan</a>, <a href="https://www.xiaohongshu.com/">RedNote</a>, <a href="https://www.swiss-ai.org/">SwissAI</a>, <a href="https://www.moonshot-ai.com/">Moonshot AI (Kimi)</a>, Baidu, Snowflake 등 다양한 기관에서 채택 및 기여하고 있습니다.</p>
<h2>verl을 활용한 주요 프로젝트</h2>
<ul>
<li><a href="https://github.com/Jiayi-Pan/TinyZero">TinyZero</a>: <strong>DeepSeek R1 Zero</strong> 레시피 재현 <img src="https://img.shields.io/github/stars/Jiayi-Pan/TinyZero" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/NovaSky-AI/SkyThought">SkyThought</a>: NovaSky AI 팀의 Sky-T1-7B RL 트레이닝 <img src="https://img.shields.io/github/stars/NovaSky-AI/SkyThought" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/hkust-nlp/simpleRL-reason">simpleRL-reason</a>: SimpleRL-Zoo: 오픈 베이스 모델의 제로 RL 연구 및 탐구 <img src="https://img.shields.io/github/stars/hkust-nlp/simpleRL-reason" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/hiyouga/EasyR1">Easy-R1</a>: <strong>멀티모달</strong> RL 트레이닝 프레임워크 <img src="https://img.shields.io/github/stars/hiyouga/EasyR1" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/OpenManus/OpenManus-RL">OpenManus-RL</a>: 다양한 에이전트 환경을 위한 LLM 에이전트 RL 튜닝 프레임워크 <img src="https://img.shields.io/github/stars/OpenManus/OpenManus-RL" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/agentica-project/rllm">rllm</a>: <a href="https://github.com/agentica-project/verl-pipeline">verl-pipeline</a> 기반 비동기 RL 트레이닝 <img src="https://img.shields.io/github/stars/agentica-project/rllm" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/PRIME-RL/PRIME">PRIME</a>: 암묵적 보상을 통한 프로세스 강화학습 <img src="https://img.shields.io/github/stars/PRIME-RL/PRIME" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/ZihanWang314/ragen">RAGEN</a>: 범용 추론 <strong>에이전트</strong> 트레이닝 프레임워크 <img src="https://img.shields.io/github/stars/ZihanWang314/ragen" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/PeterGriffinJin/Search-R1">Search-R1</a>: 추론 및 **검색(툴 콜)**이 결합된 RL <img src="https://img.shields.io/github/stars/PeterGriffinJin/Search-R1" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/pat-jj/DeepRetrieval">DeepRetrieval</a>: <strong>검색/검색결과 기반</strong> RL 트레이닝 <img src="https://img.shields.io/github/stars/pat-jj/DeepRetrieval" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/Agent-RL/ReSearch">ReSearch</a>: RL을 통한 LLM <strong>추론+검색</strong> 학습 <img src="https://img.shields.io/github/stars/Agent-RL/ReSearch" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/ganler/code-r1">Code-R1</a>: 신뢰성 높은 보상 기반 <strong>코드</strong> R1 재현 <img src="https://img.shields.io/github/stars/ganler/code-r1" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/SkyworkAI/Skywork-OR1">Skywork-OR1</a>: Skywork 오픈 리저너 시리즈 <img src="https://img.shields.io/github/stars/SkyworkAI/Skywork-OR1" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/GAIR-NLP/ToRL">ToRL</a>: 툴 통합 RL 스케일링 <img src="https://img.shields.io/github/stars/GAIR-NLP/ToRL" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/langfengQ/verl-agent">verl-agent</a>: <strong>롱-호라이즌 LLM/VLM 에이전트</strong>를 위한 확장형 트레이닝 프레임워크 및 새로운 <strong>GiGPO</strong> 알고리즘 <img src="https://img.shields.io/github/stars/langfengQ/verl-agent" alt="GitHub Repo stars" /></li>
<li><a href="https://arxiv.org/abs/2409.06957">PF-PPO</a>: 보상 신호 신뢰성 기반 정책 필터링을 통한 PPO (효율적, 견고한 RLHF)</li>
<li><a href="https://github.com/ritzz-ai/GUI-R1">GUI-R1</a>: <strong>GUI-R1</strong>: GUI 에이전트를 위한 범용 R1 스타일 비전-언어 액션 모델 <img src="https://img.shields.io/github/stars/ritzz-ai/GUI-R1" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/GAIR-NLP/DeepResearcher">DeepResearcher</a>: 실제 환경에서 RL을 통한 딥 리서치 스케일링 <img src="https://img.shields.io/github/stars/GAIR-NLP/DeepResearcher" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/RAGEN-AI/VAGEN">VAGEN</a>: 멀티턴 강화학습을 통한 VLM 에이전트 트레이닝 <img src="https://img.shields.io/github/stars/RAGEN-AI/VAGEN" alt="GitHub Repo stars" /></li>
<li><a href="https://retool-rl.github.io/">ReTool</a>: LLM의 전략적 툴 활용을 위한 강화학습 (코드 공개 예정)</li>
<li><a href="https://arxiv.org/abs/2505.02387">RM-R1</a>: 추론 보상 모델의 RL 트레이닝 <img src="https://img.shields.io/github/stars/RM-R1-UIUC/RM-R1" alt="GitHub Repo stars" /></li>
<li><a href="https://arxiv.org/abs/2505.03335">Absolute Zero Reasoner</a>: 휴먼 큐레이션 데이터 없이 자체 플레이로 추론하는 프레임워크 <img src="https://img.shields.io/github/stars/LeapLabTHU/Absolute-Zero-Reasoner" alt="GitHub Repo stars" /></li>
<li><a href="https://arxiv.org/pdf/2504.14945">LUFFY</a>: 오프폴리시 가이던스 하에서의 추론 학습 <img src="https://img.shields.io/github/stars/ElliottYan/LUFFY" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/TIGER-AI-Lab/verl-tool">verl-tool</a>: verl 기반 통합 및 확장형 툴-에이전트 트레이닝 프레임워크 <img src="https://img.shields.io/github/stars/TIGER-AI-Lab/verl-tool" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/zwhe99/DeepMath">DeepMath</a>: 수학 추론용 DeepMath-103K 데이터 및 시리즈 모델 <img src="https://img.shields.io/github/stars/zwhe99/DeepMath" alt="GitHub Repo stars" /></li>
</ul>
<p>더 많은 프로젝트는 <a href="https://raw.githubusercontent.com/volcengine/verl/main/recipe/README.md">recipe</a>에서 확인하세요.</p>
<h2>기여 안내</h2>
<p>커뮤니티의 기여를 환영합니다! <a href="https://github.com/volcengine/verl/issues/710">프로젝트 로드맵</a>과 <a href="https://github.com/volcengine/verl/issues?q=is%3Aissue%20state%3Aopen%20label%3A%22good%20first%20issue%22">good first issues</a>를 참고해 기여할 부분을 찾아보세요.</p>
<h3>코드 린팅 및 포맷팅</h3>
<p>코드 품질 향상을 위해 pre-commit을 사용합니다. 초기화 방법:</p>
<pre><code class="language-bash">pip install pre-commit
pre-commit install
</code></pre>
<p>CI 오류를 로컬에서 해결하려면 수동으로 pre-commit을 실행하세요:</p>
<pre><code class="language-bash">pre-commit run
</code></pre>
<h3>CI 테스트 추가</h3>
<p>가능하다면 새로운 기능에 대해 CI 테스트를 추가해 주세요:</p>
<ol>
<li>관련 워크플로우 yml 파일을 찾으세요(대부분 hydra 기본 config와 매칭됩니다. 예: <code>ppo_trainer</code>, <code>ppo_megatron_trainer</code>, <code>sft_trainer</code> 등).</li>
<li>관련 경로 패턴을 <code>paths</code> 섹션에 추가하세요(없다면).</li>
<li>테스트 스크립트의 작업량을 최소화하세요(기존 스크립트 참고).</li>
</ol>
<h2><a href="https://team.doubao.com/">ByteDance Seed Team</a> 소개</h2>
<p>2023년에 설립된 ByteDance Seed Team은 업계 최고의 AI 기반 모델을 개발하는 데 전념하고 있습니다. 세계적 수준의 연구팀이 되어 과학과 사회 발전에 크게 기여하는 것이 목표입니다. 아래 채널을 통해 Bytedance Seed를 더 알아보세요👇</p>
<div>
  <a href="https://team.doubao.com/">
    <img src="https://img.shields.io/badge/Website-%231e37ff?style=for-the-badge&logo=bytedance&logoColor=white"></a>
  <a href="https://github.com/user-attachments/assets/469535a8-42f2-4797-acdf-4f7a1d4a0c3e">
    <img src="https://img.shields.io/badge/WeChat-07C160?style=for-the-badge&logo=wechat&logoColor=white"></a>
 <a href="https://www.xiaohongshu.com/user/profile/668e7e15000000000303157d?xsec_token=ABl2-aqekpytY6A8TuxjrwnZskU-6BsMRE_ufQQaSAvjc%3D&xsec_source=pc_search">
    <img src="https://img.shields.io/badge/Xiaohongshu-%23FF2442?style=for-the-badge&logo=xiaohongshu&logoColor=white"></a>
  <a href="https://www.zhihu.com/org/dou-bao-da-mo-xing-tuan-dui/">
    <img src="https://img.shields.io/badge/zhihu-%230084FF?style=for-the-badge&logo=zhihu&logoColor=white"></a>
</div>
---
<p>채용 중입니다! 에이전트 RL 관련 인턴십/정규직(FTE)에 관심 있는 분들은 <a href="mailto:haibin.lin@bytedance.com">이메일</a>로 연락 바랍니다.</p>
<hr />
<p>Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx">Open Ai Tx</a> | Last indexed: 2025-06-07</p>
<hr />

        </div>
    </div>
    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Default Statcounter code for openaitx
https://openaitx.github.io/ -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>
    <!-- End of Statcounter Code -->
</body>
</html>