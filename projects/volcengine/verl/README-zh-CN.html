<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>verl - volcengine/verl zh-CN</title>
    <meta name="title" content="verl - volcengine/verl zh-CN | 👋 大家好！ verl 是由 字节跳动 Seed 团队 发起并由 verl 社区维护的强化学习训练库。 verl：Volcano Engine 大语言模型强化学习库 verl 是一个为大语言模型（LLMs）设计的灵活、高效、可用于生产的强化学习训练库。 verl 是 HybridFlow: A Flexible an...">
    <meta name="description" content="volcengine/verl - GitHub repository zh-CN documentation and information | 👋 大家好！ verl 是由 字节跳动 Seed 团队 发起并由 verl 社区维护的强化学习训练库。 verl：Volcano Engine 大语言模型强化学习库 verl 是一个为大语言模型（LLMs）设计的灵活、高效、可用于生产的强化学习训练库。 verl 是 HybridFlow: A Flexible an...">
    <meta name="keywords" content="volcengine, verl, GitHub, repository, zh-CN documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/volcengine/verl/README-zh-CN.html">
    <meta property="og:title" content="verl - volcengine/verl zh-CN | 👋 大家好！ verl 是由 字节跳动 Seed 团队 发起并由 verl 社区维护的强化学习训练库。 verl：Volcano Engine 大语言模型强化学习库 verl 是一个为大语言模型（LLMs）设计的灵活、高效、可用于生产的强化学习训练库。 verl 是 HybridFlow: A Flexible an...">
    <meta property="og:description" content="volcengine/verl - GitHub repository zh-CN documentation and information | 👋 大家好！ verl 是由 字节跳动 Seed 团队 发起并由 verl 社区维护的强化学习训练库。 verl：Volcano Engine 大语言模型强化学习库 verl 是一个为大语言模型（LLMs）设计的灵活、高效、可用于生产的强化学习训练库。 verl 是 HybridFlow: A Flexible an...">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">
    <script src="/js/marked.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        body { display: flex; flex-direction: column; min-height: 100vh; }
        .main-container { margin: 0 auto; width: 100%; max-width: 980px; padding: 0 20px; }
        @media (max-width: 768px) { .main-container { padding: 0 15px; } }
        .markdown-body img { max-width: 100%; height: auto; }
        .header { text-align: center; margin-bottom: 30px; padding: 20px; background-color: #f6f8fa; border-bottom: 1px solid #e1e4e8; position: relative; }
        .header .links { margin-top: 10px; font-size: 16px; }
        .header .links a { color: #0366d6; text-decoration: none; margin-left: 5px; }
        .header .links a:hover { text-decoration: underline; }
        .language-badges { margin-top: 15px; text-align: center; }
        .language-badges a { display: inline-block; margin: 2px; text-decoration: none; }
        .language-badges img { height: 20px; border-radius: 3px; }
        .language-badges a:hover img { opacity: 0.8; }
    </style>
</head>
<body>
    <div style="position: fixed; top: 2px; left: 2px; z-index: 2000; background: rgba(255,255,255,0.95); border-radius: 6px; box-shadow: 0 2px 8px rgba(0,0,0,0.06); padding: 4px 14px; font-size: 15px; color: #222; font-family: 'Segoe UI', Arial, sans-serif; font-weight: 500; letter-spacing: 0.5px;">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" style="color: #0366d6; text-decoration: none; font-weight: 600;">Open AI Tx</a>
    </div>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/volcengine/verl" id="githubRepoLink" target="_blank">volcengine/verl</a>
<h1 style="display: none;">👋 大家好！ verl 是由 字节跳动 Seed 团队 发起并由 verl 社区维护的强化学习训练库。 verl：Volcano Engine 大语言模型强化学习库 verl 是一个为大语言模型（LLMs）设计的灵活、高效、可用于生产的强化学习训练库。 verl 是 HybridFlow: A Flexible an...</h1>
        </div>
        <div class="language-badges" id="languageBadges">
            <!-- You can generate language badges here if needed -->
        </div>
    </div>
    <div class="main-container">
        <div class="markdown-body" id="content">
            <div align="center">
 👋 大家好！
    verl 是由 <b>字节跳动 Seed 团队</b> 发起并由 verl 社区维护的强化学习训练库。
    <br>
    <br>
</div>
<div align="center">
<p><a href="https://deepwiki.com/volcengine/verl"><img src="https://devin.ai/assets/deepwiki-badge.png" alt="Ask DeepWiki.com" height="20"/></a>
<a href="https://github.com/volcengine/verl/stargazers"><img src="https://img.shields.io/github/stars/volcengine/verl" alt="GitHub Repo stars" /></a>
<a href="https://twitter.com/verl_project"><img src="https://img.shields.io/twitter/follow/verl_project" alt="Twitter" /></a>
<a href="https://join.slack.com/t/verlgroup/shared_invite/zt-2w5p9o4c3-yy0x2Q56s_VlGLsJ93A6vA"><img src="https://img.shields.io/badge/Slack-verl-blueviolet?logo=slack&amp"></a>
<a href="https://arxiv.org/pdf/2409.19256"><img src="https://img.shields.io/static/v1?label=EuroSys&message=Paper&color=red"></a>
<a href="https://verl.readthedocs.io/en/latest/"><img src="https://img.shields.io/badge/documentation-blue" alt="Documentation" /></a>
<a href="https://raw.githubusercontent.com/eric-haibin-lin/verl-community/refs/heads/main/WeChat.JPG"><img src="https://img.shields.io/badge/微信-green?logo=wechat&amp"></a></p>
</div>
<p><img src="https://github.com/user-attachments/assets/c42e675e-497c-4508-8bb9-093ad4d1f216" alt="seed logo" /></p>
<h1 style="text-align: center;">verl：Volcano Engine 大语言模型强化学习库</h1>
<p>verl 是一个为大语言模型（LLMs）设计的灵活、高效、可用于生产的强化学习训练库。</p>
<p>verl 是 <strong><a href="https://arxiv.org/abs/2409.19256v2">HybridFlow: A Flexible and Efficient RLHF Framework</a></strong> 论文的开源实现。</p>
<p>verl 具有灵活易用的特性：</p>
<ul>
<li><strong>多样化 RL 算法的易扩展性</strong>：混合控制器编程模型可灵活表达和高效执行复杂的后训练数据流。仅需几行代码即可构建 GRPO、PPO 等强化学习数据流。</li>
<li><strong>通过模块化 API 与现有 LLM 基础设施无缝集成</strong>：计算和数据依赖分离，可与现有 LLM 框架（如 FSDP、Megatron-LM、vLLM、SGLang 等）无缝衔接。</li>
<li><strong>灵活的设备映射</strong>：支持模型在不同 GPU 组上的多样部署，实现高效的资源利用与跨集群规模扩展。</li>
<li>现成集成 HuggingFace 流行模型。</li>
</ul>
<p>verl 具有高性能：</p>
<ul>
<li><strong>业界领先的吞吐量</strong>：集成业界前沿的大模型训练与推理引擎，强化学习吞吐性能卓越。</li>
<li><strong>3D-HybridEngine 高效 actor 模型重分片</strong>：消除冗余内存，极大减少训练与生成阶段切换时的通信开销。</li>
</ul>
</p>
<h2>最新动态</h2>
<ul>
<li>[2025/06] verl 结合 Megatron 后端支持 <a href="https://verl.readthedocs.io/en/latest/perf/dpsk.html">DeepSeek-671b 和 Qwen3-236b</a> 等超大 MoE 模型。</li>
<li>[2025/06] verl 团队将于 <a href="https://www.lfasiallc.com/pytorch-day-china/">PyTorch Day China</a>（6月7日，北京）带来项目最新进展，欢迎线下交流！</li>
<li>[2025/05] <a href="https://arxiv.org/abs/2409.06957">PF-PPO</a> 已集成进 verl！PF-PPO 通过过滤噪声奖励信号并复用高质量经验，提高策略学习效率与鲁棒性，论文被 ICML 2025 接收。</li>
<li>[2025/04] 在 <a href="https://iclr.cc/virtual/2025/calendar?filter_events=Expo+Talk+Panel&amp;filter_rooms=">ICLR 2025 Expo</a>、<a href="https://open-foundation-model.github.io/">SCI-FM workshop</a>、<a href="https://lu.ma/d23nyynm">LMSys afterparty</a> 分享 verl 最新后训练技术和编程指南，讲稿见<a href="https://github.com/eric-haibin-lin/verl-community/tree/main/iclr25">这里</a>。</li>
<li>[2025/04] <a href="https://github.com/ByteDance-Seed/Seed-Thinking-v1.5/blob/main/seed-thinking-v1.5.pdf">Seed-Thinking-v1.5</a> 技术报告发布！该模型通过 verl 训练，在 AIME 2024 取得 86.7 分，Codeforces 55.0，GPQA 77.3，展现出卓越的 STEM 和编程推理能力，并在多领域泛化表现突出。</li>
<li>[2025/04] <a href="https://arxiv.org/pdf/2504.05118">VAPO</a>（基于价值的增强型 PPO）论文发布。该方法用 Qwen-32B-base 训练，在 AIME 2024 达到 60.4 分，优于 DAPO-32B。</li>
<li>[2025/03] verl v0.3.0.post1 发布！详见 <a href="https://github.com/volcengine/verl/releases/">release note</a>，相较前版本实现约 <a href="https://tongyx361.github.io/blogs/posts/verl-intro/#/verl-flexible-and-efficient-rl-for-llms">1.4 倍加速</a>。</li>
<li>[2025/03] <a href="https://dapo-sia.github.io/">DAPO</a> 开源，基于 Qwen2.5-32B 预训练模型，在 AIME 2024 取得 50 分，超越 DeepSeek GRPO（DeepSeek-R1-Zero-Qwen-32B），训练完全基于 verl，复现代码已在 <code>recipe/dapo</code>。</li>
</ul>
<details><summary>更多动态...</summary>
<ul>
  <li>[2025/05] verl 将于 [A2M 上海](https://a2m.msup.com.cn/home/?aid=4488&city=shanghai) 5/16-5/17 亮相。</li>
  <li>[2025/05] verl 将在 [GOSIM x PyTorch Day 2025](https://paris2025.gosim.org/) 展示，巴黎见！</li>
  <li>[2025/03] 在 [vLLM 北京 Meetup](https://mp.weixin.qq.com/s/n77GibL2corAtQHtVEAzfg) 和 [SGLang-LMSYS Org Meetup](https://lu.ma/ntjrr7ig)（Sunnyvale）介绍 verl 编程模型与最新进展，资料见[此处](https://github.com/eric-haibin-lin/verl-community/blob/main/slides/verl-lmsys-meetup.pdf)。</li>
  <li>[2025/03] verl（HybridFlow）将亮相 EuroSys 2025，鹿特丹见！</li>
  <li>[2025/02] verl v0.2.0.post2 发布！</li>
  <li>[2025/02] 于 <a href="https://lu.ma/ji7atxux">Bytedance/NVIDIA/Anyscale Ray Meetup</a> 展示 verl，圣何塞见！</li>
  <li>[2025/01] [豆包-1.5-pro](https://team.doubao.com/zh/special/doubao_1_5_pro) 发布，LLM & VLM 达到 SOTA 水平。RL scaling 预览模型用 verl 训练，数学榜单 OpenAI O1（AIME pass@1 70.0）。</li>
  <li>[2024/12] verl 于 Ray Forward 2024 亮相，PPT <a href="https://github.com/eric-haibin-lin/verl-community/blob/main/slides/Ray_Forward_2024_%E5%B7%AB%E9%94%A1%E6%96%8C.pdf">下载</a></li>
  <li>[2024/12] 团队于 NeurIPS 2024 展示 <a href="https://neurips.cc/Expo/Conferences/2024/workshop/100677">后训练 LLMs: 从算法到基础设施</a>，<a href="https://github.com/eric-haibin-lin/verl-data/tree/neurips">PPT</a>、<a href="https://neurips.cc/Expo/Conferences/2024/workshop/100677">视频</a>均已上线。</li>
  <li>[2024/10] verl 于 Ray Summit 亮相，<a href="https://www.youtube.com/watch?v=MrhMcXkXvJU&list=PLzTswPQNepXntmT8jr9WaNfqQ60QwW7-U&index=37">Youtube 视频</a>可观看。</li>
  <li>[2024/08] HybridFlow (verl) 被 EuroSys 2025 接收。</li>
</ul>
</details>
<h2>主要特性</h2>
<ul>
<li>训练支持 <strong>FSDP</strong>、<strong>FSDP2</strong> 和 <strong>Megatron-LM</strong>。</li>
<li>推理生成支持 <strong>vLLM</strong>、<strong>SGLang</strong> 和 <strong>HF Transformers</strong>。</li>
<li>兼容 Hugging Face Transformers 及 Modelscope Hub：<a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/grpo_trainer/run_qwen3-8b.sh">Qwen-3</a>、Qwen-2.5、Llama3.1、Gemma2、DeepSeek-LLM 等。</li>
<li>支持有监督微调。</li>
<li>强化学习支持 <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/ppo_trainer/">PPO</a>、<a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/grpo_trainer/">GRPO</a>、<a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/remax_trainer/">ReMax</a>、<a href="https://verl.readthedocs.io/en/latest/examples/config.html#algorithm">REINFORCE++</a>、<a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/rloo_trainer/">RLOO</a>、<a href="https://raw.githubusercontent.com/volcengine/verl/main/recipe/prime/">PRIME</a>、<a href="https://raw.githubusercontent.com/volcengine/verl/main/recipe/dapo/">DAPO</a>、<a href="https://raw.githubusercontent.com/volcengine/verl/main/recipe/drgrpo">DrGRPO</a> 等。
<ul>
<li>支持基于模型的奖励与基于函数的可验证奖励，适用于数学、<a href="https://raw.githubusercontent.com/volcengine/verl/main/recipe/dapo">代码</a>等领域。</li>
<li>支持视觉-语言模型（VLM）及<a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/grpo_trainer/run_qwen2_5_vl-7b.sh">多模态 RL</a>，如 Qwen2.5-vl、Kimi-VL。</li>
<li><a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/sglang_multiturn">多轮工具调用</a></li>
</ul>
</li>
<li>LLM 对齐配方如 <a href="https://raw.githubusercontent.com/volcengine/verl/main/recipe/sppo">自博弈偏好优化（SPPO）</a></li>
<li>Flash attention 2、<a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/ppo_trainer/run_qwen2-7b_seq_balance.sh">序列打包</a>、<a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/ppo_trainer/run_deepseek7b_llm_sp2.sh">序列并行</a>（DeepSpeed Ulysses）、<a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/sft/gsm8k/run_qwen_05_peft.sh">LoRA</a>、<a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/sft/gsm8k/run_qwen_05_sp2_liger.sh">Liger-kernel</a>。</li>
<li>支持 671B 超大模型与数百卡训练，具备 <a href="https://raw.githubusercontent.com/volcengine/verl/main/pull/1467">专家并行</a> 能力。</li>
<li>多卡 <a href="https://verl.readthedocs.io/en/latest/advance/ppo_lora.html">LoRA RL</a> 节省显存。</li>
<li>支持 wandb、swanlab、mlflow、tensorboard 等实验追踪。</li>
</ul>
<h2>即将上线特性与变更</h2>
<ul>
<li>路线图 https://github.com/volcengine/verl/issues/710</li>
<li>DeepSeek 671b 在 Megatron v0.11 下的优化 https://github.com/volcengine/verl/issues/708</li>
<li>多轮 rollout 与工具调用优化 https://github.com/volcengine/verl/issues/1882</li>
<li>环境交互支持 https://github.com/volcengine/verl/issues/1172</li>
<li>v0.3 以来的重大变更列表 https://github.com/volcengine/verl/discussions/943，entropy_coeff 默认值为 0</li>
<li>RL 的 Lora 支持 https://github.com/volcengine/verl/pull/1127</li>
</ul>
<h2>快速上手</h2>
<p><a href="https://verl.readthedocs.io/en/latest/index.html"><b>文档中心</b></a></p>
<p><strong>快速入门：</strong></p>
<ul>
<li><a href="https://verl.readthedocs.io/en/latest/start/install.html">安装教程</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/start/quickstart.html">快速上手</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/hybrid_flow.html">编程指南</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/algo/ppo.html">verl 中的 PPO</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/algo/grpo.html">verl 中的 GRPO</a></li>
</ul>
<p><strong>PPO 示例分步运行：</strong></p>
<ul>
<li><a href="https://verl.readthedocs.io/en/latest/preparation/prepare_data.html">后训练数据准备</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/preparation/reward_function.html">奖励函数实现</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/examples/ppo_code_architecture.html">PPO 示例架构</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/examples/config.html">配置文件说明</a></li>
</ul>
<p><strong>可复现算法基线：</strong></p>
<ul>
<li><a href="https://verl.readthedocs.io/en/latest/algo/baseline.html">代码、数学 RL 性能</a></li>
</ul>
<p><strong>代码说明与进阶扩展：</strong></p>
<ul>
<li><p>PPO Trainer 与 Workers</p>
<ul>
<li><a href="https://verl.readthedocs.io/en/latest/workers/ray_trainer.html">PPO Ray Trainer</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/workers/fsdp_workers.html">PyTorch FSDP 后端</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/index.html">Megatron-LM 后端</a></li>
</ul>
</li>
<li><p>高级用法与扩展</p>
<ul>
<li><a href="https://verl.readthedocs.io/en/latest/advance/fsdp_extension.html">添加 FSDP 后端模型</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/advance/megatron_extension.html">添加 Megatron-LM 后端模型</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/sglang_multiturn/multiturn.html">多轮 Rollout 支持</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/sglang_multiturn/search_tool_example.html">集成搜索工具</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/examples/sandbox_fusion_example.html">沙盒融合集成</a></li>
<li><a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/split_placement">分卡部署</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/advance/dpo_extension.html">扩展到其他 RL(HF) 算法</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/advance/placement.html">Ray API 设计教程</a></li>
</ul>
</li>
</ul>
<p><strong>社区博客</strong></p>
<ul>
<li><a href="https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/verl/multi-turn/verl-multiturn-rollout-Release.md">SGLang、verl、OpenBMB 与清华大学：多轮 RLHF 端到端突破</a></li>
<li><a href="https://rocm.blogs.amd.com/artificial-intelligence/verl-large-scale/README.html">verl 与 ROCm 集成于 AMD GPU 上实现人类反馈强化学习</a></li>
<li><a href="https://mp.weixin.qq.com/s/7nbqxk4knMGd-hQE9ls2tA">veMLP x verl ：玩转强化学习训练</a></li>
<li><a href="https://www.volcengine.com/docs/6459/1463942">使用 verl 进行 GRPO 分布式强化学习训练最佳实践</a></li>
<li><a href="https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/verl/readme.md">HybridFlow verl 原文浅析</a></li>
<li><a href="https://team.doubao.com/en/blog/%E6%9C%80%E9%AB%98%E6%8F%90%E5%8D%8720%E5%80%8D%E5%90%9E%E5%90%90%E9%87%8F-%E8%B1%86%E5%8C%85%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9B%A2%E9%98%9F%E5%8F%91%E5%B8%83%E5%85%A8%E6%96%B0-rlhf-%E6%A1%86%E6%9E%B6-%E7%8E%B0%E5%B7%B2%E5%BC%80%E6%BA%90">最高提升 20 倍吞吐量！豆包大模型团队发布全新 RLHF 框架，现已开源！</a></li>
</ul>
<h2>性能调优指南</h2>
<p>强化学习（on-policy RL）性能至关重要。我们已撰写详细的<a href="https://verl.readthedocs.io/en/latest/perf/perf_tuning.html">性能调优指南</a>，帮助你优化训练效率。</p>
<h2>升级到 vLLM &gt;= v0.8.2</h2>
<p>verl 现已支持在 FSDP 训练后端下使用 vLLM&gt;=0.8.2。请参考<a href="https://raw.githubusercontent.com/volcengine/verl/main/docs/README_vllm0.8.md">安装与说明文档</a>。请避免使用 vllm 0.7.x，该版本存在导致 OOM 和异常的已知 bug。</p>
<h2>最新 SGLang 支持</h2>
<p>verl 全面支持 SGLang，SGLang RL 团队持续开发如多轮智能体 RL、VLM RLHF、服务端 RL、部分 rollout 等功能。安装与说明详见<a href="https://verl.readthedocs.io/en/latest/workers/sglang_worker.html">文档</a>。</p>
<h2>升级到 FSDP2</h2>
<p>verl 已全面支持 FSDP2！FSDP2 由 torch distributed 团队推荐，具备更优吞吐和显存占用，并可与 torch.compile 等特性组合。启用 FSDP2，只需在 verl main 设置如下选项：</p>
<pre><code>actor_rollout_ref.ref.strategy=fsdp2
actor_rollout_ref.actor.strategy=fsdp2
critic.strategy=fsdp2 
reward_model.strategy=fsdp2 
</code></pre>
<p>此外，FSDP2 支持 CPU offload 与梯度累积兼容。可通过 <code>actor_rollout_ref.actor.offload_policy=True</code> 节省显存。详细说明见 https://github.com/volcengine/verl/pull/1026</p>
<h2>支持 AMD (ROCm 内核)</h2>
<p>verl 现已支持 FSDP 训练引擎（Megatron 支持即将上线），并可集成 vLLM、SGLang 推理引擎。安装与说明详见<a href="https://raw.githubusercontent.com/volcengine/verl/main/docs/amd_tutorial/amd_build_dockerfile_page.rst">AMD 教程文档</a>，vLLM 性能调优见<a href="https://raw.githubusercontent.com/volcengine/verl/main/docs/amd_tutorial/amd_vllm_page.rst">此文档</a>。</p>
<h2>引用与致谢</h2>
<p>如果本项目对您有帮助，请引用：</p>
<ul>
<li><a href="https://arxiv.org/abs/2409.19256v2">HybridFlow: A Flexible and Efficient RLHF Framework</a></li>
<li><a href="https://i.cs.hku.hk/%7Ecwu/papers/gmsheng-NL2Code24.pdf">A Framework for Training Large Language Models for Code Generation via Proximal Policy Optimization</a></li>
</ul>
<pre><code class="language-bibtex">@article{sheng2024hybridflow,
  title   = {HybridFlow: A Flexible and Efficient RLHF Framework},
  author  = {Guangming Sheng and Chi Zhang and Zilingfeng Ye and Xibin Wu and Wang Zhang and Ru Zhang and Yanghua Peng and Haibin Lin and Chuan Wu},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2409.19256}
}
</code></pre>
<p>verl 受到 Nemo-Aligner、Deepspeed-chat、OpenRLHF 等项目设计启发，并获得字节跳动、Anyscale、LMSys.org、<a href="https://github.com/QwenLM/">阿里巴巴 Qwen 团队</a>、上海 AI Lab、清华大学、UC Berkeley、UCLA、UIUC、香港大学、ke.com、<a href="https://www.all-hands.dev/">All Hands AI</a>、<a href="http://modelbest.cn/">ModelBest</a>、OpenPipe、京东 AI Lab、微软研究院、<a href="https://www.stepfun.com/">StepFun</a>、亚马逊、Linkedin、美团、<a href="https://www.camel-ai.org/">Camel-AI</a>、<a href="https://github.com/OpenManus">OpenManus</a>、小米、Prime Intellect、NVIDIA 研究院、<a href="https://www.baichuan-ai.com/home">百川</a>、<a href="https://www.xiaohongshu.com/">小红书</a>、<a href="https://www.swiss-ai.org/">SwissAI</a>、<a href="https://www.moonshot-ai.com/">Moonshot AI (Kimi)</a>、百度、Snowflake 等众多机构采用及贡献。</p>
<h2>基于 verl 的优秀开源项目</h2>
<ul>
<li><a href="https://github.com/Jiayi-Pan/TinyZero">TinyZero</a>：<strong>DeepSeek R1 Zero</strong> 配方复现！<a href="https://img.shields.io/github/stars/Jiayi-Pan/TinyZero">GitHub Repo stars</a></li>
<li><a href="https://github.com/NovaSky-AI/SkyThought">SkyThought</a>：NovaSky AI 团队 Sky-T1-7B 强化学习训练。<a href="https://img.shields.io/github/stars/NovaSky-AI/SkyThought">GitHub Repo stars</a></li>
<li><a href="https://github.com/hkust-nlp/simpleRL-reason">simpleRL-reason</a>：SimpleRL-Zoo：开放基座模型零样本强化学习探索！<a href="https://img.shields.io/github/stars/hkust-nlp/simpleRL-reason">GitHub Repo stars</a></li>
<li><a href="https://github.com/hiyouga/EasyR1">Easy-R1</a>：<strong>多模态</strong> RL 训练框架！<a href="https://img.shields.io/github/stars/hiyouga/EasyR1">GitHub Repo stars</a></li>
<li><a href="https://github.com/OpenManus/OpenManus-RL">OpenManus-RL</a>：多智能体环境 LLM Agents RL 调优框架。<a href="https://img.shields.io/github/stars/OpenManus/OpenManus-RL">GitHub Repo stars</a></li>
<li><a href="https://github.com/agentica-project/rllm">rllm</a>：<a href="https://github.com/agentica-project/verl-pipeline">verl-pipeline</a> 实现异步 RL 训练！<a href="https://img.shields.io/github/stars/agentica-project/rllm">GitHub Repo stars</a></li>
<li><a href="https://github.com/PRIME-RL/PRIME">PRIME</a>：隐式奖励流程强化！<a href="https://img.shields.io/github/stars/PRIME-RL/PRIME">GitHub Repo stars</a></li>
<li><a href="https://github.com/ZihanWang314/ragen">RAGEN</a>：通用推理智能体训练框架！<a href="https://img.shields.io/github/stars/ZihanWang314/ragen">GitHub Repo stars</a></li>
<li><a href="https://github.com/PeterGriffinJin/Search-R1">Search-R1</a>：推理与**搜索（工具调用）**交互型 LLM RL！<a href="https://img.shields.io/github/stars/PeterGriffinJin/Search-R1">GitHub Repo stars</a></li>
<li><a href="https://github.com/pat-jj/DeepRetrieval">DeepRetrieval</a>：<strong>搜索智能体</strong> RL 训练！<a href="https://img.shields.io/github/stars/pat-jj/DeepRetrieval">GitHub Repo stars</a></li>
<li><a href="https://github.com/Agent-RL/ReSearch">ReSearch</a>：LLM 搜索推理学习 RL！<a href="https://img.shields.io/github/stars/Agent-RL/ReSearch">GitHub Repo stars</a></li>
<li><a href="https://github.com/ganler/code-r1">Code-R1</a>：可靠奖励代码 RL 复现！<a href="https://img.shields.io/github/stars/ganler/code-r1">GitHub Repo stars</a></li>
<li><a href="https://github.com/SkyworkAI/Skywork-OR1">Skywork-OR1</a>：Skywork 开放推理系列！<a href="https://img.shields.io/github/stars/SkyworkAI/Skywork-OR1">GitHub Repo stars</a></li>
<li><a href="https://github.com/GAIR-NLP/ToRL">ToRL</a>：工具集成 RL 扩展！<a href="https://img.shields.io/github/stars/GAIR-NLP/ToRL">GitHub Repo stars</a></li>
<li><a href="https://github.com/langfengQ/verl-agent">verl-agent</a>：可扩展的长程 LLM/VLM 智能体 RL 训练框架，含新算法 <strong>GiGPO</strong>！<a href="https://img.shields.io/github/stars/langfengQ/verl-agent">GitHub Repo stars</a></li>
<li><a href="https://arxiv.org/abs/2409.06957">PF-PPO</a>：基于奖励信号可靠性进行策略过滤的 PPO，高效鲁棒 RLHF。</li>
<li><a href="https://github.com/ritzz-ai/GUI-R1">GUI-R1</a>：<strong>GUI-R1</strong>：面向 GUI Agent 的视觉-语言动作模型！<a href="https://img.shields.io/github/stars/ritzz-ai/GUI-R1">GitHub Repo stars</a></li>
<li><a href="https://github.com/GAIR-NLP/DeepResearcher">DeepResearcher</a>：真实环境下深度科研 RL 扩展！<a href="https://img.shields.io/github/stars/GAIR-NLP/DeepResearcher">GitHub Repo stars</a></li>
<li><a href="https://github.com/RAGEN-AI/VAGEN">VAGEN</a>：多轮 RL 训练 VLM 智能体！<a href="https://img.shields.io/github/stars/RAGEN-AI/VAGEN">GitHub Repo stars</a></li>
<li><a href="https://retool-rl.github.io/">ReTool</a>：ReTool：大模型工具使用策略 RL，代码即将开源...</li>
<li><a href="https://arxiv.org/abs/2505.02387">RM-R1</a>：推理奖励模型 RL 训练！<a href="https://img.shields.io/github/stars/RM-R1-UIUC/RM-R1">GitHub Repo stars</a></li>
<li><a href="https://arxiv.org/abs/2505.03335">Absolute Zero Reasoner</a>：零人类标注数据自博弈推理框架！<a href="https://img.shields.io/github/stars/LeapLabTHU/Absolute-Zero-Reasoner">GitHub Repo stars</a></li>
<li><a href="https://arxiv.org/pdf/2504.14945">LUFFY</a>：离策略引导下的推理学习！<a href="https://img.shields.io/github/stars/ElliottYan/LUFFY">GitHub Repo stars</a></li>
<li><a href="https://github.com/TIGER-AI-Lab/verl-tool">verl-tool</a>：基于 verl 的统一可扩展工具智能体训练框架！<a href="https://img.shields.io/github/stars/TIGER-AI-Lab/verl-tool">GitHub Repo stars</a></li>
<li><a href="https://github.com/zwhe99/DeepMath">DeepMath</a>：数学推理 DeepMath-103K 数据与系列模型！<a href="https://img.shields.io/github/stars/zwhe99/DeepMath">GitHub Repo stars</a></li>
</ul>
<p>更多优秀项目见 <a href="https://raw.githubusercontent.com/volcengine/verl/main/recipe/README.md">recipe</a>。</p>
<h2>贡献指南</h2>
<p>欢迎社区贡献！请查阅<a href="https://github.com/volcengine/verl/issues/710">项目路线图</a>和<a href="https://github.com/volcengine/verl/issues?q=is%3Aissue%20state%3Aopen%20label%3A%22good%20first%20issue%22">good first issue</a> 寻找你能贡献的领域。</p>
<h3>代码风格与格式化</h3>
<p>我们采用 pre-commit 工具提升代码质量。初始化方法：</p>
<pre><code class="language-bash">pip install pre-commit
pre-commit install
</code></pre>
<p>如需本地修复 CI 错误，可手动执行：</p>
<pre><code class="language-bash">pre-commit run
</code></pre>
<h3>添加 CI 测试</h3>
<p>如条件允许，请为新功能添加 CI 测试：</p>
<ol>
<li>找到最相关的 workflow yml 文件，通常对应一个 hydra 默认配置（如 <code>ppo_trainer</code>、<code>ppo_megatron_trainer</code>、<code>sft_trainer</code> 等）。</li>
<li>如未包含，请将相关路径模式加入 paths 部分。</li>
<li>测试脚本尽量减小工作量，可参考现有脚本。</li>
</ol>
<h2>关于 <a href="https://team.doubao.com/">字节跳动 Seed 团队</a></h2>
<p>字节跳动 Seed 团队成立于 2023 年，致力于打造业界领先的 AI 基础模型，愿成为世界级研究团队，为科学与社会进步做出贡献。了解更多 Seed 团队渠道👇</p>
<div>
  <a href="https://team.doubao.com/">
    <img src="https://img.shields.io/badge/Website-%231e37ff?style=for-the-badge&logo=bytedance&logoColor=white"></a>
  <a href="https://github.com/user-attachments/assets/469535a8-42f2-4797-acdf-4f7a1d4a0c3e">
    <img src="https://img.shields.io/badge/WeChat-07C160?style=for-the-badge&logo=wechat&logoColor=white"></a>
 <a href="https://www.xiaohongshu.com/user/profile/668e7e15000000000303157d?xsec_token=ABl2-aqekpytY6A8TuxjrwnZskU-6BsMRE_ufQQaSAvjc%3D&xsec_source=pc_search">
    <img src="https://img.shields.io/badge/Xiaohongshu-%23FF2442?style=for-the-badge&logo=xiaohongshu&logoColor=white"></a>
  <a href="https://www.zhihu.com/org/dou-bao-da-mo-xing-tuan-dui/">
    <img src="https://img.shields.io/badge/zhihu-%230084FF?style=for-the-badge&logo=zhihu&logoColor=white"></a>
</div>
---
<p>我们正在招聘！如有兴趣参与智能体 RL 实习/全职岗位，请发 <a href="mailto:haibin.lin@bytedance.com">邮件</a> 联系。</p>
<hr />
<p>Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx">Open Ai Tx</a> | Last indexed: 2025-06-07</p>
<hr />

        </div>
    </div>
    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Default Statcounter code for openaitx
https://openaitx.github.io/ -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>
    <!-- End of Statcounter Code -->
</body>
</html>