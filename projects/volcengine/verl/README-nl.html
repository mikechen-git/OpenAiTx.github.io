<!DOCTYPE html>
<html lang="nl">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>verl - volcengine/verl nl</title>
    <meta name="title" content="verl - volcengine/verl nl | 👋 Hallo allemaal! verl is een RL-trainingsbibliotheek geïnitieerd door het ByteDance Seed team en onderhouden door de verl-community. verl: Volcano Engine Rein...">
    <meta name="description" content="volcengine/verl - GitHub repository nl documentation and information | 👋 Hallo allemaal! verl is een RL-trainingsbibliotheek geïnitieerd door het ByteDance Seed team en onderhouden door de verl-community. verl: Volcano Engine Rein...">
    <meta name="keywords" content="volcengine, verl, GitHub, repository, nl documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/volcengine/verl/README-nl.html">
    <meta property="og:title" content="verl - volcengine/verl nl | 👋 Hallo allemaal! verl is een RL-trainingsbibliotheek geïnitieerd door het ByteDance Seed team en onderhouden door de verl-community. verl: Volcano Engine Rein...">
    <meta property="og:description" content="volcengine/verl - GitHub repository nl documentation and information | 👋 Hallo allemaal! verl is een RL-trainingsbibliotheek geïnitieerd door het ByteDance Seed team en onderhouden door de verl-community. verl: Volcano Engine Rein...">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">
    <script src="/js/marked.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        body { display: flex; flex-direction: column; min-height: 100vh; }
        .main-container { margin: 0 auto; width: 100%; max-width: 980px; padding: 0 20px; }
        @media (max-width: 768px) { .main-container { padding: 0 15px; } }
        .markdown-body img { max-width: 100%; height: auto; }
        .header { text-align: center; margin-bottom: 30px; padding: 20px; background-color: #f6f8fa; border-bottom: 1px solid #e1e4e8; position: relative; }
        .header .links { margin-top: 10px; font-size: 16px; }
        .header .links a { color: #0366d6; text-decoration: none; margin-left: 5px; }
        .header .links a:hover { text-decoration: underline; }
        .language-badges { margin-top: 15px; text-align: center; }
        .language-badges a { display: inline-block; margin: 2px; text-decoration: none; }
        .language-badges img { height: 20px; border-radius: 3px; }
        .language-badges a:hover img { opacity: 0.8; }
    </style>
</head>
<body>
    <div style="position: fixed; top: 2px; left: 2px; z-index: 2000; background: rgba(255,255,255,0.95); border-radius: 6px; box-shadow: 0 2px 8px rgba(0,0,0,0.06); padding: 4px 14px; font-size: 15px; color: #222; font-family: 'Segoe UI', Arial, sans-serif; font-weight: 500; letter-spacing: 0.5px;">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" style="color: #0366d6; text-decoration: none; font-weight: 600;">Open AI Tx</a>
    </div>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/volcengine/verl" id="githubRepoLink" target="_blank">volcengine/verl</a>
<h1 style="display: none;">👋 Hallo allemaal! verl is een RL-trainingsbibliotheek geïnitieerd door het ByteDance Seed team en onderhouden door de verl-community. verl: Volcano Engine Rein...</h1>
        </div>
        <div class="language-badges" id="languageBadges">
            <!-- You can generate language badges here if needed -->
        </div>
    </div>
    <div class="main-container">
        <div class="markdown-body" id="content">
            <div align="center">
 👋 Hallo allemaal! 
    verl is een RL-trainingsbibliotheek geïnitieerd door het <b>ByteDance Seed team</b> en onderhouden door de verl-community.
    <br>
    <br>
</div>
<div align="center">
<p><a href="https://deepwiki.com/volcengine/verl"><img src="https://devin.ai/assets/deepwiki-badge.png" alt="Ask DeepWiki.com" height="20"/></a>
<a href="https://github.com/volcengine/verl/stargazers"><img src="https://img.shields.io/github/stars/volcengine/verl" alt="GitHub Repo stars" /></a>
<a href="https://twitter.com/verl_project"><img src="https://img.shields.io/twitter/follow/verl_project" alt="Twitter" /></a>
<a href="https://join.slack.com/t/verlgroup/shared_invite/zt-2w5p9o4c3-yy0x2Q56s_VlGLsJ93A6vA"><img src="https://img.shields.io/badge/Slack-verl-blueviolet?logo=slack&amp"></a>
<a href="https://arxiv.org/pdf/2409.19256"><img src="https://img.shields.io/static/v1?label=EuroSys&message=Paper&color=red"></a>
<a href="https://verl.readthedocs.io/en/latest/"><img src="https://img.shields.io/badge/documentatie-blauw" alt="Documentation" /></a>
<a href="https://raw.githubusercontent.com/eric-haibin-lin/verl-community/refs/heads/main/WeChat.JPG"><img src="https://img.shields.io/badge/微信-green?logo=wechat&amp"></a></p>
</div>
<p><img src="https://github.com/user-attachments/assets/c42e675e-497c-4508-8bb9-093ad4d1f216" alt="seed logo" /></p>
<h1 style="text-align: center;">verl: Volcano Engine Reinforcement Learning voor LLMs</h1>
<p>verl is een flexibele, efficiënte en productieklare RL-trainingsbibliotheek voor grote taalmodellen (LLMs).</p>
<p>verl is de open-sourceversie van het <strong><a href="https://arxiv.org/abs/2409.19256v2">HybridFlow: A Flexible and Efficient RLHF Framework</a></strong> paper.</p>
<p>verl is flexibel en eenvoudig te gebruiken dankzij:</p>
<ul>
<li><p><strong>Eenvoudige uitbreiding van diverse RL-algoritmes</strong>: Het hybrid-controller programmeermodel maakt flexibele representatie en efficiënte uitvoering van complexe post-training dataflows mogelijk. Bouw RL-dataflows zoals GRPO, PPO in slechts enkele regels code.</p>
</li>
<li><p><strong>Naadloze integratie van bestaande LLM-infrastructuur met modulaire API's</strong>: Ontkoppelt rekenkundige en gegevensafhankelijkheden, waardoor naadloze integratie met bestaande LLM-frameworks zoals FSDP, Megatron-LM, vLLM, SGLang, etc. mogelijk is.</p>
</li>
<li><p><strong>Flexibele device mapping</strong>: Ondersteunt verschillende plaatsingen van modellen op diverse sets GPU's voor efficiënte resourcebenutting en schaalbaarheid over verschillende clusterformaten.</p>
</li>
<li><p>Directe integratie met populaire HuggingFace-modellen</p>
</li>
</ul>
<p>verl is snel dankzij:</p>
<ul>
<li><p><strong>State-of-the-art doorvoer</strong>: SOTA LLM training en inference engine-integraties en SOTA RL-doorvoer.</p>
</li>
<li><p><strong>Efficiënt herverdelen van actor-modellen met 3D-HybridEngine</strong>: Elimineert geheugenredundantie en vermindert de communicatielast aanzienlijk tijdens overgangen tussen training en generatiefase.</p>
</li>
</ul>
</p>
<h2>Nieuws</h2>
<ul>
<li>[2025/06] verl met Megatron-backend maakt grote MoE-modellen mogelijk zoals <a href="https://verl.readthedocs.io/en/latest/perf/dpsk.html">DeepSeek-671b en Qwen3-236b</a>.</li>
<li>[2025/06] verl-team zal de laatste projectupdates geven op <a href="https://www.lfasiallc.com/pytorch-day-china/">PyTorch Day China</a> op 7 juni. Ontmoet ons dev-team in Beijing!</li>
<li>[2025/05] <a href="https://arxiv.org/abs/2409.06957">PF-PPO</a>, geaccepteerd voor ICML 2025, wordt nu ondersteund in verl! PF-PPO verbetert de efficiëntie en robuustheid van policy learning door mogelijk ruisige reward-signalen te filteren en hoogwaardige ervaringen te hergebruiken via een replay buffer.</li>
<li>[2025/04] We geven een tutorial over de nieuwste post-training technieken en programmeergids voor verl op <a href="https://iclr.cc/virtual/2025/calendar?filter_events=Expo+Talk+Panel&amp;filter_rooms=">ICLR 2025 Expo</a>, <a href="https://open-foundation-model.github.io/">SCI-FM workshop</a> en <a href="https://lu.ma/d23nyynm">LMSys afterparty</a>. Presentatiematerialen beschikbaar <a href="https://github.com/eric-haibin-lin/verl-community/tree/main/iclr25">hier</a>.</li>
<li>[2025/04] <a href="https://github.com/ByteDance-Seed/Seed-Thinking-v1.5/blob/main/seed-thinking-v1.5.pdf">Seed-Thinking-v1.5</a> technisch rapport is uitgebracht! Getraind met verl behaalt Seed-Thinking-v1.5 86.7 op AIME 2024, 55.0 op Codeforces en 77.3 op GPQA, wat uitstekende redeneervermogen in STEM en codering aantoont. Naast redeneertaken toont de methode opmerkelijke generalisatie over diverse domeinen.</li>
<li>[2025/04] <a href="https://arxiv.org/pdf/2504.05118">VAPO</a> (value-based augmented PPO) paper behandelt onze nieuwste RL-methode voor redeneermodellen. Getraind vanaf Qwen-32B-base model behaalt VAPO 60.4 op AIME 2024, beter dan DAPO-32B.</li>
<li>[2025/03] verl v0.3.0.post1 is uitgebracht! Zie <a href="https://github.com/volcengine/verl/releases/">releasenote</a> voor details. Het behaalt <a href="https://tongyx361.github.io/blogs/posts/verl-intro/#/verl-flexible-and-efficient-rl-for-llms">~1.4x versnelling</a> vergeleken met eerdere versies.</li>
<li>[2025/03] <a href="https://dapo-sia.github.io/">DAPO</a> is het open-source SOTA RL-algoritme dat 50 punten behaalt op AIME 2024 op basis van het Qwen2.5-32B pre-trained model, waarmee het vorige SOTA van DeepSeek's GRPO (DeepSeek-R1-Zero-Qwen-32B) wordt overtroffen. DAPO's training is volledig aangedreven door verl en de reproductiecode is nu beschikbaar in <code>recipe/dapo</code>.</li>
</ul>
<details><summary> meer... </summary>
<ul>
  <li>[2025/05] verl zal worden gepresenteerd op [A2M Shanghai](https://a2m.msup.com.cn/home/?aid=4488&city=shanghai) op 16/5 - 17/5.</li>
  <li>[2025/05] verl zal worden gepresenteerd op [GOSIM x PyTorch Day 2025](https://paris2025.gosim.org/). Tot ziens in Parijs! </li>
  <li>[2025/03] We introduceerden het programmeermodel van verl op de [vLLM Beijing Meetup](https://mp.weixin.qq.com/s/n77GibL2corAtQHtVEAzfg) en [verl intro en updates](https://github.com/eric-haibin-lin/verl-community/blob/main/slides/verl-lmsys-meetup.pdf) op de [SGLang-LMSYS Org Meetup](https://lu.ma/ntjrr7ig) in Sunnyvale medio maart.</li>
  <li>[2025/03] We presenteren verl(HybridFlow) op EuroSys 2025. Tot ziens in Rotterdam!</li>
  <li>[2025/02] verl v0.2.0.post2 is uitgebracht!</li>
  <li>[2025/02] We presenteerden verl in de <a href="https://lu.ma/ji7atxux">Bytedance/NVIDIA/Anyscale Ray Meetup</a>. Tot ziens in San Jose!</li>
  <li>[2025/01] [Doubao-1.5-pro](https://team.doubao.com/zh/special/doubao_1_5_pro) is uitgebracht met SOTA-niveau prestaties op LLM & VLM. Het RL scaling preview model is getraind met verl en bereikt OpenAI O1-niveau prestaties op wiskundige benchmarks (70.0 pass@1 op AIME).</li>
  <li>[2024/12] verl is gepresenteerd op Ray Forward 2024. Slides beschikbaar <a href="https://github.com/eric-haibin-lin/verl-community/blob/main/slides/Ray_Forward_2024_%E5%B7%AB%E9%94%A1%E6%96%8C.pdf">hier</a></li>
  <li>[2024/12] Het team presenteerde <a href="https://neurips.cc/Expo/Conferences/2024/workshop/100677">Post-training LLMs: From Algorithms to Infrastructure</a> op NeurIPS 2024. <a href="https://github.com/eric-haibin-lin/verl-data/tree/neurips">Slides</a> en <a href="https://neurips.cc/Expo/Conferences/2024/workshop/100677">video</a> beschikbaar.</li>
  <li>[2024/10] verl is gepresenteerd op Ray Summit. <a href="https://www.youtube.com/watch?v=MrhMcXkXvJU&list=PLzTswPQNepXntmT8jr9WaNfqQ60QwW7-U&index=37">Youtube-video</a> beschikbaar.</li>
  <li>[2024/08] HybridFlow (verl) is geaccepteerd voor EuroSys 2025.</li>
</ul>   
</details>
<h2>Belangrijkste Kenmerken</h2>
<ul>
<li><strong>FSDP</strong>, <strong>FSDP2</strong> en <strong>Megatron-LM</strong> voor training.</li>
<li><strong>vLLM</strong>, <strong>SGLang</strong> en <strong>HF Transformers</strong> voor rollout-generatie.</li>
<li>Compatibel met Hugging Face Transformers en Modelscope Hub: <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/grpo_trainer/run_qwen3-8b.sh">Qwen-3</a>, Qwen-2.5, Llama3.1, Gemma2, DeepSeek-LLM, enz.</li>
<li>Supervised fine-tuning.</li>
<li>Reinforcement learning met <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/ppo_trainer/">PPO</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/grpo_trainer/">GRPO</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/remax_trainer/">ReMax</a>, <a href="https://verl.readthedocs.io/en/latest/examples/config.html#algorithm">REINFORCE++</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/rloo_trainer/">RLOO</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/recipe/prime/">PRIME</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/recipe/dapo/">DAPO</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/recipe/drgrpo">DrGRPO</a>, enz.
<ul>
<li>Ondersteunt model-based reward en function-based reward (verifieerbare beloning) voor wiskunde, <a href="https://raw.githubusercontent.com/volcengine/verl/main/recipe/dapo">coderen</a>, enz.</li>
<li>Ondersteunt vision-language modellen (VLMs) en <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/grpo_trainer/run_qwen2_5_vl-7b.sh">multi-modale RL</a> met Qwen2.5-vl, Kimi-VL</li>
<li><a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/sglang_multiturn">Multi-turn met tool calling</a></li>
</ul>
</li>
<li>LLM alignment recepten zoals <a href="https://raw.githubusercontent.com/volcengine/verl/main/recipe/sppo">Self-play preference optimization (SPPO)</a></li>
<li>Flash attention 2, <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/ppo_trainer/run_qwen2-7b_seq_balance.sh">sequence packing</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/ppo_trainer/run_deepseek7b_llm_sp2.sh">sequence parallelism</a> ondersteuning via DeepSpeed Ulysses, <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/sft/gsm8k/run_qwen_05_peft.sh">LoRA</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/sft/gsm8k/run_qwen_05_sp2_liger.sh">Liger-kernel</a>.</li>
<li>Schaalbaar tot 671B modellen en honderden GPU's met <a href="https://github.com/volcengine/verl/pull/1467">expert parallelism</a></li>
<li>Multi-gpu <a href="https://verl.readthedocs.io/en/latest/advance/ppo_lora.html">LoRA RL</a> ondersteuning om geheugen te besparen.</li>
<li>Experiment tracking met wandb, swanlab, mlflow en tensorboard.</li>
</ul>
<h2>Verwachte Functies en Wijzigingen</h2>
<ul>
<li>Roadmap https://github.com/volcengine/verl/issues/710</li>
<li>DeepSeek 671b optimalisaties met Megatron v0.11 https://github.com/volcengine/verl/issues/708</li>
<li>Multi-turn rollout en tools optimalisaties https://github.com/volcengine/verl/issues/1882</li>
<li>Omgevingsinteracties https://github.com/volcengine/verl/issues/1172</li>
<li>Lijst van breaking changes sinds v0.3 https://github.com/volcengine/verl/discussions/943, entropy_coeff standaard op 0</li>
<li>Lora voor RL https://github.com/volcengine/verl/pull/1127</li>
</ul>
<h2>Aan de Slag</h2>
<p><a href="https://verl.readthedocs.io/en/latest/index.html"><b>Documentatie</b></a></p>
<p><strong>Snelstart:</strong></p>
<ul>
<li><a href="https://verl.readthedocs.io/en/latest/start/install.html">Installatie</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/start/quickstart.html">Snelstart</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/hybrid_flow.html">Programmeerhandleiding</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/algo/ppo.html">PPO in verl</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/algo/grpo.html">GRPO in verl</a></li>
</ul>
<p><strong>Een PPO-voorbeeld stap-voor-stap uitvoeren:</strong></p>
<ul>
<li><a href="https://verl.readthedocs.io/en/latest/preparation/prepare_data.html">Data voorbereiden voor post-training</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/preparation/reward_function.html">Beloningsfunctie implementeren voor dataset</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/examples/ppo_code_architecture.html">PPO Voorbeeldarchitectuur</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/examples/config.html">Config Uitleg</a></li>
</ul>
<p><strong>Reproduceerbare algoritme-benchmarks:</strong></p>
<ul>
<li><a href="https://verl.readthedocs.io/en/latest/algo/baseline.html">RL-prestaties op coderen, wiskunde</a></li>
</ul>
<p><strong>Voor codeuitleg en gevorderd gebruik (uitbreiding):</strong></p>
<ul>
<li><p>PPO Trainer en Workers</p>
<ul>
<li><a href="https://verl.readthedocs.io/en/latest/workers/ray_trainer.html">PPO Ray Trainer</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/workers/fsdp_workers.html">PyTorch FSDP Backend</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/index.html">Megatron-LM Backend</a></li>
</ul>
</li>
<li><p>Geavanceerd gebruik en uitbreiding</p>
<ul>
<li><a href="https://verl.readthedocs.io/en/latest/advance/fsdp_extension.html">Voeg Modellen toe met de FSDP Backend</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/advance/megatron_extension.html">Voeg Modellen toe met de Megatron-LM Backend</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/sglang_multiturn/multiturn.html">Multi-turn Rollout Ondersteuning</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/sglang_multiturn/search_tool_example.html">Zoektool Integratie</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/examples/sandbox_fusion_example.html">Sandbox Fusion Integratie</a></li>
<li><a href="https://github.com/volcengine/verl/tree/main/examples/split_placement">Deployen met gescheiden GPU-resources</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/advance/dpo_extension.html">Uitbreiden naar andere RL(HF) algoritmes</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/advance/placement.html">Ray API ontwerp tutorial</a></li>
</ul>
</li>
</ul>
<p><strong>Blogs uit de community</strong></p>
<ul>
<li><a href="https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/verl/multi-turn/verl-multiturn-rollout-Release.md">SGLang, verl, OpenBMB en Tsinghua University: Pionieren met End-to-End Multi-Turn RLHF</a></li>
<li><a href="https://rocm.blogs.amd.com/artificial-intelligence/verl-large-scale/README.html">Reinforcement Learning van Menselijke Feedback op AMD GPU's met verl en ROCm-integratie</a></li>
<li><a href="https://mp.weixin.qq.com/s/7nbqxk4knMGd-hQE9ls2tA">veMLP x verl ：Spelen met reinforcement learning training</a></li>
<li><a href="https://www.volcengine.com/docs/6459/1463942">Beste praktijken voor GRPO-distributieve reinforcement learning training met verl</a></li>
<li><a href="https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/verl/readme.md">HybridFlow verl originele tekstanalyse</a></li>
<li><a href="https://team.doubao.com/en/blog/%E6%9C%80%E9%AB%98%E6%8F%90%E5%8D%8720%E5%80%8D%E5%90%9E%E5%90%90%E9%87%8F-%E8%B1%86%E5%8C%85%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9B%A2%E9%98%9F%E5%8F%91%E5%B8%83%E5%85%A8%E6%96%B0-rlhf-%E6%A1%86%E6%9E%B6-%E7%8E%B0%E5%B7%B2%E5%BC%80%E6%BA%90">Tot 20x hogere doorvoer! Doubao groot modelteam brengt volledig nieuw RLHF-framework uit, nu open-source!</a></li>
</ul>
<h2>Prestatie-tuning Gids</h2>
<p>De prestatie is essentieel voor on-policy RL-algoritmen. We hebben een gedetailleerde <a href="https://verl.readthedocs.io/en/latest/perf/perf_tuning.html">prestatie-tuning gids</a> geschreven om je te helpen de prestaties te optimaliseren.</p>
<h2>Upgrade naar vLLM &gt;= v0.8.2</h2>
<p>verl ondersteunt nu vLLM&gt;=0.8.2 bij gebruik van FSDP als trainingsbackend. Raadpleeg <a href="https://github.com/volcengine/verl/blob/main/docs/README_vllm0.8.md">dit document</a> voor de installatiehandleiding en meer informatie. Vermijd vllm 0.7.x, dit bevat bugs die kunnen leiden tot OOMs en onverwachte fouten.</p>
<h2>Gebruik de nieuwste SGLang</h2>
<p>SGLang wordt volledig ondersteund door verl, en de SGLang RL Group werkt intensief aan unieke features, waaronder multi-turn agentische RL, VLM RLHF, servergebaseerde RL en gedeeltelijke rollout. Raadpleeg <a href="https://verl.readthedocs.io/en/latest/workers/sglang_worker.html">dit document</a> voor installatiehandleiding en meer informatie.</p>
<h2>Upgrade naar FSDP2</h2>
<p>verl omarmt FSDP2 volledig! FSDP2 wordt aanbevolen door het torch distributed team, biedt betere doorvoer en geheugengebruik, en is samenstelbaar met andere features (bijv. torch.compile). Om FSDP2 te activeren, gebruik verl main en stel de volgende opties in:</p>
<pre><code>actor_rollout_ref.ref.strategy=fsdp2
actor_rollout_ref.actor.strategy=fsdp2
critic.strategy=fsdp2 
reward_model.strategy=fsdp2 
</code></pre>
<p>Bovendien is FSDP2 CPU offloading compatibel met graduele accumulatie. Je kunt dit inschakelen om geheugen te besparen met <code>actor_rollout_ref.actor.offload_policy=True</code>. Voor meer details, zie https://github.com/volcengine/verl/pull/1026</p>
<h2>AMD-ondersteuning (ROCm Kernel)</h2>
<p>verl ondersteunt nu FSDP als training engine (Megatron-ondersteuning komt binnenkort) en integreert zowel met vLLM als SGLang als inference engines. Raadpleeg <a href="https://github.com/volcengine/verl/blob/main/docs/amd_tutorial/amd_build_dockerfile_page.rst">dit document</a> voor de installatiehandleiding en meer informatie, en <a href="https://github.com/volcengine/verl/blob/main/docs/amd_tutorial/amd_vllm_page.rst">dit document</a> voor vLLM prestatie-tuning voor ROCm.</p>
<h2>Citation en erkenning</h2>
<p>Als je het project nuttig vindt, citeer dan:</p>
<ul>
<li><a href="https://arxiv.org/abs/2409.19256v2">HybridFlow: A Flexible and Efficient RLHF Framework</a></li>
<li><a href="https://i.cs.hku.hk/%7Ecwu/papers/gmsheng-NL2Code24.pdf">A Framework for Training Large Language Models for Code Generation via Proximal Policy Optimization</a></li>
</ul>
<pre><code class="language-bibtex">@article{sheng2024hybridflow,
  title   = {HybridFlow: A Flexible and Efficient RLHF Framework},
  author  = {Guangming Sheng and Chi Zhang and Zilingfeng Ye and Xibin Wu and Wang Zhang and Ru Zhang and Yanghua Peng and Haibin Lin and Chuan Wu},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2409.19256}
}
</code></pre>
<p>verl is geïnspireerd op het ontwerp van Nemo-Aligner, Deepspeed-chat en OpenRLHF. Het project is geadopteerd en bijgedragen door Bytedance, Anyscale, LMSys.org, <a href="https://github.com/QwenLM/">Alibaba Qwen team</a>, Shanghai AI Lab, Tsinghua University, UC Berkeley, UCLA, UIUC, University of Hong Kong, ke.com, <a href="https://www.all-hands.dev/">All Hands AI</a>, <a href="http://modelbest.cn/">ModelBest</a>, OpenPipe, JD AI Lab, Microsoft Research, <a href="https://www.stepfun.com/">StepFun</a>, Amazon, Linkedin, Meituan, <a href="https://www.camel-ai.org/">Camel-AI</a>, <a href="https://github.com/OpenManus">OpenManus</a>, Xiaomi, Prime Intellect, NVIDIA research, <a href="https://www.baichuan-ai.com/home">Baichuan</a>, <a href="https://www.xiaohongshu.com/">RedNote</a>, <a href="https://www.swiss-ai.org/">SwissAI</a>, <a href="https://www.moonshot-ai.com/">Moonshot AI (Kimi)</a>, Baidu, Snowflake, en vele anderen.</p>
<h2>Geweldig werk met verl</h2>
<ul>
<li><a href="https://github.com/Jiayi-Pan/TinyZero">TinyZero</a>: een reproductie van het <strong>DeepSeek R1 Zero</strong> recept voor redeneertaken <img src="https://img.shields.io/github/stars/Jiayi-Pan/TinyZero" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/NovaSky-AI/SkyThought">SkyThought</a>: RL-training voor Sky-T1-7B door het NovaSky AI team. <img src="https://img.shields.io/github/stars/NovaSky-AI/SkyThought" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/hkust-nlp/simpleRL-reason">simpleRL-reason</a>: SimpleRL-Zoo: Onderzoek en temmen van Zero Reinforcement Learning voor Open Base Modellen in het wild <img src="https://img.shields.io/github/stars/hkust-nlp/simpleRL-reason" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/hiyouga/EasyR1">Easy-R1</a>: <strong>Multi-modal</strong> RL-training framework <img src="https://img.shields.io/github/stars/hiyouga/EasyR1" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/OpenManus/OpenManus-RL">OpenManus-RL</a>: LLM Agents RL-tuning framework voor meerdere agentomgevingen. <img src="https://img.shields.io/github/stars/OpenManus/OpenManus-RL" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/agentica-project/rllm">rllm</a>: async RL-training met <a href="https://github.com/agentica-project/verl-pipeline">verl-pipeline</a> <img src="https://img.shields.io/github/stars/agentica-project/rllm" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/PRIME-RL/PRIME">PRIME</a>: Procesversterking door impliciete beloningen <img src="https://img.shields.io/github/stars/PRIME-RL/PRIME" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/ZihanWang314/ragen">RAGEN</a>: een algemeen trainingsframework voor redeneer<strong>agenten</strong> <img src="https://img.shields.io/github/stars/ZihanWang314/ragen" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/PeterGriffinJin/Search-R1">Search-R1</a>: RL met redeneer- en <strong>zoek- (tool-call)</strong> interleaved LLMs <img src="https://img.shields.io/github/stars/PeterGriffinJin/Search-R1" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/pat-jj/DeepRetrieval">DeepRetrieval</a>: RL-training van <strong>Search Agent</strong> met <strong>Search/Retrieval Outcome</strong> <img src="https://img.shields.io/github/stars/pat-jj/DeepRetrieval" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/Agent-RL/ReSearch">ReSearch</a>: Leren <strong>re</strong>deneer met <strong>search</strong> voor LLMs via reinforcement learning <img src="https://img.shields.io/github/stars/Agent-RL/ReSearch" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/ganler/code-r1">Code-R1</a>: Reproductie van R1 voor <strong>Code</strong> met betrouwbare beloningen <img src="https://img.shields.io/github/stars/ganler/code-r1" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/SkyworkAI/Skywork-OR1">Skywork-OR1</a>: Skywork open reasoner series <img src="https://img.shields.io/github/stars/SkyworkAI/Skywork-OR1" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/GAIR-NLP/ToRL">ToRL</a>: Schalen van tool-geïntegreerde RL <img src="https://img.shields.io/github/stars/GAIR-NLP/ToRL" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/langfengQ/verl-agent">verl-agent</a>: Een schaalbaar trainingsframework voor <strong>long-horizon LLM/VLM agents</strong>, samen met een nieuw algoritme <strong>GiGPO</strong> <img src="https://img.shields.io/github/stars/langfengQ/verl-agent" alt="GitHub Repo stars" /></li>
<li><a href="https://arxiv.org/abs/2409.06957">PF-PPO</a>: Policy Filtration voor PPO op basis van de betrouwbaarheid van reward-signalen voor efficiëntere en robuustere RLHF.</li>
<li><a href="https://github.com/ritzz-ai/GUI-R1">GUI-R1</a>: <strong>GUI-R1</strong>: Een generalistisch R1-stijl Vision-Language Action Model voor <strong>GUI Agents</strong> <img src="https://img.shields.io/github/stars/ritzz-ai/GUI-R1" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/GAIR-NLP/DeepResearcher">DeepResearcher</a>: Schaal diepgaand onderzoek met reinforcement learning in echte omgevingen <img src="https://img.shields.io/github/stars/GAIR-NLP/DeepResearcher" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/RAGEN-AI/VAGEN">VAGEN</a>: Training van VLM-agenten met multi-turn reinforcement learning <img src="https://img.shields.io/github/stars/RAGEN-AI/VAGEN" alt="GitHub Repo stars" /></li>
<li><a href="https://retool-rl.github.io/">ReTool</a>: ReTool: reinforcement learning voor strategisch toolgebruik in LLMs. Code release volgt...</li>
<li><a href="https://arxiv.org/abs/2505.02387">RM-R1</a>: RL training van redeneer rewardmodellen <img src="https://img.shields.io/github/stars/RM-R1-UIUC/RM-R1" alt="GitHub Repo stars" /></li>
<li><a href="https://arxiv.org/abs/2505.03335">Absolute Zero Reasoner</a>: Een self-play framework zonder handmatig samengestelde data voor redeneren<img src="https://img.shields.io/github/stars/LeapLabTHU/Absolute-Zero-Reasoner" alt="GitHub Repo stars" /></li>
<li><a href="https://arxiv.org/pdf/2504.14945">LUFFY</a>: Leren redeneren onder off-policy begeleiding<img src="https://img.shields.io/github/stars/ElliottYan/LUFFY" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/TIGER-AI-Lab/verl-tool">verl-tool</a>: Een verenigd en eenvoudig uit te breiden tool-agent trainingsframework gebaseerd op verl<img src="https://img.shields.io/github/stars/TIGER-AI-Lab/verl-tool" alt="GitHub Repo stars" /></li>
<li><a href="https://github.com/zwhe99/DeepMath">DeepMath</a>: DeepMath-103K data en serie modellen voor wiskundig redeneren<img src="https://img.shields.io/github/stars/zwhe99/DeepMath" alt="GitHub Repo stars" /></li>
</ul>
<p>en nog veel meer geweldig werk, zie <a href="https://raw.githubusercontent.com/volcengine/verl/main/recipe/README.md">recipe</a>.</p>
<h2>Bijdragen Gids</h2>
<p>Bijdragen vanuit de community zijn welkom! Bekijk onze <a href="https://github.com/volcengine/verl/issues/710">project roadmap</a> en <a href="https://github.com/volcengine/verl/issues?q=is%3Aissue%20state%3Aopen%20label%3A%22good%20first%20issue%22">good first issues</a> om te zien waar je kunt bijdragen.</p>
<h3>Code Linting en Formatteren</h3>
<p>We gebruiken pre-commit om de codekwaliteit te verbeteren. Om pre-commit te initialiseren, voer uit:</p>
<pre><code class="language-bash">pip install pre-commit
pre-commit install
</code></pre>
<p>Om CI-fouten lokaal op te lossen, kun je pre-commit handmatig uitvoeren met:</p>
<pre><code class="language-bash">pre-commit run
</code></pre>
<h3>CI-tests toevoegen</h3>
<p>Indien mogelijk, voeg graag CI-test(s) toe voor je nieuwe feature:</p>
<ol>
<li>Zoek het meest relevante workflow yml-bestand, wat meestal overeenkomt met een <code>hydra</code> default config (bijv. <code>ppo_trainer</code>, <code>ppo_megatron_trainer</code>, <code>sft_trainer</code>, enz).</li>
<li>Voeg gerelateerde padpatronen toe aan de <code>paths</code> sectie indien nog niet aanwezig.</li>
<li>Minimaliseer de workload van het testscript (zie bestaande scripts voor voorbeelden).</li>
</ol>
<h2>Over <a href="https://team.doubao.com/">ByteDance Seed Team</a></h2>
<p>Opgericht in 2023, is het ByteDance Seed Team toegewijd aan het creëren van de meest geavanceerde AI-basis modellen in de industrie. Het team streeft ernaar een wereldklasse onderzoeksteam te worden en een significante bijdrage te leveren aan de vooruitgang van wetenschap en samenleving. Je kunt Bytedance Seed beter leren kennen via de volgende kanalen👇</p>
<div>
  <a href="https://team.doubao.com/">
    <img src="https://img.shields.io/badge/Website-%231e37ff?style=for-the-badge&logo=bytedance&logoColor=white"></a>
  <a href="https://github.com/user-attachments/assets/469535a8-42f2-4797-acdf-4f7a1d4a0c3e">
    <img src="https://img.shields.io/badge/WeChat-07C160?style=for-the-badge&logo=wechat&logoColor=white"></a>
 <a href="https://www.xiaohongshu.com/user/profile/668e7e15000000000303157d?xsec_token=ABl2-aqekpytY6A8TuxjrwnZskU-6BsMRE_ufQQaSAvjc%3D&xsec_source=pc_search">
    <img src="https://img.shields.io/badge/Xiaohongshu-%23FF2442?style=for-the-badge&logo=xiaohongshu&logoColor=white"></a>
  <a href="https://www.zhihu.com/org/dou-bao-da-mo-xing-tuan-dui/">
    <img src="https://img.shields.io/badge/zhihu-%230084FF?style=for-the-badge&logo=zhihu&logoColor=white"></a>
</div>
---
<p>We ZOEKEN NIEUWE COLLEGA'S! Stuur ons een <a href="mailto:haibin.lin@bytedance.com">e-mail</a> als je geïnteresseerd bent in een stage/FTE-kans in RL voor agents.</p>
<hr />
<p>Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx">Open Ai Tx</a> | Last indexed: 2025-06-07</p>
<hr />

        </div>
    </div>
    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Default Statcounter code for openaitx
https://openaitx.github.io/ -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>
    <!-- End of Statcounter Code -->
</body>
</html>