<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DeepSeek-V3 - deepseek-ai/DeepSeek-V3 es</title>
    <meta name="title" content="DeepSeek-V3 - deepseek-ai/DeepSeek-V3 es | Paper Link👁️ Tabla de Contenidos Introducción Resumen del Modelo Descarga del Modelo Resultados de Evaluación Sitio Web de Chat y Plataforma API Cómo Ejecutar ...">
    <meta name="description" content="deepseek-ai/DeepSeek-V3 - GitHub repository es documentation and information | Paper Link👁️ Tabla de Contenidos Introducción Resumen del Modelo Descarga del Modelo Resultados de Evaluación Sitio Web de Chat y Plataforma API Cómo Ejecutar ...">
    <meta name="keywords" content="deepseek-ai, DeepSeek-V3, GitHub, repository, es documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/deepseek-ai/DeepSeek-V3/README-es.html">
    <meta property="og:title" content="DeepSeek-V3 - deepseek-ai/DeepSeek-V3 es | Paper Link👁️ Tabla de Contenidos Introducción Resumen del Modelo Descarga del Modelo Resultados de Evaluación Sitio Web de Chat y Plataforma API Cómo Ejecutar ...">
    <meta property="og:description" content="deepseek-ai/DeepSeek-V3 - GitHub repository es documentation and information | Paper Link👁️ Tabla de Contenidos Introducción Resumen del Modelo Descarga del Modelo Resultados de Evaluación Sitio Web de Chat y Plataforma API Cómo Ejecutar ...">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">
    <script src="/js/marked.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        body { display: flex; flex-direction: column; min-height: 100vh; }
        .main-container { margin: 0 auto; width: 100%; max-width: 980px; padding: 0 20px; }
        @media (max-width: 768px) { .main-container { padding: 0 15px; } }
        .markdown-body img { max-width: 100%; height: auto; }
        .header { text-align: center; margin-bottom: 30px; padding: 20px; background-color: #f6f8fa; border-bottom: 1px solid #e1e4e8; position: relative; }
        .header .links { margin-top: 10px; font-size: 16px; }
        .header .links a { color: #0366d6; text-decoration: none; margin-left: 5px; }
        .header .links a:hover { text-decoration: underline; }
        .language-badges { margin-top: 15px; text-align: center; }
        .language-badges a { display: inline-block; margin: 2px; text-decoration: none; }
        .language-badges img { height: 20px; border-radius: 3px; }
        .language-badges a:hover img { opacity: 0.8; }
    </style>
</head>
<body>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/deepseek-ai/DeepSeek-V3" id="githubRepoLink" target="_blank">deepseek-ai/DeepSeek-V3</a>
<h1 style="display: none;">Paper Link👁️ Tabla de Contenidos Introducción Resumen del Modelo Descarga del Modelo Resultados de Evaluación Sitio Web de Chat y Plataforma API Cómo Ejecutar ...</h1>
        </div>
        <div class="language-badges" id="languageBadges">
            <!-- You can generate language badges here if needed -->
        </div>
    </div>
    <div class="main-container">
        <div class="markdown-body" id="content">
            <!-- markdownlint-disable first-line-h1 -->
<!-- markdownlint-disable html -->
<!-- markdownlint-disable no-duplicate-header -->
<div align="center">
  <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" />
</div>
<hr>
<div align="center" style="line-height: 1;">
  <a href="https://www.deepseek.com/"><img alt="Homepage"
    src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true"/></a>
  <a href="https://chat.deepseek.com/"><img alt="Chat"
    src="https://img.shields.io/badge/🤖%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white"/></a>
  <a href="https://huggingface.co/deepseek-ai"><img alt="Hugging Face"
    src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white"/></a>
  <br>
  <a href="https://discord.gg/Tc7c45Zzu5"><img alt="Discord"
    src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da"/></a>
  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true"><img alt="Wechat"
    src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white"/></a>
  <a href="https://twitter.com/deepseek_ai"><img alt="Twitter Follow"
    src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white"/></a>
  <br>
  <a href="https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-CODE"><img alt="Code License"
    src="https://img.shields.io/badge/Code_License-MIT-f5de53?&color=f5de53"/></a>
  <a href="https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-MODEL"><img alt="Model License"
    src="https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&color=f5de53"/></a>
  <br>
  <a href="https://arxiv.org/pdf/2412.19437"><b>Paper Link</b>👁️</a>
</div>
<h2>Tabla de Contenidos</h2>
<ol>
<li><a href="#1-introducci%C3%B3n">Introducción</a></li>
<li><a href="#2-resumen-del-modelo">Resumen del Modelo</a></li>
<li><a href="#3-descarga-del-modelo">Descarga del Modelo</a></li>
<li><a href="#4-resultados-de-evaluaci%C3%B3n">Resultados de Evaluación</a></li>
<li><a href="#5-sitio-web-de-chat-y-plataforma-api">Sitio Web de Chat y Plataforma API</a></li>
<li><a href="#6-c%C3%B3mo-ejecutar-localmente">Cómo Ejecutar Localmente</a></li>
<li><a href="#7-licencia">Licencia</a></li>
<li><a href="#8-citaci%C3%B3n">Citación</a></li>
<li><a href="#9-contacto">Contacto</a></li>
</ol>
<h2>1. Introducción</h2>
<p>Presentamos DeepSeek-V3, un potente modelo de lenguaje Mixture-of-Experts (MoE) con 671B parámetros totales y 37B activados por cada token.<br />
Para lograr una inferencia eficiente y un entrenamiento rentable, DeepSeek-V3 adopta las arquitecturas Multi-head Latent Attention (MLA) y DeepSeekMoE, las cuales fueron validadas exhaustivamente en DeepSeek-V2.<br />
Además, DeepSeek-V3 es pionero en una estrategia de balanceo de carga sin pérdida auxiliar y establece un objetivo de entrenamiento de predicción multi-token para un rendimiento superior.<br />
Preentrenamos DeepSeek-V3 en 14.8 billones de tokens diversos y de alta calidad, seguido de etapas de Fine-Tuning Supervisado y Aprendizaje por Refuerzo para aprovechar al máximo sus capacidades.<br />
Evaluaciones exhaustivas revelan que DeepSeek-V3 supera a otros modelos open-source y alcanza un rendimiento comparable a los modelos líderes de código cerrado.<br />
A pesar de su excelente rendimiento, DeepSeek-V3 requiere solo 2.788M horas de GPU H800 para su entrenamiento completo.<br />
Además, su proceso de entrenamiento es notablemente estable.<br />
Durante todo el proceso de entrenamiento, no experimentamos picos de pérdida irrecuperables ni realizamos retrocesos.</p>
<p align="center">
  <img width="80%" src="figures/benchmark.png">
</p>
<h2>2. Resumen del Modelo</h2>
<hr />
<p><strong>Arquitectura: Estrategia Innovadora de Balanceo de Carga y Objetivo de Entrenamiento</strong></p>
<ul>
<li>Sobre la arquitectura eficiente de DeepSeek-V2, desarrollamos una estrategia de balanceo de carga sin pérdida auxiliar, que minimiza la degradación de rendimiento que surge al incentivar el balanceo de carga.</li>
<li>Investigamos un objetivo de Predicción Multi-Token (MTP) y demostramos que es beneficioso para el rendimiento del modelo.<br />
También puede usarse para decodificación especulativa para acelerar la inferencia.</li>
</ul>
<hr />
<p><strong>Preentrenamiento: Hacia la Eficiencia Máxima de Entrenamiento</strong></p>
<ul>
<li>Diseñamos un marco de entrenamiento de precisión mixta FP8 y, por primera vez, validamos la viabilidad y efectividad del entrenamiento en FP8 en un modelo de escala extremadamente grande.</li>
<li>A través del co-diseño de algoritmos, marcos y hardware, superamos el cuello de botella de comunicación en el entrenamiento MoE entre nodos, logrando casi una superposición completa entre computación y comunicación.<br />
Esto mejora significativamente nuestra eficiencia de entrenamiento y reduce los costes, permitiéndonos escalar aún más el tamaño del modelo sin gastos adicionales.</li>
<li>A un costo económico de solo 2.664M horas de GPU H800, completamos el preentrenamiento de DeepSeek-V3 en 14.8T tokens, produciendo el modelo base open-source más fuerte hasta la fecha. Las etapas subsiguientes tras el preentrenamiento requieren solo 0.1M horas GPU.</li>
</ul>
<hr />
<p><strong>Post-Entrenamiento: Destilación de Conocimientos desde DeepSeek-R1</strong></p>
<ul>
<li>Introducimos una metodología innovadora para destilar capacidades de razonamiento desde el modelo Chain-of-Thought (CoT) largo, específicamente de uno de los modelos DeepSeek R1, hacia LLMs estándar, en particular DeepSeek-V3. Nuestra canalización incorpora elegantemente los patrones de verificación y reflexión de R1 en DeepSeek-V3 y mejora notablemente su rendimiento en razonamiento. Al mismo tiempo, también mantenemos el control sobre el estilo y la longitud de salida de DeepSeek-V3.</li>
</ul>
<hr />
<h2>3. Descarga del Modelo</h2>
<div align="center">
<p>| <strong>Modelo</strong> | <strong>#Parámetros Totales</strong> | <strong>#Parámetros Activados</strong> | <strong>Longitud de Contexto</strong> | <strong>Descarga</strong> |
| :------------: | :------------: | :------------: | :------------: | :------------: |
| DeepSeek-V3-Base | 671B | 37B | 128K   | <a href="https://huggingface.co/deepseek-ai/DeepSeek-V3-Base">🤗 Hugging Face</a>   |
| DeepSeek-V3   | 671B | 37B |  128K   | <a href="https://huggingface.co/deepseek-ai/DeepSeek-V3">🤗 Hugging Face</a>   |</p>
</div>
<blockquote>
<p>[!NOTA]
El tamaño total de los modelos DeepSeek-V3 en Hugging Face es 685B, lo que incluye 671B de los pesos del Modelo Principal y 14B de los pesos del Módulo de Predicción Multi-Token (MTP).</p>
</blockquote>
<p>Para garantizar un rendimiento óptimo y flexibilidad, nos hemos asociado con comunidades open-source y proveedores de hardware para proporcionar múltiples formas de ejecutar el modelo localmente. Para una guía paso a paso, consulte la Sección 6: <a href="#6-c%C3%B3mo-ejecutar-localmente">Cómo Ejecutar Localmente</a>.</p>
<p>Para desarrolladores interesados en profundizar, recomendamos explorar <a href="./README_WEIGHTS.md">README_WEIGHTS.md</a> para detalles sobre los pesos del Modelo Principal y los Módulos de Predicción Multi-Token (MTP). Tenga en cuenta que el soporte para MTP está actualmente en desarrollo activo dentro de la comunidad, y agradecemos sus contribuciones y comentarios.</p>
<h2>4. Resultados de Evaluación</h2>
<h3>Modelo Base</h3>
<h4>Benchmarks Estándar</h4>
<div align="center">
<p>|  | Benchmark (Métrica) | # Shots | DeepSeek-V2 | Qwen2.5 72B | LLaMA3.1 405B | DeepSeek-V3 |
|---|-------------------|----------|--------|-------------|---------------|---------|
| | Arquitectura | - | MoE | Denso | Denso | MoE |
| | # Parámetros Activados | - | 21B | 72B | 405B | 37B |
| | # Parámetros Totales | - | 236B | 72B | 405B | 671B |
| Inglés | Pile-test (BPB) | - | 0.606 | 0.638 | <strong>0.542</strong> | 0.548 |
| | BBH (EM) | 3-shot | 78.8 | 79.8 | 82.9 | <strong>87.5</strong> |
| | MMLU (Prec.) | 5-shot | 78.4 | 85.0 | 84.4 | <strong>87.1</strong> |
| | MMLU-Redux (Prec.) | 5-shot | 75.6 | 83.2 | 81.3 | <strong>86.2</strong> |
| | MMLU-Pro (Prec.) | 5-shot | 51.4 | 58.3 | 52.8 | <strong>64.4</strong> |
| | DROP (F1) | 3-shot | 80.4 | 80.6 | 86.0 | <strong>89.0</strong> |
| | ARC-Easy (Prec.) | 25-shot | 97.6 | 98.4 | 98.4 | <strong>98.9</strong> |
| | ARC-Challenge (Prec.) | 25-shot | 92.2 | 94.5 | <strong>95.3</strong> | <strong>95.3</strong> |
| | HellaSwag (Prec.) | 10-shot | 87.1 | 84.8 | <strong>89.2</strong> | 88.9 |
| | PIQA (Prec.) | 0-shot | 83.9 | 82.6 | <strong>85.9</strong> | 84.7 |
| | WinoGrande (Prec.) | 5-shot | <strong>86.3</strong> | 82.3 | 85.2 | 84.9 |
| | RACE-Middle (Prec.) | 5-shot | 73.1 | 68.1 | <strong>74.2</strong> | 67.1 |
| | RACE-High (Prec.) | 5-shot | 52.6 | 50.3 | <strong>56.8</strong> | 51.3 |
| | TriviaQA (EM) | 5-shot | 80.0 | 71.9 | 82.7 | <strong>82.9</strong> |
| | NaturalQuestions (EM) | 5-shot | 38.6 | 33.2 | <strong>41.5</strong> | 40.0 |
| | AGIEval (Prec.) | 0-shot | 57.5 | 75.8 | 60.6 | <strong>79.6</strong> |
| Código | HumanEval (Pass@1) | 0-shot | 43.3 | 53.0 | 54.9 | <strong>65.2</strong> |
| | MBPP (Pass@1) | 3-shot | 65.0 | 72.6 | 68.4 | <strong>75.4</strong> |
| | LiveCodeBench-Base (Pass@1) | 3-shot | 11.6 | 12.9 | 15.5 | <strong>19.4</strong> |
| | CRUXEval-I (Prec.) | 2-shot | 52.5 | 59.1 | 58.5 | <strong>67.3</strong> |
| | CRUXEval-O (Prec.) | 2-shot | 49.8 | 59.9 | 59.9 | <strong>69.8</strong> |
| Matemáticas | GSM8K (EM) | 8-shot | 81.6 | 88.3 | 83.5 | <strong>89.3</strong> |
| | MATH (EM) | 4-shot | 43.4 | 54.4 | 49.0 | <strong>61.6</strong> |
| | MGSM (EM) | 8-shot | 63.6 | 76.2 | 69.9 | <strong>79.8</strong> |
| | CMath (EM) | 3-shot | 78.7 | 84.5 | 77.3 | <strong>90.7</strong> |
| Chino | CLUEWSC (EM) | 5-shot | 82.0 | 82.5 | <strong>83.0</strong> | 82.7 |
| | C-Eval (Prec.) | 5-shot | 81.4 | 89.2 | 72.5 | <strong>90.1</strong> |
| | CMMLU (Prec.) | 5-shot | 84.0 | <strong>89.5</strong> | 73.7 | 88.8 |
| | CMRC (EM) | 1-shot | <strong>77.4</strong> | 75.8 | 76.0 | 76.3 |
| | C3 (Prec.) | 0-shot | 77.4 | 76.7 | <strong>79.7</strong> | 78.6 |
| | CCPM (Prec.) | 0-shot | <strong>93.0</strong> | 88.5 | 78.6 | 92.0 |
| Multilingüe | MMMLU-no-Inglés (Prec.) | 5-shot | 64.0 | 74.8 | 73.8 | <strong>79.4</strong> |</p>
</div>
<blockquote>
<p>[!NOTA]
Los mejores resultados se muestran en negrita. Las puntuaciones con una diferencia no superior a 0.3 se consideran del mismo nivel. DeepSeek-V3 logra el mejor rendimiento en la mayoría de los benchmarks, especialmente en tareas de matemáticas y código.
Para más detalles de evaluación, consulte nuestro paper.</p>
</blockquote>
<h4>Ventana de Contexto</h4>
<p align="center">
  <img width="80%" src="figures/niah.png">
</p>
<p>Resultados de evaluación en las pruebas <code>Needle In A Haystack</code> (NIAH). DeepSeek-V3 rinde bien en todas las longitudes de ventana de contexto hasta <strong>128K</strong>.</p>
<h3>Modelo de Chat</h3>
<h4>Benchmarks Estándar (Modelos mayores a 67B)</h4>
<div align="center">
<p>| | <strong>Benchmark (Métrica)</strong> | <strong>DeepSeek V2-0506</strong> | <strong>DeepSeek V2.5-0905</strong> | <strong>Qwen2.5 72B-Inst.</strong> | <strong>Llama3.1 405B-Inst.</strong> | <strong>Claude-3.5-Sonnet-1022</strong> | <strong>GPT-4o 0513</strong> | <strong>DeepSeek V3</strong> |
|---|---------------------|---------------------|----------------------|---------------------|----------------------|---------------------------|----------------|----------------|
| | Arquitectura | MoE | MoE | Denso | Denso | - | - | MoE |
| | # Parámetros Activados | 21B | 21B | 72B | 405B | - | - | 37B |
| | # Parámetros Totales | 236B | 236B | 72B | 405B | - | - | 671B |
| Inglés | MMLU (EM) | 78.2 | 80.6 | 85.3 | <strong>88.6</strong> | <strong>88.3</strong> | 87.2 | <strong>88.5</strong> |
| | MMLU-Redux (EM) | 77.9 | 80.3 | 85.6 | 86.2 | <strong>88.9</strong> | 88.0 | <strong>89.1</strong> |
| | MMLU-Pro (EM) | 58.5 | 66.2 | 71.6 | 73.3 | <strong>78.0</strong> | 72.6 | 75.9 |
| | DROP (3-shot F1) | 83.0 | 87.8 | 76.7 | 88.7 | 88.3 | 83.7 | <strong>91.6</strong> |
| | IF-Eval (Prompt Estricto) | 57.7 | 80.6 | 84.1 | 86.0 | <strong>86.5</strong> | 84.3 | 86.1 |
| | GPQA-Diamond (Pass@1) | 35.3 | 41.3 | 49.0 | 51.1 | <strong>65.0</strong> | 49.9 | 59.1 |
| | SimpleQA (Correcto) | 9.0 | 10.2 | 9.1 | 17.1 | 28.4 | <strong>38.2</strong> | 24.9 |
| | FRAMES (Prec.) | 66.9 | 65.4 | 69.8 | 70.0 | 72.5 | <strong>80.5</strong> | 73.3 |
| | LongBench v2 (Prec.) | 31.6 | 35.4 | 39.4 | 36.1 | 41.0 | 48.1 | <strong>48.7</strong> |
| Código | HumanEval-Mul (Pass@1) | 69.3 | 77.4 | 77.3 | 77.2 | 81.7 | 80.5 | <strong>82.6</strong> |
| | LiveCodeBench (Pass@1-COT) | 18.8 | 29.2 | 31.1 | 28.4 | 36.3 | 33.4 | <strong>40.5</strong> |
| | LiveCodeBench (Pass@1) | 20.3 | 28.4 | 28.7 | 30.1 | 32.8 | 34.2 | <strong>37.6</strong> |
| | Codeforces (Percentil) | 17.5 | 35.6 | 24.8 | 25.3 | 20.3 | 23.6 | <strong>51.6</strong> |
| | SWE Verified (Resuelto) | - | 22.6 | 23.8 | 24.5 | <strong>50.8</strong> | 38.8 | 42.0 |
| | Aider-Edit (Prec.) | 60.3 | 71.6 | 65.4 | 63.9 | <strong>84.2</strong> | 72.9 | 79.7 |
| | Aider-Polyglot (Prec.) | - | 18.2 | 7.6 | 5.8 | 45.3 | 16.0 | <strong>49.6</strong> |
| Matemáticas | AIME 2024 (Pass@1) | 4.6 | 16.7 | 23.3 | 23.3 | 16.0 | 9.3 | <strong>39.2</strong> |
| | MATH-500 (EM) | 56.3 | 74.7 | 80.0 | 73.8 | 78.3 | 74.6 | <strong>90.2</strong> |
| | CNMO 2024 (Pass@1) | 2.8 | 10.8 | 15.9 | 6.8 | 13.1 | 10.8 | <strong>43.2</strong> |
| Chino | CLUEWSC (EM) | 89.9 | 90.4 | <strong>91.4</strong> | 84.7 | 85.4 | 87.9 | 90.9 |
| | C-Eval (EM) | 78.6 | 79.5 | 86.1 | 61.5 | 76.7 | 76.0 | <strong>86.5</strong> |
| | C-SimpleQA (Correcto) | 48.5 | 54.1 | 48.4 | 50.4 | 51.3 | 59.3 | <strong>64.8</strong> |</p>
</div>
<blockquote>
<p>[!NOTA]
Todos los modelos se evalúan en una configuración que limita la longitud de salida a 8K. Los benchmarks con menos de 1000 muestras se prueban varias veces usando diferentes temperaturas para obtener resultados robustos. DeepSeek-V3 es el modelo open-source con mejor rendimiento, y también muestra un rendimiento competitivo frente a modelos de código cerrado de vanguardia.</p>
</blockquote>
<h4>Evaluación de Generación Abierta</h4>
<div align="center">
<p>| Modelo | Arena-Hard | AlpacaEval 2.0 |
|--------|------------|----------------|
| DeepSeek-V2.5-0905 | 76.2 | 50.5 |
| Qwen2.5-72B-Instruct | 81.2 | 49.1 |
| LLaMA-3.1 405B | 69.3 | 40.5 |
| GPT-4o-0513 | 80.4 | 51.1 |
| Claude-Sonnet-3.5-1022 | 85.2 | 52.0 |
| DeepSeek-V3 | <strong>85.5</strong> | <strong>70.0</strong> |</p>
</div>
<blockquote>
<p>[!NOTA]
Evaluaciones de conversación abierta en inglés. Para AlpacaEval 2.0, usamos la tasa de victoria controlada por longitud como métrica.</p>
</blockquote>
<h2>5. Sitio Web de Chat y Plataforma API</h2>
<p>Puede chatear con DeepSeek-V3 en el sitio web oficial de DeepSeek: <a href="https://chat.deepseek.com/sign_in">chat.deepseek.com</a></p>
<p>También proporcionamos una API compatible con OpenAI en DeepSeek Platform: <a href="https://platform.deepseek.com/">platform.deepseek.com</a></p>
<h2>6. Cómo Ejecutar Localmente</h2>
<p>DeepSeek-V3 puede desplegarse localmente utilizando el siguiente hardware y software de la comunidad open-source:</p>
<ol>
<li><strong>DeepSeek-Infer Demo</strong>: Ofrecemos una demo simple y ligera para inferencia FP8 y BF16.</li>
<li><strong>SGLang</strong>: Soporta completamente el modelo DeepSeek-V3 en modos de inferencia BF16 y FP8, con Predicción Multi-Token <a href="https://github.com/sgl-project/sglang/issues/2591">próximamente</a>.</li>
<li><strong>LMDeploy</strong>: Permite inferencia eficiente FP8 y BF16 para despliegue local y en la nube.</li>
<li><strong>TensorRT-LLM</strong>: Actualmente soporta inferencia BF16 y cuantización INT4/8, con soporte FP8 próximamente.</li>
<li><strong>vLLM</strong>: Soporta el modelo DeepSeek-V3 en modos FP8 y BF16 para paralelismo tensorial y de pipeline.</li>
<li><strong>LightLLM</strong>: Permite un despliegue eficiente en uno o varios nodos para FP8 y BF16.</li>
<li><strong>AMD GPU</strong>: Permite ejecutar el modelo DeepSeek-V3 en GPUs AMD a través de SGLang en modos BF16 y FP8.</li>
<li><strong>Huawei Ascend NPU</strong>: Permite ejecutar DeepSeek-V3 en dispositivos Huawei Ascend.</li>
</ol>
<p>Dado que el entrenamiento FP8 es adoptado nativamente en nuestro framework, solo proporcionamos pesos FP8. Si requiere pesos BF16 para experimentación, puede usar el script de conversión proporcionado para realizar la transformación.</p>
<p>Aquí un ejemplo de conversión de pesos FP8 a BF16:</p>
<pre><code class="language-shell">cd inference
python fp8_cast_bf16.py --input-fp8-hf-path /ruta/a/pesos_fp8 --output-bf16-hf-path /ruta/a/pesos_bf16
</code></pre>
<blockquote>
<p>[!NOTA]
Hugging Face Transformers aún no está directamente soportado.</p>
</blockquote>
<h3>6.1 Inferencia con DeepSeek-Infer Demo (solo ejemplo)</h3>
<h4>Requisitos del Sistema</h4>
<blockquote>
<p>[!NOTA]
Solo Linux con Python 3.10. Mac y Windows no están soportados.</p>
</blockquote>
<p>Dependencias:</p>
<pre><code class="language-pip-requirements">torch==2.4.1
triton==3.0.0
transformers==4.46.3
safetensors==0.4.5
</code></pre>
<h4>Preparación de Pesos del Modelo y Código Demo</h4>
<p>Primero, clone nuestro repositorio de DeepSeek-V3 en GitHub:</p>
<pre><code class="language-shell">git clone https://github.com/deepseek-ai/DeepSeek-V3.git
</code></pre>
<p>Navegue a la carpeta <code>inference</code> e instale las dependencias listadas en <code>requirements.txt</code>. La forma más fácil es usar un gestor de paquetes como <code>conda</code> o <code>uv</code> para crear un nuevo entorno virtual e instalar las dependencias.</p>
<pre><code class="language-shell">cd DeepSeek-V3/inference
pip install -r requirements.txt
</code></pre>
<p>Descargue los pesos del modelo desde Hugging Face y colóquelos en la carpeta <code>/ruta/a/DeepSeek-V3</code>.</p>
<h4>Conversión de Pesos del Modelo</h4>
<p>Convierta los pesos del modelo Hugging Face a un formato específico:</p>
<pre><code class="language-shell">python convert.py --hf-ckpt-path /ruta/a/DeepSeek-V3 --save-path /ruta/a/DeepSeek-V3-Demo --n-experts 256 --model-parallel 16
</code></pre>
<h4>Ejecución</h4>
<p>Ahora puede chatear con DeepSeek-V3:</p>
<pre><code class="language-shell">torchrun --nnodes 2 --nproc-per-node 8 --node-rank $RANK --master-addr $ADDR generate.py --ckpt-path /ruta/a/DeepSeek-V3-Demo --config configs/config_671B.json --interactive --temperature 0.7 --max-new-tokens 200
</code></pre>
<p>O inferencia en lote sobre un archivo dado:</p>
<pre><code class="language-shell">torchrun --nnodes 2 --nproc-per-node 8 --node-rank $RANK --master-addr $ADDR generate.py --ckpt-path /ruta/a/DeepSeek-V3-Demo --config configs/config_671B.json --input-file $FILE
</code></pre>
<h3>6.2 Inferencia con SGLang (recomendado)</h3>
<p><a href="https://github.com/sgl-project/sglang">SGLang</a> soporta actualmente <a href="https://lmsys.org/blog/2024-09-04-sglang-v0-3/#deepseek-multi-head-latent-attention-mla-throughput-optimizations">optimizaciones MLA</a>, <a href="https://lmsys.org/blog/2024-12-04-sglang-v0-4/#data-parallelism-attention-for-deepseek-models">DP Attention</a>, FP8 (W8A8), FP8 KV Cache y Torch Compile, ofreciendo latencia y rendimiento de vanguardia entre los frameworks open-source.</p>
<p>En particular, <a href="https://github.com/sgl-project/sglang/releases/tag/v0.4.1">SGLang v0.4.1</a> soporta completamente la ejecución de DeepSeek-V3 tanto en <strong>GPUs NVIDIA como AMD</strong>, haciéndolo una solución altamente versátil y robusta.</p>
<p>SGLang también soporta <a href="https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3#example-serving-with-2-h208">paralelismo tensorial multi-nodo</a>, permitiéndole ejecutar el modelo en varias máquinas conectadas en red.</p>
<p>La Predicción Multi-Token (MTP) está en desarrollo, y el progreso puede seguirse en el <a href="https://github.com/sgl-project/sglang/issues/2591">plan de optimización</a>.</p>
<p>Aquí las instrucciones de lanzamiento del equipo SGLang: https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3</p>
<h3>6.3 Inferencia con LMDeploy (recomendado)</h3>
<p><a href="https://github.com/InternLM/lmdeploy">LMDeploy</a>, un framework flexible y de alto rendimiento para inferencia y servicio de grandes modelos de lenguaje, ahora soporta DeepSeek-V3. Ofrece procesamiento offline en pipeline y despliegue online, integrándose perfectamente con flujos de trabajo basados en PyTorch.</p>
<p>Para instrucciones paso a paso sobre cómo ejecutar DeepSeek-V3 con LMDeploy, consulte: https://github.com/InternLM/lmdeploy/issues/2960</p>
<h3>6.4 Inferencia con TRT-LLM (recomendado)</h3>
<p><a href="https://github.com/NVIDIA/TensorRT-LLM">TensorRT-LLM</a> ahora soporta el modelo DeepSeek-V3, ofreciendo opciones de precisión como BF16 e INT4/INT8 solo-pesos. El soporte para FP8 está en desarrollo y será lanzado próximamente. Puede acceder a la rama personalizada de TRTLLM específicamente para DeepSeek-V3 aquí: https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/deepseek_v3.</p>
<h3>6.5 Inferencia con vLLM (recomendado)</h3>
<p><a href="https://github.com/vllm-project/vllm">vLLM</a> v0.6.6 soporta inferencia DeepSeek-V3 para modos FP8 y BF16 tanto en GPUs NVIDIA como AMD. Además de técnicas estándar, vLLM ofrece <em>paralelismo de pipeline</em> permitiéndole ejecutar el modelo en varias máquinas conectadas en red. Para una guía detallada, consulte las <a href="https://docs.vllm.ai/en/latest/serving/distributed_serving.html">instrucciones vLLM</a>. Puede seguir el <a href="https://github.com/vllm-project/vllm/issues/11539">plan de mejora</a> también.</p>
<h3>6.6 Inferencia con LightLLM (recomendado)</h3>
<p><a href="https://github.com/ModelTC/lightllm/tree/main">LightLLM</a> v1.0.1 soporta despliegue tensorial en máquina única y multi-máquina para DeepSeek-R1 (FP8/BF16) y proporciona despliegue de precisión mixta, con más modos de cuantización integrándose continuamente. Para más detalles, consulte las <a href="https://lightllm-en.readthedocs.io/en/latest/getting_started/quickstart.html">instrucciones LightLLM</a>. Además, LightLLM ofrece despliegue PD-disaggregation para DeepSeek-V2, y la implementación para DeepSeek-V3 está en desarrollo.</p>
<h3>6.7 Inferencia Recomendada con GPUs AMD</h3>
<p>En colaboración con el equipo AMD, hemos logrado soporte Día-Uno para GPUs AMD usando SGLang, con compatibilidad total para precisión FP8 y BF16. Para una guía detallada, consulte las <a href="#63-inferencia-con-lmdeploy-recomendado">instrucciones SGLang</a>.</p>
<h3>6.8 Inferencia Recomendada con NPUs Huawei Ascend</h3>
<p>El framework <a href="https://www.hiascend.com/en/software/mindie">MindIE</a> de la comunidad Huawei Ascend ha adaptado exitosamente la versión BF16 de DeepSeek-V3. Para una guía paso a paso en NPUs Ascend, siga las <a href="https://modelers.cn/models/MindIE/deepseekv3">instrucciones aquí</a>.</p>
<h2>7. Licencia</h2>
<p>Este repositorio de código está licenciado bajo <a href="LICENSE-CODE">Licencia MIT</a>. El uso de los modelos DeepSeek-V3 Base/Chat está sujeto a la <a href="LICENSE-MODEL">Licencia del Modelo</a>. La serie DeepSeek-V3 (incluyendo Base y Chat) soporta uso comercial.</p>
<h2>8. Citación</h2>
<pre><code>@misc{deepseekai2024deepseekv3technicalreport,
      title={DeepSeek-V3 Technical Report}, 
      author={DeepSeek-AI},
      year={2024},
      eprint={2412.19437},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.19437}, 
}
</code></pre>
<h2>9. Contacto</h2>
<p>Si tiene alguna pregunta, por favor abra un issue o contáctenos en <a href="service@deepseek.com">service@deepseek.com</a>.</p>
<hr />
<p><a href="https://github.com/OpenAiTx/OpenAiTx">Powered By OpenAiTx</a></p>
<hr />

        </div>
    </div>
    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Default Statcounter code for openaitx
https://openaitx.github.io/ -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>
    <!-- End of Statcounter Code -->
</body>
</html>