<!DOCTYPE html>
<html lang="nl">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DeepSeek-V3 - deepseek-ai/DeepSeek-V3</title>
    <meta name="title" content="DeepSeek-V3 - deepseek-ai/DeepSeek-V3">
    <meta name="description" content="deepseek-ai/DeepSeek-V3 - GitHub repository nl documentation and informationPaper Link👁️ Inhoudsopgave Introductie Modeloverzicht Modeldownloads Evaluatieresultaten Chatwebsite &amp; API Platform Lokaal uitvoeren Licentie Citeren Conta...">
    <meta name="keywords" content="deepseek-ai, DeepSeek-V3, GitHub, repository, nl documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/deepseek-ai/DeepSeek-V3/README-nl.html">
    <meta property="og:title" content="DeepSeek-V3 - deepseek-ai/DeepSeek-V3">
    <meta property="og:description" content="deepseek-ai/DeepSeek-V3 - GitHub repository nl documentation and information">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">
    <script src="/js/marked.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        body { display: flex; flex-direction: column; min-height: 100vh; }
        .main-container { margin: 0 auto; width: 100%; max-width: 980px; padding: 0 20px; }
        @media (max-width: 768px) { .main-container { padding: 0 15px; } }
        .markdown-body img { max-width: 100%; height: auto; }
        .header { text-align: center; margin-bottom: 30px; padding: 20px; background-color: #f6f8fa; border-bottom: 1px solid #e1e4e8; position: relative; }
        .header .links { margin-top: 10px; font-size: 16px; }
        .header .links a { color: #0366d6; text-decoration: none; margin-left: 5px; }
        .header .links a:hover { text-decoration: underline; }
        .language-badges { margin-top: 15px; text-align: center; }
        .language-badges a { display: inline-block; margin: 2px; text-decoration: none; }
        .language-badges img { height: 20px; border-radius: 3px; }
        .language-badges a:hover img { opacity: 0.8; }
    </style>
</head>
<body>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/deepseek-ai/DeepSeek-V3" id="githubRepoLink" target="_blank">deepseek-ai/DeepSeek-V3</a>
<h1 style="display: none;">Paper Link👁️ Inhoudsopgave Introductie Modeloverzicht Modeldownloads Evaluatieresultaten Chatwebsite &amp; API Platform Lokaal uitvoeren Licentie Citeren Conta...</h1>
        </div>
        <div class="language-badges" id="languageBadges">
            <!-- You can generate language badges here if needed -->
        </div>
    </div>
    <div class="main-container">
        <div class="markdown-body" id="content">
            <!-- markdownlint-disable first-line-h1 -->
<!-- markdownlint-disable html -->
<!-- markdownlint-disable no-duplicate-header -->
<div align="center">
  <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" />
</div>
<hr>
<div align="center" style="line-height: 1;">
  <a href="https://www.deepseek.com/"><img alt="Homepage"
    src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true"/></a>
  <a href="https://chat.deepseek.com/"><img alt="Chat"
    src="https://img.shields.io/badge/🤖%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white"/></a>
  <a href="https://huggingface.co/deepseek-ai"><img alt="Hugging Face"
    src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white"/></a>
  <br>
  <a href="https://discord.gg/Tc7c45Zzu5"><img alt="Discord"
    src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da"/></a>
  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true"><img alt="Wechat"
    src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white"/></a>
  <a href="https://twitter.com/deepseek_ai"><img alt="Twitter Follow"
    src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white"/></a>
  <br>
  <a href="https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-CODE"><img alt="Code License"
    src="https://img.shields.io/badge/Code_License-MIT-f5de53?&color=f5de53"/></a>
  <a href="https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-MODEL"><img alt="Model License"
    src="https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&color=f5de53"/></a>
  <br>
  <a href="https://arxiv.org/pdf/2412.19437"><b>Paper Link</b>👁️</a>
</div>
<h2>Inhoudsopgave</h2>
<ol>
<li><a href="#1-introductie">Introductie</a></li>
<li><a href="#2-modeloverzicht">Modeloverzicht</a></li>
<li><a href="#3-modeldownloads">Modeldownloads</a></li>
<li><a href="#4-evaluatieresultaten">Evaluatieresultaten</a></li>
<li><a href="#5-chatwebsite--api-platform">Chatwebsite &amp; API Platform</a></li>
<li><a href="#6-lokaal-uitvoeren">Lokaal uitvoeren</a></li>
<li><a href="#7-licentie">Licentie</a></li>
<li><a href="#8-citeren">Citeren</a></li>
<li><a href="#9-contact">Contact</a></li>
</ol>
<h2>1. Introductie</h2>
<p>We presenteren DeepSeek-V3, een krachtig Mixture-of-Experts (MoE) taalmodel met 671B totale parameters waarvan 37B geactiveerd per token.
Om efficiënte inferentie en kosteneffectieve training te bereiken, hanteert DeepSeek-V3 Multi-head Latent Attention (MLA) en DeepSeekMoE-architecturen, die grondig zijn gevalideerd in DeepSeek-V2.
Bovendien introduceert DeepSeek-V3 als pionier een auxiliary-loss-vrije strategie voor load balancing en stelt het een multi-token voorspellingstrainingsdoelstelling vast voor betere prestaties.
We pre-trainen DeepSeek-V3 op 14,8 biljoen diverse en hoogwaardige tokens, gevolgd door Supervised Fine-Tuning en Reinforcement Learning stadia om het volledige potentieel te benutten.
Uitgebreide evaluaties tonen aan dat DeepSeek-V3 beter presteert dan andere open-source modellen en vergelijkbare prestaties behaalt als toonaangevende gesloten modellen.
Ondanks de uitstekende prestaties vereist DeepSeek-V3 slechts 2.788M H800 GPU-uren voor de volledige training.
Daarnaast is het trainingsproces opmerkelijk stabiel.
Gedurende het gehele trainingsproces hebben we geen onherstelbare verliespieken ervaren of rollbacks hoeven uitvoeren.</p>
<p align="center">
  <img width="80%" src="figures/benchmark.png">
</p>
<h2>2. Modeloverzicht</h2>
<hr />
<p><strong>Architectuur: Innovatieve Load Balancing Strategie en Trainingsdoelstelling</strong></p>
<ul>
<li>Bovenop de efficiënte architectuur van DeepSeek-V2 introduceren we een auxiliary-loss-vrije strategie voor load balancing, die de prestatievermindering minimaliseert die ontstaat door het stimuleren van load balancing.</li>
<li>We onderzoeken een Multi-Token Prediction (MTP) doelstelling en bewijzen het voordeel voor de modelprestaties.
Het kan ook gebruikt worden voor speculative decoding om inferentie te versnellen.</li>
</ul>
<hr />
<p><strong>Pre-Training: Naar Ultieme Trainingsefficiëntie</strong></p>
<ul>
<li>We ontwerpen een FP8 mixed precision trainingsframework en valideren voor het eerst de haalbaarheid en effectiviteit van FP8-training op een extreem grootschalig model.</li>
<li>Door co-design van algoritmes, frameworks en hardware overwinnen we de communicatiestuwing in cross-node MoE-training, en bereiken we vrijwel volledige overlap tussen berekening en communicatie.
Dit verhoogt onze trainingsefficiëntie aanzienlijk en vermindert de trainingskosten, waardoor we het model verder kunnen opschalen zonder extra overhead.</li>
<li>Tegen een economische kost van slechts 2.664M H800 GPU-uren voltooien we de pre-training van DeepSeek-V3 op 14,8T tokens, waarmee we het momenteel krachtigste open-source basismodel produceren. De daaropvolgende trainingsstadia na pre-training vereisen slechts 0,1M GPU-uren.</li>
</ul>
<hr />
<p><strong>Post-Training: Kennisdistillatie van DeepSeek-R1</strong></p>
<ul>
<li>We introduceren een innovatieve methodologie om redeneercapaciteiten te distilleren van het long-Chain-of-Thought (CoT) model, specifiek van een van de DeepSeek R1-serie modellen, naar standaard LLM's, in het bijzonder DeepSeek-V3. Onze pipeline integreert op elegante wijze de verificatie- en reflectiepatronen van R1 in DeepSeek-V3 en verbetert het redeneervermogen aanzienlijk. Ondertussen behouden we controle over de outputstijl en lengte van DeepSeek-V3.</li>
</ul>
<hr />
<h2>3. Modeldownloads</h2>
<div align="center">
<p>| <strong>Model</strong> | <strong>#Totale Params</strong> | <strong>#Geactiveerde Params</strong> | <strong>Contextlengte</strong> | <strong>Download</strong> |
| :------------: | :------------: | :------------: | :------------: | :------------: |
| DeepSeek-V3-Base | 671B | 37B | 128K   | <a href="https://huggingface.co/deepseek-ai/DeepSeek-V3-Base">🤗 Hugging Face</a>   |
| DeepSeek-V3   | 671B | 37B |  128K   | <a href="https://huggingface.co/deepseek-ai/DeepSeek-V3">🤗 Hugging Face</a>   |</p>
</div>
<blockquote>
<p>[!OPMERKING]
De totale omvang van de DeepSeek-V3 modellen op Hugging Face is 685B, wat 671B aan hoofdmodelgewichten en 14B aan Multi-Token Prediction (MTP) Module gewichten omvat.</p>
</blockquote>
<p>Om optimale prestaties en flexibiliteit te garanderen, werken we samen met open-source gemeenschappen en hardwareleveranciers om meerdere manieren te bieden om het model lokaal uit te voeren. Raadpleeg sectie 6: <a href="#6-lokaal-uitvoeren">Lokaal uitvoeren</a> voor stapsgewijze begeleiding.</p>
<p>Voor ontwikkelaars die dieper willen duiken, raden we aan <a href="./README_WEIGHTS.md">README_WEIGHTS.md</a> te bekijken voor details over de hoofdmodelgewichten en de Multi-Token Prediction (MTP) Modules. Let op: MTP-ondersteuning wordt momenteel actief ontwikkeld binnen de community, en we verwelkomen je bijdragen en feedback.</p>
<h2>4. Evaluatieresultaten</h2>
<h3>Basismodel</h3>
<h4>Standaard Benchmarks</h4>
<div align="center">
<p>|  | Benchmark (Metriek) | # Shots | DeepSeek-V2 | Qwen2.5 72B | LLaMA3.1 405B | DeepSeek-V3 |
|---|-------------------|----------|--------|-------------|---------------|---------|
| | Architectuur | - | MoE | Dense | Dense | MoE |
| | # Geactiveerde Params | - | 21B | 72B | 405B | 37B |
| | # Totale Params | - | 236B | 72B | 405B | 671B |
| Engels | Pile-test (BPB) | - | 0.606 | 0.638 | <strong>0.542</strong> | 0.548 |
| | BBH (EM) | 3-shot | 78.8 | 79.8 | 82.9 | <strong>87.5</strong> |
| | MMLU (Acc.) | 5-shot | 78.4 | 85.0 | 84.4 | <strong>87.1</strong> |
| | MMLU-Redux (Acc.) | 5-shot | 75.6 | 83.2 | 81.3 | <strong>86.2</strong> |
| | MMLU-Pro (Acc.) | 5-shot | 51.4 | 58.3 | 52.8 | <strong>64.4</strong> |
| | DROP (F1) | 3-shot | 80.4 | 80.6 | 86.0 | <strong>89.0</strong> |
| | ARC-Easy (Acc.) | 25-shot | 97.6 | 98.4 | 98.4 | <strong>98.9</strong> |
| | ARC-Challenge (Acc.) | 25-shot | 92.2 | 94.5 | <strong>95.3</strong> | <strong>95.3</strong> |
| | HellaSwag (Acc.) | 10-shot | 87.1 | 84.8 | <strong>89.2</strong> | 88.9 |
| | PIQA (Acc.) | 0-shot | 83.9 | 82.6 | <strong>85.9</strong> | 84.7 |
| | WinoGrande (Acc.) | 5-shot | <strong>86.3</strong> | 82.3 | 85.2 | 84.9 |
| | RACE-Middle (Acc.) | 5-shot | 73.1 | 68.1 | <strong>74.2</strong> | 67.1 |
| | RACE-High (Acc.) | 5-shot | 52.6 | 50.3 | <strong>56.8</strong> | 51.3 |
| | TriviaQA (EM) | 5-shot | 80.0 | 71.9 | 82.7 | <strong>82.9</strong> |
| | NaturalQuestions (EM) | 5-shot | 38.6 | 33.2 | <strong>41.5</strong> | 40.0 |
| | AGIEval (Acc.) | 0-shot | 57.5 | 75.8 | 60.6 | <strong>79.6</strong> |
| Code | HumanEval (Pass@1) | 0-shot | 43.3 | 53.0 | 54.9 | <strong>65.2</strong> |
| | MBPP (Pass@1) | 3-shot | 65.0 | 72.6 | 68.4 | <strong>75.4</strong> |
| | LiveCodeBench-Base (Pass@1) | 3-shot | 11.6 | 12.9 | 15.5 | <strong>19.4</strong> |
| | CRUXEval-I (Acc.) | 2-shot | 52.5 | 59.1 | 58.5 | <strong>67.3</strong> |
| | CRUXEval-O (Acc.) | 2-shot | 49.8 | 59.9 | 59.9 | <strong>69.8</strong> |
| Wiskunde | GSM8K (EM) | 8-shot | 81.6 | 88.3 | 83.5 | <strong>89.3</strong> |
| | MATH (EM) | 4-shot | 43.4 | 54.4 | 49.0 | <strong>61.6</strong> |
| | MGSM (EM) | 8-shot | 63.6 | 76.2 | 69.9 | <strong>79.8</strong> |
| | CMath (EM) | 3-shot | 78.7 | 84.5 | 77.3 | <strong>90.7</strong> |
| Chinees | CLUEWSC (EM) | 5-shot | 82.0 | 82.5 | <strong>83.0</strong> | 82.7 |
| | C-Eval (Acc.) | 5-shot | 81.4 | 89.2 | 72.5 | <strong>90.1</strong> |
| | CMMLU (Acc.) | 5-shot | 84.0 | <strong>89.5</strong> | 73.7 | 88.8 |
| | CMRC (EM) | 1-shot | <strong>77.4</strong> | 75.8 | 76.0 | 76.3 |
| | C3 (Acc.) | 0-shot | 77.4 | 76.7 | <strong>79.7</strong> | 78.6 |
| | CCPM (Acc.) | 0-shot | <strong>93.0</strong> | 88.5 | 78.6 | 92.0 |
| Meertalig | MMMLU-non-English (Acc.) | 5-shot | 64.0 | 74.8 | 73.8 | <strong>79.4</strong> |</p>
</div>
<blockquote>
<p>[!OPMERKING]
Beste resultaten zijn vetgedrukt. Scores met een verschil van niet meer dan 0,3 worden als gelijkwaardig beschouwd. DeepSeek-V3 behaalt de beste prestaties op de meeste benchmarks, vooral bij wiskunde- en codeertaken.
Voor meer evaluatiedetails, zie ons paper.</p>
</blockquote>
<h4>Contextvenster</h4>
<p align="center">
  <img width="80%" src="figures/niah.png">
</p>
<p>Evaluatieresultaten op de <code>Needle In A Haystack</code> (NIAH) tests. DeepSeek-V3 presteert goed over alle contextvensterlengtes tot <strong>128K</strong>.</p>
<h3>Chatmodel</h3>
<h4>Standaard Benchmarks (Modellen groter dan 67B)</h4>
<div align="center">
<p>| | <strong>Benchmark (Metriek)</strong> | <strong>DeepSeek V2-0506</strong> | <strong>DeepSeek V2.5-0905</strong> | <strong>Qwen2.5 72B-Inst.</strong> | <strong>Llama3.1 405B-Inst.</strong> | <strong>Claude-3.5-Sonnet-1022</strong> | <strong>GPT-4o 0513</strong> | <strong>DeepSeek V3</strong> |
|---|---------------------|---------------------|----------------------|---------------------|----------------------|---------------------------|----------------|----------------|
| | Architectuur | MoE | MoE | Dense | Dense | - | - | MoE |
| | # Geactiveerde Params | 21B | 21B | 72B | 405B | - | - | 37B |
| | # Totale Params | 236B | 236B | 72B | 405B | - | - | 671B |
| Engels | MMLU (EM) | 78.2 | 80.6 | 85.3 | <strong>88.6</strong> | <strong>88.3</strong> | 87.2 | <strong>88.5</strong> |
| | MMLU-Redux (EM) | 77.9 | 80.3 | 85.6 | 86.2 | <strong>88.9</strong> | 88.0 | <strong>89.1</strong> |
| | MMLU-Pro (EM) | 58.5 | 66.2 | 71.6 | 73.3 | <strong>78.0</strong> | 72.6 | 75.9 |
| | DROP (3-shot F1) | 83.0 | 87.8 | 76.7 | 88.7 | 88.3 | 83.7 | <strong>91.6</strong> |
| | IF-Eval (Prompt Strict) | 57.7 | 80.6 | 84.1 | 86.0 | <strong>86.5</strong> | 84.3 | 86.1 |
| | GPQA-Diamond (Pass@1) | 35.3 | 41.3 | 49.0 | 51.1 | <strong>65.0</strong> | 49.9 | 59.1 |
| | SimpleQA (Correct) | 9.0 | 10.2 | 9.1 | 17.1 | 28.4 | <strong>38.2</strong> | 24.9 |
| | FRAMES (Acc.) | 66.9 | 65.4 | 69.8 | 70.0 | 72.5 | <strong>80.5</strong> | 73.3 |
| | LongBench v2 (Acc.) | 31.6 | 35.4 | 39.4 | 36.1 | 41.0 | 48.1 | <strong>48.7</strong> |
| Code | HumanEval-Mul (Pass@1) | 69.3 | 77.4 | 77.3 | 77.2 | 81.7 | 80.5 | <strong>82.6</strong> |
| | LiveCodeBench (Pass@1-COT) | 18.8 | 29.2 | 31.1 | 28.4 | 36.3 | 33.4 | <strong>40.5</strong> |
| | LiveCodeBench (Pass@1) | 20.3 | 28.4 | 28.7 | 30.1 | 32.8 | 34.2 | <strong>37.6</strong> |
| | Codeforces (Percentiel) | 17.5 | 35.6 | 24.8 | 25.3 | 20.3 | 23.6 | <strong>51.6</strong> |
| | SWE Verified (Opgelost) | - | 22.6 | 23.8 | 24.5 | <strong>50.8</strong> | 38.8 | 42.0 |
| | Aider-Edit (Acc.) | 60.3 | 71.6 | 65.4 | 63.9 | <strong>84.2</strong> | 72.9 | 79.7 |
| | Aider-Polyglot (Acc.) | - | 18.2 | 7.6 | 5.8 | 45.3 | 16.0 | <strong>49.6</strong> |
| Wiskunde | AIME 2024 (Pass@1) | 4.6 | 16.7 | 23.3 | 23.3 | 16.0 | 9.3 | <strong>39.2</strong> |
| | MATH-500 (EM) | 56.3 | 74.7 | 80.0 | 73.8 | 78.3 | 74.6 | <strong>90.2</strong> |
| | CNMO 2024 (Pass@1) | 2.8 | 10.8 | 15.9 | 6.8 | 13.1 | 10.8 | <strong>43.2</strong> |
| Chinees | CLUEWSC (EM) | 89.9 | 90.4 | <strong>91.4</strong> | 84.7 | 85.4 | 87.9 | 90.9 |
| | C-Eval (EM) | 78.6 | 79.5 | 86.1 | 61.5 | 76.7 | 76.0 | <strong>86.5</strong> |
| | C-SimpleQA (Correct) | 48.5 | 54.1 | 48.4 | 50.4 | 51.3 | 59.3 | <strong>64.8</strong> |</p>
</div>
<blockquote>
<p>[!OPMERKING]
Alle modellen worden geëvalueerd in een configuratie die de outputlengte beperkt tot 8K. Benchmarks met minder dan 1000 samples worden meerdere keren getest met verschillende temperatuurinstellingen om robuuste eindresultaten te verkrijgen. DeepSeek-V3 is het best presterende open-source model en vertoont ook concurrerende prestaties ten opzichte van vooraanstaande gesloten modellen.</p>
</blockquote>
<h4>Open Ended Generation Evaluatie</h4>
<div align="center">
<p>| Model | Arena-Hard | AlpacaEval 2.0 |
|-------|------------|----------------|
| DeepSeek-V2.5-0905 | 76.2 | 50.5 |
| Qwen2.5-72B-Instruct | 81.2 | 49.1 |
| LLaMA-3.1 405B | 69.3 | 40.5 |
| GPT-4o-0513 | 80.4 | 51.1 |
| Claude-Sonnet-3.5-1022 | 85.2 | 52.0 |
| DeepSeek-V3 | <strong>85.5</strong> | <strong>70.0</strong> |</p>
</div>
<blockquote>
<p>[!OPMERKING]
Engelse open-ended gespreksbeoordelingen. Voor AlpacaEval 2.0 gebruiken we het lengte-gecontroleerde winpercentage als metriek.</p>
</blockquote>
<h2>5. Chatwebsite &amp; API Platform</h2>
<p>Je kunt chatten met DeepSeek-V3 op de officiële website van DeepSeek: <a href="https://chat.deepseek.com/sign_in">chat.deepseek.com</a></p>
<p>We bieden ook een OpenAI-compatibele API op het DeepSeek Platform: <a href="https://platform.deepseek.com/">platform.deepseek.com</a></p>
<h2>6. Lokaal uitvoeren</h2>
<p>DeepSeek-V3 kan lokaal worden ingezet met behulp van de volgende hardware en open-source community software:</p>
<ol>
<li><strong>DeepSeek-Infer Demo</strong>: We bieden een eenvoudige en lichte demo voor FP8 en BF16 inferentie.</li>
<li><strong>SGLang</strong>: Volledige ondersteuning voor het DeepSeek-V3 model in zowel BF16- als FP8-inferentiemodi, met Multi-Token Prediction <a href="https://github.com/sgl-project/sglang/issues/2591">binnenkort beschikbaar</a>.</li>
<li><strong>LMDeploy</strong>: Maakt efficiënte FP8 en BF16 inferentie mogelijk voor lokale en cloudimplementatie.</li>
<li><strong>TensorRT-LLM</strong>: Ondersteunt momenteel BF16-inferentie en INT4/8 kwantisatie, met FP8-ondersteuning binnenkort beschikbaar.</li>
<li><strong>vLLM</strong>: Ondersteunt DeepSeek-V3 model met FP8- en BF16-modi voor tensor parallelisme en pipeline parallelisme.</li>
<li><strong>LightLLM</strong>: Ondersteunt efficiënte single-node of multi-node implementatie voor FP8 en BF16.</li>
<li><strong>AMD GPU</strong>: Maakt het mogelijk om het DeepSeek-V3 model op AMD GPU's uit te voeren via SGLang in zowel BF16- als FP8-modi.</li>
<li><strong>Huawei Ascend NPU</strong>: Ondersteunt het draaien van DeepSeek-V3 op Huawei Ascend apparaten.</li>
</ol>
<p>Aangezien FP8-training standaard is geïmplementeerd in ons framework, leveren we alleen FP8-gewichten. Als je BF16-gewichten nodig hebt voor experimenten, kun je het bijgeleverde conversiescript gebruiken om de transformatie uit te voeren.</p>
<p>Hier is een voorbeeld van het converteren van FP8-gewichten naar BF16:</p>
<pre><code class="language-shell">cd inference
python fp8_cast_bf16.py --input-fp8-hf-path /path/to/fp8_weights --output-bf16-hf-path /path/to/bf16_weights
</code></pre>
<blockquote>
<p>[!OPMERKING]
Hugging Face's Transformers wordt nog niet direct ondersteund.</p>
</blockquote>
<h3>6.1 Inferentie met DeepSeek-Infer Demo (alleen voorbeeld)</h3>
<h4>Systeemeisen</h4>
<blockquote>
<p>[!OPMERKING]
Alleen Linux met Python 3.10. Mac en Windows worden niet ondersteund.</p>
</blockquote>
<p>Vereiste afhankelijkheden:</p>
<pre><code class="language-pip-requirements">torch==2.4.1
triton==3.0.0
transformers==4.46.3
safetensors==0.4.5
</code></pre>
<h4>Modelgewichten &amp; Demo Code Voorbereiding</h4>
<p>Kloon eerst onze DeepSeek-V3 GitHub repository:</p>
<pre><code class="language-shell">git clone https://github.com/deepseek-ai/DeepSeek-V3.git
</code></pre>
<p>Navigeer naar de <code>inference</code> map en installeer de afhankelijkheden uit <code>requirements.txt</code>. De makkelijkste manier is om een pakketbeheerder zoals <code>conda</code> of <code>uv</code> te gebruiken om een nieuwe virtuele omgeving te maken en de afhankelijkheden te installeren.</p>
<pre><code class="language-shell">cd DeepSeek-V3/inference
pip install -r requirements.txt
</code></pre>
<p>Download de modelgewichten van Hugging Face en plaats deze in de <code>/path/to/DeepSeek-V3</code> map.</p>
<h4>Modelgewichten Converteren</h4>
<p>Converteer Hugging Face modelgewichten naar een specifiek formaat:</p>
<pre><code class="language-shell">python convert.py --hf-ckpt-path /path/to/DeepSeek-V3 --save-path /path/to/DeepSeek-V3-Demo --n-experts 256 --model-parallel 16
</code></pre>
<h4>Uitvoeren</h4>
<p>Daarna kun je chatten met DeepSeek-V3:</p>
<pre><code class="language-shell">torchrun --nnodes 2 --nproc-per-node 8 --node-rank $RANK --master-addr $ADDR generate.py --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --interactive --temperature 0.7 --max-new-tokens 200
</code></pre>
<p>Of batch-inferentie op een opgegeven bestand:</p>
<pre><code class="language-shell">torchrun --nnodes 2 --nproc-per-node 8 --node-rank $RANK --master-addr $ADDR generate.py --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --input-file $FILE
</code></pre>
<h3>6.2 Inferentie met SGLang (aanbevolen)</h3>
<p><a href="https://github.com/sgl-project/sglang">SGLang</a> ondersteunt momenteel <a href="https://lmsys.org/blog/2024-09-04-sglang-v0-3/#deepseek-multi-head-latent-attention-mla-throughput-optimizations">MLA optimalisaties</a>, <a href="https://lmsys.org/blog/2024-12-04-sglang-v0-4/#data-parallelism-attention-for-deepseek-models">DP Attention</a>, FP8 (W8A8), FP8 KV Cache en Torch Compile, en levert state-of-the-art latency en throughput prestaties onder open-source frameworks.</p>
<p>Opmerkelijk is dat <a href="https://github.com/sgl-project/sglang/releases/tag/v0.4.1">SGLang v0.4.1</a> volledige ondersteuning biedt voor DeepSeek-V3 op zowel <strong>NVIDIA als AMD GPU's</strong>, waardoor het een veelzijdige en robuuste oplossing is.</p>
<p>SGLang ondersteunt ook <a href="https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3#example-serving-with-2-h208">multi-node tensorparallelisme</a>, waarmee je dit model op meerdere met elkaar verbonden machines kunt uitvoeren.</p>
<p>Multi-Token Prediction (MTP) is in ontwikkeling en de voortgang is te volgen in het <a href="https://github.com/sgl-project/sglang/issues/2591">optimalisatieplan</a>.</p>
<p>Hier zijn de startinstructies van het SGLang team: https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3</p>
<h3>6.3 Inferentie met LMDeploy (aanbevolen)</h3>
<p><a href="https://github.com/InternLM/lmdeploy">LMDeploy</a>, een flexibel en high-performance inferentie- en serveerframework voor grote taalmodellen, ondersteunt nu DeepSeek-V3. Het biedt zowel offline pipeline verwerking als online implementatie, en integreert naadloos met PyTorch-gebaseerde workflows.</p>
<p>Voor uitgebreide stapsgewijze instructies voor het uitvoeren van DeepSeek-V3 met LMDeploy, zie hier: https://github.com/InternLM/lmdeploy/issues/2960</p>
<h3>6.4 Inferentie met TRT-LLM (aanbevolen)</h3>
<p><a href="https://github.com/NVIDIA/TensorRT-LLM">TensorRT-LLM</a> ondersteunt nu het DeepSeek-V3 model, met precisieopties zoals BF16 en INT4/INT8 weight-only. Ondersteuning voor FP8 is momenteel in ontwikkeling en zal binnenkort worden uitgebracht. Je kunt de aangepaste branch van TRTLLM, specifiek voor DeepSeek-V3, direct hier vinden: https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/deepseek_v3.</p>
<h3>6.5 Inferentie met vLLM (aanbevolen)</h3>
<p><a href="https://github.com/vllm-project/vllm">vLLM</a> v0.6.6 ondersteunt DeepSeek-V3 inferentie voor FP8- en BF16-modi op zowel NVIDIA als AMD GPU's. Naast standaardtechnieken biedt vLLM <em>pipeline parallelism</em>, waarmee je dit model op meerdere via netwerken verbonden machines kunt uitvoeren. Zie de <a href="https://docs.vllm.ai/en/latest/serving/distributed_serving.html">vLLM instructies</a> voor gedetailleerde begeleiding. Volg ook gerust <a href="https://github.com/vllm-project/vllm/issues/11539">het verbeterplan</a>.</p>
<h3>6.6 Inferentie met LightLLM (aanbevolen)</h3>
<p><a href="https://github.com/ModelTC/lightllm/tree/main">LightLLM</a> v1.0.1 ondersteunt single-machine en multi-machine tensor parallel implementatie voor DeepSeek-R1 (FP8/BF16) en biedt mixed-precision implementatie, met steeds meer kwantisatiemodi in integratie. Zie voor meer details de <a href="https://lightllm-en.readthedocs.io/en/latest/getting_started/quickstart.html">LightLLM instructies</a>. Daarnaast biedt LightLLM PD-disaggregatie voor DeepSeek-V2, en de implementatie van PD-disaggregatie voor DeepSeek-V3 is in ontwikkeling.</p>
<h3>6.7 Aanbevolen inferentiefuncties met AMD GPU's</h3>
<p>In samenwerking met het AMD-team hebben we Day-One ondersteuning voor AMD GPU's bereikt met SGLang, met volledige compatibiliteit voor zowel FP8 als BF16 precisie. Raadpleeg de <a href="#63-inferentie-met-lmdeploy-aanbevolen">SGLang instructies</a> voor meer details.</p>
<h3>6.8 Aanbevolen inferentiefuncties met Huawei Ascend NPU's</h3>
<p>Het <a href="https://www.hiascend.com/en/software/mindie">MindIE</a> framework van de Huawei Ascend-community heeft met succes de BF16-versie van DeepSeek-V3 aangepast. Voor stapsgewijze begeleiding op Ascend NPU's, volg de <a href="https://modelers.cn/models/MindIE/deepseekv3">instructies hier</a>.</p>
<h2>7. Licentie</h2>
<p>Deze coderepository is gelicentieerd onder <a href="LICENSE-CODE">de MIT-licentie</a>. Het gebruik van DeepSeek-V3 Base/Chat modellen is onderworpen aan <a href="LICENSE-MODEL">de Model Licentie</a>. DeepSeek-V3-serie (inclusief Base en Chat) ondersteunt commercieel gebruik.</p>
<h2>8. Citeren</h2>
<pre><code>@misc{deepseekai2024deepseekv3technicalreport,
      title={DeepSeek-V3 Technical Report}, 
      author={DeepSeek-AI},
      year={2024},
      eprint={2412.19437},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.19437}, 
}
</code></pre>
<h2>9. Contact</h2>
<p>Als je vragen hebt, maak dan een issue aan of neem contact met ons op via <a href="service@deepseek.com">service@deepseek.com</a>.</p>
<hr />
<p><a href="https://github.com/OpenAiTx/OpenAiTx">Powered By OpenAiTx</a></p>
<hr />

        </div>
    </div>
    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Default Statcounter code for openaitx
https://openaitx.github.io/ -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>
    <!-- End of Statcounter Code -->
</body>
</html>