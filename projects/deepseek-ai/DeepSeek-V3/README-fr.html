<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DeepSeek-V3 - deepseek-ai/DeepSeek-V3 fr</title>
    <meta name="title" content="DeepSeek-V3 - deepseek-ai/DeepSeek-V3 fr | Lien vers l’article👁️ Table des matières Introduction Résumé du modèle Téléchargements du modèle Résultats d’évaluation Site de Chat &amp; Plateforme API Comme...">
    <meta name="description" content="deepseek-ai/DeepSeek-V3 - GitHub repository fr documentation and information | Lien vers l’article👁️ Table des matières Introduction Résumé du modèle Téléchargements du modèle Résultats d’évaluation Site de Chat &amp; Plateforme API Comme...">
    <meta name="keywords" content="deepseek-ai, DeepSeek-V3, GitHub, repository, fr documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/deepseek-ai/DeepSeek-V3/README-fr.html">
    <meta property="og:title" content="DeepSeek-V3 - deepseek-ai/DeepSeek-V3 fr | Lien vers l’article👁️ Table des matières Introduction Résumé du modèle Téléchargements du modèle Résultats d’évaluation Site de Chat &amp; Plateforme API Comme...">
    <meta property="og:description" content="deepseek-ai/DeepSeek-V3 - GitHub repository fr documentation and information | Lien vers l’article👁️ Table des matières Introduction Résumé du modèle Téléchargements du modèle Résultats d’évaluation Site de Chat &amp; Plateforme API Comme...">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">
    <script src="/js/marked.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        body { display: flex; flex-direction: column; min-height: 100vh; }
        .main-container { margin: 0 auto; width: 100%; max-width: 980px; padding: 0 20px; }
        @media (max-width: 768px) { .main-container { padding: 0 15px; } }
        .markdown-body img { max-width: 100%; height: auto; }
        .header { text-align: center; margin-bottom: 30px; padding: 20px; background-color: #f6f8fa; border-bottom: 1px solid #e1e4e8; position: relative; }
        .header .links { margin-top: 10px; font-size: 16px; }
        .header .links a { color: #0366d6; text-decoration: none; margin-left: 5px; }
        .header .links a:hover { text-decoration: underline; }
        .language-badges { margin-top: 15px; text-align: center; }
        .language-badges a { display: inline-block; margin: 2px; text-decoration: none; }
        .language-badges img { height: 20px; border-radius: 3px; }
        .language-badges a:hover img { opacity: 0.8; }
    </style>
</head>
<body>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/deepseek-ai/DeepSeek-V3" id="githubRepoLink" target="_blank">deepseek-ai/DeepSeek-V3</a>
<h1 style="display: none;">Lien vers l’article👁️ Table des matières Introduction Résumé du modèle Téléchargements du modèle Résultats d’évaluation Site de Chat &amp; Plateforme API Comme...</h1>
        </div>
        <div class="language-badges" id="languageBadges">
            <!-- You can generate language badges here if needed -->
        </div>
    </div>
    <div class="main-container">
        <div class="markdown-body" id="content">
            <!-- markdownlint-disable first-line-h1 -->
<!-- markdownlint-disable html -->
<!-- markdownlint-disable no-duplicate-header -->
<div align="center">
  <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" />
</div>
<hr>
<div align="center" style="line-height: 1;">
  <a href="https://www.deepseek.com/"><img alt="Homepage"
    src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true"/></a>
  <a href="https://chat.deepseek.com/"><img alt="Chat"
    src="https://img.shields.io/badge/🤖%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white"/></a>
  <a href="https://huggingface.co/deepseek-ai"><img alt="Hugging Face"
    src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white"/></a>
  <br>
  <a href="https://discord.gg/Tc7c45Zzu5"><img alt="Discord"
    src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da"/></a>
  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true"><img alt="Wechat"
    src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white"/></a>
  <a href="https://twitter.com/deepseek_ai"><img alt="Twitter Follow"
    src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white"/></a>
  <br>
  <a href="https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-CODE"><img alt="Code License"
    src="https://img.shields.io/badge/Code_License-MIT-f5de53?&color=f5de53"/></a>
  <a href="https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-MODEL"><img alt="Model License"
    src="https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&color=f5de53"/></a>
  <br>
  <a href="https://arxiv.org/pdf/2412.19437"><b>Lien vers l’article</b>👁️</a>
</div>
<h2>Table des matières</h2>
<ol>
<li><a href="#1-introduction">Introduction</a></li>
<li><a href="#2-r%C3%A9sum%C3%A9-du-mod%C3%A8le">Résumé du modèle</a></li>
<li><a href="#3-t%C3%A9l%C3%A9chargements-du-mod%C3%A8le">Téléchargements du modèle</a></li>
<li><a href="#4-r%C3%A9sultats-d%E2%80%99%C3%A9valuation">Résultats d’évaluation</a></li>
<li><a href="#5-site-de-chat--plateforme-api">Site de Chat &amp; Plateforme API</a></li>
<li><a href="#6-comment-ex%C3%A9cuter-localement">Comment exécuter localement</a></li>
<li><a href="#7-licence">Licence</a></li>
<li><a href="#8-citation">Citation</a></li>
<li><a href="#9-contact">Contact</a></li>
</ol>
<h2>1. Introduction</h2>
<p>Nous présentons DeepSeek-V3, un puissant modèle de langage Mixture-of-Experts (MoE) avec 671 milliards de paramètres totaux dont 37 milliards activés par jeton.<br />
Pour atteindre une inférence efficace et un entraînement économique, DeepSeek-V3 adopte les architectures Multi-head Latent Attention (MLA) et DeepSeekMoE, validées en profondeur dans DeepSeek-V2.<br />
De plus, DeepSeek-V3 innove avec une stratégie d’équilibrage de charge sans perte auxiliaire et définit un objectif d’entraînement de prédiction multi-jetons pour des performances accrues.<br />
Nous pré-entraînons DeepSeek-V3 sur 14,8 billions de jetons diversifiés et de haute qualité, suivis par des étapes de fine-tuning supervisé et d’apprentissage par renforcement pour exploiter pleinement ses capacités.<br />
Des évaluations complètes révèlent que DeepSeek-V3 surpasse les autres modèles open-source et atteint des performances comparables aux modèles propriétaires de pointe.<br />
Malgré ses excellentes performances, DeepSeek-V3 ne requiert que 2,788 millions d’heures GPU H800 pour son entraînement complet.<br />
De plus, le processus d’entraînement est remarquablement stable.<br />
Tout au long de l’entraînement, nous n’avons rencontré aucune montée de perte irrécupérable ni effectué de retour arrière.</p>
<p align="center">
  <img width="80%" src="figures/benchmark.png">
</p>
<h2>2. Résumé du modèle</h2>
<hr />
<p><strong>Architecture : Stratégie innovante d’équilibrage de charge et objectif d’entraînement</strong></p>
<ul>
<li>Sur la base de l’architecture efficace de DeepSeek-V2, nous innovons avec une stratégie d’équilibrage de charge sans perte auxiliaire, minimisant la dégradation des performances causée par l’équilibrage de charge.</li>
<li>Nous étudions un objectif de Prédiction Multi-Jetons (MTP) et prouvons son intérêt pour les performances du modèle.<br />
Il peut aussi être utilisé pour le décodage spéculatif afin d’accélérer l’inférence.</li>
</ul>
<hr />
<p><strong>Pré-entraînement : Vers une efficacité d’entraînement ultime</strong></p>
<ul>
<li>Nous concevons un cadre d’entraînement en précision mixte FP8 et, pour la première fois, validons la faisabilité et l’efficacité de l’entraînement FP8 sur un modèle à très grande échelle.</li>
<li>Grâce à la co-conception d’algorithmes, de frameworks et de matériel, nous surmontons le goulot d’étranglement de la communication dans l’entraînement MoE inter-nœuds, atteignant presque un recouvrement complet calcul-communication.<br />
Cela améliore considérablement notre efficacité d’entraînement et réduit les coûts, nous permettant d’augmenter encore la taille du modèle sans surcoût.</li>
<li>À un coût économique de seulement 2,664 millions d’heures GPU H800, nous terminons le pré-entraînement de DeepSeek-V3 sur 14,8T jetons, produisant le modèle open-source de base le plus puissant à ce jour. Les étapes d’entraînement suivantes après le pré-entraînement ne nécessitent que 0,1 million d’heures GPU.</li>
</ul>
<hr />
<p><strong>Post-entraînement : Distillation des connaissances depuis DeepSeek-R1</strong></p>
<ul>
<li>Nous introduisons une méthodologie innovante pour distiller les capacités de raisonnement du modèle long-Chain-of-Thought (CoT), en particulier d’un des modèles de la série DeepSeek R1, dans des LLM standards, notamment DeepSeek-V3. Notre pipeline intègre élégamment les motifs de vérification et de réflexion de R1 dans DeepSeek-V3 et améliore nettement ses performances de raisonnement. Nous conservons également le contrôle sur le style et la longueur de sortie de DeepSeek-V3.</li>
</ul>
<hr />
<h2>3. Téléchargements du modèle</h2>
<div align="center">
<p>| <strong>Modèle</strong> | <strong>#Paramètres totaux</strong> | <strong>#Paramètres activés</strong> | <strong>Longueur de contexte</strong> | <strong>Téléchargement</strong> |
| :------------: | :------------: | :------------: | :------------: | :------------: |
| DeepSeek-V3-Base | 671B | 37B | 128K   | <a href="https://huggingface.co/deepseek-ai/DeepSeek-V3-Base">🤗 Hugging Face</a>   |
| DeepSeek-V3   | 671B | 37B |  128K   | <a href="https://huggingface.co/deepseek-ai/DeepSeek-V3">🤗 Hugging Face</a>   |</p>
</div>
<blockquote>
<p>[!NOTE]
La taille totale des modèles DeepSeek-V3 sur Hugging Face est de 685B, ce qui inclut 671B pour les poids du modèle principal et 14B pour les poids du module Multi-Token Prediction (MTP).</p>
</blockquote>
<p>Pour garantir des performances et une flexibilité optimales, nous avons collaboré avec des communautés open-source et des fournisseurs matériels pour proposer plusieurs méthodes d’exécution locale du modèle. Pour des instructions détaillées, consultez la section 6 : <a href="#6-comment-ex%C3%A9cuter-localement">Comment exécuter localement</a>.</p>
<p>Pour les développeurs souhaitant aller plus loin, nous recommandons d’explorer <a href="./README_WEIGHTS.md">README_WEIGHTS.md</a> pour plus d’informations sur les poids du modèle principal et les modules MTP. Notez que la prise en charge de MTP est en cours de développement au sein de la communauté ; vos contributions et retours sont les bienvenus.</p>
<h2>4. Résultats d’évaluation</h2>
<h3>Modèle de base</h3>
<h4>Benchmarks standards</h4>
<div align="center">
<p>|  | Benchmark (Métrique) | # Shots | DeepSeek-V2 | Qwen2.5 72B | LLaMA3.1 405B | DeepSeek-V3 |
|---|---------------------|----------|--------|-------------|---------------|---------|
| | Architecture | - | MoE | Dense | Dense | MoE |
| | # Param. activés | - | 21B | 72B | 405B | 37B |
| | # Param. totaux | - | 236B | 72B | 405B | 671B |
| Anglais | Pile-test (BPB) | - | 0.606 | 0.638 | <strong>0.542</strong> | 0.548 |
| | BBH (EM) | 3-shot | 78.8 | 79.8 | 82.9 | <strong>87.5</strong> |
| | MMLU (Acc.) | 5-shot | 78.4 | 85.0 | 84.4 | <strong>87.1</strong> |
| | MMLU-Redux (Acc.) | 5-shot | 75.6 | 83.2 | 81.3 | <strong>86.2</strong> |
| | MMLU-Pro (Acc.) | 5-shot | 51.4 | 58.3 | 52.8 | <strong>64.4</strong> |
| | DROP (F1) | 3-shot | 80.4 | 80.6 | 86.0 | <strong>89.0</strong> |
| | ARC-Easy (Acc.) | 25-shot | 97.6 | 98.4 | 98.4 | <strong>98.9</strong> |
| | ARC-Challenge (Acc.) | 25-shot | 92.2 | 94.5 | <strong>95.3</strong> | <strong>95.3</strong> |
| | HellaSwag (Acc.) | 10-shot | 87.1 | 84.8 | <strong>89.2</strong> | 88.9 |
| | PIQA (Acc.) | 0-shot | 83.9 | 82.6 | <strong>85.9</strong> | 84.7 |
| | WinoGrande (Acc.) | 5-shot | <strong>86.3</strong> | 82.3 | 85.2 | 84.9 |
| | RACE-Middle (Acc.) | 5-shot | 73.1 | 68.1 | <strong>74.2</strong> | 67.1 |
| | RACE-High (Acc.) | 5-shot | 52.6 | 50.3 | <strong>56.8</strong> | 51.3 |
| | TriviaQA (EM) | 5-shot | 80.0 | 71.9 | 82.7 | <strong>82.9</strong> |
| | NaturalQuestions (EM) | 5-shot | 38.6 | 33.2 | <strong>41.5</strong> | 40.0 |
| | AGIEval (Acc.) | 0-shot | 57.5 | 75.8 | 60.6 | <strong>79.6</strong> |
| Code | HumanEval (Pass@1) | 0-shot | 43.3 | 53.0 | 54.9 | <strong>65.2</strong> |
| | MBPP (Pass@1) | 3-shot | 65.0 | 72.6 | 68.4 | <strong>75.4</strong> |
| | LiveCodeBench-Base (Pass@1) | 3-shot | 11.6 | 12.9 | 15.5 | <strong>19.4</strong> |
| | CRUXEval-I (Acc.) | 2-shot | 52.5 | 59.1 | 58.5 | <strong>67.3</strong> |
| | CRUXEval-O (Acc.) | 2-shot | 49.8 | 59.9 | 59.9 | <strong>69.8</strong> |
| Math | GSM8K (EM) | 8-shot | 81.6 | 88.3 | 83.5 | <strong>89.3</strong> |
| | MATH (EM) | 4-shot | 43.4 | 54.4 | 49.0 | <strong>61.6</strong> |
| | MGSM (EM) | 8-shot | 63.6 | 76.2 | 69.9 | <strong>79.8</strong> |
| | CMath (EM) | 3-shot | 78.7 | 84.5 | 77.3 | <strong>90.7</strong> |
| Chinois | CLUEWSC (EM) | 5-shot | 82.0 | 82.5 | <strong>83.0</strong> | 82.7 |
| | C-Eval (Acc.) | 5-shot | 81.4 | 89.2 | 72.5 | <strong>90.1</strong> |
| | CMMLU (Acc.) | 5-shot | 84.0 | <strong>89.5</strong> | 73.7 | 88.8 |
| | CMRC (EM) | 1-shot | <strong>77.4</strong> | 75.8 | 76.0 | 76.3 |
| | C3 (Acc.) | 0-shot | 77.4 | 76.7 | <strong>79.7</strong> | 78.6 |
| | CCPM (Acc.) | 0-shot | <strong>93.0</strong> | 88.5 | 78.6 | 92.0 |
| Multilingue | MMMLU-non-English (Acc.) | 5-shot | 64.0 | 74.8 | 73.8 | <strong>79.4</strong> |</p>
</div>
<blockquote>
<p>[!NOTE]
Les meilleurs résultats sont en gras. Les scores avec un écart inférieur ou égal à 0,3 sont considérés du même niveau. DeepSeek-V3 atteint les meilleures performances sur la plupart des benchmarks, notamment en mathématiques et en programmation.
Pour plus de détails d’évaluation, consultez notre article.</p>
</blockquote>
<h4>Fenêtre de contexte</h4>
<p align="center">
  <img width="80%" src="figures/niah.png">
</p>
<p>Résultats d’évaluation sur les tests <code>Needle In A Haystack</code> (NIAH). DeepSeek-V3 affiche de bonnes performances sur toutes les longueurs de fenêtre de contexte jusqu’à <strong>128K</strong>.</p>
<h3>Modèle de chat</h3>
<h4>Benchmarks standards (modèles &gt; 67B)</h4>
<div align="center">
<p>| | <strong>Benchmark (Métrique)</strong> | <strong>DeepSeek V2-0506</strong> | <strong>DeepSeek V2.5-0905</strong> | <strong>Qwen2.5 72B-Inst.</strong> | <strong>Llama3.1 405B-Inst.</strong> | <strong>Claude-3.5-Sonnet-1022</strong> | <strong>GPT-4o 0513</strong> | <strong>DeepSeek V3</strong> |
|---|---------------------|---------------------|----------------------|---------------------|----------------------|---------------------------|----------------|----------------|
| | Architecture | MoE | MoE | Dense | Dense | - | - | MoE |
| | # Param. activés | 21B | 21B | 72B | 405B | - | - | 37B |
| | # Param. totaux | 236B | 236B | 72B | 405B | - | - | 671B |
| Anglais | MMLU (EM) | 78.2 | 80.6 | 85.3 | <strong>88.6</strong> | <strong>88.3</strong> | 87.2 | <strong>88.5</strong> |
| | MMLU-Redux (EM) | 77.9 | 80.3 | 85.6 | 86.2 | <strong>88.9</strong> | 88.0 | <strong>89.1</strong> |
| | MMLU-Pro (EM) | 58.5 | 66.2 | 71.6 | 73.3 | <strong>78.0</strong> | 72.6 | 75.9 |
| | DROP (3-shot F1) | 83.0 | 87.8 | 76.7 | 88.7 | 88.3 | 83.7 | <strong>91.6</strong> |
| | IF-Eval (Prompt Strict) | 57.7 | 80.6 | 84.1 | 86.0 | <strong>86.5</strong> | 84.3 | 86.1 |
| | GPQA-Diamond (Pass@1) | 35.3 | 41.3 | 49.0 | 51.1 | <strong>65.0</strong> | 49.9 | 59.1 |
| | SimpleQA (Correct) | 9.0 | 10.2 | 9.1 | 17.1 | 28.4 | <strong>38.2</strong> | 24.9 |
| | FRAMES (Acc.) | 66.9 | 65.4 | 69.8 | 70.0 | 72.5 | <strong>80.5</strong> | 73.3 |
| | LongBench v2 (Acc.) | 31.6 | 35.4 | 39.4 | 36.1 | 41.0 | 48.1 | <strong>48.7</strong> |
| Code | HumanEval-Mul (Pass@1) | 69.3 | 77.4 | 77.3 | 77.2 | 81.7 | 80.5 | <strong>82.6</strong> |
| | LiveCodeBench (Pass@1-COT) | 18.8 | 29.2 | 31.1 | 28.4 | 36.3 | 33.4 | <strong>40.5</strong> |
| | LiveCodeBench (Pass@1) | 20.3 | 28.4 | 28.7 | 30.1 | 32.8 | 34.2 | <strong>37.6</strong> |
| | Codeforces (Percentile) | 17.5 | 35.6 | 24.8 | 25.3 | 20.3 | 23.6 | <strong>51.6</strong> |
| | SWE Verified (Resolved) | - | 22.6 | 23.8 | 24.5 | <strong>50.8</strong> | 38.8 | 42.0 |
| | Aider-Edit (Acc.) | 60.3 | 71.6 | 65.4 | 63.9 | <strong>84.2</strong> | 72.9 | 79.7 |
| | Aider-Polyglot (Acc.) | - | 18.2 | 7.6 | 5.8 | 45.3 | 16.0 | <strong>49.6</strong> |
| Math | AIME 2024 (Pass@1) | 4.6 | 16.7 | 23.3 | 23.3 | 16.0 | 9.3 | <strong>39.2</strong> |
| | MATH-500 (EM) | 56.3 | 74.7 | 80.0 | 73.8 | 78.3 | 74.6 | <strong>90.2</strong> |
| | CNMO 2024 (Pass@1) | 2.8 | 10.8 | 15.9 | 6.8 | 13.1 | 10.8 | <strong>43.2</strong> |
| Chinois | CLUEWSC (EM) | 89.9 | 90.4 | <strong>91.4</strong> | 84.7 | 85.4 | 87.9 | 90.9 |
| | C-Eval (EM) | 78.6 | 79.5 | 86.1 | 61.5 | 76.7 | 76.0 | <strong>86.5</strong> |
| | C-SimpleQA (Correct) | 48.5 | 54.1 | 48.4 | 50.4 | 51.3 | 59.3 | <strong>64.8</strong> |</p>
</div>
<blockquote>
<p>[!NOTE]
Tous les modèles sont évalués dans une configuration limitant la sortie à 8K. Les benchmarks comportant moins de 1000 échantillons sont testés plusieurs fois avec différentes températures afin d’obtenir des résultats robustes. DeepSeek-V3 est le meilleur modèle open-source testé, et affiche également des performances compétitives face aux modèles propriétaires de pointe.</p>
</blockquote>
<h4>Évaluation de génération ouverte</h4>
<div align="center">
<p>| Modèle | Arena-Hard | AlpacaEval 2.0 |
|--------|------------|----------------|
| DeepSeek-V2.5-0905 | 76.2 | 50.5 |
| Qwen2.5-72B-Instruct | 81.2 | 49.1 |
| LLaMA-3.1 405B | 69.3 | 40.5 |
| GPT-4o-0513 | 80.4 | 51.1 |
| Claude-Sonnet-3.5-1022 | 85.2 | 52.0 |
| DeepSeek-V3 | <strong>85.5</strong> | <strong>70.0</strong> |</p>
</div>
<blockquote>
<p>[!NOTE]
Évaluations en conversation ouverte en anglais. Pour AlpacaEval 2.0, nous utilisons le taux de victoire contrôlé en longueur comme métrique.</p>
</blockquote>
<h2>5. Site de Chat &amp; Plateforme API</h2>
<p>Vous pouvez discuter avec DeepSeek-V3 sur le site officiel de DeepSeek : <a href="https://chat.deepseek.com/sign_in">chat.deepseek.com</a></p>
<p>Nous proposons également une API compatible OpenAI sur la plateforme DeepSeek : <a href="https://platform.deepseek.com/">platform.deepseek.com</a></p>
<h2>6. Comment exécuter localement</h2>
<p>DeepSeek-V3 peut être déployé localement à l’aide du matériel et des logiciels open-source suivants :</p>
<ol>
<li><strong>DeepSeek-Infer Demo</strong> : Nous proposons une démo simple et légère pour l’inférence FP8 et BF16.</li>
<li><strong>SGLang</strong> : Prise en charge complète du modèle DeepSeek-V3 en modes d’inférence BF16 et FP8, avec Multi-Token Prediction <a href="https://github.com/sgl-project/sglang/issues/2591">à venir</a>.</li>
<li><strong>LMDeploy</strong> : Permet une inférence FP8 et BF16 efficace pour un déploiement local ou cloud.</li>
<li><strong>TensorRT-LLM</strong> : Prise en charge actuelle de l’inférence BF16 et quantification INT4/8, la compatibilité FP8 arrive prochainement.</li>
<li><strong>vLLM</strong> : Prise en charge du modèle DeepSeek-V3 en modes FP8 et BF16 pour le parallélisme de tenseur et le parallélisme de pipeline.</li>
<li><strong>LightLLM</strong> : Prise en charge d’un déploiement efficace sur un ou plusieurs nœuds pour FP8 et BF16.</li>
<li><strong>AMD GPU</strong> : Permet d’exécuter DeepSeek-V3 sur GPU AMD via SGLang en modes BF16 et FP8.</li>
<li><strong>Huawei Ascend NPU</strong> : Prise en charge de DeepSeek-V3 sur les appareils Huawei Ascend.</li>
</ol>
<p>Puisque l’entraînement FP8 est nativement adopté dans notre framework, nous ne fournissons que les poids FP8. Si vous avez besoin des poids BF16 pour vos expériences, vous pouvez utiliser le script de conversion fourni.</p>
<p>Exemple de conversion de poids FP8 vers BF16 :</p>
<pre><code class="language-shell">cd inference
python fp8_cast_bf16.py --input-fp8-hf-path /path/to/fp8_weights --output-bf16-hf-path /path/to/bf16_weights
</code></pre>
<blockquote>
<p>[!NOTE]
Transformers de Hugging Face n’est pas encore directement pris en charge.</p>
</blockquote>
<h3>6.1 Inférence avec la démo DeepSeek-Infer (exemple uniquement)</h3>
<h4>Prérequis système</h4>
<blockquote>
<p>[!NOTE]
Linux avec Python 3.10 uniquement. Mac et Windows ne sont pas supportés.</p>
</blockquote>
<p>Dépendances :</p>
<pre><code class="language-pip-requirements">torch==2.4.1
triton==3.0.0
transformers==4.46.3
safetensors==0.4.5
</code></pre>
<h4>Préparation des poids du modèle &amp; du code de démo</h4>
<p>Clonez d’abord notre dépôt GitHub DeepSeek-V3 :</p>
<pre><code class="language-shell">git clone https://github.com/deepseek-ai/DeepSeek-V3.git
</code></pre>
<p>Rendez-vous dans le dossier <code>inference</code> et installez les dépendances listées dans <code>requirements.txt</code>. Le plus simple est d’utiliser un gestionnaire d’environnements comme <code>conda</code> ou <code>uv</code> pour créer un nouvel environnement virtuel et installer les dépendances.</p>
<pre><code class="language-shell">cd DeepSeek-V3/inference
pip install -r requirements.txt
</code></pre>
<p>Téléchargez les poids du modèle depuis Hugging Face et placez-les dans le dossier <code>/path/to/DeepSeek-V3</code>.</p>
<h4>Conversion des poids du modèle</h4>
<p>Convertissez les poids du modèle Hugging Face vers le format spécifique :</p>
<pre><code class="language-shell">python convert.py --hf-ckpt-path /path/to/DeepSeek-V3 --save-path /path/to/DeepSeek-V3-Demo --n-experts 256 --model-parallel 16
</code></pre>
<h4>Exécution</h4>
<p>Vous pouvez alors discuter avec DeepSeek-V3 :</p>
<pre><code class="language-shell">torchrun --nnodes 2 --nproc-per-node 8 --node-rank $RANK --master-addr $ADDR generate.py --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --interactive --temperature 0.7 --max-new-tokens 200
</code></pre>
<p>Ou faire une inférence par lot sur un fichier donné :</p>
<pre><code class="language-shell">torchrun --nnodes 2 --nproc-per-node 8 --node-rank $RANK --master-addr $ADDR generate.py --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --input-file $FILE
</code></pre>
<h3>6.2 Inférence avec SGLang (recommandé)</h3>
<p><a href="https://github.com/sgl-project/sglang">SGLang</a> prend actuellement en charge les <a href="https://lmsys.org/blog/2024-09-04-sglang-v0-3/#deepseek-multi-head-latent-attention-mla-throughput-optimizations">optimisations MLA</a>, <a href="https://lmsys.org/blog/2024-12-04-sglang-v0-4/#data-parallelism-attention-for-deepseek-models">l’attention DP</a>, FP8 (W8A8), cache FP8 KV et Torch Compile, offrant des performances de latence et de débit de pointe parmi les frameworks open-source.</p>
<p>Notamment, <a href="https://github.com/sgl-project/sglang/releases/tag/v0.4.1">SGLang v0.4.1</a> prend en charge l’exécution de DeepSeek-V3 sur <strong>GPU NVIDIA et AMD</strong>, ce qui en fait une solution très polyvalente et robuste.</p>
<p>SGLang prend également en charge le <a href="https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3#example-serving-with-2-h208">parallélisme de tenseur multi-nœuds</a>, permettant d’exécuter ce modèle sur plusieurs machines connectées en réseau.</p>
<p>La Prédiction Multi-Jetons (MTP) est en développement, les avancées sont à suivre dans le <a href="https://github.com/sgl-project/sglang/issues/2591">plan d’optimisation</a>.</p>
<p>Voici les instructions de lancement de l’équipe SGLang : https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3</p>
<h3>6.3 Inférence avec LMDeploy (recommandé)</h3>
<p><a href="https://github.com/InternLM/lmdeploy">LMDeploy</a>, un framework d’inférence et de service flexible et haute performance dédié aux grands modèles de langage, supporte désormais DeepSeek-V3. Il propose à la fois le traitement pipeline hors ligne et le déploiement en ligne, s’intégrant parfaitement aux workflows basés sur PyTorch.</p>
<p>Pour des instructions détaillées sur l’utilisation de DeepSeek-V3 avec LMDeploy, consultez : https://github.com/InternLM/lmdeploy/issues/2960</p>
<h3>6.4 Inférence avec TRT-LLM (recommandé)</h3>
<p><a href="https://github.com/NVIDIA/TensorRT-LLM">TensorRT-LLM</a> prend désormais en charge DeepSeek-V3, offrant des options de précision telles que BF16 et INT4/INT8 (poids uniquement). La prise en charge FP8 est en cours de développement et sera bientôt disponible. Vous pouvez accéder à la branche personnalisée de TRTLLM pour DeepSeek-V3 ici : https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/deepseek_v3.</p>
<h3>6.5 Inférence avec vLLM (recommandé)</h3>
<p><a href="https://github.com/vllm-project/vllm">vLLM</a> v0.6.6 prend en charge l’inférence DeepSeek-V3 pour les modes FP8 et BF16 sur GPU NVIDIA et AMD. Outre les techniques standards, vLLM propose le <em>parallélisme de pipeline</em> permettant de faire tourner ce modèle sur plusieurs machines en réseau. Pour plus d’informations, consultez les <a href="https://docs.vllm.ai/en/latest/serving/distributed_serving.html">instructions vLLM</a>. Vous pouvez également suivre <a href="https://github.com/vllm-project/vllm/issues/11539">le plan d’amélioration</a>.</p>
<h3>6.6 Inférence avec LightLLM (recommandé)</h3>
<p><a href="https://github.com/ModelTC/lightllm/tree/main">LightLLM</a> v1.0.1 prend en charge le déploiement tensoriel parallèle mono et multi-machine pour DeepSeek-R1 (FP8/BF16) et propose un déploiement en précision mixte, avec davantage de modes de quantification intégrés en continu. Pour plus de détails, consultez la <a href="https://lightllm-en.readthedocs.io/en/latest/getting_started/quickstart.html">documentation LightLLM</a>. De plus, LightLLM offre un déploiement PD-disaggregation pour DeepSeek-V2, l’implémentation pour DeepSeek-V3 étant en cours de développement.</p>
<h3>6.7 Fonctionnalité d’inférence recommandée sur GPU AMD</h3>
<p>En collaboration avec l’équipe AMD, nous avons assuré une compatibilité Day-One des GPU AMD via SGLang, avec prise en charge complète FP8 et BF16. Pour plus de détails, consultez les <a href="#63-inf%C3%A9rence-avec-lmdeploy-recommand%C3%A9">instructions SGLang</a>.</p>
<h3>6.8 Fonctionnalité d’inférence recommandée sur Huawei Ascend NPU</h3>
<p>Le framework <a href="https://www.hiascend.com/en/software/mindie">MindIE</a> de la communauté Huawei Ascend a adapté avec succès la version BF16 de DeepSeek-V3. Pour un guide étape par étape sur Ascend NPU, consultez les <a href="https://modelers.cn/models/MindIE/deepseekv3">instructions ici</a>.</p>
<h2>7. Licence</h2>
<p>Ce dépôt de code est sous licence <a href="LICENSE-CODE">MIT License</a>. L’utilisation des modèles DeepSeek-V3 Base/Chat est soumise à <a href="LICENSE-MODEL">la licence du modèle</a>. La série DeepSeek-V3 (Base et Chat inclus) est utilisable à des fins commerciales.</p>
<h2>8. Citation</h2>
<pre><code>@misc{deepseekai2024deepseekv3technicalreport,
      title={DeepSeek-V3 Technical Report}, 
      author={DeepSeek-AI},
      year={2024},
      eprint={2412.19437},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.19437}, 
}
</code></pre>
<h2>9. Contact</h2>
<p>Pour toute question, veuillez ouvrir une issue ou nous contacter à <a href="service@deepseek.com">service@deepseek.com</a>.</p>
<hr />
<p><a href="https://github.com/OpenAiTx/OpenAiTx">Powered By OpenAiTx</a></p>
<hr />

        </div>
    </div>
    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Default Statcounter code for openaitx
https://openaitx.github.io/ -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>
    <!-- End of Statcounter Code -->
</body>
</html>