<!DOCTYPE html><html lang="en"><head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AgenticSeek: Alternativa privada e local ao Manus. - Fosowl/agenticSeek</title>

    <!-- Primary Meta Tags -->
    <meta name="title" content="AgenticSeek: Alternativa privada e local ao Manus. - Fosowl/agenticSeek">
    <meta name="description" content="Fosowl/agenticSeek - GitHub repository pt documentation and information">
    <meta name="keywords" content="Fosowl, agenticSeek, GitHub, repository, pt documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">

    <!-- Open Graph -->
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/Fosowl/agenticSeek/README-pt.html">
    <meta property="og:title" content="AgenticSeek: Alternativa privada e local ao Manus. - Fosowl/agenticSeek">
    <meta property="og:description" content="Fosowl/agenticSeek - GitHub repository pt documentation and information">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">

    <!-- Favicon -->
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">

    <!-- Marked.js for Markdown rendering -->
    <script type="text/javascript" async="" src="https://www.statcounter.com/counter/recorder.js"></script><script src="/js/marked.min.js?v=20250613"></script>
    <!-- Highlight.js for code syntax highlighting -->
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <!-- Custom CSS -->
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        /* Layout */
        body {
            display: flex;
            flex-direction: column;
            min-height: 100vh;
        }

        .main-container {
            margin: 0 auto;
            width: 100%;
            max-width: 980px;
            padding: 0 20px;
        }

        @media (max-width: 768px) {
            .main-container {
                padding: 0 15px;
            }
        }

        /* Image size restrictions */
        .markdown-body img {
            max-width: 100%;
            height: auto;
        }

        /* Existing styles */
        .header {
            text-align: center;
            margin-bottom: 30px;
            padding: 20px;
            background-color: #f6f8fa;
            border-bottom: 1px solid #e1e4e8;
            position: relative;
        }

        .back-button {
            position: absolute;
            left: 20px;
            top: 50%;
            transform: translateY(-50%);
            color: #0366d6;
            text-decoration: none;
            display: flex;
            align-items: center;
            font-size: 14px;
            padding: 5px 10px;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            background-color: #fff;
        }

        .back-button:hover {
            background-color: #f6f8fa;
            border-color: #0366d6;
        }

        .back-button::before {
            content: "←";
            margin-right: 5px;
            font-size: 16px;
        }

        .header .links {
            margin-top: 10px;
            font-size: 16px;
        }

        .header .links a {
            color: #0366d6;
            text-decoration: none;
            margin-left: 5px;
        }

        .header .links a:hover {
            text-decoration: underline;
        }
        
        /* Language badges styles */
        .language-badges {
            margin-top: 15px;
            text-align: center;
        }
        .language-badges a {
            display: inline-block;
            margin: 2px;
            text-decoration: none;
        }
        .language-badges img {
            height: 20px;
            border-radius: 3px;
        }
        .language-badges a:hover img {
            opacity: 0.8;
        }
    </style>
</head>

<body>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/Fosowl/agenticSeek" id="githubRepoLink" target="_blank">Fosowl/agenticSeek</a>
        </div>
        <div class="language-badges" id="languageBadges"><a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=en"><img src="https://img.shields.io/badge/EN-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=zh-CN"><img src="https://img.shields.io/badge/简中-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=zh-TW"><img src="https://img.shields.io/badge/繁中-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=ja"><img src="https://img.shields.io/badge/日本語-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=ko"><img src="https://img.shields.io/badge/한국어-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=th"><img src="https://img.shields.io/badge/ไทย-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=fr"><img src="https://img.shields.io/badge/Français-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=de"><img src="https://img.shields.io/badge/Deutsch-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=es"><img src="https://img.shields.io/badge/Español-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=it"><img src="https://img.shields.io/badge/Italiano-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=ru"><img src="https://img.shields.io/badge/Русский-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=pt"><img src="https://img.shields.io/badge/Português-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=nl"><img src="https://img.shields.io/badge/Nederlands-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=pl"><img src="https://img.shields.io/badge/Polski-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=ar"><img src="https://img.shields.io/badge/العربية-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=tr"><img src="https://img.shields.io/badge/Türkçe-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=vi"><img src="https://img.shields.io/badge/Tiếng Việt-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=hi"><img src="https://img.shields.io/badge/हिंदी-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=fa"><img src="https://img.shields.io/badge/فارسی-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=id"><img src="https://img.shields.io/badge/Bahasa Indonesia-white" alt="version"></a></div>
    </div>

    <div class="main-container">
        <div class="markdown-body" id="content"><h1>AgenticSeek: Alternativa privada e local ao Manus.</h1>
<p align="center">
<img align="center" src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/agentic_seek_logo.png" width="300" height="300" alt="Agentic Seek Logo">
</p><p>

</p><p>  English | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_CHS.md">中文</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_CHT.md">繁體中文</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_FR.md">Français</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_JP.md">日本語</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_PTBR.md">Português (Brasil)</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_ES.md">Español</a></p>
<p><em>Uma <strong>alternativa 100% local ao Manus AI</strong>, este assistente de IA habilitado por voz navega autonomamente na web, escreve código e planeja tarefas enquanto mantém todos os dados em seu dispositivo. Projetado para modelos de raciocínio locais, ele roda inteiramente no seu hardware, garantindo total privacidade e nenhuma dependência da nuvem.</em></p>
<p><a href="https://fosowl.github.io/agenticSeek.html"><img src="https://img.shields.io/static/v1?label=Website&amp;message=AgenticSeek&amp;color=blue&amp;style=flat-square" alt="Visite AgenticSeek"></a> <img src="https://img.shields.io/badge/license-GPL--3.0-green" alt="Licença"> <a href="https://discord.gg/8hGDaME3TC"><img src="https://img.shields.io/badge/Discord-Join%20Us-7289DA?logo=discord&amp;logoColor=white" alt="Discord"></a> <a href="https://x.com/Martin993886460"><img src="https://img.shields.io/twitter/url/https/twitter.com/fosowl.svg?style=social&amp;label=Update%20%40Fosowl" alt="Twitter"></a> <a href="https://github.com/Fosowl/agenticSeek/stargazers"><img src="https://img.shields.io/github/stars/Fosowl/agenticSeek?style=social" alt="Estrelas no GitHub"></a></p>
<h3>Por que AgenticSeek?</h3>
<ul>
<li><p>🔒 Totalmente Local &amp; Privado - Tudo roda em sua máquina — sem nuvem, sem compartilhamento de dados. Seus arquivos, conversas e pesquisas permanecem privados.</p>
</li>
<li><p>🌐 Navegação Inteligente na Web - O AgenticSeek pode navegar na internet sozinho — pesquisar, ler, extrair informações, preencher formulários — tudo sem usar as mãos.</p>
</li>
<li><p>💻 Assistente Autônomo de Programação - Precisa de código? Ele pode escrever, depurar e executar programas em Python, C, Go, Java e mais — tudo sem supervisão.</p>
</li>
<li><p>🧠 Seleção Inteligente de Agentes - Você pede, ele descobre automaticamente o melhor agente para a tarefa. Como ter uma equipe de especialistas pronta para ajudar.</p>
</li>
<li><p>📋 Planeja &amp; Executa Tarefas Complexas - De planejamento de viagens a projetos complexos — ele pode dividir grandes tarefas em etapas e realizar usando múltiplos agentes de IA.</p>
</li>
<li><p>🎙️ Habilitado por Voz - Voz limpa, rápida e futurista e conversão de fala para texto, permitindo que você converse como se fosse sua IA pessoal de um filme de ficção científica. (Em desenvolvimento)</p>
</li>
</ul>
<h3><strong>Demo</strong></h3>
<blockquote>
<p><em>Você pode pesquisar pelo projeto agenticSeek, aprender quais habilidades são necessárias, depois abrir o CV_candidates.zip e então me dizer quais candidatos combinam melhor com o projeto</em></p>
</blockquote>
<p><a href="https://github.com/user-attachments/assets/b8ca60e9-7b3b-4533-840e-08f9ac426316">https://github.com/user-attachments/assets/b8ca60e9-7b3b-4533-840e-08f9ac426316</a></p>
<p>Aviso: Esta demonstração, incluindo todos os arquivos que aparecem (ex: CV_candidates.zip), é inteiramente fictícia. Não somos uma corporação, buscamos colaboradores open-source, não candidatos.</p>
<blockquote>
<p>🛠⚠️️ <strong>Trabalho Ativo em Andamento</strong></p>
</blockquote>
<blockquote>
<p>🙏 Este projeto começou como um projeto paralelo e não tem roteiro nem financiamento. Cresceu muito além do esperado, chegando ao GitHub Trending. Contribuições, feedback e paciência são profundamente apreciados.</p>
</blockquote>
<h2>Pré-requisitos</h2>
<p>Antes de começar, certifique-se de ter o seguinte software instalado:</p>
<ul>
<li><strong>Git:</strong> Para clonar o repositório. <a href="https://git-scm.com/downloads">Baixe o Git</a></li>
<li><strong>Python 3.10.x:</strong> Recomendamos fortemente o uso da versão Python 3.10.x. Usar outras versões pode causar erros de dependência. <a href="https://www.python.org/downloads/release/python-3100/">Baixe o Python 3.10</a> (escolha uma versão 3.10.x).</li>
<li><strong>Docker Engine &amp; Docker Compose:</strong> Para rodar serviços agrupados como o SearxNG.<ul>
<li>Instale o Docker Desktop (que inclui Docker Compose V2): <a href="https://docs.docker.com/desktop/install/windows-install/">Windows</a> | <a href="https://docs.docker.com/desktop/install/mac-install/">Mac</a> | <a href="https://docs.docker.com/desktop/install/linux-install/">Linux</a></li>
<li>Alternativamente, instale o Docker Engine e o Docker Compose separadamente no Linux: <a href="https://docs.docker.com/engine/install/">Docker Engine</a> | <a href="https://docs.docker.com/compose/install/">Docker Compose</a> (certifique-se de instalar o Compose V2, ex: <code>sudo apt-get install docker-compose-plugin</code>).</li>
</ul>
</li>
</ul>
<h3>1. <strong>Clone o repositório e configure</strong></h3>
<pre><code class="language-sh hljs language-bash">git <span class="hljs-built_in">clone</span> https://github.com/Fosowl/agenticSeek.git
<span class="hljs-built_in">cd</span> agenticSeek
<span class="hljs-built_in">mv</span> .env.example .<span class="hljs-built_in">env</span>
</code></pre>
<h3>2. Altere o conteúdo do arquivo .env</h3>
<pre><code class="language-sh hljs language-bash">SEARXNG_BASE_URL=<span class="hljs-string">"http://127.0.0.1:8080"</span>
REDIS_BASE_URL=<span class="hljs-string">"redis://redis:6379/0"</span>
WORK_DIR=<span class="hljs-string">"/Users/mlg/Documents/workspace_for_ai"</span>
OLLAMA_PORT=<span class="hljs-string">"11434"</span>
LM_STUDIO_PORT=<span class="hljs-string">"1234"</span>
CUSTOM_ADDITIONAL_LLM_PORT=<span class="hljs-string">"11435"</span>
OPENAI_API_KEY=<span class="hljs-string">'opcional'</span>
DEEPSEEK_API_KEY=<span class="hljs-string">'opcional'</span>
OPENROUTER_API_KEY=<span class="hljs-string">'opcional'</span>
TOGETHER_API_KEY=<span class="hljs-string">'opcional'</span>
GOOGLE_API_KEY=<span class="hljs-string">'opcional'</span>
ANTHROPIC_API_KEY=<span class="hljs-string">'opcional'</span>
</code></pre>
<p>Atualize o arquivo <code>.env</code> com seus próprios valores conforme necessário:</p>
<ul>
<li><strong>SEARXNG_BASE_URL</strong>: Deixe inalterado</li>
<li><strong>REDIS_BASE_URL</strong>: Deixe inalterado</li>
<li><strong>WORK_DIR</strong>: Caminho para seu diretório de trabalho em sua máquina local. O AgenticSeek poderá ler e interagir com estes arquivos.</li>
<li><strong>OLLAMA_PORT</strong>: Número da porta para o serviço Ollama.</li>
<li><strong>LM_STUDIO_PORT</strong>: Número da porta para o serviço LM Studio.</li>
<li><strong>CUSTOM_ADDITIONAL_LLM_PORT</strong>: Porta para qualquer serviço LLM personalizado adicional.</li>
</ul>
<p><strong>As chaves de API são totalmente opcionais para usuários que optam por rodar LLM localmente. Esse é o objetivo principal deste projeto. Deixe em branco se você possuir hardware suficiente</strong></p>
<h3>3. <strong>Inicie o Docker</strong></h3>
<p>Certifique-se de que o Docker está instalado e em execução em seu sistema. Você pode iniciar o Docker usando os seguintes comandos:</p>
<ul>
<li><p><strong>No Linux/macOS:</strong><br>  Abra um terminal e execute:</p>
<pre><code class="language-sh hljs language-bash">sudo systemctl start docker
</code></pre>
<p>  Ou inicie o Docker Desktop a partir do menu de aplicativos, se instalado.</p>
</li>
<li><p><strong>No Windows:</strong><br>  Inicie o Docker Desktop pelo menu Iniciar.</p>
</li>
</ul>
<p>Você pode verificar se o Docker está rodando executando:</p>
<pre><code class="language-sh hljs language-bash">docker info
</code></pre>
<p>Se você visualizar informações sobre sua instalação do Docker, ele está funcionando corretamente.</p>
<p>Veja a tabela de <a href="#list-of-local-providers">Provedores Locais</a> abaixo para um resumo.</p>
<p>Próximo passo: <a href="#start-services-and-run">Execute o AgenticSeek localmente</a></p>
<p><em>Veja a seção <a href="#troubleshooting">Solução de Problemas</a> se estiver tendo problemas.</em><br><em>Se seu hardware não conseguir rodar LLMs localmente, veja <a href="#setup-to-run-with-an-api">Configuração para rodar com uma API</a>.</em><br><em>Para explicações detalhadas do <code>config.ini</code>, veja a <a href="#config">Seção de Configuração</a>.</em></p>
<hr>
<h2>Configuração para rodar LLM localmente em sua máquina</h2>
<p><strong>Requisitos de Hardware:</strong></p>
<p>Para rodar LLMs localmente, você precisará de hardware suficiente. No mínimo, é necessário uma GPU capaz de rodar Magistral, Qwen ou Deepseek 14B. Veja o FAQ para recomendações detalhadas de modelo/desempenho.</p>
<p><strong>Configure seu provedor local</strong></p>
<p>Inicie seu provedor local, por exemplo, com o ollama:</p>
<pre><code class="language-sh hljs language-bash">ollama serve
</code></pre>
<p>Veja abaixo uma lista de provedores locais suportados.</p>
<p><strong>Atualize o config.ini</strong></p>
<p>Altere o arquivo config.ini para definir o provider_name para um provedor suportado e provider_model para um LLM suportado por seu provedor. Recomendamos modelos de raciocínio como <em>Magistral</em> ou <em>Deepseek</em>.</p>
<p>Veja o <strong>FAQ</strong> no final do README para hardware necessário.</p>
<pre><code class="language-sh hljs language-bash">[MAIN]
is_local = True <span class="hljs-comment"># Sempre que estiver rodando localmente ou com provedor remoto.</span>
provider_name = ollama <span class="hljs-comment"># ou lm-studio, openai, etc..</span>
provider_model = deepseek-r1:14b <span class="hljs-comment"># escolha um modelo que se encaixe em seu hardware</span>
provider_server_address = 127.0.0.1:11434
agent_name = Jarvis <span class="hljs-comment"># nome da sua IA</span>
recover_last_session = True <span class="hljs-comment"># se deseja recuperar a sessão anterior</span>
save_session = True <span class="hljs-comment"># se deseja lembrar a sessão atual</span>
speak = False <span class="hljs-comment"># texto para fala</span>
listen = False <span class="hljs-comment"># fala para texto, apenas para CLI, experimental</span>
jarvis_personality = False <span class="hljs-comment"># Se deseja usar uma personalidade mais "Jarvis" (experimental)</span>
languages = en zh <span class="hljs-comment"># Lista de idiomas, Texto para fala usará o primeiro idioma da lista como padrão</span>
[BROWSER]
headless_browser = True <span class="hljs-comment"># deixe inalterado, a menos que use CLI no host.</span>
stealth_mode = True <span class="hljs-comment"># Use selenium não detectado para reduzir detecção do navegador</span>
</code></pre>
<p><strong>Aviso</strong>:</p>
<ul>
<li><p>O formato do arquivo <code>config.ini</code> não suporta comentários.<br>Não copie e cole a configuração de exemplo diretamente, pois os comentários causarão erros. Em vez disso, modifique manualmente o arquivo <code>config.ini</code> com suas configurações desejadas, excluindo quaisquer comentários.</p>
</li>
<li><p><em>NÃO</em> defina provider_name como <code>openai</code> se estiver usando LM-studio para rodar LLMs. Defina como <code>lm-studio</code>.</p>
</li>
<li><p>Alguns provedores (ex: lm-studio) exigem que você tenha <code>http://</code> na frente do IP. Por exemplo, <code>http://127.0.0.1:1234</code></p>
</li>
</ul>
<p><strong>Lista de provedores locais</strong></p>
<table>
<thead>
<tr>
<th>Provedor</th>
<th>Local?</th>
<th>Descrição</th>
</tr>
</thead>
<tbody><tr>
<td>ollama</td>
<td>Sim</td>
<td>Rode LLMs localmente com facilidade usando ollama como provedor</td>
</tr>
<tr>
<td>lm-studio</td>
<td>Sim</td>
<td>Rode LLM localmente com o LM studio (defina <code>provider_name</code> como <code>lm-studio</code>)</td>
</tr>
<tr>
<td>openai</td>
<td>Sim</td>
<td>Use API compatível com openai (ex: servidor llama.cpp)</td>
</tr>
</tbody></table>
<p>Próximo passo: <a href="#Start-services-and-Run">Inicie os serviços e rode o AgenticSeek</a>  </p>
<p><em>Veja a seção <a href="#troubleshooting">Solução de Problemas</a> se estiver tendo problemas.</em><br><em>Se seu hardware não conseguir rodar LLMs localmente, veja <a href="#setup-to-run-with-an-api">Configuração para rodar com uma API</a>.</em><br><em>Para explicações detalhadas do <code>config.ini</code>, veja a <a href="#config">Seção de Configuração</a>.</em></p>
<h2>Configuração para rodar com uma API</h2>
<p>Esta configuração usa provedores de LLM externos, baseados em nuvem. Você precisará de uma chave de API do serviço escolhido.</p>
<p><strong>1. Escolha um Provedor de API e obtenha uma chave de API:</strong></p>
<p>Consulte a <a href="#list-of-api-providers">Lista de Provedores de API</a> abaixo. Visite os sites deles para se cadastrar e obter uma chave de API.</p>
<p><strong>2. Defina sua chave de API como uma variável de ambiente:</strong></p>
<ul>
<li><strong>Linux/macOS:</strong><br>Abra seu terminal e use o comando <code>export</code>. É melhor adicionar isso ao arquivo de perfil do seu shell (ex: <code>~/.bashrc</code>, <code>~/.zshrc</code>) para persistência.<pre><code class="language-sh hljs language-bash"><span class="hljs-built_in">export</span> PROVIDER_API_KEY=<span class="hljs-string">"sua_chave_api_aqui"</span>
<span class="hljs-comment"># Substitua PROVIDER_API_KEY pelo nome da variável específica, ex: OPENAI_API_KEY, GOOGLE_API_KEY</span>
</code></pre>
Exemplo para TogetherAI:<pre><code class="language-sh hljs language-bash"><span class="hljs-built_in">export</span> TOGETHER_API_KEY=<span class="hljs-string">"xxxxxxxxxxxxxxxxxxxxxx"</span>
</code></pre>
</li>
<li><strong>Windows:</strong></li>
<li><strong>Prompt de Comando (Temporário para a sessão atual):</strong><pre><code class="language-cmd">set PROVIDER_API_KEY=your_api_key_here
</code></pre>
</li>
<li><strong>PowerShell (Temporário para a sessão atual):</strong><pre><code class="language-powershell">$env:PROVIDER_API_KEY="your_api_key_here"
</code></pre>
</li>
<li><strong>Permanentemente:</strong> Pesquise por "variáveis de ambiente" na barra de pesquisa do Windows, clique em "Editar as variáveis de ambiente do sistema" e, em seguida, clique no botão "Variáveis de Ambiente...". Adicione uma nova variável de Usuário com o nome apropriado (por exemplo, <code>OPENAI_API_KEY</code>) e sua chave como valor.</li>
</ul>
<p><em>(Veja o FAQ: <a href="#how-do-i-set-api-keys">Como configuro chaves de API?</a> para mais detalhes).</em></p>
<p><strong>3. Atualize o <code>config.ini</code>:</strong></p>
<pre><code class="language-ini hljs"><span class="hljs-section">[MAIN]</span>
<span class="hljs-attr">is_local</span> = <span class="hljs-literal">False</span>
<span class="hljs-attr">provider_name</span> = openai <span class="hljs-comment"># Ou google, deepseek, togetherAI, huggingface</span>
<span class="hljs-attr">provider_model</span> = gpt-<span class="hljs-number">3.5</span>-turbo <span class="hljs-comment"># Ou gemini-1.5-flash, deepseek-chat, mistralai/Mixtral-8x7B-Instruct-v0.1 etc.</span>
provider_server_address = <span class="hljs-comment"># Normalmente ignorado ou pode ser deixado em branco quando is_local = False para a maioria das APIs</span>
<span class="hljs-comment"># ... outras configurações ...</span>
</code></pre>
<p><em>Atenção:</em> Certifique-se de que não há espaços no final dos valores no <code>config.ini</code>.</p>
<p><strong>Lista de Provedores de API</strong></p>
<table>
<thead>
<tr>
<th>Provedor</th>
<th><code>provider_name</code></th>
<th>Local?</th>
<th>Descrição</th>
<th>Link da chave da API (Exemplos)</th>
</tr>
</thead>
<tbody><tr>
<td>OpenAI</td>
<td><code>openai</code></td>
<td>Não</td>
<td>Usa modelos ChatGPT via API da OpenAI.</td>
<td><a href="https://platform.openai.com/signup">platform.openai.com/signup</a></td>
</tr>
<tr>
<td>Google Gemini</td>
<td><code>google</code></td>
<td>Não</td>
<td>Usa modelos Google Gemini via Google AI Studio.</td>
<td><a href="https://aistudio.google.com/keys">aistudio.google.com/keys</a></td>
</tr>
<tr>
<td>Deepseek</td>
<td><code>deepseek</code></td>
<td>Não</td>
<td>Usa modelos Deepseek via sua API.</td>
<td><a href="https://platform.deepseek.com">platform.deepseek.com</a></td>
</tr>
<tr>
<td>Hugging Face</td>
<td><code>huggingface</code></td>
<td>Não</td>
<td>Usa modelos da Hugging Face Inference API.</td>
<td><a href="https://huggingface.co/settings/tokens">huggingface.co/settings/tokens</a></td>
</tr>
<tr>
<td>TogetherAI</td>
<td><code>togetherAI</code></td>
<td>Não</td>
<td>Usa diversos modelos open-source via TogetherAI API.</td>
<td><a href="https://api.together.ai/settings/api-keys">api.together.ai/settings/api-keys</a></td>
</tr>
</tbody></table>
<p><em>Nota:</em></p>
<ul>
<li>Não recomendamos o uso de <code>gpt-4o</code> ou outros modelos da OpenAI para navegação web complexa e planejamento de tarefas, pois as otimizações atuais de prompt são voltadas para modelos como Deepseek.</li>
<li>Tarefas de codificação/bash podem apresentar problemas com o Gemini, pois ele pode não seguir rigorosamente os prompts de formatação otimizados para o Deepseek.</li>
<li>O campo <code>provider_server_address</code> no <code>config.ini</code> geralmente não é usado quando <code>is_local = False</code>, pois o endpoint da API normalmente é definido na biblioteca do provedor.</li>
</ul>
<p>Próxima etapa: <a href="#Start-services-and-Run">Inicie os serviços e rode o AgenticSeek</a></p>
<p><em>Consulte a seção <strong>Problemas conhecidos</strong> caso esteja enfrentando problemas</em></p>
<p><em>Consulte a seção <strong>Config</strong> para uma explicação detalhada do arquivo de configuração.</em></p>
<hr>
<h2>Inicie os serviços e execute</h2>
<p>Por padrão, o AgenticSeek é executado totalmente em docker.</p>
<p>Inicie os serviços necessários. Isso iniciará todos os serviços do docker-compose.yml, incluindo:<br>    - searxng<br>    - redis (necessário pelo searxng)<br>    - frontend<br>    - backend (se usar <code>full</code>)</p>
<pre><code class="language-sh hljs language-bash">./start_services.sh full <span class="hljs-comment"># MacOS</span>
start ./start_services.cmd full <span class="hljs-comment"># Windows</span>
</code></pre>
<p><strong>Atenção:</strong> Esta etapa irá baixar e carregar todas as imagens do Docker, o que pode levar até 30 minutos. Após iniciar os serviços, aguarde até que o serviço backend esteja totalmente rodando (você deve ver <strong>backend: "GET /health HTTP/1.1" 200 OK</strong> no log) antes de enviar qualquer mensagem. Os serviços backend podem levar até 5 minutos para iniciar na primeira execução.</p>
<p>Acesse <code>http://localhost:3000/</code> e você deverá ver a interface web.</p>
<p><em>Resolução de problemas ao iniciar o serviço:</em> Se esses scripts falharem, certifique-se de que o Docker Engine está em execução e que o Docker Compose (V2, <code>docker compose</code>) está instalado corretamente. Verifique a saída no terminal para mensagens de erro. Veja <a href="#faq-troubleshooting">FAQ: Help! I get an error when running AgenticSeek or its scripts.</a></p>
<p><strong>Opcional:</strong> Rodar na máquina local (modo CLI):</p>
<p>Para rodar com interface CLI você precisará instalar o pacote na máquina local:</p>
<pre><code class="language-sh hljs language-bash">./install.sh
./install.bat <span class="hljs-comment"># windows</span>
</code></pre>
<p>Inicie os serviços:</p>
<pre><code class="language-sh hljs language-bash">./start_services.sh <span class="hljs-comment"># MacOS</span>
start ./start_services.cmd <span class="hljs-comment"># Windows</span>
</code></pre>
<p>Use o CLI: <code>python3 cli.py</code></p>
<hr>
<h2>Uso</h2>
<p>Certifique-se de que os serviços estão ativos e em execução com <code>./start_services.sh full</code> e acesse <code>localhost:3000</code> para a interface web.</p>
<p>Você também pode usar reconhecimento de voz (speech to text) configurando <code>listen = True</code> no config. Apenas para o modo CLI.</p>
<p>Para sair, simplesmente diga/digite <code>goodbye</code>.</p>
<p>Aqui estão alguns exemplos de uso:</p>
<blockquote>
<p><em>Crie um jogo da cobrinha em python!</em></p>
</blockquote>
<blockquote>
<p><em>Pesquise na web pelos melhores cafés em Rennes, França, e salve uma lista de três com seus endereços em rennes_cafes.txt.</em></p>
</blockquote>
<blockquote>
<p><em>Escreva um programa em Go para calcular o fatorial de um número, salve como factorial.go em sua área de trabalho</em></p>
</blockquote>
<blockquote>
<p><em>Procure na pasta summer_pictures todos os arquivos JPG, renomeie-os com a data de hoje e salve uma lista dos arquivos renomeados em photos_list.txt</em></p>
</blockquote>
<blockquote>
<p><em>Pesquise online por filmes de ficção científica populares de 2024 e escolha três para assistir hoje à noite. Salve a lista em movie_night.txt.</em></p>
</blockquote>
<blockquote>
<p><em>Pesquise na web os artigos mais recentes sobre IA de 2025, selecione três e escreva um script Python para capturar seus títulos e resumos. Salve o script como news_scraper.py e os resumos em ai_news.txt em /home/projects</em></p>
</blockquote>
<blockquote>
<p><em>Sexta-feira, pesquise na web por uma API gratuita de cotação de ações, registre-se com <a href="mailto:supersuper7434567@gmail.com">supersuper7434567@gmail.com</a>, depois escreva um script Python para buscar, usando a API, os preços diários da Tesla, e salve os resultados em stock_prices.csv</em></p>
</blockquote>
<p><em>Observe que as capacidades de preenchimento de formulários ainda são experimentais e podem falhar.</em></p>
<p>Após digitar sua consulta, o AgenticSeek irá alocar o melhor agente para a tarefa.</p>
<p>Como este é um protótipo inicial, o sistema de roteamento de agentes pode não alocar sempre o agente correto com base na sua consulta.</p>
<p>Portanto, seja muito explícito no que deseja e em como a IA deve proceder. Por exemplo, se quiser que ela faça uma busca na web, não diga:</p>
<p><code>Você conhece alguns bons países para viajar sozinho?</code></p>
<p>Em vez disso, peça:</p>
<p><code>Faça uma busca na web e descubra quais são os melhores países para viajar sozinho</code></p>
<hr>
<h2><strong>Configuração para rodar o LLM em seu próprio servidor</strong></h2>
<p>Se você tem um computador potente ou um servidor disponível, mas deseja usá-lo a partir do seu laptop, você pode rodar o LLM em um servidor remoto usando nosso servidor LLM personalizado.</p>
<p>No seu "servidor" que irá rodar o modelo de IA, obtenha o endereço IP</p>
<pre><code class="language-sh hljs language-bash">ip a | grep <span class="hljs-string">"inet "</span> | grep -v 127.0.0.1 | awk <span class="hljs-string">'{print $2}'</span> | <span class="hljs-built_in">cut</span> -d/ -f1 <span class="hljs-comment"># ip local</span>
curl https://ipinfo.io/ip <span class="hljs-comment"># ip público</span>
</code></pre>
<p>Nota: Para Windows ou macOS, use ipconfig ou ifconfig respectivamente para encontrar o endereço IP.</p>
<p>Clone o repositório e entre na pasta <code>server/</code>.</p>
<pre><code class="language-sh hljs language-bash">git <span class="hljs-built_in">clone</span> --depth 1 https://github.com/Fosowl/agenticSeek.git
<span class="hljs-built_in">cd</span> agenticSeek/llm_server/
</code></pre>
<p>Instale os requisitos específicos do servidor:</p>
<pre><code class="language-sh hljs language-bash">pip3 install -r requirements.txt
</code></pre>
<p>Execute o script do servidor.</p>
<pre><code class="language-sh hljs language-bash">python3 app.py --provider ollama --port 3333
</code></pre>
<p>Você pode escolher entre usar <code>ollama</code> e <code>llamacpp</code> como serviço LLM.</p>
<p>Agora no seu computador pessoal:</p>
<p>Altere o arquivo <code>config.ini</code> para definir <code>provider_name</code> como <code>server</code> e <code>provider_model</code> como <code>deepseek-r1:xxb</code>.<br>Defina o <code>provider_server_address</code> para o endereço IP da máquina que irá rodar o modelo.</p>
<pre><code class="language-sh hljs language-bash">[MAIN]
is_local = False
provider_name = server
provider_model = deepseek-r1:70b
provider_server_address = x.x.x.x:3333
</code></pre>
<p>Próxima etapa: <a href="#Start-services-and-Run">Inicie os serviços e rode o AgenticSeek</a>  </p>
<hr>
<h2>Reconhecimento de fala (Speech to Text)</h2>
<p>Atenção: o reconhecimento de fala só funciona no modo CLI no momento.</p>
<p>Por favor, note que atualmente o reconhecimento de fala só funciona em inglês.</p>
<p>A funcionalidade de reconhecimento de fala vem desabilitada por padrão. Para habilitá-la, defina a opção listen como True no arquivo config.ini:</p>
<pre><code class="hljs language-ini"><span class="hljs-attr">listen</span> = <span class="hljs-literal">True</span>
</code></pre>
<p>Quando habilitada, a função de reconhecimento de fala escuta por uma palavra-chave de ativação, que é o nome do agente, antes de começar a processar sua entrada. Você pode personalizar o nome do agente atualizando o valor <code>agent_name</code> no arquivo <em>config.ini</em>:</p>
<pre><code class="hljs language-ini"><span class="hljs-attr">agent_name</span> = Friday
</code></pre>
<p>Para reconhecimento ideal, recomendamos o uso de um nome comum em inglês como "John" ou "Emma" como nome do agente</p>
<p>Assim que você ver a transcrição começar a aparecer, diga o nome do agente em voz alta para acordá-lo (por exemplo, "Friday").</p>
<p>Fale sua pergunta claramente.</p>
<p>Encerre seu pedido com uma frase de confirmação para sinalizar ao sistema que deve prosseguir. Exemplos de frases de confirmação incluem:</p>
<pre><code class="hljs language-bash"><span class="hljs-string">"faça isso"</span>, <span class="hljs-string">"vai em frente"</span>, <span class="hljs-string">"execute"</span>, <span class="hljs-string">"rodar"</span>, <span class="hljs-string">"iniciar"</span>, <span class="hljs-string">"obrigado"</span>, <span class="hljs-string">"pode ser?"</span>, <span class="hljs-string">"por favor"</span>, <span class="hljs-string">"ok?"</span>, <span class="hljs-string">"prossiga"</span>, <span class="hljs-string">"continue"</span>, <span class="hljs-string">"vá em frente"</span>, <span class="hljs-string">"faça isso"</span>, <span class="hljs-string">"entendeu?"</span>
</code></pre>
<h2>Config</h2>
<p>Exemplo de configuração:</p>
<pre><code class="hljs language-ini"><span class="hljs-section">[MAIN]</span>
<span class="hljs-attr">is_local</span> = <span class="hljs-literal">True</span>
<span class="hljs-attr">provider_name</span> = ollama
<span class="hljs-attr">provider_model</span> = deepseek-r1:<span class="hljs-number">32</span>b
<span class="hljs-attr">provider_server_address</span> = http://<span class="hljs-number">127.0</span>.<span class="hljs-number">0.1</span>:<span class="hljs-number">11434</span> <span class="hljs-comment"># Exemplo para Ollama; use http://127.0.0.1:1234 para LM-Studio</span>
<span class="hljs-attr">agent_name</span> = Friday
<span class="hljs-attr">recover_last_session</span> = <span class="hljs-literal">False</span>
<span class="hljs-attr">save_session</span> = <span class="hljs-literal">False</span>
<span class="hljs-attr">speak</span> = <span class="hljs-literal">False</span>
<span class="hljs-attr">listen</span> = <span class="hljs-literal">False</span>

<span class="hljs-attr">jarvis_personality</span> = <span class="hljs-literal">False</span>
<span class="hljs-attr">languages</span> = en zh <span class="hljs-comment"># Lista de idiomas para TTS e potencialmente roteamento.</span>
<span class="hljs-section">[BROWSER]</span>
<span class="hljs-attr">headless_browser</span> = <span class="hljs-literal">False</span>
<span class="hljs-attr">stealth_mode</span> = <span class="hljs-literal">False</span>
</code></pre>
<p><strong>Explicação das configurações do <code>config.ini</code></strong>:</p>
<ul>
<li><strong>Seção <code>[MAIN]</code>:</strong><ul>
<li><code>is_local</code>: <code>True</code> se estiver usando um provedor LLM local (Ollama, LM-Studio, servidor local compatível com OpenAI) ou a opção de servidor auto-hospedado. <code>False</code> se estiver usando uma API em nuvem (OpenAI, Google, etc.).</li>
<li><code>provider_name</code>: Especifica o provedor LLM.<ul>
<li>Opções locais: <code>ollama</code>, <code>lm-studio</code>, <code>openai</code> (para servidores locais compatíveis com OpenAI), <code>server</code> (para configuração de servidor auto-hospedado).</li>
<li>Opções de API: <code>openai</code>, <code>google</code>, <code>deepseek</code>, <code>huggingface</code>, <code>togetherAI</code>.</li>
</ul>
</li>
<li><code>provider_model</code>: O nome ou ID do modelo específico para o provedor escolhido (por exemplo, <code>deepseekcoder:6.7b</code> para Ollama, <code>gpt-3.5-turbo</code> para API OpenAI, <code>mistralai/Mixtral-8x7B-Instruct-v0.1</code> para TogetherAI).</li>
<li><code>provider_server_address</code>: O endereço do seu provedor LLM.<ul>
<li>Para provedores locais: por exemplo, <code>http://127.0.0.1:11434</code> para Ollama, <code>http://127.0.0.1:1234</code> para LM-Studio.</li>
<li>Para o tipo de provedor <code>server</code>: o endereço do seu servidor LLM auto-hospedado (por exemplo, <code>http://seu_ip_servidor:3333</code>).</li>
<li>Para APIs em nuvem (<code>is_local = False</code>): geralmente é ignorado ou pode ser deixado em branco, pois o endpoint da API é normalmente gerenciado pela biblioteca cliente.</li>
</ul>
</li>
<li><code>agent_name</code>: Nome do assistente de IA (por exemplo, Friday). Usado como palavra de ativação para reconhecimento de fala, se ativado.</li>
<li><code>recover_last_session</code>: <code>True</code> para tentar restaurar o estado da sessão anterior, <code>False</code> para iniciar do zero.</li>
<li><code>save_session</code>: <code>True</code> para salvar o estado da sessão atual para possível recuperação, <code>False</code> caso contrário.</li>
<li><code>speak</code>: <code>True</code> para ativar saída de voz via texto para fala, <code>False</code> para desativar.</li>
<li><code>listen</code>: <code>True</code> para ativar entrada de voz via reconhecimento de fala (somente modo CLI), <code>False</code> para desativar.</li>
<li><code>work_dir</code>: <strong>Crucial:</strong> O diretório onde o AgenticSeek irá ler/gravar arquivos. <strong>Certifique-se de que este caminho é válido e acessível em seu sistema.</strong></li>
<li><code>jarvis_personality</code>: <code>True</code> para usar um prompt de sistema mais "estilo Jarvis" (experimental), <code>False</code> para o prompt padrão.</li>
<li><code>languages</code>: Uma lista de idiomas separada por vírgulas (por exemplo, <code>en, zh, fr</code>). Usada para seleção de voz TTS (padrão é o primeiro) e pode auxiliar o roteador LLM. Evite muitos idiomas ou idiomas muito semelhantes para eficiência do roteador.</li>
</ul>
</li>
<li><strong>Seção <code>[BROWSER]</code>:</strong><ul>
<li><code>headless_browser</code>: <code>True</code> para rodar o navegador automatizado sem janela visível (recomendado para interface web ou uso não interativo). <code>False</code> para exibir a janela do navegador (útil para modo CLI ou depuração).</li>
<li><code>stealth_mode</code>: <code>True</code> para ativar medidas que dificultam a detecção da automação do navegador. Pode exigir instalação manual de extensões como anticaptcha.</li>
</ul>
</li>
</ul>
<p>Esta seção resume os tipos de provedores LLM suportados. Configure-os no <code>config.ini</code>.</p>
<p><strong>Provedores Locais (Executados no Seu Próprio Hardware):</strong></p>
<table>
<thead>
<tr>
<th>Nome do Provedor no <code>config.ini</code></th>
<th><code>is_local</code></th>
<th>Descrição</th>
<th>Seção de Configuração</th>
</tr>
</thead>
<tbody><tr>
<td><code>ollama</code></td>
<td><code>True</code></td>
<td>Usa o Ollama para servir LLMs locais.</td>
<td><a href="#setup-for-running-llm-locally-on-your-machine">Configuração para rodar LLM localmente</a></td>
</tr>
<tr>
<td><code>lm-studio</code></td>
<td><code>True</code></td>
<td>Usa o LM-Studio para servir LLMs locais.</td>
<td><a href="#setup-for-running-llm-locally-on-your-machine">Configuração para rodar LLM localmente</a></td>
</tr>
<tr>
<td><code>openai</code> (para servidor local)</td>
<td><code>True</code></td>
<td>Conecta a um servidor local que expõe uma API compatível com OpenAI (ex.: llama.cpp).</td>
<td><a href="#setup-for-running-llm-locally-on-your-machine">Configuração para rodar LLM localmente</a></td>
</tr>
<tr>
<td><code>server</code></td>
<td><code>False</code></td>
<td>Conecta ao servidor LLM auto-hospedado do AgenticSeek rodando em outra máquina.</td>
<td><a href="#setup-to-run-the-llm-on-your-own-server">Configuração para rodar o LLM no seu próprio servidor</a></td>
</tr>
</tbody></table>
<p><strong>Provedores de API (Baseados em Nuvem):</strong></p>
<table>
<thead>
<tr>
<th>Nome do Provedor no <code>config.ini</code></th>
<th><code>is_local</code></th>
<th>Descrição</th>
<th>Seção de Configuração</th>
</tr>
</thead>
<tbody><tr>
<td><code>openai</code></td>
<td><code>False</code></td>
<td>Usa a API oficial da OpenAI (ex.: GPT-3.5, GPT-4).</td>
<td><a href="#setup-to-run-with-an-api">Configuração para rodar com uma API</a></td>
</tr>
<tr>
<td><code>google</code></td>
<td><code>False</code></td>
<td>Usa modelos Gemini do Google via API.</td>
<td><a href="#setup-to-run-with-an-api">Configuração para rodar com uma API</a></td>
</tr>
<tr>
<td><code>deepseek</code></td>
<td><code>False</code></td>
<td>Usa a API oficial Deepseek.</td>
<td><a href="#setup-to-run-with-an-api">Configuração para rodar com uma API</a></td>
</tr>
<tr>
<td><code>huggingface</code></td>
<td><code>False</code></td>
<td>Usa a API de Inferência do Hugging Face.</td>
<td><a href="#setup-to-run-with-an-api">Configuração para rodar com uma API</a></td>
</tr>
<tr>
<td><code>togetherAI</code></td>
<td><code>False</code></td>
<td>Usa a API TogetherAI para vários modelos abertos.</td>
<td><a href="#setup-to-run-with-an-api">Configuração para rodar com uma API</a></td>
</tr>
</tbody></table>
<hr>
<h2>Solução de Problemas</h2>
<p>Se você encontrar problemas, esta seção traz orientações.</p>
<h1>Problemas Conhecidos</h1>
<h2>Problemas com o ChromeDriver</h2>
<p><strong>Exemplo de Erro:</strong> <code>SessionNotCreatedException: Message: session not created: This version of ChromeDriver only supports Chrome version XXX</code></p>
<ul>
<li><strong>Causa:</strong> A versão do ChromeDriver instalada é incompatível com a versão do seu navegador Google Chrome.</li>
<li><strong>Solução:</strong><ol>
<li><strong>Verifique a Versão do Chrome:</strong> Abra o Google Chrome, vá em <code>Configurações &gt; Sobre o Chrome</code> para encontrar sua versão (exemplo: "Versão 120.0.6099.110").</li>
<li><strong>Baixe o ChromeDriver Correspondente:</strong><ul>
<li>Para versões do Chrome 115 ou mais recentes: Acesse os <a href="https://googlechromelabs.github.io/chrome-for-testing/">Endpoints JSON do Chrome for Testing (CfT)</a>. Encontre o canal "stable" e baixe o ChromeDriver para seu SO que corresponda à versão principal do seu Chrome.</li>
<li>Para versões mais antigas (menos comuns): Você pode encontrá-las na página <a href="https://chromedriver.chromium.org/downloads">ChromeDriver - WebDriver for Chrome</a>.</li>
<li>A imagem abaixo mostra um exemplo da página CfT:<br><img src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/chromedriver_readme.png" alt="Baixe a versão específica do Chromedriver na página Chrome for Testing"></li>
</ul>
</li>
<li><strong>Instale o ChromeDriver:</strong><ul>
<li>Certifique-se de que o <code>chromedriver</code> baixado (ou <code>chromedriver.exe</code> no Windows) esteja em um diretório listado na variável de ambiente PATH do seu sistema (ex.: <code>/usr/local/bin</code> no Linux/macOS ou uma pasta de scripts personalizada adicionada ao PATH no Windows).</li>
<li>Alternativamente, coloque-o no diretório raiz do projeto <code>agenticSeek</code>.</li>
<li>Certifique-se de que o driver seja executável (ex.: <code>chmod +x chromedriver</code> no Linux/macOS).</li>
</ul>
</li>
<li>Consulte a seção <a href="#chromedriver-installation">Instalação do ChromeDriver</a> no guia principal de Instalação para mais detalhes.</li>
</ol>
</li>
</ul>
<p>Se esta seção estiver incompleta ou você encontrar outros problemas com o ChromeDriver, considere pesquisar nos <a href="https://github.com/Fosowl/agenticSeek/issues">Issues do GitHub</a> existentes ou abrir um novo.</p>
<p><code>Exception: Failed to initialize browser: Message: session not created: This version of ChromeDriver only supports Chrome version 113 Current browser version is 134.0.6998.89 with binary path</code></p>
<p>Isso ocorre se houver incompatibilidade entre a versão do navegador e do chromedriver.</p>
<p>Você precisa acessar e baixar a versão mais recente:</p>
<p><a href="https://developer.chrome.com/docs/chromedriver/downloads">https://developer.chrome.com/docs/chromedriver/downloads</a></p>
<p>Se você estiver usando o Chrome versão 115 ou mais recente, acesse:</p>
<p><a href="https://googlechromelabs.github.io/chrome-for-testing/">https://googlechromelabs.github.io/chrome-for-testing/</a></p>
<p>E baixe a versão do chromedriver correspondente ao seu SO.</p>
<p><img src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/chromedriver_readme.png" alt="alt text"></p>
<p>Se esta seção estiver incompleta, por favor, abra uma issue.</p>
<h2>Problemas com connection adapters</h2>
<pre><code class="hljs language-vbnet"><span class="hljs-symbol">Exception:</span> Provider lm-studio failed: HTTP request failed: No connection adapters were found <span class="hljs-keyword">for</span> <span class="hljs-comment">'127.0.0.1:1234/v1/chat/completions'` (Nota: a porta pode variar)</span>
</code></pre>
<ul>
<li><strong>Causa:</strong> O <code>provider_server_address</code> no <code>config.ini</code> para o <code>lm-studio</code> (ou outros servidores locais compatíveis com OpenAI) está sem o prefixo <code>http://</code> ou está apontando para a porta errada.</li>
<li><strong>Solução:</strong><ul>
<li>Certifique-se de que o endereço inclua <code>http://</code>. O LM-Studio normalmente usa <code>http://127.0.0.1:1234</code>.</li>
<li>Corrija no <code>config.ini</code>: <code>provider_server_address = http://127.0.0.1:1234</code> (ou a porta real do seu servidor LM-Studio).</li>
</ul>
</li>
</ul>
<h2>SearxNG Base URL Não Fornecido</h2>
<pre><code class="hljs language-csharp"><span class="hljs-function">raise <span class="hljs-title">ValueError</span>(<span class="hljs-params"><span class="hljs-string">"SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable."</span></span>)
ValueError: SearxNG <span class="hljs-keyword">base</span> URL must be provided either <span class="hljs-keyword">as</span> an argument <span class="hljs-keyword">or</span> via the SEARXNG_BASE_URL environment variable.`
</span></code></pre>
<h2>Perguntas Frequentes (FAQ)</h2>
<p><strong>P: Que hardware eu preciso?</strong>  </p>
<table>
<thead>
<tr>
<th>Tamanho do Modelo</th>
<th>GPU</th>
<th>Comentário</th>
</tr>
</thead>
<tbody><tr>
<td>7B</td>
<td>8GB Vram</td>
<td>⚠️ Não recomendado. Baixo desempenho, alucinações frequentes, agentes planejadores devem falhar.</td>
</tr>
<tr>
<td>14B</td>
<td>12 GB VRAM (ex.: RTX 3060)</td>
<td>✅ Utilizável para tarefas simples. Pode ter dificuldades com navegação web e planejamento.</td>
</tr>
<tr>
<td>32B</td>
<td>24+ GB VRAM (ex.: RTX 4090)</td>
<td>🚀 Sucesso na maioria das tarefas, pode ainda ter dificuldades com planejamento de tarefas</td>
</tr>
<tr>
<td>70B+</td>
<td>48+ GB Vram</td>
<td>💪 Excelente. Recomendado para casos de uso avançados.</td>
</tr>
</tbody></table>
<p><strong>P: Recebo um erro, o que faço?</strong>  </p>
<p>Certifique-se de que o local está rodando (<code>ollama serve</code>), que seu <code>config.ini</code> corresponde ao seu provedor e que as dependências estão instaladas. Se nada funcionar, sinta-se à vontade para abrir uma issue.</p>
<p><strong>P: Realmente pode rodar 100% localmente?</strong>  </p>
<p>Sim, com Ollama, lm-studio ou provedores de servidor, todo reconhecimento de fala, LLM e modelo de texto para fala rodam localmente. Opções não locais (OpenAI ou outras APIs) são opcionais.</p>
<p><strong>P: Por que devo usar o AgenticSeek se já tenho o Manus?</strong></p>
<p>Ao contrário do Manus, o AgenticSeek prioriza independência de sistemas externos, oferecendo mais controle, privacidade e evitando custos de API.</p>
<p><strong>P: Quem está por trás do projeto?</strong></p>
<p>O projeto foi criado por mim, junto com dois amigos que atuam como mantenedores e colaboradores da comunidade open source no GitHub. Somos apenas um grupo de pessoas apaixonadas, não uma startup ou afiliados a qualquer organização.</p>
<p>Qualquer conta do AgenticSeek no X além da minha pessoal (<a href="https://x.com/Martin993886460">https://x.com/Martin993886460</a>) é uma falsificação.</p>
<h2>Contribua</h2>
<p>Estamos procurando desenvolvedores para melhorar o AgenticSeek! Veja as issues abertas ou discussões.</p>
<p><a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/docs/CONTRIBUTING.md">Guia de contribuição</a></p>
<p><a href="https://www.star-history.com/#Fosowl/agenticSeek&amp;Date"><img src="https://api.star-history.com/svg?repos=Fosowl/agenticSeek&amp;type=Date" alt="Star History Chart"></a></p>
<h2>Mantenedores:</h2>
<blockquote>
<p><a href="https://github.com/Fosowl">Fosowl</a> | Horário de Paris </p>
</blockquote>
<blockquote>
<p><a href="https://github.com/antoineVIVIES">antoineVIVIES</a> | Horário de Taipei</p>
</blockquote>
<blockquote>
<p><a href="https://github.com/steveh8758">steveh8758</a> | Horário de Taipei</p>
</blockquote>
<h2>Agradecimentos Especiais:</h2>
<blockquote>
<p><a href="https://github.com/tcsenpai">tcsenpai</a> e <a href="https://github.com/plitc">plitc</a> pela ajuda com dockerização do backend</p>
</blockquote>
<h2>Patrocinadores:</h2>
<p>Patrocinadores mensais de 5 dólares ou mais aparecem aqui:</p>
<ul>
<li><strong>tatra-labs</strong></li>
</ul>
<p>I'm sorry, but you haven't provided the content of Part 4 of 4 to translate. Please provide the text you'd like translated.</p>
<hr>
<p>Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx">Open Ai Tx</a> | Last indexed: 2025-06-16</p>
<hr>
</div>
    </div>

    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Default Statcounter code for openaitx
https://openaitx.github.io/ -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async=""></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>
    <!-- End of Statcounter Code -->
    


</body></html>