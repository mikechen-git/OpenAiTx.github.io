<!DOCTYPE html>
<html lang="de">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>agenticSeek - Fosowl/agenticSeek</title>
    <meta name="title" content="agenticSeek - Fosowl/agenticSeek">
    <meta name="description" content="Fosowl/agenticSeek - GitHub repository de documentation and informationAgenticSeek: Private, lokale Manus-Alternative. English | 中文 | 繁體中文 | Français | 日本語 | Português (Brasil) | Español Eine 100% lokale Alternative zu Manus AI, di...">
    <meta name="keywords" content="Fosowl, agenticSeek, GitHub, repository, de documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/Fosowl/agenticSeek/README-de.html">
    <meta property="og:title" content="agenticSeek - Fosowl/agenticSeek">
    <meta property="og:description" content="Fosowl/agenticSeek - GitHub repository de documentation and information">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">
    <script src="/js/marked.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        body { display: flex; flex-direction: column; min-height: 100vh; }
        .main-container { margin: 0 auto; width: 100%; max-width: 980px; padding: 0 20px; }
        @media (max-width: 768px) { .main-container { padding: 0 15px; } }
        .markdown-body img { max-width: 100%; height: auto; }
        .header { text-align: center; margin-bottom: 30px; padding: 20px; background-color: #f6f8fa; border-bottom: 1px solid #e1e4e8; position: relative; }
        .header .links { margin-top: 10px; font-size: 16px; }
        .header .links a { color: #0366d6; text-decoration: none; margin-left: 5px; }
        .header .links a:hover { text-decoration: underline; }
        .language-badges { margin-top: 15px; text-align: center; }
        .language-badges a { display: inline-block; margin: 2px; text-decoration: none; }
        .language-badges img { height: 20px; border-radius: 3px; }
        .language-badges a:hover img { opacity: 0.8; }
    </style>
</head>
<body>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/Fosowl/agenticSeek" id="githubRepoLink" target="_blank">Fosowl/agenticSeek</a>
<h1 style="display: none;">AgenticSeek: Private, lokale Manus-Alternative. English | 中文 | 繁體中文 | Français | 日本語 | Português (Brasil) | Español Eine 100% lokale Alternative zu Manus AI, di...</h1>
        </div>
        <div class="language-badges" id="languageBadges">
            <!-- You can generate language badges here if needed -->
        </div>
    </div>
    <div class="main-container">
        <div class="markdown-body" id="content">
            <h1>AgenticSeek: Private, lokale Manus-Alternative.</h1>
<p align="center">
<img align="center" src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/agentic_seek_logo.png" width="300" height="300" alt="Agentic Seek Logo">
<p>
<p>English | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_CHS.md">中文</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_CHT.md">繁體中文</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_FR.md">Français</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_JP.md">日本語</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_PTBR.md">Português (Brasil)</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_ES.md">Español</a></p>
<p><em>Eine <strong>100% lokale Alternative zu Manus AI</strong>, dieser sprachgesteuerte KI-Assistent durchsucht autonom das Web, schreibt Code und plant Aufgaben, während alle Daten auf Ihrem Gerät bleiben. Entwickelt für lokale Reasoning-Modelle läuft er vollständig auf Ihrer eigenen Hardware, gewährleistet vollständige Privatsphäre und keinerlei Cloud-Abhängigkeit.</em></p>
<p><a href="https://fosowl.github.io/agenticSeek.html"><img src="https://img.shields.io/static/v1?label=Website&amp;message=AgenticSeek&amp;color=blue&amp;style=flat-square" alt="Visit AgenticSeek" /></a> <img src="https://img.shields.io/badge/license-GPL--3.0-green" alt="License" /> <a href="https://discord.gg/8hGDaME3TC"><img src="https://img.shields.io/badge/Discord-Join%20Us-7289DA?logo=discord&amp;logoColor=white" alt="Discord" /></a> <a href="https://x.com/Martin993886460"><img src="https://img.shields.io/twitter/url/https/twitter.com/fosowl.svg?style=social&amp;label=Update%20%40Fosowl" alt="Twitter" /></a> <a href="https://github.com/Fosowl/agenticSeek/stargazers"><img src="https://img.shields.io/github/stars/Fosowl/agenticSeek?style=social" alt="GitHub stars" /></a></p>
<h3>Warum AgenticSeek?</h3>
<ul>
<li><p>🔒 Vollständig lokal &amp; privat – Alles läuft auf Ihrem Computer — keine Cloud, keine Datenweitergabe. Ihre Dateien, Unterhaltungen und Suchanfragen bleiben privat.</p>
</li>
<li><p>🌐 Intelligentes Web-Browsing – AgenticSeek kann das Internet selbstständig durchsuchen — suchen, lesen, Informationen extrahieren, Webformulare ausfüllen — komplett freihändig.</p>
</li>
<li><p>💻 Autonomer Coding-Assistent – Brauchen Sie Code? Er kann Programme in Python, C, Go, Java und mehr schreiben, debuggen und ausführen — ganz ohne Aufsicht.</p>
</li>
<li><p>🧠 Intelligente Agenten-Auswahl – Sie fragen, er findet automatisch den besten Agenten für die Aufgabe. Wie ein Expertenteam, das bereit ist zu helfen.</p>
</li>
<li><p>📋 Plant &amp; führt komplexe Aufgaben aus – Von Reiseplanung bis zu komplexen Projekten — er kann große Aufgaben in Schritte unterteilen und mit mehreren KI-Agenten erledigen.</p>
</li>
<li><p>🎙️ Sprachsteuerung – Klare, schnelle und futuristische Sprach-zu-Text- und Text-zu-Sprache-Funktionen, die es ermöglichen, mit ihm zu sprechen, als wäre er Ihr persönlicher KI-Assistent aus einem Sci-Fi-Film. (In Entwicklung)</p>
</li>
</ul>
<h3><strong>Demo</strong></h3>
<blockquote>
<p><em>Kannst du nach dem agenticSeek-Projekt suchen, herausfinden, welche Fähigkeiten benötigt werden, dann die Datei CV_candidates.zip öffnen und mir anschließend mitteilen, welche am besten zum Projekt passen?</em></p>
</blockquote>
<p>https://github.com/user-attachments/assets/b8ca60e9-7b3b-4533-840e-08f9ac426316</p>
<p>Hinweis: Diese Demo sowie alle darin erscheinenden Dateien (z. B. CV_candidates.zip) sind vollständig fiktiv. Wir sind kein Unternehmen, sondern suchen Open-Source-Mitwirkende, keine Bewerber.</p>
<blockquote>
<p>🛠⚠️️ <strong>Aktive Entwicklung</strong></p>
</blockquote>
<blockquote>
<p>🙏 Dieses Projekt begann als Nebenprojekt und hat weder eine Roadmap noch Finanzierung. Es ist viel größer geworden als erwartet und landete in GitHub Trending. Beiträge, Feedback und Geduld sind sehr willkommen.</p>
</blockquote>
<h2>Voraussetzungen</h2>
<p>Bevor Sie beginnen, stellen Sie sicher, dass folgende Software installiert ist:</p>
<ul>
<li><strong>Git:</strong> Zum Klonen des Repositories. <a href="https://git-scm.com/downloads">Git herunterladen</a></li>
<li><strong>Python 3.10.x:</strong> Wir empfehlen nachdrücklich die Python-Version 3.10.x zu verwenden. Andere Versionen können zu Abhängigkeitsfehlern führen. <a href="https://www.python.org/downloads/release/python-3100/">Python 3.10 herunterladen</a> (Wählen Sie eine 3.10.x-Version).</li>
<li><strong>Docker Engine &amp; Docker Compose:</strong> Zum Ausführen gebündelter Dienste wie SearxNG.
<ul>
<li>Installieren Sie Docker Desktop (enthält Docker Compose V2): <a href="https://docs.docker.com/desktop/install/windows-install/">Windows</a> | <a href="https://docs.docker.com/desktop/install/mac-install/">Mac</a> | <a href="https://docs.docker.com/desktop/install/linux-install/">Linux</a></li>
<li>Alternativ können Sie Docker Engine und Docker Compose unter Linux separat installieren: <a href="https://docs.docker.com/engine/install/">Docker Engine</a> | <a href="https://docs.docker.com/compose/install/">Docker Compose</a> (Stellen Sie sicher, dass Compose V2 installiert ist, z. B. <code>sudo apt-get install docker-compose-plugin</code>).</li>
</ul>
</li>
</ul>
<h3>1. <strong>Repository klonen und Setup</strong></h3>
<pre><code class="language-sh">git clone https://github.com/Fosowl/agenticSeek.git
cd agenticSeek
mv .env.example .env
</code></pre>
<h3>2. Inhalt der .env-Datei anpassen</h3>
<pre><code class="language-sh">SEARXNG_BASE_URL=&quot;http://127.0.0.1:8080&quot;
REDIS_BASE_URL=&quot;redis://redis:6379/0&quot;
WORK_DIR=&quot;/Users/mlg/Documents/workspace_for_ai&quot;
OLLAMA_PORT=&quot;11434&quot;
LM_STUDIO_PORT=&quot;1234&quot;
CUSTOM_ADDITIONAL_LLM_PORT=&quot;11435&quot;
OPENAI_API_KEY='optional'
DEEPSEEK_API_KEY='optional'
OPENROUTER_API_KEY='optional'
TOGETHER_API_KEY='optional'
GOOGLE_API_KEY='optional'
ANTHROPIC_API_KEY='optional'
</code></pre>
<p>Aktualisieren Sie die <code>.env</code>-Datei nach Bedarf mit Ihren eigenen Werten:</p>
<ul>
<li><strong>SEARXNG_BASE_URL</strong>: Unverändert lassen</li>
<li><strong>REDIS_BASE_URL</strong>: Unverändert lassen</li>
<li><strong>WORK_DIR</strong>: Pfad zu Ihrem Arbeitsverzeichnis auf Ihrem lokalen Rechner. AgenticSeek kann diese Dateien lesen und mit ihnen interagieren.</li>
<li><strong>OLLAMA_PORT</strong>: Portnummer für den Ollama-Dienst.</li>
<li><strong>LM_STUDIO_PORT</strong>: Portnummer für den LM Studio-Dienst.</li>
<li><strong>CUSTOM_ADDITIONAL_LLM_PORT</strong>: Port für einen zusätzlichen benutzerdefinierten LLM-Dienst.</li>
</ul>
<p><strong>API-Keys sind völlig optional für Nutzer, die LLM lokal ausführen. Das ist der Hauptzweck dieses Projekts. Lassen Sie die Felder leer, wenn Sie über ausreichend Hardware verfügen.</strong></p>
<h3>3. <strong>Docker starten</strong></h3>
<p>Stellen Sie sicher, dass Docker auf Ihrem System installiert und gestartet ist. Sie können Docker mit folgenden Befehlen starten:</p>
<ul>
<li><p><strong>Unter Linux/macOS:</strong><br />
Öffnen Sie ein Terminal und führen Sie aus:</p>
<pre><code class="language-sh">sudo systemctl start docker
</code></pre>
<p>Oder starten Sie Docker Desktop über das Anwendungsmenü, falls installiert.</p>
</li>
<li><p><strong>Unter Windows:</strong><br />
Starten Sie Docker Desktop über das Startmenü.</p>
</li>
</ul>
<p>Sie können überprüfen, ob Docker läuft, indem Sie folgenden Befehl ausführen:</p>
<pre><code class="language-sh">docker info
</code></pre>
<p>Wenn Sie Informationen zu Ihrer Docker-Installation sehen, läuft Docker korrekt.</p>
<p>Eine Zusammenfassung finden Sie in der Tabelle der <a href="#list-of-local-providers">lokalen Anbieter</a> unten.</p>
<p>Nächster Schritt: <a href="#start-services-and-run">AgenticSeek lokal ausführen</a></p>
<p><em>Siehe den Abschnitt <a href="#troubleshooting">Fehlerbehebung</a>, falls Sie Probleme haben.</em>
<em>Wenn Ihre Hardware keine lokalen LLMs ausführen kann, siehe <a href="#setup-to-run-with-an-api">Setup zur Nutzung mit einer API</a>.</em>
<em>Für detaillierte Erklärungen zu <code>config.ini</code>, siehe <a href="#config">Config-Abschnitt</a>.</em></p>
<hr />
<h2>Setup zum lokalen Ausführen von LLM auf Ihrem Rechner</h2>
<p><strong>Hardware-Anforderungen:</strong></p>
<p>Um LLMs lokal auszuführen, benötigen Sie entsprechende Hardware. Mindestens eine GPU, die Magistral, Qwen oder Deepseek 14B ausführen kann, ist erforderlich. Siehe FAQ für detaillierte Modell-/Performance-Empfehlungen.</p>
<p><strong>Lokalen Anbieter einrichten</strong></p>
<p>Starten Sie Ihren lokalen Anbieter, zum Beispiel mit ollama:</p>
<pre><code class="language-sh">ollama serve
</code></pre>
<p>Eine Liste der unterstützten lokalen Anbieter finden Sie unten.</p>
<p><strong>config.ini aktualisieren</strong></p>
<p>Passen Sie die config.ini-Datei an, um provider_name auf einen unterstützten Anbieter und provider_model auf ein von Ihrem Anbieter unterstütztes LLM zu setzen. Wir empfehlen Reasoning-Modelle wie <em>Magistral</em> oder <em>Deepseek</em>.</p>
<p>Siehe <strong>FAQ</strong> am Ende des README für benötigte Hardware.</p>
<pre><code class="language-sh">[MAIN]
is_local = True # Ob Sie lokal oder mit Remote-Anbieter arbeiten.
provider_name = ollama # oder lm-studio, openai, etc..
provider_model = deepseek-r1:14b # Wählen Sie ein Modell, das zu Ihrer Hardware passt
provider_server_address = 127.0.0.1:11434
agent_name = Jarvis # Name Ihrer KI
recover_last_session = True # Ob die letzte Sitzung wiederhergestellt werden soll
save_session = True # Ob die aktuelle Sitzung gespeichert werden soll
speak = False # Text zu Sprache
listen = False # Sprache zu Text, nur für CLI, experimentell
jarvis_personality = False # Ob eine „Jarvis“-ähnliche Persönlichkeit genutzt werden soll (experimentell)
languages = en zh # Liste der Sprachen, Text zu Sprache nutzt standardmäßig die erste auf der Liste
[BROWSER]
headless_browser = True # Unverändert lassen, außer bei Nutzung von CLI auf dem Host.
stealth_mode = True # Nutze undetected selenium zur Reduzierung der Browser-Erkennung
</code></pre>
<p><strong>Warnung</strong>:</p>
<ul>
<li><p>Das Format der <code>config.ini</code>-Datei unterstützt keine Kommentare.
Kopieren Sie NICHT die Beispiel-Konfiguration direkt, da Kommentare zu Fehlern führen. Passen Sie die <code>config.ini</code> manuell mit Ihren gewünschten Einstellungen an – ohne Kommentare.</p>
</li>
<li><p>Setzen Sie <em>NICHT</em> provider_name auf <code>openai</code>, wenn Sie LM-studio zum Ausführen von LLMs verwenden. Setzen Sie es stattdessen auf <code>lm-studio</code>.</p>
</li>
<li><p>Einige Anbieter (z. B. lm-studio) erfordern, dass Sie <code>http://</code> vor die IP-Adresse setzen. Zum Beispiel <code>http://127.0.0.1:1234</code></p>
</li>
</ul>
<p><strong>Liste lokaler Anbieter</strong></p>
<p>| Anbieter   | Lokal? | Beschreibung                                               |
|------------|--------|------------------------------------------------------------|
| ollama     | Ja     | Führen Sie LLMs lokal einfach mit ollama als LLM-Anbieter aus |
| lm-studio  | Ja     | Führen Sie LLM lokal mit LM Studio aus (setze <code>provider_name</code> auf <code>lm-studio</code>)|
| openai     | Ja     |  Nutzung einer OpenAI-kompatiblen API (z. B. llama.cpp server)  |</p>
<p>Nächster Schritt: <a href="#Start-services-and-Run">Dienste starten und AgenticSeek ausführen</a></p>
<p><em>Siehe den Abschnitt <a href="#troubleshooting">Fehlerbehebung</a>, falls Sie Probleme haben.</em>
<em>Wenn Ihre Hardware keine lokalen LLMs ausführen kann, siehe <a href="#setup-to-run-with-an-api">Setup zur Nutzung mit einer API</a>.</em>
<em>Für detaillierte Erklärungen zu <code>config.ini</code>, siehe <a href="#config">Config-Abschnitt</a>.</em></p>
<h2>Setup zur Nutzung mit einer API</h2>
<p>Dieses Setup verwendet externe, cloudbasierte LLM-Anbieter. Sie benötigen einen API-Key von Ihrem gewählten Dienst.</p>
<p><strong>1. Wählen Sie einen API-Anbieter und erhalten Sie einen API-Key:</strong></p>
<p>Siehe <a href="#list-of-api-providers">Liste der API-Anbieter</a> unten. Besuchen Sie deren Websites, um sich zu registrieren und einen API-Key zu erhalten.</p>
<p><strong>2. Setzen Sie Ihren API-Key als Umgebungsvariable:</strong></p>
<ul>
<li><strong>Linux/macOS:</strong>
Öffnen Sie Ihr Terminal und verwenden Sie den Befehl <code>export</code>. Am besten fügen Sie diesen Befehl in die Profil-Datei Ihrer Shell ein (z. B. <code>~/.bashrc</code>, <code>~/.zshrc</code>) für dauerhafte Speicherung.
<pre><code class="language-sh">export PROVIDER_API_KEY=&quot;your_api_key_here&quot; 
# Ersetzen Sie PROVIDER_API_KEY durch den spezifischen Variablennamen, z. B. OPENAI_API_KEY, GOOGLE_API_KEY
</code></pre>
Beispiel für TogetherAI:
<pre><code class="language-sh">export TOGETHER_API_KEY=&quot;xxxxxxxxxxxxxxxxxxxxxx&quot;
</code></pre>
</li>
<li><strong>Windows:</strong></li>
<li><strong>Eingabeaufforderung (temporär für aktuelle Sitzung):</strong>
<pre><code class="language-cmd">set PROVIDER_API_KEY=dein_api_schlüssel_hier
</code></pre>
</li>
<li><strong>PowerShell (temporär für aktuelle Sitzung):</strong>
<pre><code class="language-powershell">$env:PROVIDER_API_KEY=&quot;dein_api_schlüssel_hier&quot;
</code></pre>
</li>
<li><strong>Permanent:</strong> Suche nach „Umgebungsvariablen“ in der Windows-Suchleiste, klicke auf „Systemumgebungsvariablen bearbeiten“ und dann auf die Schaltfläche „Umgebungsvariablen...“. Füge eine neue Benutzervariable mit dem passenden Namen (z. B. <code>OPENAI_API_KEY</code>) und deinem Schlüssel als Wert hinzu.</li>
</ul>
<p><em>(Siehe FAQ: <a href="#how-do-i-set-api-keys">Wie setze ich API-Schlüssel?</a> für weitere Details).</em></p>
<p><strong>3. Aktualisiere <code>config.ini</code>:</strong></p>
<pre><code class="language-ini">[MAIN]
is_local = False
provider_name = openai # Oder google, deepseek, togetherAI, huggingface
provider_model = gpt-3.5-turbo # Oder gemini-1.5-flash, deepseek-chat, mistralai/Mixtral-8x7B-Instruct-v0.1 etc.
provider_server_address = # Wird typischerweise ignoriert oder kann leer bleiben, wenn is_local = False für die meisten APIs
# ... andere Einstellungen ...
</code></pre>
<p><em>Warnung:</em> Stelle sicher, dass keine Leerzeichen am Ende der Werte in der <code>config.ini</code> stehen.</p>
<p><strong>Liste der API-Anbieter</strong></p>
<p>| Anbieter     | <code>provider_name</code> | Lokal? | Beschreibung                                      | API-Schlüssel-Link (Beispiele)                       |
|--------------|-----------------|--------|---------------------------------------------------|------------------------------------------------------|
| OpenAI       | <code>openai</code>        | Nein   | Nutzung der ChatGPT-Modelle über die OpenAI-API.  | <a href="https://platform.openai.com/signup">platform.openai.com/signup</a> |
| Google Gemini| <code>google</code>        | Nein   | Nutzung der Google Gemini-Modelle via Google AI Studio. | <a href="https://aistudio.google.com/keys">aistudio.google.com/keys</a> |
| Deepseek     | <code>deepseek</code>      | Nein   | Nutzung von Deepseek-Modellen über deren API.      | <a href="https://platform.deepseek.com">platform.deepseek.com</a> |
| Hugging Face | <code>huggingface</code>   | Nein   | Nutzung von Modellen der Hugging Face Inference API. | <a href="https://huggingface.co/settings/tokens">huggingface.co/settings/tokens</a> |
| TogetherAI   | <code>togetherAI</code>    | Nein   | Nutzung verschiedener Open-Source-Modelle über TogetherAI API. | <a href="https://api.together.ai/settings/api-keys">api.together.ai/settings/api-keys</a> |</p>
<p><em>Hinweis:</em></p>
<ul>
<li>Wir raten davon ab, <code>gpt-4o</code> oder andere OpenAI-Modelle für komplexes Web-Browsing und Aufgabenplanung zu verwenden, da die aktuellen Prompt-Optimierungen auf Modelle wie Deepseek ausgerichtet sind.</li>
<li>Coding-/Bash-Aufgaben könnten mit Gemini auf Probleme stoßen, da das Modell eventuell Formatierungsanweisungen, die für Deepseek optimiert sind, nicht strikt befolgt.</li>
<li>Die <code>provider_server_address</code> in <code>config.ini</code> wird generell nicht verwendet, wenn <code>is_local = False</code>, da der API-Endpunkt in der Regel in der jeweiligen Provider-Bibliothek fest hinterlegt ist.</li>
</ul>
<p>Nächster Schritt: <a href="#Start-services-and-Run">Dienste starten und AgenticSeek ausführen</a></p>
<p><em>Siehe den Abschnitt <strong>Bekannte Probleme</strong>, falls du auf Schwierigkeiten stößt.</em></p>
<p><em>Siehe den Abschnitt <strong>Config</strong> für eine ausführliche Erklärung der Konfigurationsdatei.</em></p>
<hr />
<h2>Dienste starten und AgenticSeek ausführen</h2>
<p>Standardmäßig läuft AgenticSeek vollständig in Docker.</p>
<p>Starte die benötigten Dienste. Dies startet alle Dienste aus der docker-compose.yml, einschließlich:
- searxng
- redis (benötigt von searxng)
- frontend
- backend (bei Nutzung von <code>full</code>)</p>
<pre><code class="language-sh">./start_services.sh full # MacOS
start ./start_services.cmd full # Windows
</code></pre>
<p><strong>Warnung:</strong> Dieser Schritt lädt und startet alle Docker-Images, was bis zu 30 Minuten dauern kann. Warte nach dem Starten der Dienste, bis der Backend-Dienst vollständig läuft (du solltest <strong>backend: &quot;GET /health HTTP/1.1&quot; 200 OK</strong> im Log sehen), bevor du Nachrichten sendest. Das Backend kann beim ersten Start bis zu 5 Minuten benötigen.</p>
<p>Gehe zu <code>http://localhost:3000/</code> – dort solltest du die Weboberfläche sehen.</p>
<p><em>Fehlerbehebung beim Dienststart:</em> Wenn diese Skripte fehlschlagen, stelle sicher, dass die Docker Engine läuft und Docker Compose (V2, <code>docker compose</code>) korrekt installiert ist. Überprüfe die Ausgaben im Terminal auf Fehlermeldungen. Siehe <a href="#faq-troubleshooting">FAQ: Hilfe! Ich erhalte einen Fehler beim Ausführen von AgenticSeek oder dessen Skripten.</a></p>
<p><strong>Optional:</strong> Auf dem Host ausführen (CLI-Modus):</p>
<p>Um die CLI-Oberfläche zu nutzen, musst du das Paket auf dem Host installieren:</p>
<pre><code class="language-sh">./install.sh
./install.bat # Windows
</code></pre>
<p>Dienste starten:</p>
<pre><code class="language-sh">./start_services.sh # MacOS
start ./start_services.cmd # Windows
</code></pre>
<p>Nutze die CLI: <code>python3 cli.py</code></p>
<hr />
<h2>Nutzung</h2>
<p>Stelle sicher, dass die Dienste mit <code>./start_services.sh full</code> laufen und öffne <code>localhost:3000</code> für die Weboberfläche.</p>
<p>Du kannst auch Sprache-zu-Text nutzen, indem du <code>listen = True</code> in der config aktivierst. Nur im CLI-Modus verfügbar.</p>
<p>Um zu beenden, sage oder tippe einfach <code>goodbye</code>.</p>
<p>Hier einige Anwendungsbeispiele:</p>
<blockquote>
<p><em>Erstelle ein Snake-Spiel in Python!</em></p>
</blockquote>
<blockquote>
<p><em>Durchsuche das Web nach den besten Cafés in Rennes, Frankreich, und speichere drei mit Adresse in rennes_cafes.txt.</em></p>
</blockquote>
<blockquote>
<p><em>Schreibe ein Go-Programm, das die Fakultät einer Zahl berechnet, und speichere es als factorial.go in deinem Arbeitsbereich.</em></p>
</blockquote>
<blockquote>
<p><em>Durchsuche meinen summer_pictures-Ordner nach allen JPG-Dateien, benenne sie mit dem heutigen Datum um und speichere eine Liste der umbenannten Dateien in photos_list.txt.</em></p>
</blockquote>
<blockquote>
<p><em>Suche online nach beliebten Sci-Fi-Filmen aus 2024 und wähle drei für heute Abend aus. Speichere die Liste in movie_night.txt.</em></p>
</blockquote>
<blockquote>
<p><em>Durchsuche das Web nach den neuesten KI-Nachrichtenartikeln aus 2025, wähle drei aus und schreibe ein Python-Skript, das deren Titel und Zusammenfassungen extrahiert. Speichere das Skript als news_scraper.py und die Zusammenfassungen in ai_news.txt im Verzeichnis /home/projects.</em></p>
</blockquote>
<blockquote>
<p><em>Suche am Freitag im Web nach einer kostenlosen Aktienkurs-API, registriere dich mit supersuper7434567@gmail.com und schreibe dann ein Python-Skript, das täglich die Preise von Tesla abruft und die Ergebnisse in stock_prices.csv speichert.</em></p>
</blockquote>
<p><em>Beachte, dass das Ausfüllen von Formularen noch experimentell ist und fehlschlagen kann.</em></p>
<p>Nachdem du deine Anfrage eingegeben hast, weist AgenticSeek den am besten geeigneten Agenten für die Aufgabe zu.</p>
<p>Da dies ein frühes Prototypstadium ist, kann es sein, dass das Agenten-Routing-System nicht immer den passenden Agenten basierend auf deiner Anfrage auswählt.</p>
<p>Daher solltest du sehr explizit formulieren, was du möchtest und wie die KI vorgehen soll. Wenn du beispielsweise eine Websuche möchtest, sage nicht:</p>
<p><code>Kennst du gute Länder für Solo-Reisen?</code></p>
<p>Sondern frage:</p>
<p><code>Führe eine Websuche durch und finde heraus, welche Länder sich am besten für Solo-Reisen eignen</code></p>
<hr />
<h2><strong>Setup zur Ausführung des LLM auf deinem eigenen Server</strong></h2>
<p>Wenn du einen leistungsfähigen Computer oder Server hast, den du nutzen möchtest, aber von deinem Laptop aus darauf zugreifen willst, kannst du das LLM auf einem entfernten Server mit unserem eigenen LLM-Server ausführen.</p>
<p>Auf deinem „Server“, der das KI-Modell ausführen soll, ermittle die IP-Adresse:</p>
<pre><code class="language-sh">ip a | grep &quot;inet &quot; | grep -v 127.0.0.1 | awk '{print $2}' | cut -d/ -f1 # lokale IP
curl https://ipinfo.io/ip # öffentliche IP
</code></pre>
<p>Hinweis: Für Windows oder macOS nutze jeweils ipconfig oder ifconfig, um die IP-Adresse zu ermitteln.</p>
<p>Klonen das Repository und wechsle in den Ordner <code>server/</code>.</p>
<pre><code class="language-sh">git clone --depth 1 https://github.com/Fosowl/agenticSeek.git
cd agenticSeek/llm_server/
</code></pre>
<p>Installiere die spezifischen Anforderungen für den Server:</p>
<pre><code class="language-sh">pip3 install -r requirements.txt
</code></pre>
<p>Starte das Server-Skript.</p>
<pre><code class="language-sh">python3 app.py --provider ollama --port 3333
</code></pre>
<p>Du hast die Wahl zwischen der Nutzung von <code>ollama</code> und <code>llamacpp</code> als LLM-Service.</p>
<p>Nun auf deinem persönlichen Computer:</p>
<p>Ändere die Datei <code>config.ini</code>, setze <code>provider_name</code> auf <code>server</code> und <code>provider_model</code> auf <code>deepseek-r1:xxb</code>.
Setze <code>provider_server_address</code> auf die IP-Adresse des Rechners, der das Modell ausführt.</p>
<pre><code class="language-sh">[MAIN]
is_local = False
provider_name = server
provider_model = deepseek-r1:70b
provider_server_address = x.x.x.x:3333
</code></pre>
<p>Nächster Schritt: <a href="#Start-services-and-Run">Dienste starten und AgenticSeek ausführen</a></p>
<hr />
<h2>Sprache-zu-Text</h2>
<p>Achtung: Sprache-zu-Text funktioniert derzeit nur im CLI-Modus.</p>
<p>Beachte, dass Sprache-zu-Text aktuell nur auf Englisch funktioniert.</p>
<p>Die Sprache-zu-Text-Funktion ist standardmäßig deaktiviert. Um sie zu aktivieren, setze die Option listen in der config.ini-Datei auf True:</p>
<pre><code>listen = True
</code></pre>
<p>Wenn aktiviert, wartet die Sprache-zu-Text-Funktion auf ein Auslöse-Schlüsselwort, das der Name des Agenten ist, bevor deine Eingabe verarbeitet wird. Du kannst den Namen des Agenten anpassen, indem du den Wert <code>agent_name</code> in der <em>config.ini</em> aktualisierst:</p>
<pre><code>agent_name = Friday
</code></pre>
<p>Für eine optimale Erkennung empfehlen wir, einen geläufigen englischen Namen wie „John“ oder „Emma“ als Agentennamen zu verwenden.</p>
<p>Sobald das Transkript angezeigt wird, sagen Sie den Namen des Agenten laut, um ihn zu aktivieren (z.B. „Friday“).</p>
<p>Sprechen Sie Ihre Anfrage deutlich aus.</p>
<p>Beenden Sie Ihre Anfrage mit einer Bestätigungsphrase, um das System zum Fortfahren zu veranlassen. Beispiele für Bestätigungsphrasen sind:</p>
<pre><code>&quot;do it&quot;, &quot;go ahead&quot;, &quot;execute&quot;, &quot;run&quot;, &quot;start&quot;, &quot;thanks&quot;, &quot;would ya&quot;, &quot;please&quot;, &quot;okay?&quot;, &quot;proceed&quot;, &quot;continue&quot;, &quot;go on&quot;, &quot;do that&quot;, &quot;go it&quot;, &quot;do you understand?&quot;
</code></pre>
<h2>Config</h2>
<p>Beispiel-Konfiguration:</p>
<pre><code>[MAIN]
is_local = True
provider_name = ollama
provider_model = deepseek-r1:32b
provider_server_address = http://127.0.0.1:11434 # Beispiel für Ollama; benutze http://127.0.0.1:1234 für LM-Studio
agent_name = Friday
recover_last_session = False
save_session = False
speak = False
listen = False

jarvis_personality = False
languages = en zh # Liste der Sprachen für TTS und ggf. Routing.
[BROWSER]
headless_browser = False
stealth_mode = False
</code></pre>
<p><strong>Erklärung der <code>config.ini</code>-Einstellungen</strong>:</p>
<ul>
<li><strong><code>[MAIN]</code> Abschnitt:</strong>
<ul>
<li><code>is_local</code>: <code>True</code>, wenn ein lokaler LLM-Provider verwendet wird (Ollama, LM-Studio, lokaler OpenAI-kompatibler Server) oder die selbst gehostete Server-Option. <code>False</code>, wenn eine Cloud-basierte API (OpenAI, Google, etc.) verwendet wird.</li>
<li><code>provider_name</code>: Gibt den LLM-Provider an.
<ul>
<li>Lokale Optionen: <code>ollama</code>, <code>lm-studio</code>, <code>openai</code> (für lokale OpenAI-kompatible Server), <code>server</code> (für das selbst gehostete Server-Setup).</li>
<li>API-Optionen: <code>openai</code>, <code>google</code>, <code>deepseek</code>, <code>huggingface</code>, <code>togetherAI</code>.</li>
</ul>
</li>
<li><code>provider_model</code>: Der spezifische Modellname oder die ID für den gewählten Provider (z.B. <code>deepseekcoder:6.7b</code> für Ollama, <code>gpt-3.5-turbo</code> für die OpenAI-API, <code>mistralai/Mixtral-8x7B-Instruct-v0.1</code> für TogetherAI).</li>
<li><code>provider_server_address</code>: Die Adresse Ihres LLM-Providers.
<ul>
<li>Für lokale Provider: z.B. <code>http://127.0.0.1:11434</code> für Ollama, <code>http://127.0.0.1:1234</code> für LM-Studio.</li>
<li>Für den <code>server</code>-Provider-Typ: Die Adresse Ihres selbst gehosteten LLM-Servers (z.B. <code>http://your_server_ip:3333</code>).</li>
<li>Für Cloud-APIs (<code>is_local = False</code>): Wird meist ignoriert oder kann leer gelassen werden, da der API-Endpunkt i.d.R. durch die Client-Bibliothek verarbeitet wird.</li>
</ul>
</li>
<li><code>agent_name</code>: Name des KI-Assistenten (z.B. Friday). Wird als Triggerwort für Sprache-zu-Text verwendet, falls aktiviert.</li>
<li><code>recover_last_session</code>: <code>True</code>, um den Zustand der vorherigen Sitzung wiederherzustellen, <code>False</code> für einen Neustart.</li>
<li><code>save_session</code>: <code>True</code>, um den aktuellen Sitzungsstatus für eine mögliche Wiederherstellung zu speichern, sonst <code>False</code>.</li>
<li><code>speak</code>: <code>True</code>, um Text-zu-Sprache-Sprachausgabe zu aktivieren, <code>False</code> zum Deaktivieren.</li>
<li><code>listen</code>: <code>True</code>, um Sprache-zu-Text-Eingabe zu aktivieren (nur CLI-Modus), <code>False</code> zum Deaktivieren.</li>
<li><code>work_dir</code>: <strong>Wichtig:</strong> Das Verzeichnis, in dem AgenticSeek Dateien liest/schreibt. <strong>Stellen Sie sicher, dass dieser Pfad auf Ihrem System gültig und zugänglich ist.</strong></li>
<li><code>jarvis_personality</code>: <code>True</code> für einen experimentellen, „Jarvis-ähnlichen“ Systemprompt, <code>False</code> für den Standardprompt.</li>
<li><code>languages</code>: Kommagetrennte Liste von Sprachen (z.B. <code>en, zh, fr</code>). Wird für die Auswahl der TTS-Stimme verwendet (Standard ist die erste) und kann dem LLM-Router helfen. Vermeiden Sie zu viele oder sehr ähnliche Sprachen für die Effizienz des Routers.</li>
</ul>
</li>
<li><strong><code>[BROWSER]</code> Abschnitt:</strong>
<ul>
<li><code>headless_browser</code>: <code>True</code>, um den automatisierten Browser ohne sichtbares Fenster auszuführen (empfohlen für Web-Oberfläche oder nicht-interaktive Nutzung). <code>False</code>, um das Browserfenster anzuzeigen (nützlich für CLI-Modus oder Debugging).</li>
<li><code>stealth_mode</code>: <code>True</code>, um Maßnahmen zu aktivieren, die die Erkennung von Browserautomatisierung erschweren. Dies kann die manuelle Installation von Browser-Erweiterungen wie anticaptcha erfordern.</li>
</ul>
</li>
</ul>
<p>Dieser Abschnitt fasst die unterstützten LLM-Provider-Typen zusammen. Konfigurieren Sie diese in der <code>config.ini</code>.</p>
<p><strong>Lokale Provider (laufen auf Ihrer eigenen Hardware):</strong></p>
<p>| Provider-Name in <code>config.ini</code> | <code>is_local</code> | Beschreibung                                                                 | Setup-Abschnitt                                                    |
|-------------------------------|------------|-----------------------------------------------------------------------------|--------------------------------------------------------------------|
| <code>ollama</code>                      | <code>True</code>     | Nutzt Ollama zum Bereitstellen lokaler LLMs.                                | <a href="#setup-for-running-llm-locally-on-your-machine">Setup für lokalen LLM-Betrieb</a> |
| <code>lm-studio</code>                   | <code>True</code>     | Nutzt LM-Studio zum Bereitstellen lokaler LLMs.                             | <a href="#setup-for-running-llm-locally-on-your-machine">Setup für lokalen LLM-Betrieb</a> |
| <code>openai</code> (für lokalen Server) | <code>True</code>     | Verbindung zu lokalem Server mit OpenAI-kompatibler API (z.B. llama.cpp).   | <a href="#setup-for-running-llm-locally-on-your-machine">Setup für lokalen LLM-Betrieb</a> |
| <code>server</code>                      | <code>False</code>    | Verbindung zum AgenticSeek-LLM-Server auf einem anderen Rechner.             | <a href="#setup-to-run-the-llm-on-your-own-server">Setup für eigenen LLM-Server</a> |</p>
<p><strong>API-Provider (Cloud-basiert):</strong></p>
<p>| Provider-Name in <code>config.ini</code> | <code>is_local</code> | Beschreibung                                      | Setup-Abschnitt                                      |
|-------------------------------|------------|--------------------------------------------------|------------------------------------------------------|
| <code>openai</code>                      | <code>False</code>    | Nutzung der offiziellen OpenAI-API (z.B. GPT-3.5, GPT-4). | <a href="#setup-to-run-with-an-api">Setup für API-Betrieb</a>         |
| <code>google</code>                      | <code>False</code>    | Nutzung der Gemini-Modelle von Google via API.    | <a href="#setup-to-run-with-an-api">Setup für API-Betrieb</a>         |
| <code>deepseek</code>                    | <code>False</code>    | Nutzung der offiziellen Deepseek-API.             | <a href="#setup-to-run-with-an-api">Setup für API-Betrieb</a>         |
| <code>huggingface</code>                 | <code>False</code>    | Nutzung der Hugging Face Inference API.           | <a href="#setup-to-run-with-an-api">Setup für API-Betrieb</a>         |
| <code>togetherAI</code>                  | <code>False</code>    | Nutzung der TogetherAI-API für verschiedene Open-Modelle. | <a href="#setup-to-run-with-an-api">Setup für API-Betrieb</a>         |</p>
<hr />
<h2>Fehlerbehebung</h2>
<p>Wenn Sie auf Probleme stoßen, finden Sie in diesem Abschnitt Hilfestellungen.</p>
<h1>Bekannte Probleme</h1>
<h2>ChromeDriver-Probleme</h2>
<p><strong>Fehlerbeispiel:</strong> <code>SessionNotCreatedException: Message: session not created: This version of ChromeDriver only supports Chrome version XXX</code></p>
<ul>
<li><strong>Ursache:</strong> Die installierte ChromeDriver-Version ist nicht kompatibel mit Ihrer Google Chrome-Version.</li>
<li><strong>Lösung:</strong>
<ol>
<li><strong>Chrome-Version prüfen:</strong> Öffnen Sie Google Chrome, gehen Sie zu <code>Einstellungen &gt; Über Chrome</code>, um Ihre Version zu finden (z.B. „Version 120.0.6099.110“).</li>
<li><strong>Passenden ChromeDriver herunterladen:</strong>
<ul>
<li>Für Chrome-Versionen 115 und neuer: Gehen Sie zu den <a href="https://googlechromelabs.github.io/chrome-for-testing/">Chrome for Testing (CfT) JSON Endpoints</a>. Finden Sie den &quot;stable&quot;-Kanal und laden Sie den ChromeDriver für Ihr Betriebssystem herunter, der zu Ihrer Chrome-Hauptversion passt.</li>
<li>Für ältere Versionen (seltener): Sie finden diese ggf. auf der Seite <a href="https://chromedriver.chromium.org/downloads">ChromeDriver - WebDriver for Chrome</a>.</li>
<li>Das untenstehende Bild zeigt ein Beispiel von der CfT-Seite:
<img src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/chromedriver_readme.png" alt="Download Chromedriver specific version from Chrome for Testing page" /></li>
</ul>
</li>
<li><strong>ChromeDriver installieren:</strong>
<ul>
<li>Stellen Sie sicher, dass der heruntergeladene <code>chromedriver</code> (bzw. <code>chromedriver.exe</code> unter Windows) in einem Verzeichnis liegt, das in Ihrer PATH-Umgebungsvariablen aufgeführt ist (z.B. <code>/usr/local/bin</code> unter Linux/macOS oder einen eigenen Scripts-Ordner unter Windows).</li>
<li>Alternativ können Sie ihn im Hauptverzeichnis des <code>agenticSeek</code>-Projekts ablegen.</li>
<li>Machen Sie den Treiber ausführbar (z.B. <code>chmod +x chromedriver</code> unter Linux/macOS).</li>
</ul>
</li>
<li>Siehe auch den Abschnitt <a href="#chromedriver-installation">ChromeDriver-Installation</a> im Haupt-Installationsleitfaden für weitere Details.</li>
</ol>
</li>
</ul>
<p>Wenn dieser Abschnitt unvollständig ist oder Sie auf weitere ChromeDriver-Probleme stoßen, durchsuchen Sie bitte die bestehenden <a href="https://github.com/Fosowl/agenticSeek/issues">GitHub Issues</a> oder erstellen Sie ein neues.</p>
<p><code>Exception: Failed to initialize browser: Message: session not created: This version of ChromeDriver only supports Chrome version 113 Current browser version is 134.0.6998.89 with binary path</code></p>
<p>Dies passiert, wenn die Versionen Ihres Browsers und des Chromedrivers nicht übereinstimmen.</p>
<p>Sie müssen die neueste Version herunterladen:</p>
<p>https://developer.chrome.com/docs/chromedriver/downloads</p>
<p>Wenn Sie Chrome Version 115 oder neuer verwenden, gehen Sie zu:</p>
<p>https://googlechromelabs.github.io/chrome-for-testing/</p>
<p>Und laden Sie die zum Betriebssystem passende Chromedriver-Version herunter.</p>
<p><img src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/chromedriver_readme.png" alt="alt text" /></p>
<p>Wenn dieser Abschnitt unvollständig ist, erstellen Sie bitte ein Issue.</p>
<h2>Probleme mit Connection Adapters</h2>
<pre><code>Exception: Provider lm-studio failed: HTTP request failed: No connection adapters were found for '127.0.0.1:1234/v1/chat/completions'` (Hinweis: Port kann variieren)
</code></pre>
<ul>
<li><strong>Ursache:</strong> Die <code>provider_server_address</code> in der <code>config.ini</code> für <code>lm-studio</code> (oder andere lokale OpenAI-kompatible Server) fehlt das Präfix <code>http://</code> oder sie verweist auf den falschen Port.</li>
<li><strong>Lösung:</strong>
<ul>
<li>Stellen Sie sicher, dass die Adresse mit <code>http://</code> beginnt. LM-Studio verwendet typischerweise <code>http://127.0.0.1:1234</code>.</li>
<li>Korrigierte <code>config.ini</code>: <code>provider_server_address = http://127.0.0.1:1234</code> (oder Ihr tatsächlicher LM-Studio-Server-Port).</li>
</ul>
</li>
</ul>
<h2>SearxNG-Basis-URL nicht angegeben</h2>
<pre><code>raise ValueError(&quot;SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.&quot;)
ValueError: SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.`
</code></pre>
<h2>FAQ</h2>
<p><strong>F: Welche Hardware benötige ich?</strong></p>
<p>| Modellgröße | GPU      | Kommentar                                                                                         |
|-------------|----------|--------------------------------------------------------------------------------------------------|
| 7B          | 8 GB VRAM| ⚠️ Nicht empfohlen. Schlechte Performance, häufige Halluzinationen, und Planungsagenten schlagen fehl. |
| 14B         | 12 GB VRAM (z.B. RTX 3060) | ✅ Für einfache Aufgaben nutzbar. Kann bei Web-Browsing und Planung ins Straucheln geraten. |
| 32B         | 24+ GB VRAM (z.B. RTX 4090) | 🚀 Erfolgreich bei den meisten Aufgaben, könnte bei komplexer Planung an Grenzen stoßen.   |
| 70B+        | 48+ GB VRAM| 💪 Exzellent. Empfohlen für fortgeschrittene Anwendungsfälle.                                 |</p>
<p><strong>F: Ich erhalte einen Fehler – was tun?</strong></p>
<p>Stellen Sie sicher, dass lokal ausgeführt wird (<code>ollama serve</code>), Ihre <code>config.ini</code> zum Provider passt und alle Abhängigkeiten installiert sind. Wenn nichts hilft, gerne ein Issue erstellen.</p>
<p><strong>F: Kann es wirklich 100% lokal laufen?</strong></p>
<p>Ja, mit Ollama, lm-studio oder server-Providern laufen alle Sprachmodelle und TTS/STT-Modelle lokal. Nicht-lokale Optionen (OpenAI oder andere APIs) sind optional.</p>
<p><strong>F: Warum AgenticSeek nutzen, wenn ich Manus habe?</strong></p>
<p>Im Gegensatz zu Manus setzt AgenticSeek auf Unabhängigkeit von externen Systemen, bietet mehr Kontrolle, Privatsphäre und vermeidet API-Kosten.</p>
<p><strong>F: Wer steckt hinter dem Projekt?</strong></p>
<p>Das Projekt wurde von mir ins Leben gerufen, zusammen mit zwei Freunden, die als Maintainer und Mitwirkende aus der Open-Source-Community auf GitHub agieren. Wir sind einfach eine Gruppe engagierter Leute, kein Startup oder Unternehmen.</p>
<p>Jedes AgenticSeek-Konto auf X außer meinem persönlichen (https://x.com/Martin993886460) ist eine Nachahmung.</p>
<h2>Mitwirken</h2>
<p>Wir suchen Entwickler, die AgenticSeek verbessern! Schau dir offene Issues oder Diskussionen an.</p>
<p><a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/docs/CONTRIBUTING.md">Beitragsleitfaden</a></p>
<p><a href="https://www.star-history.com/#Fosowl/agenticSeek&amp;Date"><img src="https://api.star-history.com/svg?repos=Fosowl/agenticSeek&amp;type=Date" alt="Star History Chart" /></a></p>
<h2>Maintainer:</h2>
<blockquote>
<p><a href="https://github.com/Fosowl">Fosowl</a> | Pariser Zeit</p>
</blockquote>
<blockquote>
<p><a href="https://github.com/antoineVIVIES">antoineVIVIES</a> | Taipeh Zeit</p>
</blockquote>
<blockquote>
<p><a href="https://github.com/steveh8758">steveh8758</a> | Taipeh Zeit</p>
</blockquote>
<h2>Besonderer Dank:</h2>
<blockquote>
<p><a href="https://github.com/tcsenpai">tcsenpai</a> und <a href="https://github.com/plitc">plitc</a> für die Unterstützung bei der Backend-Dockerisierung</p>
</blockquote>
<h2>Sponsoren:</h2>
<p>5$ oder mehr monatliche Sponsoren werden hier genannt:</p>
<ul>
<li><strong>tatra-labs</strong>
Certainly! Please provide the content for Part 4 of 4 that you wish to have translated.</li>
</ul>
<hr />
<p>Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx">Open Ai Tx</a> | Last indexed: 2025-06-16</p>
<hr />

        </div>
    </div>
    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Statcounter and other scripts can be added here -->
</body>
</html>