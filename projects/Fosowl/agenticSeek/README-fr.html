<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>agenticSeek - Fosowl/agenticSeek fr</title>
    <meta name="title" content="agenticSeek - Fosowl/agenticSeek fr | AgenticSeek : Alternative privée et locale à Manus. English | 中文 | 繁體中文 | Français | 日本語 | Português (Brasil) | Español Une alternative 100% locale à Manus AI, ...">
    <meta name="description" content="Fosowl/agenticSeek - GitHub repository fr documentation and information | AgenticSeek : Alternative privée et locale à Manus. English | 中文 | 繁體中文 | Français | 日本語 | Português (Brasil) | Español Une alternative 100% locale à Manus AI, ...">
    <meta name="keywords" content="Fosowl, agenticSeek, GitHub, repository, fr documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/Fosowl/agenticSeek/README-fr.html">
    <meta property="og:title" content="agenticSeek - Fosowl/agenticSeek fr | AgenticSeek : Alternative privée et locale à Manus. English | 中文 | 繁體中文 | Français | 日本語 | Português (Brasil) | Español Une alternative 100% locale à Manus AI, ...">
    <meta property="og:description" content="Fosowl/agenticSeek - GitHub repository fr documentation and information | AgenticSeek : Alternative privée et locale à Manus. English | 中文 | 繁體中文 | Français | 日本語 | Português (Brasil) | Español Une alternative 100% locale à Manus AI, ...">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">
    <script src="/js/marked.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        body { display: flex; flex-direction: column; min-height: 100vh; }
        .main-container { margin: 0 auto; width: 100%; max-width: 980px; padding: 0 20px; }
        @media (max-width: 768px) { .main-container { padding: 0 15px; } }
        .markdown-body img { max-width: 100%; height: auto; }
        .header { text-align: center; margin-bottom: 30px; padding: 20px; background-color: #f6f8fa; border-bottom: 1px solid #e1e4e8; position: relative; }
        .header .links { margin-top: 10px; font-size: 16px; }
        .header .links a { color: #0366d6; text-decoration: none; margin-left: 5px; }
        .header .links a:hover { text-decoration: underline; }
        .language-badges { margin-top: 15px; text-align: center; }
        .language-badges a { display: inline-block; margin: 2px; text-decoration: none; }
        .language-badges img { height: 20px; border-radius: 3px; }
        .language-badges a:hover img { opacity: 0.8; }
    </style>
</head>
<body>
    <div style="position: fixed; top: 2px; left: 2px; z-index: 2000; background: rgba(255,255,255,0.95); border-radius: 6px; box-shadow: 0 2px 8px rgba(0,0,0,0.06); padding: 4px 14px; font-size: 15px; color: #222; font-family: 'Segoe UI', Arial, sans-serif; font-weight: 500; letter-spacing: 0.5px;">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" style="color: #0366d6; text-decoration: none; font-weight: 600;">Open AI Tx</a>
    </div>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/Fosowl/agenticSeek" id="githubRepoLink" target="_blank">Fosowl/agenticSeek</a>
<h1 style="display: none;">AgenticSeek : Alternative privée et locale à Manus. English | 中文 | 繁體中文 | Français | 日本語 | Português (Brasil) | Español Une alternative 100% locale à Manus AI, ...</h1>
        </div>
        <div class="language-badges" id="languageBadges">
            <!-- You can generate language badges here if needed -->
        </div>
    </div>
    <div class="main-container">
        <div class="markdown-body" id="content">
            <h1>AgenticSeek : Alternative privée et locale à Manus.</h1>
<p align="center">
<img align="center" src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/agentic_seek_logo.png" width="300" height="300" alt="Agentic Seek Logo">
<p>
<p>English | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_CHS.md">中文</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_CHT.md">繁體中文</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_FR.md">Français</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_JP.md">日本語</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_PTBR.md">Português (Brasil)</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_ES.md">Español</a></p>
<p><em>Une <strong>alternative 100% locale à Manus AI</strong>, cet assistant IA activé par la voix navigue de façon autonome sur le web, écrit du code et planifie des tâches tout en gardant toutes les données sur votre appareil. Conçu pour les modèles de raisonnement locaux, il fonctionne entièrement sur votre matériel, garantissant une confidentialité totale et aucune dépendance au cloud.</em></p>
<p><a href="https://fosowl.github.io/agenticSeek.html"><img src="https://img.shields.io/static/v1?label=Website&amp;message=AgenticSeek&amp;color=blue&amp;style=flat-square" alt="Visiter AgenticSeek" /></a> <img src="https://img.shields.io/badge/license-GPL--3.0-green" alt="License" /> <a href="https://discord.gg/8hGDaME3TC"><img src="https://img.shields.io/badge/Discord-Join%20Us-7289DA?logo=discord&amp;logoColor=white" alt="Discord" /></a> <a href="https://x.com/Martin993886460"><img src="https://img.shields.io/twitter/url/https/twitter.com/fosowl.svg?style=social&amp;label=Update%20%40Fosowl" alt="Twitter" /></a> <a href="https://github.com/Fosowl/agenticSeek/stargazers"><img src="https://img.shields.io/github/stars/Fosowl/agenticSeek?style=social" alt="GitHub stars" /></a></p>
<h3>Pourquoi AgenticSeek ?</h3>
<ul>
<li><p>🔒 Entièrement local &amp; privé - Tout s'exécute sur votre machine — pas de cloud, pas de partage de données. Vos fichiers, conversations et recherches restent privés.</p>
</li>
<li><p>🌐 Navigation web intelligente - AgenticSeek peut naviguer sur Internet de façon autonome — rechercher, lire, extraire des informations, remplir des formulaires web — tout cela en mode mains libres.</p>
</li>
<li><p>💻 Assistant de codage autonome - Besoin de code ? Il peut écrire, déboguer et exécuter des programmes en Python, C, Go, Java, et plus encore — sans supervision.</p>
</li>
<li><p>🧠 Sélection intelligente d’agents - Vous demandez, il choisit automatiquement le meilleur agent pour la tâche. Comme si vous aviez une équipe d’experts à disposition.</p>
</li>
<li><p>📋 Planifie &amp; exécute des tâches complexes - De la planification de voyage à la gestion de projets complexes — il peut découper de grandes tâches en étapes et les réaliser en utilisant plusieurs agents IA.</p>
</li>
<li><p>🎙️ Activation vocale - Voix propre, rapide et futuriste, et transcription vocale permettant de lui parler comme à votre IA personnelle d’un film de science-fiction. (En cours de développement)</p>
</li>
</ul>
<h3><strong>Démo</strong></h3>
<blockquote>
<p><em>Peux-tu rechercher le projet agenticSeek, apprendre quelles compétences sont requises, puis ouvrir le CV_candidates.zip et ensuite me dire lesquelles correspondent le mieux au projet</em></p>
</blockquote>
<p>https://github.com/user-attachments/assets/b8ca60e9-7b3b-4533-840e-08f9ac426316</p>
<p>Avertissement : Cette démo, y compris tous les fichiers qui apparaissent (ex : CV_candidates.zip), est entièrement fictive. Nous ne sommes pas une entreprise, nous recherchons des contributeurs open-source, pas des candidats.</p>
<blockquote>
<p>🛠⚠️️ <strong>Travail en cours actif</strong></p>
</blockquote>
<blockquote>
<p>🙏 Ce projet a commencé comme un projet annexe, sans feuille de route ni financement. Il a largement dépassé mes attentes en finissant dans GitHub Trending. Les contributions, retours et votre patience sont grandement appréciés.</p>
</blockquote>
<h2>Prérequis</h2>
<p>Avant de commencer, assurez-vous d’avoir les logiciels suivants installés :</p>
<ul>
<li><strong>Git :</strong> Pour cloner le dépôt. <a href="https://git-scm.com/downloads">Télécharger Git</a></li>
<li><strong>Python 3.10.x :</strong> Nous recommandons fortement d’utiliser Python version 3.10.x. L’utilisation d’autres versions peut entraîner des erreurs de dépendances. <a href="https://www.python.org/downloads/release/python-3100/">Télécharger Python 3.10</a> (choisissez une version 3.10.x).</li>
<li><strong>Docker Engine &amp; Docker Compose :</strong> Pour exécuter les services groupés comme SearxNG.
<ul>
<li>Installer Docker Desktop (qui inclut Docker Compose V2) : <a href="https://docs.docker.com/desktop/install/windows-install/">Windows</a> | <a href="https://docs.docker.com/desktop/install/mac-install/">Mac</a> | <a href="https://docs.docker.com/desktop/install/linux-install/">Linux</a></li>
<li>Alternativement, installez Docker Engine et Docker Compose séparément sur Linux : <a href="https://docs.docker.com/engine/install/">Docker Engine</a> | <a href="https://docs.docker.com/compose/install/">Docker Compose</a> (assurez-vous d’installer Compose V2, ex : <code>sudo apt-get install docker-compose-plugin</code>).</li>
</ul>
</li>
</ul>
<h3>1. <strong>Cloner le dépôt et configurer</strong></h3>
<pre><code class="language-sh">git clone https://github.com/Fosowl/agenticSeek.git
cd agenticSeek
mv .env.example .env
</code></pre>
<h3>2. Modifier le contenu du fichier .env</h3>
<pre><code class="language-sh">SEARXNG_BASE_URL=&quot;http://127.0.0.1:8080&quot;
REDIS_BASE_URL=&quot;redis://redis:6379/0&quot;
WORK_DIR=&quot;/Users/mlg/Documents/workspace_for_ai&quot;
OLLAMA_PORT=&quot;11434&quot;
LM_STUDIO_PORT=&quot;1234&quot;
CUSTOM_ADDITIONAL_LLM_PORT=&quot;11435&quot;
OPENAI_API_KEY='optional'
DEEPSEEK_API_KEY='optional'
OPENROUTER_API_KEY='optional'
TOGETHER_API_KEY='optional'
GOOGLE_API_KEY='optional'
ANTHROPIC_API_KEY='optional'
</code></pre>
<p>Mettez à jour le fichier <code>.env</code> avec vos propres valeurs si besoin :</p>
<ul>
<li><strong>SEARXNG_BASE_URL</strong> : Laisser inchangé</li>
<li><strong>REDIS_BASE_URL</strong> : Laisser inchangé</li>
<li><strong>WORK_DIR</strong> : Chemin vers votre dossier de travail local. AgenticSeek pourra lire et interagir avec ces fichiers.</li>
<li><strong>OLLAMA_PORT</strong> : Numéro de port pour le service Ollama.</li>
<li><strong>LM_STUDIO_PORT</strong> : Numéro de port pour le service LM Studio.</li>
<li><strong>CUSTOM_ADDITIONAL_LLM_PORT</strong> : Port pour tout service LLM personnalisé supplémentaire.</li>
</ul>
<p><strong>Les clés API sont totalement optionnelles pour les utilisateurs qui choisissent de faire tourner les LLM localement. C’est le but principal de ce projet. Laissez vide si vous avez un matériel suffisant</strong></p>
<h3>3. <strong>Démarrer Docker</strong></h3>
<p>Assurez-vous que Docker est installé et en cours d’exécution sur votre système. Vous pouvez démarrer Docker avec les commandes suivantes :</p>
<ul>
<li><p><strong>Sur Linux/macOS :</strong><br />
Ouvrez un terminal et lancez :</p>
<pre><code class="language-sh">sudo systemctl start docker
</code></pre>
<p>Ou lancez Docker Desktop depuis votre menu d’applications si installé.</p>
</li>
<li><p><strong>Sur Windows :</strong><br />
Lancez Docker Desktop depuis le menu Démarrer.</p>
</li>
</ul>
<p>Vous pouvez vérifier que Docker fonctionne en exécutant :</p>
<pre><code class="language-sh">docker info
</code></pre>
<p>Si vous voyez des informations sur votre installation Docker, cela fonctionne correctement.</p>
<p>Voir le tableau des <a href="#list-of-local-providers">Fournisseurs locaux</a> ci-dessous pour un résumé.</p>
<p>Prochaine étape : <a href="#start-services-and-run">Lancer AgenticSeek en local</a></p>
<p><em>Voir la section <a href="#troubleshooting">Dépannage</a> si vous rencontrez des problèmes.</em>
<em>Si votre matériel ne peut pas exécuter les LLM localement, voir <a href="#setup-to-run-with-an-api">Configuration pour utilisation via API</a>.</em>
<em>Pour une explication détaillée de <code>config.ini</code>, voir la <a href="#config">Section Config</a>.</em></p>
<hr />
<h2>Configuration pour faire tourner un LLM localement sur votre machine</h2>
<p><strong>Configuration matérielle requise :</strong></p>
<p>Pour faire tourner les LLM localement, vous aurez besoin d’un matériel suffisant. Au minimum, un GPU capable d’exécuter Magistral, Qwen ou Deepseek 14B est requis. Voir la FAQ pour des recommandations détaillées sur les modèles/performances.</p>
<p><strong>Démarrez votre fournisseur local</strong></p>
<p>Lancez votre fournisseur local, par exemple avec ollama :</p>
<pre><code class="language-sh">ollama serve
</code></pre>
<p>Voir ci-dessous la liste des fournisseurs locaux pris en charge.</p>
<p><strong>Mettre à jour le config.ini</strong></p>
<p>Modifiez le fichier config.ini pour définir provider_name sur un fournisseur pris en charge et provider_model sur un LLM supporté par votre fournisseur. Nous recommandons un modèle de raisonnement tel que <em>Magistral</em> ou <em>Deepseek</em>.</p>
<p>Voir la <strong>FAQ</strong> à la fin du README pour le matériel requis.</p>
<pre><code class="language-sh">[MAIN]
is_local = True # Si vous exécutez localement ou avec un fournisseur distant.
provider_name = ollama # ou lm-studio, openai, etc..
provider_model = deepseek-r1:14b # choisissez un modèle adapté à votre matériel
provider_server_address = 127.0.0.1:11434
agent_name = Jarvis # nom de votre IA
recover_last_session = True # pour récupérer la session précédente
save_session = True # pour mémoriser la session courante
speak = False # texte vers parole
listen = False # reconnaissance vocale, uniquement pour CLI, expérimental
jarvis_personality = False # Pour un comportement plus &quot;Jarvis&quot; (expérimental)
languages = en zh # Liste des langues, la synthèse vocale utilisera la première langue de la liste
[BROWSER]
headless_browser = True # laisser inchangé sauf utilisation CLI sur hôte.
stealth_mode = True # Utilise selenium indétectable pour réduire la détection par les sites
</code></pre>
<p><strong>Attention</strong> :</p>
<ul>
<li><p>Le format du fichier <code>config.ini</code> ne supporte pas les commentaires.
Ne copiez/collez pas la configuration d’exemple directement, car les commentaires provoqueront des erreurs. Modifiez plutôt manuellement le fichier <code>config.ini</code> selon vos besoins, sans commentaires.</p>
</li>
<li><p>Ne mettez <em>PAS</em> provider_name à <code>openai</code> si vous utilisez LM-studio pour exécuter les LLM. Utilisez <code>lm-studio</code>.</p>
</li>
<li><p>Certains fournisseurs (ex : lm-studio) exigent <code>http://</code> devant l’IP. Par exemple <code>http://127.0.0.1:1234</code></p>
</li>
</ul>
<p><strong>Liste des fournisseurs locaux</strong></p>
<p>| Fournisseur  | Local ? | Description                                               |
|--------------|---------|-----------------------------------------------------------|
| ollama       | Oui     | Exécutez des LLM localement facilement avec ollama        |
| lm-studio    | Oui     | Exécutez un LLM localement avec LM studio (utiliser <code>provider_name</code> à <code>lm-studio</code>)|
| openai       | Oui     | Utilisez une API compatible openai (ex : serveur llama.cpp)  |</p>
<p>Prochaine étape : <a href="#Start-services-and-Run">Démarrer les services et lancer AgenticSeek</a></p>
<p><em>Voir la section <a href="#troubleshooting">Dépannage</a> si vous rencontrez des problèmes.</em>
<em>Si votre matériel ne peut pas exécuter les LLM localement, voir <a href="#setup-to-run-with-an-api">Configuration pour utilisation via API</a>.</em>
<em>Pour une explication détaillée de <code>config.ini</code>, voir la <a href="#config">Section Config</a>.</em></p>
<h2>Configuration pour utilisation via API</h2>
<p>Cette configuration utilise des fournisseurs LLM externes, basés sur le cloud. Vous aurez besoin d’une clé API provenant du service choisi.</p>
<p><strong>1. Choisissez un fournisseur API et obtenez une clé API :</strong></p>
<p>Consultez la <a href="#list-of-api-providers">Liste des fournisseurs API</a> ci-dessous. Rendez-vous sur leur site web pour vous inscrire et obtenir une clé API.</p>
<p><strong>2. Définissez votre clé API comme variable d’environnement :</strong></p>
<ul>
<li><strong>Linux/macOS :</strong>
Ouvrez votre terminal et utilisez la commande <code>export</code>. Il est conseillé de l’ajouter à votre fichier de profil de shell (ex : <code>~/.bashrc</code>, <code>~/.zshrc</code>) pour la persistance.
<pre><code class="language-sh">export PROVIDER_API_KEY=&quot;votre_cle_api_ici&quot; 
# Remplacez PROVIDER_API_KEY par le nom de variable spécifique, ex : OPENAI_API_KEY, GOOGLE_API_KEY
</code></pre>
Exemple pour TogetherAI :
<pre><code class="language-sh">export TOGETHER_API_KEY=&quot;xxxxxxxxxxxxxxxxxxxxxx&quot;
</code></pre>
</li>
<li><strong>Windows :</strong></li>
<li><strong>Invite de commandes (Temporaire pour la session en cours) :</strong>
<pre><code class="language-cmd">set PROVIDER_API_KEY=your_api_key_here
</code></pre>
</li>
<li><strong>PowerShell (Temporaire pour la session en cours) :</strong>
<pre><code class="language-powershell">$env:PROVIDER_API_KEY=&quot;your_api_key_here&quot;
</code></pre>
</li>
<li><strong>De façon permanente :</strong> Recherchez &quot;variables d'environnement&quot; dans la barre de recherche Windows, cliquez sur &quot;Modifier les variables d'environnement système&quot;, puis cliquez sur le bouton &quot;Variables d'environnement...&quot;. Ajoutez une nouvelle variable utilisateur avec le nom approprié (par exemple, <code>OPENAI_API_KEY</code>) et votre clé comme valeur.</li>
</ul>
<p><em>(Voir la FAQ : <a href="#how-do-i-set-api-keys">Comment définir les clés API ?</a> pour plus de détails).</em></p>
<p><strong>3. Mettre à jour <code>config.ini</code> :</strong></p>
<pre><code class="language-ini">[MAIN]
is_local = False
provider_name = openai # Ou google, deepseek, togetherAI, huggingface
provider_model = gpt-3.5-turbo # Ou gemini-1.5-flash, deepseek-chat, mistralai/Mixtral-8x7B-Instruct-v0.1 etc.
provider_server_address = # Généralement ignoré ou peut rester vide lorsque is_local = False pour la plupart des APIs
# ... autres paramètres ...
</code></pre>
<p><em>Attention :</em> Assurez-vous qu'il n'y a pas d'espaces à la fin des valeurs dans le fichier <code>config.ini</code>.</p>
<p><strong>Liste des fournisseurs d'API</strong></p>
<p>| Fournisseur   | <code>provider_name</code> | Local ? | Description                                           | Lien de clé API (Exemples)                     |
|---------------|-----------------|---------|------------------------------------------------------|-----------------------------------------------|
| OpenAI        | <code>openai</code>        | Non     | Utilise les modèles ChatGPT via l'API d'OpenAI.      | <a href="https://platform.openai.com/signup">platform.openai.com/signup</a> |
| Google Gemini | <code>google</code>        | Non     | Utilise les modèles Google Gemini via Google AI Studio.| <a href="https://aistudio.google.com/keys">aistudio.google.com/keys</a> |
| Deepseek      | <code>deepseek</code>      | Non     | Utilise les modèles Deepseek via leur API.            | <a href="https://platform.deepseek.com">platform.deepseek.com</a> |
| Hugging Face  | <code>huggingface</code>   | Non     | Utilise les modèles de l'API Hugging Face Inference.  | <a href="https://huggingface.co/settings/tokens">huggingface.co/settings/tokens</a> |
| TogetherAI    | <code>togetherAI</code>    | Non     | Utilise divers modèles open source via l'API TogetherAI.| <a href="https://api.together.ai/settings/api-keys">api.together.ai/settings/api-keys</a> |</p>
<p><em>Remarque :</em></p>
<ul>
<li>Nous déconseillons l'utilisation de <code>gpt-4o</code> ou d'autres modèles OpenAI pour la navigation web complexe et la planification de tâches, car les optimisations actuelles des prompts sont adaptées à des modèles comme Deepseek.</li>
<li>Les tâches de codage/bash peuvent rencontrer des problèmes avec Gemini, car il ne suit pas toujours strictement le formatage des prompts optimisés pour Deepseek.</li>
<li>Le champ <code>provider_server_address</code> dans <code>config.ini</code> n'est généralement pas utilisé lorsque <code>is_local = False</code> car le point d'accès API est généralement codé en dur dans la bibliothèque du fournisseur concerné.</li>
</ul>
<p>Étape suivante : <a href="#Start-services-and-Run">Démarrer les services et lancer AgenticSeek</a></p>
<p><em>Voir la section <strong>Problèmes connus</strong> si vous rencontrez des difficultés</em></p>
<p><em>Voir la section <strong>Config</strong> pour une explication détaillée du fichier de configuration.</em></p>
<hr />
<h2>Démarrer les services et lancer AgenticSeek</h2>
<p>Par défaut, AgenticSeek fonctionne entièrement dans Docker.</p>
<p>Démarrez les services requis. Cela démarrera tous les services du fichier docker-compose.yml, y compris :
- searxng
- redis (requis par searxng)
- frontend
- backend (si vous utilisez <code>full</code>)</p>
<pre><code class="language-sh">./start_services.sh full # MacOS
start ./start_services.cmd full # Windows
</code></pre>
<p><strong>Attention :</strong> Cette étape téléchargera et chargera toutes les images Docker, ce qui peut prendre jusqu'à 30 minutes. Après le démarrage des services, veuillez attendre que le service backend soit complètement opérationnel (vous devriez voir <strong>backend: &quot;GET /health HTTP/1.1&quot; 200 OK</strong> dans le journal) avant d'envoyer des messages. Les services backend peuvent prendre 5 minutes à démarrer lors du premier lancement.</p>
<p>Accédez à <code>http://localhost:3000/</code> et vous devriez voir l'interface web.</p>
<p><em>Dépannage du démarrage des services :</em> Si ces scripts échouent, assurez-vous que Docker Engine est en cours d'exécution et que Docker Compose (V2, <code>docker compose</code>) est correctement installé. Vérifiez les messages d'erreur dans le terminal. Voir <a href="#faq-troubleshooting">FAQ : Aide ! J'obtiens une erreur lors de l'exécution d'AgenticSeek ou de ses scripts.</a></p>
<p><strong>Optionnel :</strong> Exécuter sur l'hôte (mode CLI) :</p>
<p>Pour utiliser l'interface CLI, vous devez installer le paquet sur l'hôte :</p>
<pre><code class="language-sh">./install.sh
./install.bat # windows
</code></pre>
<p>Démarrez les services :</p>
<pre><code class="language-sh">./start_services.sh # MacOS
start ./start_services.cmd # Windows
</code></pre>
<p>Utilisez la CLI : <code>python3 cli.py</code></p>
<hr />
<h2>Utilisation</h2>
<p>Assurez-vous que les services sont actifs avec <code>./start_services.sh full</code> et rendez-vous sur <code>localhost:3000</code> pour l'interface web.</p>
<p>Vous pouvez également utiliser la reconnaissance vocale en définissant <code>listen = True</code> dans la configuration. Uniquement en mode CLI.</p>
<p>Pour quitter, dites/écrivez simplement <code>goodbye</code>.</p>
<p>Voici quelques exemples d'utilisation :</p>
<blockquote>
<p><em>Créer un jeu du serpent en python !</em></p>
</blockquote>
<blockquote>
<p><em>Cherche sur le web les meilleurs cafés à Rennes, France, et sauvegarde une liste de trois avec leurs adresses dans rennes_cafes.txt.</em></p>
</blockquote>
<blockquote>
<p><em>Écris un programme Go pour calculer la factorielle d'un nombre, sauvegarde-le sous factorial.go dans ton espace de travail</em></p>
</blockquote>
<blockquote>
<p><em>Recherche dans mon dossier summer_pictures tous les fichiers JPG, renomme-les avec la date d’aujourd’hui, et sauvegarde la liste des fichiers renommés dans photos_list.txt</em></p>
</blockquote>
<blockquote>
<p><em>Recherche en ligne les films de science-fiction populaires de 2024 et choisis-en trois à regarder ce soir. Sauvegarde la liste dans movie_night.txt.</em></p>
</blockquote>
<blockquote>
<p><em>Cherche sur le web les derniers articles d’actualité IA de 2025, sélectionne-en trois, et écris un script Python pour extraire leurs titres et résumés. Sauvegarde le script sous news_scraper.py et les résumés dans ai_news.txt dans /home/projects</em></p>
</blockquote>
<blockquote>
<p><em>Vendredi, cherche sur le web une API gratuite de prix d’actions, inscris-toi avec supersuper7434567@gmail.com puis écris un script Python pour récupérer les prix quotidiens de Tesla via l’API et sauvegarde les résultats dans stock_prices.csv</em></p>
</blockquote>
<p><em>Notez que les capacités de remplissage de formulaire sont encore expérimentales et peuvent échouer.</em></p>
<p>Après avoir saisi votre requête, AgenticSeek allouera le meilleur agent pour la tâche.</p>
<p>Comme il s'agit d'un prototype précoce, le système de routage des agents peut ne pas toujours choisir le bon agent en fonction de votre requête.</p>
<p>Par conséquent, vous devez être très explicite sur ce que vous voulez et comment l’IA doit procéder ; par exemple, si vous souhaitez qu’elle effectue une recherche web, ne dites pas :</p>
<p><code>Connais-tu de bons pays pour voyager en solo ?</code></p>
<p>Demandez plutôt :</p>
<p><code>Fais une recherche web et trouve quels sont les meilleurs pays pour voyager en solo</code></p>
<hr />
<h2><strong>Configuration pour exécuter le LLM sur votre propre serveur</strong></h2>
<p>Si vous disposez d'un ordinateur puissant ou d'un serveur que vous pouvez utiliser, mais que vous souhaitez l'utiliser à distance depuis votre ordinateur portable, vous avez la possibilité d'exécuter le LLM sur un serveur distant en utilisant notre serveur llm personnalisé.</p>
<p>Sur votre &quot;serveur&quot; qui exécutera le modèle IA, récupérez l'adresse IP</p>
<pre><code class="language-sh">ip a | grep &quot;inet &quot; | grep -v 127.0.0.1 | awk '{print $2}' | cut -d/ -f1 # ip locale
curl https://ipinfo.io/ip # ip publique
</code></pre>
<p>Remarque : Pour Windows ou macOS, utilisez respectivement ipconfig ou ifconfig pour trouver l'adresse IP.</p>
<p>Clonez le dépôt et entrez dans le dossier <code>server/</code>.</p>
<pre><code class="language-sh">git clone --depth 1 https://github.com/Fosowl/agenticSeek.git
cd agenticSeek/llm_server/
</code></pre>
<p>Installez les dépendances spécifiques au serveur :</p>
<pre><code class="language-sh">pip3 install -r requirements.txt
</code></pre>
<p>Exécutez le script serveur.</p>
<pre><code class="language-sh">python3 app.py --provider ollama --port 3333
</code></pre>
<p>Vous avez le choix entre utiliser <code>ollama</code> ou <code>llamacpp</code> comme service LLM.</p>
<p>Maintenant sur votre ordinateur personnel :</p>
<p>Modifiez le fichier <code>config.ini</code> pour définir <code>provider_name</code> sur <code>server</code> et <code>provider_model</code> sur <code>deepseek-r1:xxb</code>.
Définissez <code>provider_server_address</code> sur l'adresse IP de la machine qui exécutera le modèle.</p>
<pre><code class="language-sh">[MAIN]
is_local = False
provider_name = server
provider_model = deepseek-r1:70b
provider_server_address = x.x.x.x:3333
</code></pre>
<p>Étape suivante : <a href="#Start-services-and-Run">Démarrer les services et lancer AgenticSeek</a></p>
<hr />
<h2>Reconnaissance vocale (Speech to Text)</h2>
<p>Attention : la reconnaissance vocale ne fonctionne actuellement qu’en mode CLI.</p>
<p>Veuillez noter qu’actuellement la reconnaissance vocale ne fonctionne qu’en anglais.</p>
<p>La fonctionnalité de reconnaissance vocale est désactivée par défaut. Pour l’activer, définissez l’option listen sur True dans le fichier config.ini :</p>
<pre><code>listen = True
</code></pre>
<p>Lorsqu’elle est activée, la reconnaissance vocale attend un mot-clé déclencheur, qui est le nom de l’agent, avant de commencer à traiter votre entrée. Vous pouvez personnaliser le nom de l’agent en modifiant la valeur <code>agent_name</code> dans le fichier <em>config.ini</em> :</p>
<pre><code>agent_name = Friday
</code></pre>
<p>Pour une reconnaissance optimale, nous recommandons d'utiliser un prénom anglais courant comme &quot;John&quot; ou &quot;Emma&quot; comme nom d'agent.</p>
<p>Une fois que vous voyez la transcription apparaître, dites le nom de l'agent à voix haute pour le réveiller (ex. : &quot;Friday&quot;).</p>
<p>Énoncez clairement votre requête.</p>
<p>Terminez votre demande par une phrase de confirmation pour indiquer au système de procéder. Exemples de phrases de confirmation :</p>
<pre><code>&quot;do it&quot;, &quot;go ahead&quot;, &quot;execute&quot;, &quot;run&quot;, &quot;start&quot;, &quot;thanks&quot;, &quot;would ya&quot;, &quot;please&quot;, &quot;okay?&quot;, &quot;proceed&quot;, &quot;continue&quot;, &quot;go on&quot;, &quot;do that&quot;, &quot;go it&quot;, &quot;do you understand?&quot;
</code></pre>
<h2>Config</h2>
<p>Exemple de configuration :</p>
<pre><code>[MAIN]
is_local = True
provider_name = ollama
provider_model = deepseek-r1:32b
provider_server_address = http://127.0.0.1:11434 # Exemple pour Ollama ; utilisez http://127.0.0.1:1234 pour LM-Studio
agent_name = Friday
recover_last_session = False
save_session = False
speak = False
listen = False

jarvis_personality = False
languages = en zh # Liste des langues pour la synthèse vocale et potentiellement le routage.
[BROWSER]
headless_browser = False
stealth_mode = False
</code></pre>
<p><strong>Explication des paramètres <code>config.ini</code></strong> :</p>
<ul>
<li><strong>Section <strong><code>[MAIN]</code></strong> :</strong>
<ul>
<li><code>is_local</code> : <code>True</code> si vous utilisez un fournisseur LLM local (Ollama, LM-Studio, serveur compatible OpenAI local) ou l'option serveur auto-hébergé. <code>False</code> si vous utilisez une API cloud (OpenAI, Google, etc.).</li>
<li><code>provider_name</code> : Spécifie le fournisseur LLM.
<ul>
<li>Options locales : <code>ollama</code>, <code>lm-studio</code>, <code>openai</code> (pour les serveurs compatibles OpenAI locaux), <code>server</code> (pour l'installation en auto-hébergement).</li>
<li>Options API : <code>openai</code>, <code>google</code>, <code>deepseek</code>, <code>huggingface</code>, <code>togetherAI</code>.</li>
</ul>
</li>
<li><code>provider_model</code> : Le nom ou ID du modèle spécifique pour le fournisseur choisi (ex. : <code>deepseekcoder:6.7b</code> pour Ollama, <code>gpt-3.5-turbo</code> pour l'API OpenAI, <code>mistralai/Mixtral-8x7B-Instruct-v0.1</code> pour TogetherAI).</li>
<li><code>provider_server_address</code> : L'adresse de votre fournisseur LLM.
<ul>
<li>Pour les fournisseurs locaux : ex. <code>http://127.0.0.1:11434</code> pour Ollama, <code>http://127.0.0.1:1234</code> pour LM-Studio.</li>
<li>Pour le type de fournisseur <code>server</code> : l'adresse de votre serveur LLM auto-hébergé (ex. : <code>http://votre_ip_serveur:3333</code>).</li>
<li>Pour les API cloud (<code>is_local = False</code>) : cela est souvent ignoré ou peut rester vide, car le point de terminaison de l'API est généralement géré par la bibliothèque cliente.</li>
</ul>
</li>
<li><code>agent_name</code> : Nom de l'assistant IA (ex. : Friday). Utilisé comme mot déclencheur pour la reconnaissance vocale si activée.</li>
<li><code>recover_last_session</code> : <code>True</code> pour tenter de restaurer l'état de la session précédente, <code>False</code> pour recommencer à zéro.</li>
<li><code>save_session</code> : <code>True</code> pour sauvegarder l'état actuel de la session pour une éventuelle récupération, <code>False</code> sinon.</li>
<li><code>speak</code> : <code>True</code> pour activer la synthèse vocale, <code>False</code> pour désactiver.</li>
<li><code>listen</code> : <code>True</code> pour activer la reconnaissance vocale (mode CLI uniquement), <code>False</code> pour désactiver.</li>
<li><code>work_dir</code> : <strong>Crucial :</strong> Le répertoire dans lequel AgenticSeek lira/écrira les fichiers. <strong>Assurez-vous que ce chemin est valide et accessible sur votre système.</strong></li>
<li><code>jarvis_personality</code> : <code>True</code> pour utiliser un prompt système de type &quot;Jarvis&quot; (expérimental), <code>False</code> pour le prompt standard.</li>
<li><code>languages</code> : Liste de langues séparées par des virgules (ex. : <code>en, zh, fr</code>). Utilisé pour la sélection de la voix TTS (par défaut la première) et peut aider le routeur LLM. Évitez d’indiquer trop de langues ou des langues très similaires pour l’efficacité du routeur.</li>
</ul>
</li>
<li><strong>Section <strong><code>[BROWSER]</code></strong> :</strong>
<ul>
<li><code>headless_browser</code> : <code>True</code> pour exécuter le navigateur automatisé sans fenêtre visible (recommandé pour l’interface web ou une utilisation non interactive). <code>False</code> pour afficher la fenêtre du navigateur (utile pour le mode CLI ou le débogage).</li>
<li><code>stealth_mode</code> : <code>True</code> pour activer des mesures rendant l’automatisation du navigateur plus difficile à détecter. Peut nécessiter l’installation manuelle d’extensions comme anticaptcha.</li>
</ul>
</li>
</ul>
<p>Cette section résume les types de fournisseurs LLM supportés. Configurez-les dans <code>config.ini</code>.</p>
<p><strong>Fournisseurs locaux (exécutés sur votre matériel) :</strong></p>
<p>| Nom du fournisseur dans <code>config.ini</code> | <code>is_local</code> | Description                                                                 | Section de configuration                                                    |
|-------------------------------|------------|-----------------------------------------------------------------------------|-----------------------------------------------------------------------------|
| <code>ollama</code>                      | <code>True</code>     | Utilise Ollama pour servir des LLM locaux.                                  | <a href="#setup-for-running-llm-locally-on-your-machine">Configuration pour exécuter un LLM localement</a> |
| <code>lm-studio</code>                   | <code>True</code>     | Utilise LM-Studio pour servir des LLM locaux.                               | <a href="#setup-for-running-llm-locally-on-your-machine">Configuration pour exécuter un LLM localement</a> |
| <code>openai</code> (pour serveur local) | <code>True</code>     | Connexion à un serveur local exposant une API compatible OpenAI (ex : llama.cpp). | <a href="#setup-for-running-llm-locally-on-your-machine">Configuration pour exécuter un LLM localement</a> |
| <code>server</code>                      | <code>False</code>    | Connexion au serveur LLM AgenticSeek auto-hébergé sur une autre machine.    | <a href="#setup-to-run-the-llm-on-your-own-server">Configuration pour exécuter le LLM sur votre propre serveur</a> |</p>
<p><strong>Fournisseurs API (basés sur le cloud) :</strong></p>
<p>| Nom du fournisseur dans <code>config.ini</code> | <code>is_local</code> | Description                                        | Section de configuration                                 |
|-------------------------------|------------|----------------------------------------------------|----------------------------------------------------------|
| <code>openai</code>                      | <code>False</code>    | Utilise l’API officielle d’OpenAI (ex. GPT-3.5, GPT-4). | <a href="#setup-to-run-with-an-api">Configuration pour utiliser une API</a> |
| <code>google</code>                      | <code>False</code>    | Utilise les modèles Gemini de Google via API.           | <a href="#setup-to-run-with-an-api">Configuration pour utiliser une API</a> |
| <code>deepseek</code>                    | <code>False</code>    | Utilise l’API officielle de Deepseek.                  | <a href="#setup-to-run-with-an-api">Configuration pour utiliser une API</a> |
| <code>huggingface</code>                 | <code>False</code>    | Utilise l’API d’inférence de Hugging Face.             | <a href="#setup-to-run-with-an-api">Configuration pour utiliser une API</a> |
| <code>togetherAI</code>                  | <code>False</code>    | Utilise l’API TogetherAI pour divers modèles open source. | <a href="#setup-to-run-with-an-api">Configuration pour utiliser une API</a> |</p>
<hr />
<h2>Dépannage</h2>
<p>Si vous rencontrez des problèmes, cette section vous guide.</p>
<h1>Problèmes connus</h1>
<h2>Problèmes de ChromeDriver</h2>
<p><strong>Exemple d’erreur :</strong> <code>SessionNotCreatedException: Message: session not created: This version of ChromeDriver only supports Chrome version XXX</code></p>
<ul>
<li><strong>Cause :</strong> La version de ChromeDriver installée est incompatible avec la version de votre navigateur Google Chrome.</li>
<li><strong>Solution :</strong>
<ol>
<li><strong>Vérifiez la version de Chrome :</strong> Ouvrez Google Chrome, allez dans <code>Paramètres &gt; À propos de Chrome</code> pour trouver votre version (ex. : &quot;Version 120.0.6099.110&quot;).</li>
<li><strong>Téléchargez le ChromeDriver correspondant :</strong>
<ul>
<li>Pour Chrome version 115 et supérieure : Rendez-vous sur les <a href="https://googlechromelabs.github.io/chrome-for-testing/">Chrome for Testing (CfT) JSON Endpoints</a>. Trouvez le canal &quot;stable&quot; et téléchargez le ChromeDriver pour votre OS correspondant à la version majeure de votre Chrome.</li>
<li>Pour les versions plus anciennes (plus rare) : Vous pouvez les trouver sur la page <a href="https://chromedriver.chromium.org/downloads">ChromeDriver - WebDriver for Chrome</a>.</li>
<li>L'image ci-dessous montre un exemple depuis la page CfT :
<img src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/chromedriver_readme.png" alt="Télécharger une version spécifique de Chromedriver sur la page Chrome for Testing" /></li>
</ul>
</li>
<li><strong>Installez ChromeDriver :</strong>
<ul>
<li>Assurez-vous que le <code>chromedriver</code> téléchargé (ou <code>chromedriver.exe</code> sur Windows) est placé dans un dossier référencé par la variable d’environnement PATH de votre système (ex. : <code>/usr/local/bin</code> sous Linux/macOS, ou un dossier de scripts ajouté au PATH sous Windows).</li>
<li>Alternativement, placez-le dans le répertoire racine du projet <code>agenticSeek</code>.</li>
<li>Assurez-vous que le pilote est exécutable (ex. : <code>chmod +x chromedriver</code> sous Linux/macOS).</li>
</ul>
</li>
<li>Consultez la section <a href="#chromedriver-installation">Installation de ChromeDriver</a> dans le guide principal d'installation pour plus de détails.</li>
</ol>
</li>
</ul>
<p>Si cette section est incomplète ou si vous rencontrez d’autres problèmes avec ChromeDriver, veuillez consulter les <a href="https://github.com/Fosowl/agenticSeek/issues">Issues GitHub existants</a> ou en ouvrir un nouveau.</p>
<p><code>Exception: Failed to initialize browser: Message: session not created: This version of ChromeDriver only supports Chrome version 113 Current browser version is 134.0.6998.89 with binary path</code></p>
<p>Cela arrive s'il y a un décalage entre la version de votre navigateur et celle de chromedriver.</p>
<p>Vous devez télécharger la dernière version :</p>
<p>https://developer.chrome.com/docs/chromedriver/downloads</p>
<p>Si vous utilisez Chrome version 115 ou supérieure, allez sur :</p>
<p>https://googlechromelabs.github.io/chrome-for-testing/</p>
<p>Et téléchargez la version de chromedriver correspondant à votre système d’exploitation.</p>
<p><img src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/chromedriver_readme.png" alt="alt text" /></p>
<p>Si cette section est incomplète, veuillez ouvrir une issue.</p>
<h2>Problèmes d’adaptateurs de connexion</h2>
<pre><code>Exception: Provider lm-studio failed: HTTP request failed: No connection adapters were found for '127.0.0.1:1234/v1/chat/completions'` (Note : le port peut varier)
</code></pre>
<ul>
<li><strong>Cause :</strong> Le paramètre <code>provider_server_address</code> dans <code>config.ini</code> pour <code>lm-studio</code> (ou tout autre serveur local compatible OpenAI) ne comporte pas le préfixe <code>http://</code> ou pointe vers le mauvais port.</li>
<li><strong>Solution :</strong>
<ul>
<li>Assurez-vous que l’adresse commence par <code>http://</code>. LM-Studio utilise généralement <code>http://127.0.0.1:1234</code> par défaut.</li>
<li>Correction dans <code>config.ini</code> : <code>provider_server_address = http://127.0.0.1:1234</code> (ou le port réel de votre serveur LM-Studio).</li>
</ul>
</li>
</ul>
<h2>URL de base SearxNG non fournie</h2>
<pre><code>raise ValueError(&quot;SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.&quot;)
ValueError: SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.`
</code></pre>
<h2>FAQ</h2>
<p><strong>Q : De quel matériel ai-je besoin ?</strong></p>
<p>| Taille du modèle  | GPU  | Remarques                                               |
|-------------------|------|--------------------------------------------------------|
| 7B                | 8GB Vram | ⚠️ Non recommandé. Performances faibles, hallucinations fréquentes, les agents planificateurs échoueront probablement. |
| 14B               | 12 GB VRAM (ex. RTX 3060) | ✅ Utilisable pour des tâches simples. Peut peiner sur la navigation web et la planification. |
| 32B               | 24+ GB VRAM (ex. RTX 4090) | 🚀 Réussite sur la plupart des tâches, peut encore avoir des difficultés pour la planification. |
| 70B+              | 48+ GB Vram | 💪 Excellent. Recommandé pour des usages avancés. |</p>
<p><strong>Q : J’ai une erreur, que faire ?</strong></p>
<p>Assurez-vous que le local est lancé (<code>ollama serve</code>), que votre <code>config.ini</code> correspond à votre fournisseur, et que les dépendances sont installées. Si rien ne fonctionne, n’hésitez pas à ouvrir une issue.</p>
<p><strong>Q : Peut-on vraiment tout faire tourner en local à 100% ?</strong></p>
<p>Oui, avec Ollama, lm-studio ou les fournisseurs <code>server</code>, toute la reconnaissance vocale, LLM et la synthèse vocale tournent en local. Les options non-locales (OpenAI ou autres API) sont facultatives.</p>
<p><strong>Q : Pourquoi utiliser AgenticSeek alors que j’ai Manus ?</strong></p>
<p>Contrairement à Manus, AgenticSeek privilégie l’indépendance vis-à-vis des systèmes externes, vous donnant plus de contrôle, de confidentialité et évitant les coûts d’API.</p>
<p><strong>Q : Qui est à l’origine du projet ?</strong></p>
<p>Le projet a été créé par moi-même, avec deux amis qui sont mainteneurs et contributeurs issus de la communauté open source sur GitHub. Nous ne sommes qu’un groupe de passionnés, pas une startup ni affiliés à une organisation.</p>
<p>Tout compte AgenticSeek sur X autre que mon compte personnel (https://x.com/Martin993886460) est une usurpation.</p>
<h2>Contribuer</h2>
<p>Nous recherchons des développeurs pour améliorer AgenticSeek ! Consultez les issues ouvertes ou les discussions.</p>
<p><a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/docs/CONTRIBUTING.md">Guide de contribution</a></p>
<p><a href="https://www.star-history.com/#Fosowl/agenticSeek&amp;Date"><img src="https://api.star-history.com/svg?repos=Fosowl/agenticSeek&amp;type=Date" alt="Star History Chart" /></a></p>
<h2>Mainteneurs :</h2>
<blockquote>
<p><a href="https://github.com/Fosowl">Fosowl</a> | Fuseau horaire de Paris</p>
</blockquote>
<blockquote>
<p><a href="https://github.com/antoineVIVIES">antoineVIVIES</a> | Fuseau horaire de Taipei</p>
</blockquote>
<blockquote>
<p><a href="https://github.com/steveh8758">steveh8758</a> | Fuseau horaire de Taipei</p>
</blockquote>
<h2>Remerciements particuliers :</h2>
<blockquote>
<p><a href="https://github.com/tcsenpai">tcsenpai</a> et <a href="https://github.com/plitc">plitc</a> pour leur aide à la dockerisation du backend</p>
</blockquote>
<h2>Sponsors :</h2>
<p>Les sponsors mensuels de 5 $ ou plus apparaissent ici :</p>
<ul>
<li><strong>tatra-labs</strong></li>
</ul>
<p>Certainly! However, you have not provided the content of &quot;Part 4 of 4&quot; to be translated. Please provide the text of the technical document you want translated into French, and I will proceed accordingly.</p>
<hr />
<p>Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx">Open Ai Tx</a> | Last indexed: 2025-06-16</p>
<hr />

        </div>
    </div>
    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Default Statcounter code for openaitx
https://openaitx.github.io/ -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>
    <!-- End of Statcounter Code -->
</body>
</html>