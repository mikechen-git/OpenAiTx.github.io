<!DOCTYPE html>
<html lang="pl">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>agenticSeek - Fosowl/agenticSeek pl</title>
    <meta name="title" content="agenticSeek - Fosowl/agenticSeek pl | AgenticSeek: Prywatna, lokalna alternatywa dla Manus English | 中文 | 繁體中文 | Français | 日本語 | Português (Brasil) | Español W pełni lokalna alternatywa dla Manus A...">
    <meta name="description" content="Fosowl/agenticSeek - GitHub repository pl documentation and information | AgenticSeek: Prywatna, lokalna alternatywa dla Manus English | 中文 | 繁體中文 | Français | 日本語 | Português (Brasil) | Español W pełni lokalna alternatywa dla Manus A...">
    <meta name="keywords" content="Fosowl, agenticSeek, GitHub, repository, pl documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/Fosowl/agenticSeek/README-pl.html">
    <meta property="og:title" content="agenticSeek - Fosowl/agenticSeek pl | AgenticSeek: Prywatna, lokalna alternatywa dla Manus English | 中文 | 繁體中文 | Français | 日本語 | Português (Brasil) | Español W pełni lokalna alternatywa dla Manus A...">
    <meta property="og:description" content="Fosowl/agenticSeek - GitHub repository pl documentation and information | AgenticSeek: Prywatna, lokalna alternatywa dla Manus English | 中文 | 繁體中文 | Français | 日本語 | Português (Brasil) | Español W pełni lokalna alternatywa dla Manus A...">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">
    <script src="/js/marked.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        body { display: flex; flex-direction: column; min-height: 100vh; }
        .main-container { margin: 0 auto; width: 100%; max-width: 980px; padding: 0 20px; }
        @media (max-width: 768px) { .main-container { padding: 0 15px; } }
        .markdown-body img { max-width: 100%; height: auto; }
        .header { text-align: center; margin-bottom: 30px; padding: 20px; background-color: #f6f8fa; border-bottom: 1px solid #e1e4e8; position: relative; }
        .header .links { margin-top: 10px; font-size: 16px; }
        .header .links a { color: #0366d6; text-decoration: none; margin-left: 5px; }
        .header .links a:hover { text-decoration: underline; }
        .language-badges { margin-top: 15px; text-align: center; }
        .language-badges a { display: inline-block; margin: 2px; text-decoration: none; }
        .language-badges img { height: 20px; border-radius: 3px; }
        .language-badges a:hover img { opacity: 0.8; }
    </style>
</head>
<body>
    <div style="position: fixed; top: 2px; left: 2px; z-index: 2000; background: rgba(255,255,255,0.95); border-radius: 6px; box-shadow: 0 2px 8px rgba(0,0,0,0.06); padding: 4px 14px; font-size: 15px; color: #222; font-family: 'Segoe UI', Arial, sans-serif; font-weight: 500; letter-spacing: 0.5px;">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" style="color: #0366d6; text-decoration: none; font-weight: 600;">Open AI Tx</a>
    </div>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/Fosowl/agenticSeek" id="githubRepoLink" target="_blank">Fosowl/agenticSeek</a>
<h1 style="display: none;">AgenticSeek: Prywatna, lokalna alternatywa dla Manus English | 中文 | 繁體中文 | Français | 日本語 | Português (Brasil) | Español W pełni lokalna alternatywa dla Manus A...</h1>
        </div>
        <div class="language-badges" id="languageBadges">
            <!-- You can generate language badges here if needed -->
        </div>
    </div>
    <div class="main-container">
        <div class="markdown-body" id="content">
            <h1>AgenticSeek: Prywatna, lokalna alternatywa dla Manus</h1>
<p align="center">
<img align="center" src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/agentic_seek_logo.png" width="300" height="300" alt="Agentic Seek Logo">
<p>
<p>English | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_CHS.md">中文</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_CHT.md">繁體中文</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_FR.md">Français</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_JP.md">日本語</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_PTBR.md">Português (Brasil)</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_ES.md">Español</a></p>
<p><em>W pełni <strong>lokalna alternatywa dla Manus AI</strong> – ten asystent AI z obsługą głosu autonomicznie przegląda internet, pisze kod i planuje zadania, zachowując wszystkie dane na Twoim urządzeniu. Dostosowany do lokalnych modeli rozumowania, działa całkowicie na Twoim sprzęcie, zapewniając pełną prywatność i brak zależności od chmury.</em></p>
<p><a href="https://fosowl.github.io/agenticSeek.html"><img src="https://img.shields.io/static/v1?label=Website&amp;message=AgenticSeek&amp;color=blue&amp;style=flat-square" alt="Odwiedź AgenticSeek" /></a> <img src="https://img.shields.io/badge/license-GPL--3.0-green" alt="License" /> <a href="https://discord.gg/8hGDaME3TC"><img src="https://img.shields.io/badge/Discord-Join%20Us-7289DA?logo=discord&amp;logoColor=white" alt="Discord" /></a> <a href="https://x.com/Martin993886460"><img src="https://img.shields.io/twitter/url/https/twitter.com/fosowl.svg?style=social&amp;label=Update%20%40Fosowl" alt="Twitter" /></a> <a href="https://github.com/Fosowl/agenticSeek/stargazers"><img src="https://img.shields.io/github/stars/Fosowl/agenticSeek?style=social" alt="GitHub stars" /></a></p>
<h3>Dlaczego AgenticSeek?</h3>
<ul>
<li><p>🔒 W pełni lokalny i prywatny – Wszystko działa na Twoim komputerze — bez chmury, bez udostępniania danych. Twoje pliki, rozmowy i wyszukiwania pozostają prywatne.</p>
</li>
<li><p>🌐 Inteligentne przeglądanie sieci – AgenticSeek może samodzielnie przeglądać internet — wyszukiwać, czytać, wyodrębniać informacje, wypełniać formularze — wszystko bez użycia rąk.</p>
</li>
<li><p>💻 Autonomiczny asystent kodowania – Potrzebujesz kodu? Może pisać, debugować i uruchamiać programy w Pythonie, C, Go, Javie i innych — wszystko bez nadzoru.</p>
</li>
<li><p>🧠 Inteligentny wybór agenta – Zadajesz pytanie, a on sam wybiera najlepszego agenta do zadania. Jak zespół ekspertów gotowych do pomocy.</p>
</li>
<li><p>📋 Planuje i realizuje złożone zadania – Od planowania podróży po skomplikowane projekty — potrafi rozbić duże zadania na kroki i wykonać je, korzystając z wielu agentów AI.</p>
</li>
<li><p>🎙️ Obsługa głosu – Czysty, szybki, futurystyczny głos oraz zamiana mowy na tekst pozwala rozmawiać z nim jak z własnym AI z filmu science fiction. (W trakcie opracowania)</p>
</li>
</ul>
<h3><strong>Demo</strong></h3>
<blockquote>
<p><em>Czy możesz wyszukać projekt agenticSeek, dowiedzieć się, jakie są wymagane umiejętności, następnie otworzyć CV_candidates.zip i powiedzieć mi, które najlepiej pasują do projektu</em></p>
</blockquote>
<p>https://github.com/user-attachments/assets/b8ca60e9-7b3b-4533-840e-08f9ac426316</p>
<p>Zastrzeżenie: To demo, w tym wszystkie pojawiające się pliki (np.: CV_candidates.zip), są całkowicie fikcyjne. Nie jesteśmy korporacją, szukamy współtwórców open-source, a nie kandydatów.</p>
<blockquote>
<p>🛠⚠️️ <strong>Aktywnie rozwijane</strong></p>
</blockquote>
<blockquote>
<p>🙏 Ten projekt powstał jako poboczny i nie posiada żadnej mapy drogowej ani finansowania. Rozrósł się znacznie bardziej, niż się spodziewałem, kończąc na GitHub Trending. Wszelkie wkłady, opinie i cierpliwość są bardzo cenione.</p>
</blockquote>
<h2>Wymagania wstępne</h2>
<p>Przed rozpoczęciem upewnij się, że masz zainstalowane następujące oprogramowanie:</p>
<ul>
<li><strong>Git:</strong> Do klonowania repozytorium. <a href="https://git-scm.com/downloads">Pobierz Git</a></li>
<li><strong>Python 3.10.x:</strong> Zdecydowanie zalecamy użycie wersji Python 3.10.x. Używanie innych wersji może prowadzić do błędów zależności. <a href="https://www.python.org/downloads/release/python-3100/">Pobierz Python 3.10</a> (wybierz wersję 3.10.x).</li>
<li><strong>Docker Engine i Docker Compose:</strong> Do uruchamiania usług takich jak SearxNG.
<ul>
<li>Zainstaluj Docker Desktop (zawiera Docker Compose V2): <a href="https://docs.docker.com/desktop/install/windows-install/">Windows</a> | <a href="https://docs.docker.com/desktop/install/mac-install/">Mac</a> | <a href="https://docs.docker.com/desktop/install/linux-install/">Linux</a></li>
<li>Alternatywnie, zainstaluj Docker Engine i Docker Compose oddzielnie na Linuksie: <a href="https://docs.docker.com/engine/install/">Docker Engine</a> | <a href="https://docs.docker.com/compose/install/">Docker Compose</a> (upewnij się, że instalujesz Compose V2, np. <code>sudo apt-get install docker-compose-plugin</code>).</li>
</ul>
</li>
</ul>
<h3>1. <strong>Sklonuj repozytorium i skonfiguruj</strong></h3>
<pre><code class="language-sh">git clone https://github.com/Fosowl/agenticSeek.git
cd agenticSeek
mv .env.example .env
</code></pre>
<h3>2. Zmień zawartość pliku .env</h3>
<pre><code class="language-sh">SEARXNG_BASE_URL=&quot;http://127.0.0.1:8080&quot;
REDIS_BASE_URL=&quot;redis://redis:6379/0&quot;
WORK_DIR=&quot;/Users/mlg/Documents/workspace_for_ai&quot;
OLLAMA_PORT=&quot;11434&quot;
LM_STUDIO_PORT=&quot;1234&quot;
CUSTOM_ADDITIONAL_LLM_PORT=&quot;11435&quot;
OPENAI_API_KEY='optional'
DEEPSEEK_API_KEY='optional'
OPENROUTER_API_KEY='optional'
TOGETHER_API_KEY='optional'
GOOGLE_API_KEY='optional'
ANTHROPIC_API_KEY='optional'
</code></pre>
<p>Zaktualizuj plik <code>.env</code> własnymi wartościami, jeśli to konieczne:</p>
<ul>
<li><strong>SEARXNG_BASE_URL</strong>: Pozostaw bez zmian</li>
<li><strong>REDIS_BASE_URL</strong>: Pozostaw bez zmian</li>
<li><strong>WORK_DIR</strong>: Ścieżka do Twojego katalogu roboczego na komputerze lokalnym. AgenticSeek będzie mógł czytać i pracować na tych plikach.</li>
<li><strong>OLLAMA_PORT</strong>: Numer portu dla usługi Ollama.</li>
<li><strong>LM_STUDIO_PORT</strong>: Numer portu dla LM Studio.</li>
<li><strong>CUSTOM_ADDITIONAL_LLM_PORT</strong>: Port dla dodatkowej, własnej usługi LLM.</li>
</ul>
<p><strong>Klucze API są całkowicie opcjonalne dla użytkowników, którzy zdecydują się uruchamiać LLM lokalnie. Jest to główny cel tego projektu. Pozostaw puste, jeśli masz wystarczająco wydajny sprzęt</strong></p>
<h3>3. <strong>Uruchom Docker</strong></h3>
<p>Upewnij się, że Docker jest zainstalowany i działa na Twoim systemie. Możesz uruchomić Dockera następującymi poleceniami:</p>
<ul>
<li><p><strong>Na Linux/macOS:</strong><br />
Otwórz terminal i wpisz:</p>
<pre><code class="language-sh">sudo systemctl start docker
</code></pre>
<p>Lub uruchom Docker Desktop z menu aplikacji, jeśli jest zainstalowany.</p>
</li>
<li><p><strong>Na Windows:</strong><br />
Uruchom Docker Desktop z menu Start.</p>
</li>
</ul>
<p>Możesz sprawdzić, czy Docker działa, wykonując:</p>
<pre><code class="language-sh">docker info
</code></pre>
<p>Jeśli zobaczysz informacje o instalacji Dockera, wszystko działa poprawnie.</p>
<p>Zobacz tabelę <a href="#list-of-local-providers">Dostawcy lokalni</a> poniżej podsumowanie.</p>
<p>Następny krok: <a href="#start-services-and-run">Uruchom AgenticSeek lokalnie</a></p>
<p><em>Zobacz sekcję <a href="#troubleshooting">Rozwiązywanie problemów</a>, jeśli masz problemy.</em>
<em>Jeśli Twój sprzęt nie pozwala na lokalne uruchomienie LLM, zobacz <a href="#setup-to-run-with-an-api">Konfiguracja z API</a>.</em>
<em>Szczegółowe wyjaśnienia pliku <code>config.ini</code> w sekcji <a href="#config">Konfiguracja</a>.</em></p>
<hr />
<h2>Konfiguracja do lokalnego uruchamiania LLM</h2>
<p><strong>Wymagania sprzętowe:</strong></p>
<p>Aby uruchamiać LLM lokalnie, potrzebujesz odpowiedniego sprzętu. Minimalnie wymagana jest karta GPU zdolna do obsługi Magistral, Qwen lub Deepseek 14B. Szczegółowe zalecenia dotyczące modeli/wydajności znajdziesz w FAQ.</p>
<p><strong>Uruchom lokalnego dostawcę</strong></p>
<p>Uruchom swojego lokalnego dostawcę, np. ollama:</p>
<pre><code class="language-sh">ollama serve
</code></pre>
<p>Poniżej znajdziesz listę obsługiwanych lokalnych dostawców.</p>
<p><strong>Zaktualizuj config.ini</strong></p>
<p>Zmień plik config.ini, ustawiając provider_name na obsługiwanego dostawcę oraz provider_model na model LLM obsługiwany przez dostawcę. Zalecamy modele rozumowania takie jak <em>Magistral</em> lub <em>Deepseek</em>.</p>
<p>Szczegóły sprzętowe znajdziesz w <strong>FAQ</strong> na końcu README.</p>
<pre><code class="language-sh">[MAIN]
is_local = True # Czy uruchamiasz lokalnie lub zdalnie.
provider_name = ollama # lub lm-studio, openai, itd.
provider_model = deepseek-r1:14b # wybierz model odpowiedni do swojego sprzętu
provider_server_address = 127.0.0.1:11434
agent_name = Jarvis # nazwa Twojej AI
recover_last_session = True # czy odzyskiwać ostatnią sesję
save_session = True # czy zapamiętywać bieżącą sesję
speak = False # zamiana tekstu na mowę
listen = False # zamiana mowy na tekst, tylko CLI, eksperymentalne
jarvis_personality = False # czy używać bardziej &quot;Jarvisowej&quot; osobowości (eksperymentalne)
languages = en zh # Lista języków, zamiana tekstu na mowę domyślnie w pierwszym języku z listy
[BROWSER]
headless_browser = True # pozostaw bez zmian, chyba że używasz CLI na hoście
stealth_mode = True # Użyj selenium nie do wykrycia, by zmniejszyć wykrywanie przeglądarki
</code></pre>
<p><strong>Uwaga</strong>:</p>
<ul>
<li><p>Plik <code>config.ini</code> nie obsługuje komentarzy.
Nie kopiuj i nie wklejaj przykładowej konfiguracji bezpośrednio, ponieważ komentarze spowodują błędy. Zamiast tego ręcznie zmodyfikuj plik <code>config.ini</code>, wpisując wybrane ustawienia bez komentarzy.</p>
</li>
<li><p><em>NIE</em> ustawiaj provider_name na <code>openai</code>, jeśli korzystasz z LM-studio do lokalnych LLM. Ustaw na <code>lm-studio</code>.</p>
</li>
<li><p>Niektórzy dostawcy (np. lm-studio) wymagają dodania <code>http://</code> przed adresem IP. Przykład: <code>http://127.0.0.1:1234</code></p>
</li>
</ul>
<p><strong>Lista lokalnych dostawców</strong></p>
<p>| Dostawca     | Lokalny? | Opis                                                                 |
|--------------|----------|----------------------------------------------------------------------|
| ollama       | Tak      | Uruchamiaj LLM lokalnie z łatwością używając ollama jako dostawcy     |
| lm-studio    | Tak      | Lokalny LLM przez LM studio (ustaw <code>provider_name</code> na <code>lm-studio</code>)    |
| openai       | Tak      | Użyj kompatybilnego API openai (np. serwer llama.cpp)                 |</p>
<p>Następny krok: <a href="#Start-services-and-Run">Uruchom usługi i AgenticSeek</a></p>
<p><em>Zobacz sekcję <a href="#troubleshooting">Rozwiązywanie problemów</a>, jeśli masz problemy.</em>
<em>Jeśli Twój sprzęt nie pozwala na lokalne uruchomienie LLM, zobacz <a href="#setup-to-run-with-an-api">Konfiguracja z API</a>.</em>
<em>Szczegółowe wyjaśnienia pliku <code>config.ini</code> w sekcji <a href="#config">Konfiguracja</a>.</em></p>
<h2>Konfiguracja do uruchamiania przez API</h2>
<p>To ustawienie wykorzystuje zewnętrznych, chmurowych dostawców LLM. Potrzebujesz klucza API wybranego serwisu.</p>
<p><strong>1. Wybierz dostawcę API i uzyskaj klucz:</strong></p>
<p>Zobacz <a href="#list-of-api-providers">Listę dostawców API</a> poniżej. Odwiedź ich strony, zarejestruj się i pobierz klucz API.</p>
<p><strong>2. Ustaw swój klucz API jako zmienną środowiskową:</strong></p>
<ul>
<li><strong>Linux/macOS:</strong>
Otwórz terminal i użyj polecenia <code>export</code>. Najlepiej dodać to do pliku profilu swojej powłoki (np. <code>~/.bashrc</code>, <code>~/.zshrc</code>), aby było trwałe.
<pre><code class="language-sh">export PROVIDER_API_KEY=&quot;tutaj_twój_klucz_api&quot;
# Zamień PROVIDER_API_KEY na konkretną zmienną, np. OPENAI_API_KEY, GOOGLE_API_KEY
</code></pre>
Przykład dla TogetherAI:
<pre><code class="language-sh">export TOGETHER_API_KEY=&quot;xxxxxxxxxxxxxxxxxxxxxx&quot;
</code></pre>
</li>
<li><strong>Windows:</strong></li>
<li><strong>Wiersz poleceń (Tymczasowo dla bieżącej sesji):</strong>
<pre><code class="language-cmd">set PROVIDER_API_KEY=twoj_klucz_api_tutaj
</code></pre>
</li>
<li><strong>PowerShell (Tymczasowo dla bieżącej sesji):</strong>
<pre><code class="language-powershell">$env:PROVIDER_API_KEY=&quot;twoj_klucz_api_tutaj&quot;
</code></pre>
</li>
<li><strong>Na stałe:</strong> Wyszukaj &quot;zmienne środowiskowe&quot; w pasku wyszukiwania Windows, kliknij &quot;Edytuj zmienne środowiskowe systemu&quot;, a następnie kliknij przycisk &quot;Zmienne środowiskowe...&quot;. Dodaj nową zmienną użytkownika z odpowiednią nazwą (np. <code>OPENAI_API_KEY</code>) i Twoim kluczem jako wartością.</li>
</ul>
<p><em>(Zobacz FAQ: <a href="#how-do-i-set-api-keys">Jak ustawić klucze API?</a> po więcej szczegółów).</em></p>
<p><strong>3. Zaktualizuj <code>config.ini</code>:</strong></p>
<pre><code class="language-ini">[MAIN]
is_local = False
provider_name = openai # Lub google, deepseek, togetherAI, huggingface
provider_model = gpt-3.5-turbo # Lub gemini-1.5-flash, deepseek-chat, mistralai/Mixtral-8x7B-Instruct-v0.1 itd.
provider_server_address = # Zazwyczaj ignorowane lub można pozostawić puste, gdy is_local = False dla większości API
# ... inne ustawienia ...
</code></pre>
<p><em>Uwaga:</em> Upewnij się, że w wartościach <code>config.ini</code> nie ma spacji na końcu linii.</p>
<p><strong>Lista dostawców API</strong></p>
<p>| Dostawca      | <code>provider_name</code> | Lokalny? | Opis                                              | Link do klucza API (przykłady)              |
|---------------|-----------------|----------|---------------------------------------------------|---------------------------------------------|
| OpenAI        | <code>openai</code>        | Nie      | Użyj modeli ChatGPT przez API OpenAI.             | <a href="https://platform.openai.com/signup">platform.openai.com/signup</a> |
| Google Gemini | <code>google</code>        | Nie      | Użyj modeli Google Gemini przez Google AI Studio. | <a href="https://aistudio.google.com/keys">aistudio.google.com/keys</a> |
| Deepseek      | <code>deepseek</code>      | Nie      | Użyj modeli Deepseek przez ich API.               | <a href="https://platform.deepseek.com">platform.deepseek.com</a> |
| Hugging Face  | <code>huggingface</code>   | Nie      | Użyj modeli z Hugging Face Inference API.         | <a href="https://huggingface.co/settings/tokens">huggingface.co/settings/tokens</a> |
| TogetherAI    | <code>togetherAI</code>    | Nie      | Użyj różnych modeli open-source przez TogetherAI. | <a href="https://api.together.ai/settings/api-keys">api.together.ai/settings/api-keys</a> |</p>
<p><em>Uwaga:</em></p>
<ul>
<li>Odradzamy używanie <code>gpt-4o</code> lub innych modeli OpenAI do złożonego przeglądania internetu i planowania zadań, ponieważ aktualne optymalizacje promptów są skierowane pod modele takie jak Deepseek.</li>
<li>Zadania związane z kodowaniem/bash mogą napotkać problemy z Gemini, gdyż może nie przestrzegać ściśle formatowania promptów zoptymalizowanych pod Deepseek.</li>
<li><code>provider_server_address</code> w pliku <code>config.ini</code> jest zazwyczaj nieużywane, gdy <code>is_local = False</code>, ponieważ adres API jest zwykle zapisany na stałe w odpowiedniej bibliotece dostawcy.</li>
</ul>
<p>Następny krok: <a href="#Start-services-and-Run">Uruchom usługi i AgenticSeek</a></p>
<p><em>Zobacz sekcję <strong>Znane problemy</strong> jeśli napotkasz problemy</em></p>
<p><em>Zobacz sekcję <strong>Config</strong> po szczegółowe wyjaśnienie pliku konfiguracyjnego.</em></p>
<hr />
<h2>Uruchom usługi i AgenticSeek</h2>
<p>Domyślnie AgenticSeek uruchamiany jest w całości w dockerze.</p>
<p>Uruchom wymagane usługi. Rozpocznie to wszystkie usługi z pliku docker-compose.yml, w tym:
- searxng
- redis (wymagany przez searxng)
- frontend
- backend (jeśli używasz <code>full</code>)</p>
<pre><code class="language-sh">./start_services.sh full # MacOS
start ./start_services.cmd full # Windows
</code></pre>
<p><strong>Uwaga:</strong> Ten krok pobierze i załaduje wszystkie obrazy Dockera, co może zająć do 30 minut. Po uruchomieniu usług poczekaj, aż usługa backend będzie w pełni uruchomiona (powinieneś zobaczyć <strong>backend: &quot;GET /health HTTP/1.1&quot; 200 OK</strong> w logu), zanim wyślesz jakiekolwiek wiadomości. Usługi backend mogą potrzebować do 5 minut na pierwsze uruchomienie.</p>
<p>Przejdź do <code>http://localhost:3000/</code> i powinieneś zobaczyć interfejs webowy.</p>
<p><em>Rozwiązywanie problemów z uruchomieniem usług:</em> Jeśli te skrypty się nie powiodą, upewnij się, że Docker Engine jest uruchomiony oraz Docker Compose (V2, <code>docker compose</code>) jest poprawnie zainstalowany. Sprawdź komunikaty o błędach w terminalu. Zobacz <a href="#faq-troubleshooting">FAQ: Pomoc! Wystąpił błąd podczas uruchamiania AgenticSeek lub jego skryptów.</a></p>
<p><strong>Opcjonalnie:</strong> Uruchom na hoście (tryb CLI):</p>
<p>Aby uruchomić w interfejsie CLI musisz zainstalować pakiet na hoście:</p>
<pre><code class="language-sh">./install.sh
./install.bat # windows
</code></pre>
<p>Uruchom usługi:</p>
<pre><code class="language-sh">./start_services.sh # MacOS
start ./start_services.cmd # Windows
</code></pre>
<p>Użyj CLI: <code>python3 cli.py</code></p>
<hr />
<h2>Użytkowanie</h2>
<p>Upewnij się, że usługi są uruchomione poprzez <code>./start_services.sh full</code> i przejdź do <code>localhost:3000</code> aby uzyskać dostęp do interfejsu webowego.</p>
<p>Możesz także użyć funkcji rozpoznawania mowy ustawiając <code>listen = True</code> w konfiguracji. Tylko w trybie CLI.</p>
<p>Aby zakończyć, po prostu powiedz/napisz <code>goodbye</code>.</p>
<p>Oto przykłady użycia:</p>
<blockquote>
<p><em>Stwórz grę w węża w pythonie!</em></p>
</blockquote>
<blockquote>
<p><em>Wyszukaj w internecie najlepsze kawiarnie w Rennes, Francja, i zapisz listę trzech z ich adresami w pliku rennes_cafes.txt.</em></p>
</blockquote>
<blockquote>
<p><em>Napisz program w Go obliczający silnię liczby, zapisz go jako factorial.go w swoim katalogu roboczym</em></p>
</blockquote>
<blockquote>
<p><em>Wyszukaj w folderze summer_pictures wszystkie pliki JPG, zmień ich nazwy na dzisiejszą datę i zapisz listę zmienionych plików w photos_list.txt</em></p>
</blockquote>
<blockquote>
<p><em>Wyszukaj online popularne filmy sci-fi z 2024 i wybierz trzy do obejrzenia dziś wieczorem. Zapisz listę w movie_night.txt.</em></p>
</blockquote>
<blockquote>
<p><em>Wyszukaj w internecie najnowsze artykuły o AI z 2025, wybierz trzy, i napisz skrypt w Pythonie, który pobierze ich tytuły i podsumowania. Zapisz skrypt jako news_scraper.py, a podsumowania w ai_news.txt w /home/projects</em></p>
</blockquote>
<blockquote>
<p><em>W piątek wyszukaj w internecie darmowe API do notowań giełdowych, zarejestruj się jako supersuper7434567@gmail.com, potem napisz skrypt w Pythonie pobierający dzienne ceny Tesli i zapisz wyniki w stock_prices.csv</em></p>
</blockquote>
<p><em>Zwróć uwagę, że możliwości wypełniania formularzy są nadal eksperymentalne i mogą nie działać poprawnie.</em></p>
<p>Po wpisaniu zapytania AgenticSeek przydzieli najlepszego agenta do zadania.</p>
<p>Ponieważ to wczesny prototyp, system routingu agentów może nie zawsze przydzielić właściwego agenta na podstawie zapytania.</p>
<p>Dlatego powinieneś być bardzo precyzyjny w tym, czego oczekujesz i jak AI ma postępować — np. jeśli chcesz, by wyszukał informacje w internecie, nie pisz:</p>
<p><code>Czy znasz jakieś dobre kraje na podróż solo?</code></p>
<p>Zamiast tego, zapytaj:</p>
<p><code>Wyszukaj w internecie i dowiedz się, które kraje są najlepsze do podróżowania solo</code></p>
<hr />
<h2><strong>Konfiguracja uruchomienia LLM na własnym serwerze</strong></h2>
<p>Jeśli masz wydajny komputer lub serwer, z którego możesz korzystać, ale chcesz używać go z laptopa, masz możliwość uruchomienia LLM na zdalnym serwerze za pomocą naszego własnego serwera llm.</p>
<p>Na swoim &quot;serwerze&quot;, który będzie uruchamiał model AI, pobierz adres IP</p>
<pre><code class="language-sh">ip a | grep &quot;inet &quot; | grep -v 127.0.0.1 | awk '{print $2}' | cut -d/ -f1 # lokalny IP
curl https://ipinfo.io/ip # publiczny IP
</code></pre>
<p>Uwaga: Dla Windows lub macOS użyj odpowiednio ipconfig lub ifconfig, aby znaleźć adres IP.</p>
<p>Sklonuj repozytorium i wejdź do folderu <code>server/</code>.</p>
<pre><code class="language-sh">git clone --depth 1 https://github.com/Fosowl/agenticSeek.git
cd agenticSeek/llm_server/
</code></pre>
<p>Zainstaluj wymagania serwera:</p>
<pre><code class="language-sh">pip3 install -r requirements.txt
</code></pre>
<p>Uruchom skrypt serwera.</p>
<pre><code class="language-sh">python3 app.py --provider ollama --port 3333
</code></pre>
<p>Masz wybór pomiędzy <code>ollama</code> i <code>llamacpp</code> jako usługą LLM.</p>
<p>Teraz na swoim komputerze osobistym:</p>
<p>Zmień plik <code>config.ini</code>, ustawiając <code>provider_name</code> na <code>server</code> oraz <code>provider_model</code> na <code>deepseek-r1:xxb</code>.
Ustaw <code>provider_server_address</code> na adres IP maszyny, która będzie uruchamiała model.</p>
<pre><code class="language-sh">[MAIN]
is_local = False
provider_name = server
provider_model = deepseek-r1:70b
provider_server_address = x.x.x.x:3333
</code></pre>
<p>Następny krok: <a href="#Start-services-and-Run">Uruchom usługi i AgenticSeek</a></p>
<hr />
<h2>Rozpoznawanie mowy (Speech to Text)</h2>
<p>Uwaga: rozpoznawanie mowy działa obecnie tylko w trybie CLI.</p>
<p>Pamiętaj, że obecnie rozpoznawanie mowy działa tylko w języku angielskim.</p>
<p>Funkcjonalność rozpoznawania mowy jest domyślnie wyłączona. Aby ją włączyć, ustaw opcję listen na True w pliku config.ini:</p>
<pre><code>listen = True
</code></pre>
<p>Po włączeniu, funkcja rozpoznawania mowy czeka na słowo kluczowe (nazwę agenta), zanim zacznie przetwarzać Twoje polecenie. Możesz dostosować nazwę agenta, aktualizując wartość <code>agent_name</code> w pliku <em>config.ini</em>:</p>
<pre><code>agent_name = Friday
</code></pre>
<p>Dla optymalnego rozpoznawania zalecamy użycie popularnego angielskiego imienia, takiego jak &quot;John&quot; lub &quot;Emma&quot; jako nazwy agenta</p>
<p>Gdy zobaczysz, że transkrypcja zaczyna się pojawiać, wypowiedz głośno imię agenta, aby go obudzić (np. &quot;Friday&quot;).</p>
<p>Wypowiedz swoje zapytanie wyraźnie.</p>
<p>Zakończ swoją prośbę frazą potwierdzającą, aby zasygnalizować systemowi, że ma przejść dalej. Przykłady fraz potwierdzających to:</p>
<pre><code>&quot;do it&quot;, &quot;go ahead&quot;, &quot;execute&quot;, &quot;run&quot;, &quot;start&quot;, &quot;thanks&quot;, &quot;would ya&quot;, &quot;please&quot;, &quot;okay?&quot;, &quot;proceed&quot;, &quot;continue&quot;, &quot;go on&quot;, &quot;do that&quot;, &quot;go it&quot;, &quot;do you understand?&quot;
</code></pre>
<h2>Konfiguracja</h2>
<p>Przykładowa konfiguracja:</p>
<pre><code>[MAIN]
is_local = True
provider_name = ollama
provider_model = deepseek-r1:32b
provider_server_address = http://127.0.0.1:11434 # Przykład dla Ollama; użyj http://127.0.0.1:1234 dla LM-Studio
agent_name = Friday
recover_last_session = False
save_session = False
speak = False
listen = False

jarvis_personality = False
languages = en zh # Lista języków dla TTS i potencjalnie routingu.
[BROWSER]
headless_browser = False
stealth_mode = False
</code></pre>
<p><strong>Wyjaśnienie ustawień <code>config.ini</code></strong>:</p>
<ul>
<li><strong>Sekcja <code>[MAIN]</code>:</strong>
<ul>
<li><code>is_local</code>: <code>True</code> jeśli korzystasz z lokalnego dostawcy LLM (Ollama, LM-Studio, lokalny serwer zgodny z OpenAI) lub opcji serwera self-hosted. <code>False</code> jeśli używasz API w chmurze (OpenAI, Google, itd.).</li>
<li><code>provider_name</code>: Określa dostawcę LLM.
<ul>
<li>Opcje lokalne: <code>ollama</code>, <code>lm-studio</code>, <code>openai</code> (dla lokalnych serwerów zgodnych z OpenAI), <code>server</code> (dla własnego serwera self-hosted).</li>
<li>Opcje API: <code>openai</code>, <code>google</code>, <code>deepseek</code>, <code>huggingface</code>, <code>togetherAI</code>.</li>
</ul>
</li>
<li><code>provider_model</code>: Konkretna nazwa lub ID modelu dla wybranego dostawcy (np. <code>deepseekcoder:6.7b</code> dla Ollama, <code>gpt-3.5-turbo</code> dla OpenAI API, <code>mistralai/Mixtral-8x7B-Instruct-v0.1</code> dla TogetherAI).</li>
<li><code>provider_server_address</code>: Adres twojego dostawcy LLM.
<ul>
<li>Dla dostawców lokalnych: np. <code>http://127.0.0.1:11434</code> dla Ollama, <code>http://127.0.0.1:1234</code> dla LM-Studio.</li>
<li>Dla typu <code>server</code>: Adres twojego własnego serwera LLM (np. <code>http://your_server_ip:3333</code>).</li>
<li>Dla API w chmurze (<code>is_local = False</code>): Zazwyczaj ignorowane lub można zostawić puste, ponieważ endpoint API jest zazwyczaj obsługiwany przez bibliotekę kliencką.</li>
</ul>
</li>
<li><code>agent_name</code>: Nazwa asystenta AI (np. Friday). Używana jako słowo wybudzające dla rozpoznawania mowy, jeśli jest włączone.</li>
<li><code>recover_last_session</code>: <code>True</code> aby przywrócić stan poprzedniej sesji, <code>False</code> aby rozpocząć od nowa.</li>
<li><code>save_session</code>: <code>True</code> aby zapisać stan bieżącej sesji do ewentualnego przywrócenia, <code>False</code> w przeciwnym wypadku.</li>
<li><code>speak</code>: <code>True</code> aby włączyć syntezę mowy (TTS), <code>False</code> aby wyłączyć.</li>
<li><code>listen</code>: <code>True</code> aby włączyć rozpoznawanie mowy (STT) w trybie CLI, <code>False</code> aby wyłączyć.</li>
<li><code>work_dir</code>: <strong>Istotne:</strong> Katalog, w którym AgenticSeek będzie czytać/zapisywać pliki. <strong>Upewnij się, że ta ścieżka jest poprawna i dostępna na twoim systemie.</strong></li>
<li><code>jarvis_personality</code>: <code>True</code> aby użyć bardziej &quot;Jarvisowego&quot; prompta systemowego (eksperymentalne), <code>False</code> dla standardowego prompta.</li>
<li><code>languages</code>: Lista języków rozdzielona przecinkami (np. <code>en, zh, fr</code>). Używana do wyboru głosu TTS (domyślnie pierwszy) i może pomagać routerowi LLM. Unikaj zbyt wielu lub bardzo podobnych języków dla efektywności routera.</li>
</ul>
</li>
<li><strong>Sekcja <code>[BROWSER]</code>:</strong>
<ul>
<li><code>headless_browser</code>: <code>True</code> aby uruchomić zautomatyzowaną przeglądarkę bez widocznego okna (zalecane do interfejsu webowego lub użytku nieinteraktywnego). <code>False</code> aby pokazać okno przeglądarki (przydatne w trybie CLI lub debugowaniu).</li>
<li><code>stealth_mode</code>: <code>True</code> aby włączyć środki utrudniające wykrycie automatyzacji przeglądarki. Może wymagać ręcznej instalacji rozszerzeń, takich jak anticaptcha.</li>
</ul>
</li>
</ul>
<p>Ta sekcja podsumowuje obsługiwane typy dostawców LLM. Skonfiguruj je w <code>config.ini</code>.</p>
<p><strong>Dostawcy lokalni (uruchamiani na twoim sprzęcie):</strong></p>
<p>| Nazwa dostawcy w <code>config.ini</code> | <code>is_local</code> | Opis                                                                       | Sekcja konfiguracji                                                 |
|-------------------------------|------------|----------------------------------------------------------------------------|---------------------------------------------------------------------|
| <code>ollama</code>                      | <code>True</code>     | Użyj Ollama do serwowania lokalnych LLM.                                   | <a href="#setup-for-running-llm-locally-on-your-machine">Konfiguracja lokalnego LLM</a> |
| <code>lm-studio</code>                   | <code>True</code>     | Użyj LM-Studio do serwowania lokalnych LLM.                                | <a href="#setup-for-running-llm-locally-on-your-machine">Konfiguracja lokalnego LLM</a> |
| <code>openai</code> (dla lokalnego serwera) | <code>True</code> | Połącz z lokalnym serwerem udostępniającym API zgodne z OpenAI (np. llama.cpp). | <a href="#setup-for-running-llm-locally-on-your-machine">Konfiguracja lokalnego LLM</a> |
| <code>server</code>                      | <code>False</code>    | Połącz z własnym serwerem LLM AgenticSeek uruchomionym na innej maszynie.  | <a href="#setup-to-run-the-llm-on-your-own-server">Konfiguracja własnego serwera LLM</a> |</p>
<p><strong>Dostawcy API (chmurowi):</strong></p>
<p>| Nazwa dostawcy w <code>config.ini</code> | <code>is_local</code> | Opis                                            | Sekcja konfiguracji                                    |
|-------------------------------|------------|--------------------------------------------------|--------------------------------------------------------|
| <code>openai</code>                      | <code>False</code>    | Użyj oficjalnego API OpenAI (np. GPT-3.5, GPT-4).| <a href="#setup-to-run-with-an-api">Konfiguracja z API</a>         |
| <code>google</code>                      | <code>False</code>    | Użyj modeli Gemini Google przez API.             | <a href="#setup-to-run-with-an-api">Konfiguracja z API</a>         |
| <code>deepseek</code>                    | <code>False</code>    | Użyj oficjalnego API Deepseek.                   | <a href="#setup-to-run-with-an-api">Konfiguracja z API</a>         |
| <code>huggingface</code>                 | <code>False</code>    | Użyj Hugging Face Inference API.                 | <a href="#setup-to-run-with-an-api">Konfiguracja z API</a>         |
| <code>togetherAI</code>                  | <code>False</code>    | Użyj API TogetherAI dla różnych modeli open.     | <a href="#setup-to-run-with-an-api">Konfiguracja z API</a>         |</p>
<hr />
<h2>Rozwiązywanie problemów</h2>
<p>Jeśli napotkasz problemy, ta sekcja zawiera wskazówki.</p>
<h1>Znane problemy</h1>
<h2>Problemy z ChromeDriver</h2>
<p><strong>Przykład błędu:</strong> <code>SessionNotCreatedException: Message: session not created: This version of ChromeDriver only supports Chrome version XXX</code></p>
<ul>
<li><strong>Przyczyna:</strong> Zainstalowana wersja ChromeDriver jest niezgodna z wersją przeglądarki Google Chrome.</li>
<li><strong>Rozwiązanie:</strong>
<ol>
<li><strong>Sprawdź wersję Chrome:</strong> Otwórz Google Chrome, przejdź do <code>Ustawienia &gt; O Google Chrome</code>, aby znaleźć wersję (np. &quot;Wersja 120.0.6099.110&quot;).</li>
<li><strong>Pobierz pasującą wersję ChromeDriver:</strong>
<ul>
<li>Dla Chrome w wersji 115 i nowszych: Przejdź do <a href="https://googlechromelabs.github.io/chrome-for-testing/">Chrome for Testing (CfT) JSON Endpoints</a>. Znajdź kanał &quot;stable&quot; i pobierz ChromeDriver dla swojego systemu operacyjnego, zgodny z główną wersją przeglądarki Chrome.</li>
<li>Dla starszych wersji (rzadziej spotykane): Możesz je znaleźć na stronie <a href="https://chromedriver.chromium.org/downloads">ChromeDriver - WebDriver for Chrome</a>.</li>
<li>Poniższy obrazek przedstawia przykład ze strony CfT:
<img src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/chromedriver_readme.png" alt="Download Chromedriver specific version from Chrome for Testing page" /></li>
</ul>
</li>
<li><strong>Zainstaluj ChromeDriver:</strong>
<ul>
<li>Upewnij się, że pobrany plik <code>chromedriver</code> (lub <code>chromedriver.exe</code> w Windows) znajduje się w katalogu, który jest na liście PATH w systemie (np. <code>/usr/local/bin</code> w Linux/macOS lub własny folder skryptów dodany do PATH w Windows).</li>
<li>Alternatywnie umieść go w katalogu głównym projektu <code>agenticSeek</code>.</li>
<li>Upewnij się, że sterownik ma prawa wykonywania (np. <code>chmod +x chromedriver</code> w Linux/macOS).</li>
</ul>
</li>
<li>Zajrzyj do sekcji <a href="#chromedriver-installation">ChromeDriver Installation</a> w głównym przewodniku instalacji po więcej szczegółów.</li>
</ol>
</li>
</ul>
<p>Jeśli ta sekcja jest niepełna lub napotkasz inne problemy z ChromeDriver, rozważ sprawdzenie istniejących <a href="https://github.com/Fosowl/agenticSeek/issues">GitHub Issues</a> lub zgłoszenie nowego problemu.</p>
<p><code>Exception: Failed to initialize browser: Message: session not created: This version of ChromeDriver only supports Chrome version 113 Current browser version is 134.0.6998.89 with binary path</code></p>
<p>To dzieje się, jeśli wersje przeglądarki i chromedrivera się nie zgadzają.</p>
<p>Musisz przejść do pobrania najnowszej wersji:</p>
<p>https://developer.chrome.com/docs/chromedriver/downloads</p>
<p>Jeśli używasz Chrome w wersji 115 lub nowszej przejdź do:</p>
<p>https://googlechromelabs.github.io/chrome-for-testing/</p>
<p>I pobierz wersję chromedrivera odpowiednią dla twojego systemu operacyjnego.</p>
<p><img src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/chromedriver_readme.png" alt="alt text" /></p>
<p>Jeśli ta sekcja jest niepełna, zgłoś problem.</p>
<h2>Problemy z connection adapters</h2>
<pre><code>Exception: Provider lm-studio failed: HTTP request failed: No connection adapters were found for '127.0.0.1:1234/v1/chat/completions'` (Uwaga: port może być inny)
</code></pre>
<ul>
<li><strong>Przyczyna:</strong> <code>provider_server_address</code> w <code>config.ini</code> dla <code>lm-studio</code> (lub innych podobnych lokalnych serwerów zgodnych z OpenAI) nie zawiera prefiksu <code>http://</code> lub wskazuje zły port.</li>
<li><strong>Rozwiązanie:</strong>
<ul>
<li>Upewnij się, że adres zawiera <code>http://</code>. LM-Studio domyślnie to <code>http://127.0.0.1:1234</code>.</li>
<li>Popraw <code>config.ini</code>: <code>provider_server_address = http://127.0.0.1:1234</code> (lub twój faktyczny port serwera LM-Studio).</li>
</ul>
</li>
</ul>
<h2>Nie podano podstawowego adresu URL SearxNG</h2>
<pre><code>raise ValueError(&quot;SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.&quot;)
ValueError: SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.`
</code></pre>
<h2>FAQ</h2>
<p><strong>Q: Jakiego sprzętu potrzebuję?</strong></p>
<p>| Rozmiar modelu | GPU  | Komentarz                                                   |
|----------------|------|------------------------------------------------------------|
| 7B             | 8GB Vram | ⚠️ Niezalecane. Słaba wydajność, częste halucynacje, agenci planujący prawdopodobnie zawiodą. |
| 14B            | 12 GB VRAM (np. RTX 3060) | ✅ Używalny do prostych zadań. Może mieć trudności z przeglądaniem sieci i zadaniami planistycznymi. |
| 32B            | 24+ GB VRAM (np. RTX 4090) | 🚀 Sukces w większości zadań, nadal może mieć trudności z planowaniem zadań |
| 70B+           | 48+ GB Vram | 💪 Doskonały. Zalecany do zaawansowanych zastosowań.      |</p>
<p><strong>Q: Otrzymuję błąd, co mam zrobić?</strong></p>
<p>Upewnij się, że lokalny serwer działa (<code>ollama serve</code>), twoje <code>config.ini</code> pasuje do wybranego dostawcy, a zależności są zainstalowane. Jeśli żadne nie działa, śmiało zgłoś problem.</p>
<p><strong>Q: Czy to naprawdę może działać w 100% lokalnie?</strong></p>
<p>Tak, z dostawcami Ollama, lm-studio lub server wszystkie modele STT, LLM i TTS działają lokalnie. Opcje nielokalne (OpenAI lub inne API) są opcjonalne.</p>
<p><strong>Q: Dlaczego mam używać AgenticSeek, skoro mam Manus?</strong></p>
<p>W przeciwieństwie do Manus, AgenticSeek stawia na niezależność od zewnętrznych systemów, dając ci większą kontrolę, prywatność i brak kosztów API.</p>
<p><strong>Q: Kto stoi za projektem?</strong></p>
<p>Projekt został stworzony przeze mnie, wraz z dwoma przyjaciółmi, którzy są maintainerami i kontrybutorami z open-source na GitHubie. Jesteśmy po prostu grupą pasjonatów, a nie startupem ani organizacją.</p>
<p>Każde konto AgenticSeek na X poza moim osobistym kontem (https://x.com/Martin993886460) jest podszywaniem się.</p>
<h2>Współtwórz</h2>
<p>Szukamy developerów do rozwoju AgenticSeek! Zajrzyj do otwartych zgłoszeń lub dyskusji.</p>
<p><a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/docs/CONTRIBUTING.md">Przewodnik kontrybucji</a></p>
<p><a href="https://www.star-history.com/#Fosowl/agenticSeek&amp;Date"><img src="https://api.star-history.com/svg?repos=Fosowl/agenticSeek&amp;type=Date" alt="Star History Chart" /></a></p>
<h2>Maintainerzy:</h2>
<blockquote>
<p><a href="https://github.com/Fosowl">Fosowl</a> | czas paryski</p>
</blockquote>
<blockquote>
<p><a href="https://github.com/antoineVIVIES">antoineVIVIES</a> | czas tajpejski</p>
</blockquote>
<blockquote>
<p><a href="https://github.com/steveh8758">steveh8758</a> | czas tajpejski</p>
</blockquote>
<h2>Specjalne podziękowania:</h2>
<blockquote>
<p><a href="https://github.com/tcsenpai">tcsenpai</a> oraz <a href="https://github.com/plitc">plitc</a> za pomoc w dockerowaniu backendu</p>
</blockquote>
<h2>Sponsorzy:</h2>
<p>Sponsorzy z miesięczną wpłatą 5$ lub więcej pojawią się tutaj:</p>
<ul>
<li><strong>tatra-labs</strong>
It seems you haven't provided the text of the technical document that needs to be translated. Please provide the content you'd like translated (Part 4 of 4), and I'll proceed with the translation while preserving the original Markdown format and updating the relative paths as requested.</li>
</ul>
<hr />
<p>Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx">Open Ai Tx</a> | Last indexed: 2025-06-16</p>
<hr />

        </div>
    </div>
    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Default Statcounter code for openaitx
https://openaitx.github.io/ -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>
    <!-- End of Statcounter Code -->
</body>
</html>