<!DOCTYPE html><html lang="en"><head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AgenticSeek: Prywatna, lokalna alternatywa dla Manus - Fosowl/agenticSeek</title>

    <!-- Primary Meta Tags -->
    <meta name="title" content="AgenticSeek: Prywatna, lokalna alternatywa dla Manus - Fosowl/agenticSeek">
    <meta name="description" content="Fosowl/agenticSeek - GitHub repository pl documentation and information">
    <meta name="keywords" content="Fosowl, agenticSeek, GitHub, repository, pl documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">

    <!-- Open Graph -->
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/Fosowl/agenticSeek/README-pl.html">
    <meta property="og:title" content="AgenticSeek: Prywatna, lokalna alternatywa dla Manus - Fosowl/agenticSeek">
    <meta property="og:description" content="Fosowl/agenticSeek - GitHub repository pl documentation and information">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">

    <!-- Favicon -->
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">

    <!-- Marked.js for Markdown rendering -->
    <script type="text/javascript" async="" src="https://www.statcounter.com/counter/recorder.js"></script><script src="/js/marked.min.js?v=20250613"></script>
    <!-- Highlight.js for code syntax highlighting -->
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <!-- Custom CSS -->
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        /* Layout */
        body {
            display: flex;
            flex-direction: column;
            min-height: 100vh;
        }

        .main-container {
            margin: 0 auto;
            width: 100%;
            max-width: 980px;
            padding: 0 20px;
        }

        @media (max-width: 768px) {
            .main-container {
                padding: 0 15px;
            }
        }

        /* Image size restrictions */
        .markdown-body img {
            max-width: 100%;
            height: auto;
        }

        /* Existing styles */
        .header {
            text-align: center;
            margin-bottom: 30px;
            padding: 20px;
            background-color: #f6f8fa;
            border-bottom: 1px solid #e1e4e8;
            position: relative;
        }

        .back-button {
            position: absolute;
            left: 20px;
            top: 50%;
            transform: translateY(-50%);
            color: #0366d6;
            text-decoration: none;
            display: flex;
            align-items: center;
            font-size: 14px;
            padding: 5px 10px;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            background-color: #fff;
        }

        .back-button:hover {
            background-color: #f6f8fa;
            border-color: #0366d6;
        }

        .back-button::before {
            content: "←";
            margin-right: 5px;
            font-size: 16px;
        }

        .header .links {
            margin-top: 10px;
            font-size: 16px;
        }

        .header .links a {
            color: #0366d6;
            text-decoration: none;
            margin-left: 5px;
        }

        .header .links a:hover {
            text-decoration: underline;
        }
        
        /* Language badges styles */
        .language-badges {
            margin-top: 15px;
            text-align: center;
        }
        .language-badges a {
            display: inline-block;
            margin: 2px;
            text-decoration: none;
        }
        .language-badges img {
            height: 20px;
            border-radius: 3px;
        }
        .language-badges a:hover img {
            opacity: 0.8;
        }
    </style>
</head>

<body>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/Fosowl/agenticSeek" id="githubRepoLink" target="_blank">Fosowl/agenticSeek</a>
        </div>
        <div class="language-badges" id="languageBadges"><a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=en"><img src="https://img.shields.io/badge/EN-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=zh-CN"><img src="https://img.shields.io/badge/简中-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=zh-TW"><img src="https://img.shields.io/badge/繁中-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=ja"><img src="https://img.shields.io/badge/日本語-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=ko"><img src="https://img.shields.io/badge/한국어-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=th"><img src="https://img.shields.io/badge/ไทย-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=fr"><img src="https://img.shields.io/badge/Français-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=de"><img src="https://img.shields.io/badge/Deutsch-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=es"><img src="https://img.shields.io/badge/Español-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=it"><img src="https://img.shields.io/badge/Italiano-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=ru"><img src="https://img.shields.io/badge/Русский-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=pt"><img src="https://img.shields.io/badge/Português-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=nl"><img src="https://img.shields.io/badge/Nederlands-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=pl"><img src="https://img.shields.io/badge/Polski-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=ar"><img src="https://img.shields.io/badge/العربية-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=tr"><img src="https://img.shields.io/badge/Türkçe-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=vi"><img src="https://img.shields.io/badge/Tiếng Việt-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=hi"><img src="https://img.shields.io/badge/हिंदी-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=fa"><img src="https://img.shields.io/badge/فارسی-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=id"><img src="https://img.shields.io/badge/Bahasa Indonesia-white" alt="version"></a></div>
    </div>

    <div class="main-container">
        <div class="markdown-body" id="content"><h1>AgenticSeek: Prywatna, lokalna alternatywa dla Manus</h1>
<p align="center">
<img align="center" src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/agentic_seek_logo.png" width="300" height="300" alt="Agentic Seek Logo">
</p><p>

</p><p>  English | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_CHS.md">中文</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_CHT.md">繁體中文</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_FR.md">Français</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_JP.md">日本語</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_PTBR.md">Português (Brasil)</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_ES.md">Español</a></p>
<p><em>W pełni <strong>lokalna alternatywa dla Manus AI</strong> – ten asystent AI z obsługą głosu autonomicznie przegląda internet, pisze kod i planuje zadania, zachowując wszystkie dane na Twoim urządzeniu. Dostosowany do lokalnych modeli rozumowania, działa całkowicie na Twoim sprzęcie, zapewniając pełną prywatność i brak zależności od chmury.</em></p>
<p><a href="https://fosowl.github.io/agenticSeek.html"><img src="https://img.shields.io/static/v1?label=Website&amp;message=AgenticSeek&amp;color=blue&amp;style=flat-square" alt="Odwiedź AgenticSeek"></a> <img src="https://img.shields.io/badge/license-GPL--3.0-green" alt="License"> <a href="https://discord.gg/8hGDaME3TC"><img src="https://img.shields.io/badge/Discord-Join%20Us-7289DA?logo=discord&amp;logoColor=white" alt="Discord"></a> <a href="https://x.com/Martin993886460"><img src="https://img.shields.io/twitter/url/https/twitter.com/fosowl.svg?style=social&amp;label=Update%20%40Fosowl" alt="Twitter"></a> <a href="https://github.com/Fosowl/agenticSeek/stargazers"><img src="https://img.shields.io/github/stars/Fosowl/agenticSeek?style=social" alt="GitHub stars"></a></p>
<h3>Dlaczego AgenticSeek?</h3>
<ul>
<li><p>🔒 W pełni lokalny i prywatny – Wszystko działa na Twoim komputerze — bez chmury, bez udostępniania danych. Twoje pliki, rozmowy i wyszukiwania pozostają prywatne.</p>
</li>
<li><p>🌐 Inteligentne przeglądanie sieci – AgenticSeek może samodzielnie przeglądać internet — wyszukiwać, czytać, wyodrębniać informacje, wypełniać formularze — wszystko bez użycia rąk.</p>
</li>
<li><p>💻 Autonomiczny asystent kodowania – Potrzebujesz kodu? Może pisać, debugować i uruchamiać programy w Pythonie, C, Go, Javie i innych — wszystko bez nadzoru.</p>
</li>
<li><p>🧠 Inteligentny wybór agenta – Zadajesz pytanie, a on sam wybiera najlepszego agenta do zadania. Jak zespół ekspertów gotowych do pomocy.</p>
</li>
<li><p>📋 Planuje i realizuje złożone zadania – Od planowania podróży po skomplikowane projekty — potrafi rozbić duże zadania na kroki i wykonać je, korzystając z wielu agentów AI.</p>
</li>
<li><p>🎙️ Obsługa głosu – Czysty, szybki, futurystyczny głos oraz zamiana mowy na tekst pozwala rozmawiać z nim jak z własnym AI z filmu science fiction. (W trakcie opracowania)</p>
</li>
</ul>
<h3><strong>Demo</strong></h3>
<blockquote>
<p><em>Czy możesz wyszukać projekt agenticSeek, dowiedzieć się, jakie są wymagane umiejętności, następnie otworzyć CV_candidates.zip i powiedzieć mi, które najlepiej pasują do projektu</em></p>
</blockquote>
<p><a href="https://github.com/user-attachments/assets/b8ca60e9-7b3b-4533-840e-08f9ac426316">https://github.com/user-attachments/assets/b8ca60e9-7b3b-4533-840e-08f9ac426316</a></p>
<p>Zastrzeżenie: To demo, w tym wszystkie pojawiające się pliki (np.: CV_candidates.zip), są całkowicie fikcyjne. Nie jesteśmy korporacją, szukamy współtwórców open-source, a nie kandydatów.</p>
<blockquote>
<p>🛠⚠️️ <strong>Aktywnie rozwijane</strong></p>
</blockquote>
<blockquote>
<p>🙏 Ten projekt powstał jako poboczny i nie posiada żadnej mapy drogowej ani finansowania. Rozrósł się znacznie bardziej, niż się spodziewałem, kończąc na GitHub Trending. Wszelkie wkłady, opinie i cierpliwość są bardzo cenione.</p>
</blockquote>
<h2>Wymagania wstępne</h2>
<p>Przed rozpoczęciem upewnij się, że masz zainstalowane następujące oprogramowanie:</p>
<ul>
<li><strong>Git:</strong> Do klonowania repozytorium. <a href="https://git-scm.com/downloads">Pobierz Git</a></li>
<li><strong>Python 3.10.x:</strong> Zdecydowanie zalecamy użycie wersji Python 3.10.x. Używanie innych wersji może prowadzić do błędów zależności. <a href="https://www.python.org/downloads/release/python-3100/">Pobierz Python 3.10</a> (wybierz wersję 3.10.x).</li>
<li><strong>Docker Engine i Docker Compose:</strong> Do uruchamiania usług takich jak SearxNG.<ul>
<li>Zainstaluj Docker Desktop (zawiera Docker Compose V2): <a href="https://docs.docker.com/desktop/install/windows-install/">Windows</a> | <a href="https://docs.docker.com/desktop/install/mac-install/">Mac</a> | <a href="https://docs.docker.com/desktop/install/linux-install/">Linux</a></li>
<li>Alternatywnie, zainstaluj Docker Engine i Docker Compose oddzielnie na Linuksie: <a href="https://docs.docker.com/engine/install/">Docker Engine</a> | <a href="https://docs.docker.com/compose/install/">Docker Compose</a> (upewnij się, że instalujesz Compose V2, np. <code>sudo apt-get install docker-compose-plugin</code>).</li>
</ul>
</li>
</ul>
<h3>1. <strong>Sklonuj repozytorium i skonfiguruj</strong></h3>
<pre><code class="language-sh hljs language-bash">git <span class="hljs-built_in">clone</span> https://github.com/Fosowl/agenticSeek.git
<span class="hljs-built_in">cd</span> agenticSeek
<span class="hljs-built_in">mv</span> .env.example .<span class="hljs-built_in">env</span>
</code></pre>
<h3>2. Zmień zawartość pliku .env</h3>
<pre><code class="language-sh hljs language-bash">SEARXNG_BASE_URL=<span class="hljs-string">"http://127.0.0.1:8080"</span>
REDIS_BASE_URL=<span class="hljs-string">"redis://redis:6379/0"</span>
WORK_DIR=<span class="hljs-string">"/Users/mlg/Documents/workspace_for_ai"</span>
OLLAMA_PORT=<span class="hljs-string">"11434"</span>
LM_STUDIO_PORT=<span class="hljs-string">"1234"</span>
CUSTOM_ADDITIONAL_LLM_PORT=<span class="hljs-string">"11435"</span>
OPENAI_API_KEY=<span class="hljs-string">'optional'</span>
DEEPSEEK_API_KEY=<span class="hljs-string">'optional'</span>
OPENROUTER_API_KEY=<span class="hljs-string">'optional'</span>
TOGETHER_API_KEY=<span class="hljs-string">'optional'</span>
GOOGLE_API_KEY=<span class="hljs-string">'optional'</span>
ANTHROPIC_API_KEY=<span class="hljs-string">'optional'</span>
</code></pre>
<p>Zaktualizuj plik <code>.env</code> własnymi wartościami, jeśli to konieczne:</p>
<ul>
<li><strong>SEARXNG_BASE_URL</strong>: Pozostaw bez zmian</li>
<li><strong>REDIS_BASE_URL</strong>: Pozostaw bez zmian</li>
<li><strong>WORK_DIR</strong>: Ścieżka do Twojego katalogu roboczego na komputerze lokalnym. AgenticSeek będzie mógł czytać i pracować na tych plikach.</li>
<li><strong>OLLAMA_PORT</strong>: Numer portu dla usługi Ollama.</li>
<li><strong>LM_STUDIO_PORT</strong>: Numer portu dla LM Studio.</li>
<li><strong>CUSTOM_ADDITIONAL_LLM_PORT</strong>: Port dla dodatkowej, własnej usługi LLM.</li>
</ul>
<p><strong>Klucze API są całkowicie opcjonalne dla użytkowników, którzy zdecydują się uruchamiać LLM lokalnie. Jest to główny cel tego projektu. Pozostaw puste, jeśli masz wystarczająco wydajny sprzęt</strong></p>
<h3>3. <strong>Uruchom Docker</strong></h3>
<p>Upewnij się, że Docker jest zainstalowany i działa na Twoim systemie. Możesz uruchomić Dockera następującymi poleceniami:</p>
<ul>
<li><p><strong>Na Linux/macOS:</strong><br>  Otwórz terminal i wpisz:</p>
<pre><code class="language-sh hljs language-bash">sudo systemctl start docker
</code></pre>
<p>  Lub uruchom Docker Desktop z menu aplikacji, jeśli jest zainstalowany.</p>
</li>
<li><p><strong>Na Windows:</strong><br>  Uruchom Docker Desktop z menu Start.</p>
</li>
</ul>
<p>Możesz sprawdzić, czy Docker działa, wykonując:</p>
<pre><code class="language-sh hljs language-bash">docker info
</code></pre>
<p>Jeśli zobaczysz informacje o instalacji Dockera, wszystko działa poprawnie.</p>
<p>Zobacz tabelę <a href="#list-of-local-providers">Dostawcy lokalni</a> poniżej podsumowanie.</p>
<p>Następny krok: <a href="#start-services-and-run">Uruchom AgenticSeek lokalnie</a></p>
<p><em>Zobacz sekcję <a href="#troubleshooting">Rozwiązywanie problemów</a>, jeśli masz problemy.</em><br><em>Jeśli Twój sprzęt nie pozwala na lokalne uruchomienie LLM, zobacz <a href="#setup-to-run-with-an-api">Konfiguracja z API</a>.</em><br><em>Szczegółowe wyjaśnienia pliku <code>config.ini</code> w sekcji <a href="#config">Konfiguracja</a>.</em></p>
<hr>
<h2>Konfiguracja do lokalnego uruchamiania LLM</h2>
<p><strong>Wymagania sprzętowe:</strong></p>
<p>Aby uruchamiać LLM lokalnie, potrzebujesz odpowiedniego sprzętu. Minimalnie wymagana jest karta GPU zdolna do obsługi Magistral, Qwen lub Deepseek 14B. Szczegółowe zalecenia dotyczące modeli/wydajności znajdziesz w FAQ.</p>
<p><strong>Uruchom lokalnego dostawcę</strong>  </p>
<p>Uruchom swojego lokalnego dostawcę, np. ollama:</p>
<pre><code class="language-sh hljs language-bash">ollama serve
</code></pre>
<p>Poniżej znajdziesz listę obsługiwanych lokalnych dostawców.</p>
<p><strong>Zaktualizuj config.ini</strong></p>
<p>Zmień plik config.ini, ustawiając provider_name na obsługiwanego dostawcę oraz provider_model na model LLM obsługiwany przez dostawcę. Zalecamy modele rozumowania takie jak <em>Magistral</em> lub <em>Deepseek</em>.</p>
<p>Szczegóły sprzętowe znajdziesz w <strong>FAQ</strong> na końcu README.</p>
<pre><code class="language-sh hljs language-bash">[MAIN]
is_local = True <span class="hljs-comment"># Czy uruchamiasz lokalnie lub zdalnie.</span>
provider_name = ollama <span class="hljs-comment"># lub lm-studio, openai, itd.</span>
provider_model = deepseek-r1:14b <span class="hljs-comment"># wybierz model odpowiedni do swojego sprzętu</span>
provider_server_address = 127.0.0.1:11434
agent_name = Jarvis <span class="hljs-comment"># nazwa Twojej AI</span>
recover_last_session = True <span class="hljs-comment"># czy odzyskiwać ostatnią sesję</span>
save_session = True <span class="hljs-comment"># czy zapamiętywać bieżącą sesję</span>
speak = False <span class="hljs-comment"># zamiana tekstu na mowę</span>
listen = False <span class="hljs-comment"># zamiana mowy na tekst, tylko CLI, eksperymentalne</span>
jarvis_personality = False <span class="hljs-comment"># czy używać bardziej "Jarvisowej" osobowości (eksperymentalne)</span>
languages = en zh <span class="hljs-comment"># Lista języków, zamiana tekstu na mowę domyślnie w pierwszym języku z listy</span>
[BROWSER]
headless_browser = True <span class="hljs-comment"># pozostaw bez zmian, chyba że używasz CLI na hoście</span>
stealth_mode = True <span class="hljs-comment"># Użyj selenium nie do wykrycia, by zmniejszyć wykrywanie przeglądarki</span>
</code></pre>
<p><strong>Uwaga</strong>:</p>
<ul>
<li><p>Plik <code>config.ini</code> nie obsługuje komentarzy.<br>Nie kopiuj i nie wklejaj przykładowej konfiguracji bezpośrednio, ponieważ komentarze spowodują błędy. Zamiast tego ręcznie zmodyfikuj plik <code>config.ini</code>, wpisując wybrane ustawienia bez komentarzy.</p>
</li>
<li><p><em>NIE</em> ustawiaj provider_name na <code>openai</code>, jeśli korzystasz z LM-studio do lokalnych LLM. Ustaw na <code>lm-studio</code>.</p>
</li>
<li><p>Niektórzy dostawcy (np. lm-studio) wymagają dodania <code>http://</code> przed adresem IP. Przykład: <code>http://127.0.0.1:1234</code></p>
</li>
</ul>
<p><strong>Lista lokalnych dostawców</strong></p>
<table>
<thead>
<tr>
<th>Dostawca</th>
<th>Lokalny?</th>
<th>Opis</th>
</tr>
</thead>
<tbody><tr>
<td>ollama</td>
<td>Tak</td>
<td>Uruchamiaj LLM lokalnie z łatwością używając ollama jako dostawcy</td>
</tr>
<tr>
<td>lm-studio</td>
<td>Tak</td>
<td>Lokalny LLM przez LM studio (ustaw <code>provider_name</code> na <code>lm-studio</code>)</td>
</tr>
<tr>
<td>openai</td>
<td>Tak</td>
<td>Użyj kompatybilnego API openai (np. serwer llama.cpp)</td>
</tr>
</tbody></table>
<p>Następny krok: <a href="#Start-services-and-Run">Uruchom usługi i AgenticSeek</a></p>
<p><em>Zobacz sekcję <a href="#troubleshooting">Rozwiązywanie problemów</a>, jeśli masz problemy.</em><br><em>Jeśli Twój sprzęt nie pozwala na lokalne uruchomienie LLM, zobacz <a href="#setup-to-run-with-an-api">Konfiguracja z API</a>.</em><br><em>Szczegółowe wyjaśnienia pliku <code>config.ini</code> w sekcji <a href="#config">Konfiguracja</a>.</em></p>
<h2>Konfiguracja do uruchamiania przez API</h2>
<p>To ustawienie wykorzystuje zewnętrznych, chmurowych dostawców LLM. Potrzebujesz klucza API wybranego serwisu.</p>
<p><strong>1. Wybierz dostawcę API i uzyskaj klucz:</strong></p>
<p>Zobacz <a href="#list-of-api-providers">Listę dostawców API</a> poniżej. Odwiedź ich strony, zarejestruj się i pobierz klucz API.</p>
<p><strong>2. Ustaw swój klucz API jako zmienną środowiskową:</strong></p>
<ul>
<li><strong>Linux/macOS:</strong><br>Otwórz terminal i użyj polecenia <code>export</code>. Najlepiej dodać to do pliku profilu swojej powłoki (np. <code>~/.bashrc</code>, <code>~/.zshrc</code>), aby było trwałe.<pre><code class="language-sh hljs language-bash"><span class="hljs-built_in">export</span> PROVIDER_API_KEY=<span class="hljs-string">"tutaj_twój_klucz_api"</span>
<span class="hljs-comment"># Zamień PROVIDER_API_KEY na konkretną zmienną, np. OPENAI_API_KEY, GOOGLE_API_KEY</span>
</code></pre>
Przykład dla TogetherAI:<pre><code class="language-sh hljs language-bash"><span class="hljs-built_in">export</span> TOGETHER_API_KEY=<span class="hljs-string">"xxxxxxxxxxxxxxxxxxxxxx"</span>
</code></pre>
</li>
<li><strong>Windows:</strong></li>
<li><strong>Wiersz poleceń (Tymczasowo dla bieżącej sesji):</strong><pre><code class="language-cmd">set PROVIDER_API_KEY=twoj_klucz_api_tutaj
</code></pre>
</li>
<li><strong>PowerShell (Tymczasowo dla bieżącej sesji):</strong><pre><code class="language-powershell">$env:PROVIDER_API_KEY="twoj_klucz_api_tutaj"
</code></pre>
</li>
<li><strong>Na stałe:</strong> Wyszukaj "zmienne środowiskowe" w pasku wyszukiwania Windows, kliknij "Edytuj zmienne środowiskowe systemu", a następnie kliknij przycisk "Zmienne środowiskowe...". Dodaj nową zmienną użytkownika z odpowiednią nazwą (np. <code>OPENAI_API_KEY</code>) i Twoim kluczem jako wartością.</li>
</ul>
<p><em>(Zobacz FAQ: <a href="#how-do-i-set-api-keys">Jak ustawić klucze API?</a> po więcej szczegółów).</em></p>
<p><strong>3. Zaktualizuj <code>config.ini</code>:</strong></p>
<pre><code class="language-ini hljs"><span class="hljs-section">[MAIN]</span>
<span class="hljs-attr">is_local</span> = <span class="hljs-literal">False</span>
<span class="hljs-attr">provider_name</span> = openai <span class="hljs-comment"># Lub google, deepseek, togetherAI, huggingface</span>
<span class="hljs-attr">provider_model</span> = gpt-<span class="hljs-number">3.5</span>-turbo <span class="hljs-comment"># Lub gemini-1.5-flash, deepseek-chat, mistralai/Mixtral-8x7B-Instruct-v0.1 itd.</span>
provider_server_address = <span class="hljs-comment"># Zazwyczaj ignorowane lub można pozostawić puste, gdy is_local = False dla większości API</span>
<span class="hljs-comment"># ... inne ustawienia ...</span>
</code></pre>
<p><em>Uwaga:</em> Upewnij się, że w wartościach <code>config.ini</code> nie ma spacji na końcu linii.</p>
<p><strong>Lista dostawców API</strong></p>
<table>
<thead>
<tr>
<th>Dostawca</th>
<th><code>provider_name</code></th>
<th>Lokalny?</th>
<th>Opis</th>
<th>Link do klucza API (przykłady)</th>
</tr>
</thead>
<tbody><tr>
<td>OpenAI</td>
<td><code>openai</code></td>
<td>Nie</td>
<td>Użyj modeli ChatGPT przez API OpenAI.</td>
<td><a href="https://platform.openai.com/signup">platform.openai.com/signup</a></td>
</tr>
<tr>
<td>Google Gemini</td>
<td><code>google</code></td>
<td>Nie</td>
<td>Użyj modeli Google Gemini przez Google AI Studio.</td>
<td><a href="https://aistudio.google.com/keys">aistudio.google.com/keys</a></td>
</tr>
<tr>
<td>Deepseek</td>
<td><code>deepseek</code></td>
<td>Nie</td>
<td>Użyj modeli Deepseek przez ich API.</td>
<td><a href="https://platform.deepseek.com">platform.deepseek.com</a></td>
</tr>
<tr>
<td>Hugging Face</td>
<td><code>huggingface</code></td>
<td>Nie</td>
<td>Użyj modeli z Hugging Face Inference API.</td>
<td><a href="https://huggingface.co/settings/tokens">huggingface.co/settings/tokens</a></td>
</tr>
<tr>
<td>TogetherAI</td>
<td><code>togetherAI</code></td>
<td>Nie</td>
<td>Użyj różnych modeli open-source przez TogetherAI.</td>
<td><a href="https://api.together.ai/settings/api-keys">api.together.ai/settings/api-keys</a></td>
</tr>
</tbody></table>
<p><em>Uwaga:</em></p>
<ul>
<li>Odradzamy używanie <code>gpt-4o</code> lub innych modeli OpenAI do złożonego przeglądania internetu i planowania zadań, ponieważ aktualne optymalizacje promptów są skierowane pod modele takie jak Deepseek.</li>
<li>Zadania związane z kodowaniem/bash mogą napotkać problemy z Gemini, gdyż może nie przestrzegać ściśle formatowania promptów zoptymalizowanych pod Deepseek.</li>
<li><code>provider_server_address</code> w pliku <code>config.ini</code> jest zazwyczaj nieużywane, gdy <code>is_local = False</code>, ponieważ adres API jest zwykle zapisany na stałe w odpowiedniej bibliotece dostawcy.</li>
</ul>
<p>Następny krok: <a href="#Start-services-and-Run">Uruchom usługi i AgenticSeek</a></p>
<p><em>Zobacz sekcję <strong>Znane problemy</strong> jeśli napotkasz problemy</em></p>
<p><em>Zobacz sekcję <strong>Config</strong> po szczegółowe wyjaśnienie pliku konfiguracyjnego.</em></p>
<hr>
<h2>Uruchom usługi i AgenticSeek</h2>
<p>Domyślnie AgenticSeek uruchamiany jest w całości w dockerze.</p>
<p>Uruchom wymagane usługi. Rozpocznie to wszystkie usługi z pliku docker-compose.yml, w tym:<br>    - searxng<br>    - redis (wymagany przez searxng)<br>    - frontend<br>    - backend (jeśli używasz <code>full</code>)</p>
<pre><code class="language-sh hljs language-bash">./start_services.sh full <span class="hljs-comment"># MacOS</span>
start ./start_services.cmd full <span class="hljs-comment"># Windows</span>
</code></pre>
<p><strong>Uwaga:</strong> Ten krok pobierze i załaduje wszystkie obrazy Dockera, co może zająć do 30 minut. Po uruchomieniu usług poczekaj, aż usługa backend będzie w pełni uruchomiona (powinieneś zobaczyć <strong>backend: "GET /health HTTP/1.1" 200 OK</strong> w logu), zanim wyślesz jakiekolwiek wiadomości. Usługi backend mogą potrzebować do 5 minut na pierwsze uruchomienie.</p>
<p>Przejdź do <code>http://localhost:3000/</code> i powinieneś zobaczyć interfejs webowy.</p>
<p><em>Rozwiązywanie problemów z uruchomieniem usług:</em> Jeśli te skrypty się nie powiodą, upewnij się, że Docker Engine jest uruchomiony oraz Docker Compose (V2, <code>docker compose</code>) jest poprawnie zainstalowany. Sprawdź komunikaty o błędach w terminalu. Zobacz <a href="#faq-troubleshooting">FAQ: Pomoc! Wystąpił błąd podczas uruchamiania AgenticSeek lub jego skryptów.</a></p>
<p><strong>Opcjonalnie:</strong> Uruchom na hoście (tryb CLI):</p>
<p>Aby uruchomić w interfejsie CLI musisz zainstalować pakiet na hoście:</p>
<pre><code class="language-sh hljs language-bash">./install.sh
./install.bat <span class="hljs-comment"># windows</span>
</code></pre>
<p>Uruchom usługi:</p>
<pre><code class="language-sh hljs language-bash">./start_services.sh <span class="hljs-comment"># MacOS</span>
start ./start_services.cmd <span class="hljs-comment"># Windows</span>
</code></pre>
<p>Użyj CLI: <code>python3 cli.py</code></p>
<hr>
<h2>Użytkowanie</h2>
<p>Upewnij się, że usługi są uruchomione poprzez <code>./start_services.sh full</code> i przejdź do <code>localhost:3000</code> aby uzyskać dostęp do interfejsu webowego.</p>
<p>Możesz także użyć funkcji rozpoznawania mowy ustawiając <code>listen = True</code> w konfiguracji. Tylko w trybie CLI.</p>
<p>Aby zakończyć, po prostu powiedz/napisz <code>goodbye</code>.</p>
<p>Oto przykłady użycia:</p>
<blockquote>
<p><em>Stwórz grę w węża w pythonie!</em></p>
</blockquote>
<blockquote>
<p><em>Wyszukaj w internecie najlepsze kawiarnie w Rennes, Francja, i zapisz listę trzech z ich adresami w pliku rennes_cafes.txt.</em></p>
</blockquote>
<blockquote>
<p><em>Napisz program w Go obliczający silnię liczby, zapisz go jako factorial.go w swoim katalogu roboczym</em></p>
</blockquote>
<blockquote>
<p><em>Wyszukaj w folderze summer_pictures wszystkie pliki JPG, zmień ich nazwy na dzisiejszą datę i zapisz listę zmienionych plików w photos_list.txt</em></p>
</blockquote>
<blockquote>
<p><em>Wyszukaj online popularne filmy sci-fi z 2024 i wybierz trzy do obejrzenia dziś wieczorem. Zapisz listę w movie_night.txt.</em></p>
</blockquote>
<blockquote>
<p><em>Wyszukaj w internecie najnowsze artykuły o AI z 2025, wybierz trzy, i napisz skrypt w Pythonie, który pobierze ich tytuły i podsumowania. Zapisz skrypt jako news_scraper.py, a podsumowania w ai_news.txt w /home/projects</em></p>
</blockquote>
<blockquote>
<p><em>W piątek wyszukaj w internecie darmowe API do notowań giełdowych, zarejestruj się jako <a href="mailto:supersuper7434567@gmail.com">supersuper7434567@gmail.com</a>, potem napisz skrypt w Pythonie pobierający dzienne ceny Tesli i zapisz wyniki w stock_prices.csv</em></p>
</blockquote>
<p><em>Zwróć uwagę, że możliwości wypełniania formularzy są nadal eksperymentalne i mogą nie działać poprawnie.</em></p>
<p>Po wpisaniu zapytania AgenticSeek przydzieli najlepszego agenta do zadania.</p>
<p>Ponieważ to wczesny prototyp, system routingu agentów może nie zawsze przydzielić właściwego agenta na podstawie zapytania.</p>
<p>Dlatego powinieneś być bardzo precyzyjny w tym, czego oczekujesz i jak AI ma postępować — np. jeśli chcesz, by wyszukał informacje w internecie, nie pisz:</p>
<p><code>Czy znasz jakieś dobre kraje na podróż solo?</code></p>
<p>Zamiast tego, zapytaj:</p>
<p><code>Wyszukaj w internecie i dowiedz się, które kraje są najlepsze do podróżowania solo</code></p>
<hr>
<h2><strong>Konfiguracja uruchomienia LLM na własnym serwerze</strong></h2>
<p>Jeśli masz wydajny komputer lub serwer, z którego możesz korzystać, ale chcesz używać go z laptopa, masz możliwość uruchomienia LLM na zdalnym serwerze za pomocą naszego własnego serwera llm.</p>
<p>Na swoim "serwerze", który będzie uruchamiał model AI, pobierz adres IP</p>
<pre><code class="language-sh hljs language-bash">ip a | grep <span class="hljs-string">"inet "</span> | grep -v 127.0.0.1 | awk <span class="hljs-string">'{print $2}'</span> | <span class="hljs-built_in">cut</span> -d/ -f1 <span class="hljs-comment"># lokalny IP</span>
curl https://ipinfo.io/ip <span class="hljs-comment"># publiczny IP</span>
</code></pre>
<p>Uwaga: Dla Windows lub macOS użyj odpowiednio ipconfig lub ifconfig, aby znaleźć adres IP.</p>
<p>Sklonuj repozytorium i wejdź do folderu <code>server/</code>.</p>
<pre><code class="language-sh hljs language-bash">git <span class="hljs-built_in">clone</span> --depth 1 https://github.com/Fosowl/agenticSeek.git
<span class="hljs-built_in">cd</span> agenticSeek/llm_server/
</code></pre>
<p>Zainstaluj wymagania serwera:</p>
<pre><code class="language-sh hljs language-bash">pip3 install -r requirements.txt
</code></pre>
<p>Uruchom skrypt serwera.</p>
<pre><code class="language-sh hljs language-bash">python3 app.py --provider ollama --port 3333
</code></pre>
<p>Masz wybór pomiędzy <code>ollama</code> i <code>llamacpp</code> jako usługą LLM.</p>
<p>Teraz na swoim komputerze osobistym:</p>
<p>Zmień plik <code>config.ini</code>, ustawiając <code>provider_name</code> na <code>server</code> oraz <code>provider_model</code> na <code>deepseek-r1:xxb</code>.<br>Ustaw <code>provider_server_address</code> na adres IP maszyny, która będzie uruchamiała model.</p>
<pre><code class="language-sh hljs language-bash">[MAIN]
is_local = False
provider_name = server
provider_model = deepseek-r1:70b
provider_server_address = x.x.x.x:3333
</code></pre>
<p>Następny krok: <a href="#Start-services-and-Run">Uruchom usługi i AgenticSeek</a>  </p>
<hr>
<h2>Rozpoznawanie mowy (Speech to Text)</h2>
<p>Uwaga: rozpoznawanie mowy działa obecnie tylko w trybie CLI.</p>
<p>Pamiętaj, że obecnie rozpoznawanie mowy działa tylko w języku angielskim.</p>
<p>Funkcjonalność rozpoznawania mowy jest domyślnie wyłączona. Aby ją włączyć, ustaw opcję listen na True w pliku config.ini:</p>
<pre><code class="hljs language-ini"><span class="hljs-attr">listen</span> = <span class="hljs-literal">True</span>
</code></pre>
<p>Po włączeniu, funkcja rozpoznawania mowy czeka na słowo kluczowe (nazwę agenta), zanim zacznie przetwarzać Twoje polecenie. Możesz dostosować nazwę agenta, aktualizując wartość <code>agent_name</code> w pliku <em>config.ini</em>:</p>
<pre><code class="hljs language-ini"><span class="hljs-attr">agent_name</span> = Friday
</code></pre>
<p>Dla optymalnego rozpoznawania zalecamy użycie popularnego angielskiego imienia, takiego jak "John" lub "Emma" jako nazwy agenta</p>
<p>Gdy zobaczysz, że transkrypcja zaczyna się pojawiać, wypowiedz głośno imię agenta, aby go obudzić (np. "Friday").</p>
<p>Wypowiedz swoje zapytanie wyraźnie.</p>
<p>Zakończ swoją prośbę frazą potwierdzającą, aby zasygnalizować systemowi, że ma przejść dalej. Przykłady fraz potwierdzających to:</p>
<pre><code class="hljs language-bash"><span class="hljs-string">"do it"</span>, <span class="hljs-string">"go ahead"</span>, <span class="hljs-string">"execute"</span>, <span class="hljs-string">"run"</span>, <span class="hljs-string">"start"</span>, <span class="hljs-string">"thanks"</span>, <span class="hljs-string">"would ya"</span>, <span class="hljs-string">"please"</span>, <span class="hljs-string">"okay?"</span>, <span class="hljs-string">"proceed"</span>, <span class="hljs-string">"continue"</span>, <span class="hljs-string">"go on"</span>, <span class="hljs-string">"do that"</span>, <span class="hljs-string">"go it"</span>, <span class="hljs-string">"do you understand?"</span>
</code></pre>
<h2>Konfiguracja</h2>
<p>Przykładowa konfiguracja:</p>
<pre><code class="hljs language-ini"><span class="hljs-section">[MAIN]</span>
<span class="hljs-attr">is_local</span> = <span class="hljs-literal">True</span>
<span class="hljs-attr">provider_name</span> = ollama
<span class="hljs-attr">provider_model</span> = deepseek-r1:<span class="hljs-number">32</span>b
<span class="hljs-attr">provider_server_address</span> = http://<span class="hljs-number">127.0</span>.<span class="hljs-number">0.1</span>:<span class="hljs-number">11434</span> <span class="hljs-comment"># Przykład dla Ollama; użyj http://127.0.0.1:1234 dla LM-Studio</span>
<span class="hljs-attr">agent_name</span> = Friday
<span class="hljs-attr">recover_last_session</span> = <span class="hljs-literal">False</span>
<span class="hljs-attr">save_session</span> = <span class="hljs-literal">False</span>
<span class="hljs-attr">speak</span> = <span class="hljs-literal">False</span>
<span class="hljs-attr">listen</span> = <span class="hljs-literal">False</span>

<span class="hljs-attr">jarvis_personality</span> = <span class="hljs-literal">False</span>
<span class="hljs-attr">languages</span> = en zh <span class="hljs-comment"># Lista języków dla TTS i potencjalnie routingu.</span>
<span class="hljs-section">[BROWSER]</span>
<span class="hljs-attr">headless_browser</span> = <span class="hljs-literal">False</span>
<span class="hljs-attr">stealth_mode</span> = <span class="hljs-literal">False</span>
</code></pre>
<p><strong>Wyjaśnienie ustawień <code>config.ini</code></strong>:</p>
<ul>
<li><strong>Sekcja <code>[MAIN]</code>:</strong><ul>
<li><code>is_local</code>: <code>True</code> jeśli korzystasz z lokalnego dostawcy LLM (Ollama, LM-Studio, lokalny serwer zgodny z OpenAI) lub opcji serwera self-hosted. <code>False</code> jeśli używasz API w chmurze (OpenAI, Google, itd.).</li>
<li><code>provider_name</code>: Określa dostawcę LLM.<ul>
<li>Opcje lokalne: <code>ollama</code>, <code>lm-studio</code>, <code>openai</code> (dla lokalnych serwerów zgodnych z OpenAI), <code>server</code> (dla własnego serwera self-hosted).</li>
<li>Opcje API: <code>openai</code>, <code>google</code>, <code>deepseek</code>, <code>huggingface</code>, <code>togetherAI</code>.</li>
</ul>
</li>
<li><code>provider_model</code>: Konkretna nazwa lub ID modelu dla wybranego dostawcy (np. <code>deepseekcoder:6.7b</code> dla Ollama, <code>gpt-3.5-turbo</code> dla OpenAI API, <code>mistralai/Mixtral-8x7B-Instruct-v0.1</code> dla TogetherAI).</li>
<li><code>provider_server_address</code>: Adres twojego dostawcy LLM.<ul>
<li>Dla dostawców lokalnych: np. <code>http://127.0.0.1:11434</code> dla Ollama, <code>http://127.0.0.1:1234</code> dla LM-Studio.</li>
<li>Dla typu <code>server</code>: Adres twojego własnego serwera LLM (np. <code>http://your_server_ip:3333</code>).</li>
<li>Dla API w chmurze (<code>is_local = False</code>): Zazwyczaj ignorowane lub można zostawić puste, ponieważ endpoint API jest zazwyczaj obsługiwany przez bibliotekę kliencką.</li>
</ul>
</li>
<li><code>agent_name</code>: Nazwa asystenta AI (np. Friday). Używana jako słowo wybudzające dla rozpoznawania mowy, jeśli jest włączone.</li>
<li><code>recover_last_session</code>: <code>True</code> aby przywrócić stan poprzedniej sesji, <code>False</code> aby rozpocząć od nowa.</li>
<li><code>save_session</code>: <code>True</code> aby zapisać stan bieżącej sesji do ewentualnego przywrócenia, <code>False</code> w przeciwnym wypadku.</li>
<li><code>speak</code>: <code>True</code> aby włączyć syntezę mowy (TTS), <code>False</code> aby wyłączyć.</li>
<li><code>listen</code>: <code>True</code> aby włączyć rozpoznawanie mowy (STT) w trybie CLI, <code>False</code> aby wyłączyć.</li>
<li><code>work_dir</code>: <strong>Istotne:</strong> Katalog, w którym AgenticSeek będzie czytać/zapisywać pliki. <strong>Upewnij się, że ta ścieżka jest poprawna i dostępna na twoim systemie.</strong></li>
<li><code>jarvis_personality</code>: <code>True</code> aby użyć bardziej "Jarvisowego" prompta systemowego (eksperymentalne), <code>False</code> dla standardowego prompta.</li>
<li><code>languages</code>: Lista języków rozdzielona przecinkami (np. <code>en, zh, fr</code>). Używana do wyboru głosu TTS (domyślnie pierwszy) i może pomagać routerowi LLM. Unikaj zbyt wielu lub bardzo podobnych języków dla efektywności routera.</li>
</ul>
</li>
<li><strong>Sekcja <code>[BROWSER]</code>:</strong><ul>
<li><code>headless_browser</code>: <code>True</code> aby uruchomić zautomatyzowaną przeglądarkę bez widocznego okna (zalecane do interfejsu webowego lub użytku nieinteraktywnego). <code>False</code> aby pokazać okno przeglądarki (przydatne w trybie CLI lub debugowaniu).</li>
<li><code>stealth_mode</code>: <code>True</code> aby włączyć środki utrudniające wykrycie automatyzacji przeglądarki. Może wymagać ręcznej instalacji rozszerzeń, takich jak anticaptcha.</li>
</ul>
</li>
</ul>
<p>Ta sekcja podsumowuje obsługiwane typy dostawców LLM. Skonfiguruj je w <code>config.ini</code>.</p>
<p><strong>Dostawcy lokalni (uruchamiani na twoim sprzęcie):</strong></p>
<table>
<thead>
<tr>
<th>Nazwa dostawcy w <code>config.ini</code></th>
<th><code>is_local</code></th>
<th>Opis</th>
<th>Sekcja konfiguracji</th>
</tr>
</thead>
<tbody><tr>
<td><code>ollama</code></td>
<td><code>True</code></td>
<td>Użyj Ollama do serwowania lokalnych LLM.</td>
<td><a href="#setup-for-running-llm-locally-on-your-machine">Konfiguracja lokalnego LLM</a></td>
</tr>
<tr>
<td><code>lm-studio</code></td>
<td><code>True</code></td>
<td>Użyj LM-Studio do serwowania lokalnych LLM.</td>
<td><a href="#setup-for-running-llm-locally-on-your-machine">Konfiguracja lokalnego LLM</a></td>
</tr>
<tr>
<td><code>openai</code> (dla lokalnego serwera)</td>
<td><code>True</code></td>
<td>Połącz z lokalnym serwerem udostępniającym API zgodne z OpenAI (np. llama.cpp).</td>
<td><a href="#setup-for-running-llm-locally-on-your-machine">Konfiguracja lokalnego LLM</a></td>
</tr>
<tr>
<td><code>server</code></td>
<td><code>False</code></td>
<td>Połącz z własnym serwerem LLM AgenticSeek uruchomionym na innej maszynie.</td>
<td><a href="#setup-to-run-the-llm-on-your-own-server">Konfiguracja własnego serwera LLM</a></td>
</tr>
</tbody></table>
<p><strong>Dostawcy API (chmurowi):</strong></p>
<table>
<thead>
<tr>
<th>Nazwa dostawcy w <code>config.ini</code></th>
<th><code>is_local</code></th>
<th>Opis</th>
<th>Sekcja konfiguracji</th>
</tr>
</thead>
<tbody><tr>
<td><code>openai</code></td>
<td><code>False</code></td>
<td>Użyj oficjalnego API OpenAI (np. GPT-3.5, GPT-4).</td>
<td><a href="#setup-to-run-with-an-api">Konfiguracja z API</a></td>
</tr>
<tr>
<td><code>google</code></td>
<td><code>False</code></td>
<td>Użyj modeli Gemini Google przez API.</td>
<td><a href="#setup-to-run-with-an-api">Konfiguracja z API</a></td>
</tr>
<tr>
<td><code>deepseek</code></td>
<td><code>False</code></td>
<td>Użyj oficjalnego API Deepseek.</td>
<td><a href="#setup-to-run-with-an-api">Konfiguracja z API</a></td>
</tr>
<tr>
<td><code>huggingface</code></td>
<td><code>False</code></td>
<td>Użyj Hugging Face Inference API.</td>
<td><a href="#setup-to-run-with-an-api">Konfiguracja z API</a></td>
</tr>
<tr>
<td><code>togetherAI</code></td>
<td><code>False</code></td>
<td>Użyj API TogetherAI dla różnych modeli open.</td>
<td><a href="#setup-to-run-with-an-api">Konfiguracja z API</a></td>
</tr>
</tbody></table>
<hr>
<h2>Rozwiązywanie problemów</h2>
<p>Jeśli napotkasz problemy, ta sekcja zawiera wskazówki.</p>
<h1>Znane problemy</h1>
<h2>Problemy z ChromeDriver</h2>
<p><strong>Przykład błędu:</strong> <code>SessionNotCreatedException: Message: session not created: This version of ChromeDriver only supports Chrome version XXX</code></p>
<ul>
<li><strong>Przyczyna:</strong> Zainstalowana wersja ChromeDriver jest niezgodna z wersją przeglądarki Google Chrome.</li>
<li><strong>Rozwiązanie:</strong><ol>
<li><strong>Sprawdź wersję Chrome:</strong> Otwórz Google Chrome, przejdź do <code>Ustawienia &gt; O Google Chrome</code>, aby znaleźć wersję (np. "Wersja 120.0.6099.110").</li>
<li><strong>Pobierz pasującą wersję ChromeDriver:</strong><ul>
<li>Dla Chrome w wersji 115 i nowszych: Przejdź do <a href="https://googlechromelabs.github.io/chrome-for-testing/">Chrome for Testing (CfT) JSON Endpoints</a>. Znajdź kanał "stable" i pobierz ChromeDriver dla swojego systemu operacyjnego, zgodny z główną wersją przeglądarki Chrome.</li>
<li>Dla starszych wersji (rzadziej spotykane): Możesz je znaleźć na stronie <a href="https://chromedriver.chromium.org/downloads">ChromeDriver - WebDriver for Chrome</a>.</li>
<li>Poniższy obrazek przedstawia przykład ze strony CfT:<br><img src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/chromedriver_readme.png" alt="Download Chromedriver specific version from Chrome for Testing page"></li>
</ul>
</li>
<li><strong>Zainstaluj ChromeDriver:</strong><ul>
<li>Upewnij się, że pobrany plik <code>chromedriver</code> (lub <code>chromedriver.exe</code> w Windows) znajduje się w katalogu, który jest na liście PATH w systemie (np. <code>/usr/local/bin</code> w Linux/macOS lub własny folder skryptów dodany do PATH w Windows).</li>
<li>Alternatywnie umieść go w katalogu głównym projektu <code>agenticSeek</code>.</li>
<li>Upewnij się, że sterownik ma prawa wykonywania (np. <code>chmod +x chromedriver</code> w Linux/macOS).</li>
</ul>
</li>
<li>Zajrzyj do sekcji <a href="#chromedriver-installation">ChromeDriver Installation</a> w głównym przewodniku instalacji po więcej szczegółów.</li>
</ol>
</li>
</ul>
<p>Jeśli ta sekcja jest niepełna lub napotkasz inne problemy z ChromeDriver, rozważ sprawdzenie istniejących <a href="https://github.com/Fosowl/agenticSeek/issues">GitHub Issues</a> lub zgłoszenie nowego problemu.</p>
<p><code>Exception: Failed to initialize browser: Message: session not created: This version of ChromeDriver only supports Chrome version 113 Current browser version is 134.0.6998.89 with binary path</code></p>
<p>To dzieje się, jeśli wersje przeglądarki i chromedrivera się nie zgadzają.</p>
<p>Musisz przejść do pobrania najnowszej wersji:</p>
<p><a href="https://developer.chrome.com/docs/chromedriver/downloads">https://developer.chrome.com/docs/chromedriver/downloads</a></p>
<p>Jeśli używasz Chrome w wersji 115 lub nowszej przejdź do:</p>
<p><a href="https://googlechromelabs.github.io/chrome-for-testing/">https://googlechromelabs.github.io/chrome-for-testing/</a></p>
<p>I pobierz wersję chromedrivera odpowiednią dla twojego systemu operacyjnego.</p>
<p><img src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/chromedriver_readme.png" alt="alt text"></p>
<p>Jeśli ta sekcja jest niepełna, zgłoś problem.</p>
<h2>Problemy z connection adapters</h2>
<pre><code class="hljs language-csharp">Exception: Provider lm-studio failed: HTTP request failed: No connection adapters were found <span class="hljs-keyword">for</span> <span class="hljs-string">'127.0.0.1:1234/v1/chat/completions'</span>` (Uwaga: port może <span class="hljs-keyword">by</span>ć inny)
</code></pre>
<ul>
<li><strong>Przyczyna:</strong> <code>provider_server_address</code> w <code>config.ini</code> dla <code>lm-studio</code> (lub innych podobnych lokalnych serwerów zgodnych z OpenAI) nie zawiera prefiksu <code>http://</code> lub wskazuje zły port.</li>
<li><strong>Rozwiązanie:</strong><ul>
<li>Upewnij się, że adres zawiera <code>http://</code>. LM-Studio domyślnie to <code>http://127.0.0.1:1234</code>.</li>
<li>Popraw <code>config.ini</code>: <code>provider_server_address = http://127.0.0.1:1234</code> (lub twój faktyczny port serwera LM-Studio).</li>
</ul>
</li>
</ul>
<h2>Nie podano podstawowego adresu URL SearxNG</h2>
<pre><code class="hljs language-csharp"><span class="hljs-function">raise <span class="hljs-title">ValueError</span>(<span class="hljs-params"><span class="hljs-string">"SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable."</span></span>)
ValueError: SearxNG <span class="hljs-keyword">base</span> URL must be provided either <span class="hljs-keyword">as</span> an argument <span class="hljs-keyword">or</span> via the SEARXNG_BASE_URL environment variable.`
</span></code></pre>
<h2>FAQ</h2>
<p><strong>Q: Jakiego sprzętu potrzebuję?</strong>  </p>
<table>
<thead>
<tr>
<th>Rozmiar modelu</th>
<th>GPU</th>
<th>Komentarz</th>
</tr>
</thead>
<tbody><tr>
<td>7B</td>
<td>8GB Vram</td>
<td>⚠️ Niezalecane. Słaba wydajność, częste halucynacje, agenci planujący prawdopodobnie zawiodą.</td>
</tr>
<tr>
<td>14B</td>
<td>12 GB VRAM (np. RTX 3060)</td>
<td>✅ Używalny do prostych zadań. Może mieć trudności z przeglądaniem sieci i zadaniami planistycznymi.</td>
</tr>
<tr>
<td>32B</td>
<td>24+ GB VRAM (np. RTX 4090)</td>
<td>🚀 Sukces w większości zadań, nadal może mieć trudności z planowaniem zadań</td>
</tr>
<tr>
<td>70B+</td>
<td>48+ GB Vram</td>
<td>💪 Doskonały. Zalecany do zaawansowanych zastosowań.</td>
</tr>
</tbody></table>
<p><strong>Q: Otrzymuję błąd, co mam zrobić?</strong>  </p>
<p>Upewnij się, że lokalny serwer działa (<code>ollama serve</code>), twoje <code>config.ini</code> pasuje do wybranego dostawcy, a zależności są zainstalowane. Jeśli żadne nie działa, śmiało zgłoś problem.</p>
<p><strong>Q: Czy to naprawdę może działać w 100% lokalnie?</strong>  </p>
<p>Tak, z dostawcami Ollama, lm-studio lub server wszystkie modele STT, LLM i TTS działają lokalnie. Opcje nielokalne (OpenAI lub inne API) są opcjonalne.</p>
<p><strong>Q: Dlaczego mam używać AgenticSeek, skoro mam Manus?</strong></p>
<p>W przeciwieństwie do Manus, AgenticSeek stawia na niezależność od zewnętrznych systemów, dając ci większą kontrolę, prywatność i brak kosztów API.</p>
<p><strong>Q: Kto stoi za projektem?</strong></p>
<p>Projekt został stworzony przeze mnie, wraz z dwoma przyjaciółmi, którzy są maintainerami i kontrybutorami z open-source na GitHubie. Jesteśmy po prostu grupą pasjonatów, a nie startupem ani organizacją.</p>
<p>Każde konto AgenticSeek na X poza moim osobistym kontem (<a href="https://x.com/Martin993886460">https://x.com/Martin993886460</a>) jest podszywaniem się.</p>
<h2>Współtwórz</h2>
<p>Szukamy developerów do rozwoju AgenticSeek! Zajrzyj do otwartych zgłoszeń lub dyskusji.</p>
<p><a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/docs/CONTRIBUTING.md">Przewodnik kontrybucji</a></p>
<p><a href="https://www.star-history.com/#Fosowl/agenticSeek&amp;Date"><img src="https://api.star-history.com/svg?repos=Fosowl/agenticSeek&amp;type=Date" alt="Star History Chart"></a></p>
<h2>Maintainerzy:</h2>
<blockquote>
<p><a href="https://github.com/Fosowl">Fosowl</a> | czas paryski </p>
</blockquote>
<blockquote>
<p><a href="https://github.com/antoineVIVIES">antoineVIVIES</a> | czas tajpejski </p>
</blockquote>
<blockquote>
<p><a href="https://github.com/steveh8758">steveh8758</a> | czas tajpejski </p>
</blockquote>
<h2>Specjalne podziękowania:</h2>
<blockquote>
<p><a href="https://github.com/tcsenpai">tcsenpai</a> oraz <a href="https://github.com/plitc">plitc</a> za pomoc w dockerowaniu backendu</p>
</blockquote>
<h2>Sponsorzy:</h2>
<p>Sponsorzy z miesięczną wpłatą 5$ lub więcej pojawią się tutaj:</p>
<ul>
<li><strong>tatra-labs</strong><br>It seems you haven't provided the text of the technical document that needs to be translated. Please provide the content you'd like translated (Part 4 of 4), and I'll proceed with the translation while preserving the original Markdown format and updating the relative paths as requested.</li>
</ul>
<hr>
<p>Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx">Open Ai Tx</a> | Last indexed: 2025-06-16</p>
<hr>
</div>
    </div>

    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Default Statcounter code for openaitx
https://openaitx.github.io/ -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async=""></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>
    <!-- End of Statcounter Code -->
    


</body></html>