<!DOCTYPE html>
<html lang="it">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>agenticSeek - Fosowl/agenticSeek</title>
    <meta name="title" content="agenticSeek - Fosowl/agenticSeek">
    <meta name="description" content="Fosowl/agenticSeek - GitHub repository it documentation and informationAgenticSeek: Alternativa privata e locale a Manus. English | 中文 | 繁體中文 | Français | 日本語 | Português (Brasil) | Español Un'alternativa 100% locale a Manus AI, qu...">
    <meta name="keywords" content="Fosowl, agenticSeek, GitHub, repository, it documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/Fosowl/agenticSeek/README-it.html">
    <meta property="og:title" content="agenticSeek - Fosowl/agenticSeek">
    <meta property="og:description" content="Fosowl/agenticSeek - GitHub repository it documentation and information">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">
    <script src="/js/marked.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        body { display: flex; flex-direction: column; min-height: 100vh; }
        .main-container { margin: 0 auto; width: 100%; max-width: 980px; padding: 0 20px; }
        @media (max-width: 768px) { .main-container { padding: 0 15px; } }
        .markdown-body img { max-width: 100%; height: auto; }
        .header { text-align: center; margin-bottom: 30px; padding: 20px; background-color: #f6f8fa; border-bottom: 1px solid #e1e4e8; position: relative; }
        .header .links { margin-top: 10px; font-size: 16px; }
        .header .links a { color: #0366d6; text-decoration: none; margin-left: 5px; }
        .header .links a:hover { text-decoration: underline; }
        .language-badges { margin-top: 15px; text-align: center; }
        .language-badges a { display: inline-block; margin: 2px; text-decoration: none; }
        .language-badges img { height: 20px; border-radius: 3px; }
        .language-badges a:hover img { opacity: 0.8; }
    </style>
</head>
<body>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/Fosowl/agenticSeek" id="githubRepoLink" target="_blank">Fosowl/agenticSeek</a>
<h1 style="display: none;">AgenticSeek: Alternativa privata e locale a Manus. English | 中文 | 繁體中文 | Français | 日本語 | Português (Brasil) | Español Un'alternativa 100% locale a Manus AI, qu...</h1>
        </div>
        <div class="language-badges" id="languageBadges">
            <!-- You can generate language badges here if needed -->
        </div>
    </div>
    <div class="main-container">
        <div class="markdown-body" id="content">
            <h1>AgenticSeek: Alternativa privata e locale a Manus.</h1>
<p align="center">
<img align="center" src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/agentic_seek_logo.png" width="300" height="300" alt="Agentic Seek Logo">
<p>
<p>English | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_CHS.md">中文</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_CHT.md">繁體中文</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_FR.md">Français</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_JP.md">日本語</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_PTBR.md">Português (Brasil)</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_ES.md">Español</a></p>
<p><em>Un'alternativa <strong>100% locale a Manus AI</strong>, questo assistente vocale abilitato all'IA naviga autonomamente sul web, scrive codice e pianifica attività mantenendo tutti i dati sul tuo dispositivo. Progettato per modelli di ragionamento locali, funziona interamente sull'hardware locale, garantendo completa privacy e zero dipendenza dal cloud.</em></p>
<p><a href="https://fosowl.github.io/agenticSeek.html"><img src="https://img.shields.io/static/v1?label=Website&amp;message=AgenticSeek&amp;color=blue&amp;style=flat-square" alt="Visita AgenticSeek" /></a> <img src="https://img.shields.io/badge/license-GPL--3.0-green" alt="License" /> <a href="https://discord.gg/8hGDaME3TC"><img src="https://img.shields.io/badge/Discord-Join%20Us-7289DA?logo=discord&amp;logoColor=white" alt="Discord" /></a> <a href="https://x.com/Martin993886460"><img src="https://img.shields.io/twitter/url/https/twitter.com/fosowl.svg?style=social&amp;label=Update%20%40Fosowl" alt="Twitter" /></a> <a href="https://github.com/Fosowl/agenticSeek/stargazers"><img src="https://img.shields.io/github/stars/Fosowl/agenticSeek?style=social" alt="GitHub stars" /></a></p>
<h3>Perché AgenticSeek?</h3>
<ul>
<li><p>🔒 Completamente Locale e Privato - Tutto funziona sulla tua macchina — nessun cloud, nessuna condivisione dei dati. I tuoi file, conversazioni e ricerche rimangono privati.</p>
</li>
<li><p>🌐 Navigazione Web Intelligente - AgenticSeek può navigare autonomamente in internet — cercare, leggere, estrarre informazioni, compilare moduli web — tutto a mani libere.</p>
</li>
<li><p>💻 Assistente di Codifica Autonomo - Hai bisogno di codice? Può scrivere, effettuare il debug ed eseguire programmi in Python, C, Go, Java e altro — tutto senza supervisione.</p>
</li>
<li><p>🧠 Selezione Intelligente dell’Agente - Tu chiedi, lui individua automaticamente l'agente migliore per il compito. Come avere un team di esperti sempre pronti ad aiutarti.</p>
</li>
<li><p>📋 Pianifica ed Esegue Compiti Complessi - Dalla pianificazione di viaggi a progetti complessi — può suddividere grandi attività in step e portarle a termine usando molteplici agenti IA.</p>
</li>
<li><p>🎙️ Voce Abilitata - Voce pulita, veloce, futuristica e riconoscimento vocale che ti permette di parlarci come se fosse la tua IA personale da un film di fantascienza. (In sviluppo)</p>
</li>
</ul>
<h3><strong>Demo</strong></h3>
<blockquote>
<p><em>Puoi cercare il progetto agenticSeek, capire quali competenze sono richieste, poi aprire CV_candidates.zip e dirmi quali corrispondono meglio al progetto?</em></p>
</blockquote>
<p>https://github.com/user-attachments/assets/b8ca60e9-7b3b-4533-840e-08f9ac426316</p>
<p>Disclaimer: Questa demo, inclusi tutti i file che compaiono (es: CV_candidates.zip), è completamente fittizia. Non siamo una società, cerchiamo contributori open-source, non candidati.</p>
<blockquote>
<p>🛠⚠️️ <strong>Lavoro attivo in corso</strong></p>
</blockquote>
<blockquote>
<p>🙏 Questo progetto è iniziato come un side-project e non ha roadmap né finanziamenti. È cresciuto oltre le aspettative finendo nei GitHub Trending. Contributi, feedback e pazienza sono profondamente apprezzati.</p>
</blockquote>
<h2>Prerequisiti</h2>
<p>Prima di iniziare, assicurati di avere installato il seguente software:</p>
<ul>
<li><strong>Git:</strong> Per clonare il repository. <a href="https://git-scm.com/downloads">Scarica Git</a></li>
<li><strong>Python 3.10.x:</strong> Consigliamo vivamente Python versione 3.10.x. L’uso di altre versioni può causare errori di dipendenza. <a href="https://www.python.org/downloads/release/python-3100/">Scarica Python 3.10</a> (scegli una versione 3.10.x).</li>
<li><strong>Docker Engine &amp; Docker Compose:</strong> Per l’esecuzione di servizi integrati come SearxNG.
<ul>
<li>Installa Docker Desktop (che include Docker Compose V2): <a href="https://docs.docker.com/desktop/install/windows-install/">Windows</a> | <a href="https://docs.docker.com/desktop/install/mac-install/">Mac</a> | <a href="https://docs.docker.com/desktop/install/linux-install/">Linux</a></li>
<li>In alternativa, installa Docker Engine e Docker Compose separatamente su Linux: <a href="https://docs.docker.com/engine/install/">Docker Engine</a> | <a href="https://docs.docker.com/compose/install/">Docker Compose</a> (assicurati di installare Compose V2, ad esempio, <code>sudo apt-get install docker-compose-plugin</code>).</li>
</ul>
</li>
</ul>
<h3>1. <strong>Clona il repository e configura</strong></h3>
<pre><code class="language-sh">git clone https://github.com/Fosowl/agenticSeek.git
cd agenticSeek
mv .env.example .env
</code></pre>
<h3>2. Modifica il contenuto del file .env</h3>
<pre><code class="language-sh">SEARXNG_BASE_URL=&quot;http://127.0.0.1:8080&quot;
REDIS_BASE_URL=&quot;redis://redis:6379/0&quot;
WORK_DIR=&quot;/Users/mlg/Documents/workspace_for_ai&quot;
OLLAMA_PORT=&quot;11434&quot;
LM_STUDIO_PORT=&quot;1234&quot;
CUSTOM_ADDITIONAL_LLM_PORT=&quot;11435&quot;
OPENAI_API_KEY='optional'
DEEPSEEK_API_KEY='optional'
OPENROUTER_API_KEY='optional'
TOGETHER_API_KEY='optional'
GOOGLE_API_KEY='optional'
ANTHROPIC_API_KEY='optional'
</code></pre>
<p>Aggiorna il file <code>.env</code> con i tuoi valori ove necessario:</p>
<ul>
<li><strong>SEARXNG_BASE_URL</strong>: Lascia invariato</li>
<li><strong>REDIS_BASE_URL</strong>: Lascia invariato</li>
<li><strong>WORK_DIR</strong>: Percorso della tua cartella di lavoro sulla tua macchina locale. AgenticSeek potrà leggere e interagire con questi file.</li>
<li><strong>OLLAMA_PORT</strong>: Porta per il servizio Ollama.</li>
<li><strong>LM_STUDIO_PORT</strong>: Porta per il servizio LM Studio.</li>
<li><strong>CUSTOM_ADDITIONAL_LLM_PORT</strong>: Porta per eventuali servizi LLM personalizzati aggiuntivi.</li>
</ul>
<p><strong>Le chiavi API sono totalmente opzionali per gli utenti che scelgono di eseguire LLM localmente, che è lo scopo principale di questo progetto. Lascia vuoto se hai hardware sufficiente</strong></p>
<h3>3. <strong>Avvia Docker</strong></h3>
<p>Assicurati che Docker sia installato e in esecuzione sul tuo sistema. Puoi avviare Docker con i seguenti comandi:</p>
<ul>
<li><p><strong>Su Linux/macOS:</strong><br />
Apri un terminale ed esegui:</p>
<pre><code class="language-sh">sudo systemctl start docker
</code></pre>
<p>Oppure lancia Docker Desktop dal menu delle applicazioni se installato.</p>
</li>
<li><p><strong>Su Windows:</strong><br />
Avvia Docker Desktop dal menu Start.</p>
</li>
</ul>
<p>Puoi verificare che Docker sia in esecuzione eseguendo:</p>
<pre><code class="language-sh">docker info
</code></pre>
<p>Se vedi informazioni sulla tua installazione di Docker, allora è attivo correttamente.</p>
<p>Consulta la tabella dei <a href="#list-of-local-providers">Provider Locali</a> qui sotto per un riassunto.</p>
<p>Prossimo passo: <a href="#start-services-and-run">Esegui AgenticSeek in locale</a></p>
<p><em>Consulta la sezione <a href="#troubleshooting">Risoluzione dei problemi</a> se hai problemi.</em>
<em>Se il tuo hardware non può eseguire LLM in locale, vedi <a href="#setup-to-run-with-an-api">Configurazione per esecuzione con una API</a>.</em>
<em>Per dettagli su <code>config.ini</code>, vedi <a href="#config">Sezione Config</a>.</em></p>
<hr />
<h2>Configurazione per esecuzione LLM in locale sulla tua macchina</h2>
<p><strong>Requisiti hardware:</strong></p>
<p>Per eseguire LLM localmente, avrai bisogno di hardware sufficiente. Al minimo, è richiesta una GPU in grado di eseguire Magistral, Qwen o Deepseek 14B. Consulta le FAQ per raccomandazioni dettagliate su modelli/prestazioni.</p>
<p><strong>Configura il tuo provider locale</strong></p>
<p>Avvia il tuo provider locale, ad esempio con ollama:</p>
<pre><code class="language-sh">ollama serve
</code></pre>
<p>Consulta sotto per un elenco di provider locali supportati.</p>
<p><strong>Aggiorna il config.ini</strong></p>
<p>Modifica il file config.ini per impostare provider_name su un provider supportato e provider_model su un LLM supportato dal tuo provider. Consigliamo modelli di ragionamento come <em>Magistral</em> o <em>Deepseek</em>.</p>
<p>Consulta le <strong>FAQ</strong> alla fine del README per l’hardware necessario.</p>
<pre><code class="language-sh">[MAIN]
is_local = True # Se stai eseguendo localmente o con un provider remoto.
provider_name = ollama # oppure lm-studio, openai, ecc.
provider_model = deepseek-r1:14b # scegli un modello compatibile col tuo hardware
provider_server_address = 127.0.0.1:11434
agent_name = Jarvis # nome della tua IA
recover_last_session = True # se recuperare la sessione precedente
save_session = True # se ricordare la sessione attuale
speak = False # sintesi vocale
listen = False # riconoscimento vocale, solo per CLI, sperimentale
jarvis_personality = False # se usare una personalità tipo &quot;Jarvis&quot; (sperimentale)
languages = en zh # Elenco lingue, la sintesi vocale userà la prima della lista
[BROWSER]
headless_browser = True # lascia invariato a meno che usi CLI sull’host.
stealth_mode = True # Usa selenium non rilevabile per ridurre la detection del browser
</code></pre>
<p><strong>Attenzione</strong>:</p>
<ul>
<li><p>Il formato del file <code>config.ini</code> non supporta commenti.
Non copiare e incollare direttamente la configurazione di esempio, poiché i commenti causeranno errori. Modifica manualmente il file <code>config.ini</code> con le impostazioni desiderate, escludendo qualsiasi commento.</p>
</li>
<li><p>NON impostare provider_name su <code>openai</code> se usi LM-studio per eseguire LLM. Impostalo su <code>lm-studio</code>.</p>
</li>
<li><p>Alcuni provider (es: lm-studio) richiedono che ci sia <code>http://</code> davanti all’IP. Ad esempio <code>http://127.0.0.1:1234</code></p>
</li>
</ul>
<p><strong>Elenco dei provider locali</strong></p>
<p>| Provider  | Locale? | Descrizione                                               |
|-----------|--------|-----------------------------------------------------------|
| ollama    | Sì     | Esegui LLM localmente con ollama come provider LLM        |
| lm-studio | Sì     | Esegui LLM localmente con LM studio (imposta <code>provider_name</code> su <code>lm-studio</code>)|
| openai    | Sì     |  Usa API compatibili openai (es: server llama.cpp)        |</p>
<p>Prossimo passo: <a href="#Start-services-and-Run">Avvia i servizi ed esegui AgenticSeek</a></p>
<p><em>Consulta la sezione <a href="#troubleshooting">Risoluzione dei problemi</a> se hai problemi.</em>
<em>Se il tuo hardware non può eseguire LLM in locale, vedi <a href="#setup-to-run-with-an-api">Configurazione per esecuzione con una API</a>.</em>
<em>Per dettagli su <code>config.ini</code>, vedi <a href="#config">Sezione Config</a>.</em></p>
<h2>Configurazione per esecuzione tramite API</h2>
<p>Questa configurazione utilizza provider LLM esterni basati su cloud. Avrai bisogno di una chiave API dal servizio scelto.</p>
<p><strong>1. Scegli un provider API e ottieni una chiave API:</strong></p>
<p>Consulta l’<a href="#list-of-api-providers">Elenco dei provider API</a> qui sotto. Visita i loro siti web per registrarti e ottenere una chiave API.</p>
<p><strong>2. Imposta la tua chiave API come variabile d’ambiente:</strong></p>
<ul>
<li><strong>Linux/macOS:</strong>
Apri il terminale e usa il comando <code>export</code>. È meglio aggiungerlo al file di profilo della shell (es: <code>~/.bashrc</code>, <code>~/.zshrc</code>) per la persistenza.
<pre><code class="language-sh">export PROVIDER_API_KEY=&quot;your_api_key_here&quot; 
# Sostituisci PROVIDER_API_KEY con il nome della variabile specifica, es: OPENAI_API_KEY, GOOGLE_API_KEY
</code></pre>
Esempio per TogetherAI:
<pre><code class="language-sh">export TOGETHER_API_KEY=&quot;xxxxxxxxxxxxxxxxxxxxxx&quot;
</code></pre>
</li>
<li><strong>Windows:</strong></li>
<li><strong>Prompt dei comandi (temporaneo per la sessione corrente):</strong>
<pre><code class="language-cmd">set PROVIDER_API_KEY=your_api_key_here
</code></pre>
</li>
<li><strong>PowerShell (temporaneo per la sessione corrente):</strong>
<pre><code class="language-powershell">$env:PROVIDER_API_KEY=&quot;your_api_key_here&quot;
</code></pre>
</li>
<li><strong>Permanente:</strong> Cerca &quot;variabili d'ambiente&quot; nella barra di ricerca di Windows, clicca su &quot;Modifica le variabili d'ambiente di sistema,&quot; poi clicca sul pulsante &quot;Variabili d'ambiente...&quot;. Aggiungi una nuova variabile utente con il nome appropriato (es. <code>OPENAI_API_KEY</code>) e la tua chiave come valore.</li>
</ul>
<p><em>(Vedi FAQ: <a href="#how-do-i-set-api-keys">Come imposto le chiavi API?</a> per maggiori dettagli).</em></p>
<p><strong>3. Aggiorna <code>config.ini</code>:</strong></p>
<pre><code class="language-ini">[MAIN]
is_local = False
provider_name = openai # Oppure google, deepseek, togetherAI, huggingface
provider_model = gpt-3.5-turbo # Oppure gemini-1.5-flash, deepseek-chat, mistralai/Mixtral-8x7B-Instruct-v0.1 ecc.
provider_server_address = # Tipicamente ignorato o può essere lasciato vuoto quando is_local = False per la maggior parte delle API
# ... altre impostazioni ...
</code></pre>
<p><em>Attenzione:</em> Assicurati che non ci siano spazi finali nei valori di <code>config.ini</code>.</p>
<p><strong>Elenco dei fornitori API</strong></p>
<p>| Fornitore     | <code>provider_name</code> | Locale? | Descrizione                                         | Link chiave API (Esempi)                              |
|---------------|-----------------|---------|-----------------------------------------------------|--------------------------------------------------------|
| OpenAI        | <code>openai</code>        | No      | Utilizza i modelli ChatGPT tramite l'API di OpenAI. | <a href="https://platform.openai.com/signup">platform.openai.com/signup</a> |
| Google Gemini | <code>google</code>        | No      | Utilizza i modelli Gemini di Google tramite AI Studio.| <a href="https://aistudio.google.com/keys">aistudio.google.com/keys</a> |
| Deepseek      | <code>deepseek</code>      | No      | Utilizza i modelli Deepseek tramite la loro API.    | <a href="https://platform.deepseek.com">platform.deepseek.com</a> |
| Hugging Face  | <code>huggingface</code>   | No      | Utilizza modelli tramite Hugging Face Inference API.| <a href="https://huggingface.co/settings/tokens">huggingface.co/settings/tokens</a> |
| TogetherAI    | <code>togetherAI</code>    | No      | Utilizza vari modelli open-source tramite TogetherAI API.| <a href="https://api.together.ai/settings/api-keys">api.together.ai/settings/api-keys</a> |</p>
<p><em>Nota:</em></p>
<ul>
<li>Sconsigliamo l'uso di <code>gpt-4o</code> o altri modelli OpenAI per la navigazione web complessa e la pianificazione di task poiché le attuali ottimizzazioni dei prompt sono orientate verso modelli come Deepseek.</li>
<li>Task di coding/bash potrebbero avere problemi con Gemini, in quanto potrebbe non seguire rigorosamente i prompt di formattazione ottimizzati per Deepseek.</li>
<li>Il campo <code>provider_server_address</code> in <code>config.ini</code> generalmente non viene utilizzato quando <code>is_local = False</code> poiché l'endpoint API è solitamente hardcoded nella rispettiva libreria del fornitore.</li>
</ul>
<p>Prossimo passo: <a href="#Start-services-and-Run">Avvia servizi ed esegui AgenticSeek</a></p>
<p><em>Consulta la sezione <strong>Problemi noti</strong> se hai problemi</em></p>
<p><em>Consulta la sezione <strong>Config</strong> per una spiegazione dettagliata del file di configurazione.</em></p>
<hr />
<h2>Avvia servizi ed Esegui</h2>
<p>Per impostazione predefinita AgenticSeek viene eseguito completamente in docker.</p>
<p>Avvia i servizi richiesti. Questo avvierà tutti i servizi dal docker-compose.yml, inclusi:</p>
<ul>
<li>searxng</li>
<li>redis (necessario per searxng)</li>
<li>frontend</li>
<li>backend (se si utilizza <code>full</code>)</li>
</ul>
<pre><code class="language-sh">./start_services.sh full # MacOS
start ./start_services.cmd full # Window
</code></pre>
<p><strong>Attenzione:</strong> Questo passaggio scaricherà e caricherà tutte le immagini Docker, il che potrebbe richiedere fino a 30 minuti. Dopo l'avvio dei servizi, attendi che il servizio backend sia completamente operativo (dovresti vedere <strong>backend: &quot;GET /health HTTP/1.1&quot; 200 OK</strong> nel log) prima di inviare qualsiasi messaggio. I servizi backend potrebbero impiegare 5 minuti per avviarsi al primo avvio.</p>
<p>Vai su <code>http://localhost:3000/</code> e dovresti vedere l'interfaccia web.</p>
<p><em>Risoluzione dei problemi di avvio servizio:</em> Se questi script falliscono, assicurati che Docker Engine sia in esecuzione e che Docker Compose (V2, <code>docker compose</code>) sia installato correttamente. Controlla l'output nel terminale per eventuali messaggi di errore. Vedi <a href="#faq-troubleshooting">FAQ: Aiuto! Ricevo un errore quando eseguo AgenticSeek o i suoi script.</a></p>
<p><strong>Opzionale:</strong> Esegui su host (modalità CLI):</p>
<p>Per eseguire con l'interfaccia CLI devi installare il pacchetto sull'host:</p>
<pre><code class="language-sh">./install.sh
./install.bat # windows
</code></pre>
<p>Avvia i servizi:</p>
<pre><code class="language-sh">./start_services.sh # MacOS
start ./start_services.cmd # Window
</code></pre>
<p>Usa la CLI: <code>python3 cli.py</code></p>
<hr />
<h2>Utilizzo</h2>
<p>Assicurati che i servizi siano attivi e funzionanti con <code>./start_services.sh full</code> e vai su <code>localhost:3000</code> per l'interfaccia web.</p>
<p>Puoi anche utilizzare la funzione speech to text impostando <code>listen = True</code> nel config. Solo in modalità CLI.</p>
<p>Per uscire, basta dire/digitare <code>goodbye</code>.</p>
<p>Ecco alcuni esempi di utilizzo:</p>
<blockquote>
<p><em>Crea un gioco snake in python!</em></p>
</blockquote>
<blockquote>
<p><em>Cerca sul web i migliori caffè a Rennes, Francia, e salva una lista di tre con i loro indirizzi in rennes_cafes.txt.</em></p>
</blockquote>
<blockquote>
<p><em>Scrivi un programma Go per calcolare il fattoriale di un numero, salvalo come factorial.go nella tua workspace</em></p>
</blockquote>
<blockquote>
<p><em>Cerca nella mia cartella summer_pictures tutti i file JPG, rinominali con la data odierna e salva la lista dei file rinominati in photos_list.txt</em></p>
</blockquote>
<blockquote>
<p><em>Cerca online film di fantascienza popolari del 2024 e scegline tre da guardare stasera. Salva la lista in movie_night.txt.</em></p>
</blockquote>
<blockquote>
<p><em>Cerca sul web gli ultimi articoli di notizie sull'IA dal 2025, seleziona tre e scrivi uno script Python per estrarre i titoli e i riassunti. Salva lo script come news_scraper.py e i riassunti in ai_news.txt in /home/projects</em></p>
</blockquote>
<blockquote>
<p><em>Venerdì, cerca sul web una API gratuita per i prezzi delle azioni, registrati con supersuper7434567@gmail.com, quindi scrivi uno script Python per recuperare tramite l'API i prezzi giornalieri di Tesla e salva i risultati in stock_prices.csv</em></p>
</blockquote>
<p><em>Nota che le capacità di compilazione moduli sono ancora sperimentali e potrebbero fallire.</em></p>
<p>Dopo aver digitato la tua richiesta, AgenticSeek assegnerà il miglior agente per il compito.</p>
<p>Poiché si tratta di un prototipo iniziale, il sistema di instradamento degli agenti potrebbe non sempre assegnare l'agente giusto in base alla tua richiesta.</p>
<p>Pertanto, dovresti essere molto esplicito su ciò che desideri e su come l'IA potrebbe procedere; ad esempio, se vuoi che effettui una ricerca web, non dire:</p>
<p><code>Conosci alcuni buoni paesi per viaggiare da soli?</code></p>
<p>Invece, chiedi:</p>
<p><code>Fai una ricerca web e scopri quali sono i migliori paesi per viaggiare da soli</code></p>
<hr />
<h2><strong>Configurazione per eseguire l'LLM sul tuo server</strong></h2>
<p>Se disponi di un computer potente o di un server che puoi utilizzare, ma vuoi usarlo dal tuo portatile, hai la possibilità di eseguire l'LLM su un server remoto utilizzando il nostro llm server personalizzato.</p>
<p>Sul tuo &quot;server&quot; che eseguirà il modello AI, ottieni l'indirizzo IP</p>
<pre><code class="language-sh">ip a | grep &quot;inet &quot; | grep -v 127.0.0.1 | awk '{print $2}' | cut -d/ -f1 # ip locale
curl https://ipinfo.io/ip # ip pubblico
</code></pre>
<p>Nota: Per Windows o macOS, utilizza rispettivamente ipconfig o ifconfig per trovare l'indirizzo IP.</p>
<p>Clona il repository ed entra nella cartella <code>server/</code>.</p>
<pre><code class="language-sh">git clone --depth 1 https://github.com/Fosowl/agenticSeek.git
cd agenticSeek/llm_server/
</code></pre>
<p>Installa i requisiti specifici del server:</p>
<pre><code class="language-sh">pip3 install -r requirements.txt
</code></pre>
<p>Esegui lo script server.</p>
<pre><code class="language-sh">python3 app.py --provider ollama --port 3333
</code></pre>
<p>Puoi scegliere tra utilizzare <code>ollama</code> e <code>llamacpp</code> come servizio LLM.</p>
<p>Ora sul tuo computer personale:</p>
<p>Modifica il file <code>config.ini</code> per impostare <code>provider_name</code> su <code>server</code> e <code>provider_model</code> su <code>deepseek-r1:xxb</code>.
Imposta <code>provider_server_address</code> sull'indirizzo IP della macchina che eseguirà il modello.</p>
<pre><code class="language-sh">[MAIN]
is_local = False
provider_name = server
provider_model = deepseek-r1:70b
provider_server_address = x.x.x.x:3333
</code></pre>
<p>Prossimo passo: <a href="#Start-services-and-Run">Avvia servizi ed esegui AgenticSeek</a></p>
<hr />
<h2>Speech to Text</h2>
<p>Attenzione: la funzione speech to text funziona solo in modalità CLI al momento.</p>
<p>Nota che attualmente il riconoscimento vocale funziona solo in inglese.</p>
<p>La funzionalità di speech-to-text è disabilitata per impostazione predefinita. Per abilitarla, imposta l'opzione listen su True nel file config.ini:</p>
<pre><code>listen = True
</code></pre>
<p>Quando abilitata, la funzione speech-to-text ascolta una parola chiave di attivazione, che è il nome dell'agente, prima di iniziare a elaborare il tuo input. Puoi personalizzare il nome dell'agente aggiornando il valore <code>agent_name</code> nel file <em>config.ini</em>:</p>
<pre><code>agent_name = Friday
</code></pre>
<p>Per un riconoscimento ottimale, si consiglia di utilizzare un nome inglese comune come &quot;John&quot; o &quot;Emma&quot; come nome dell'agente</p>
<p>Una volta che vedrai iniziare ad apparire la trascrizione, pronuncia ad alta voce il nome dell’agente per attivarlo (es. &quot;Friday&quot;).</p>
<p>Pronuncia chiaramente la tua richiesta.</p>
<p>Termina la richiesta con una frase di conferma per segnalare al sistema di procedere. Esempi di frasi di conferma includono:</p>
<pre><code>&quot;do it&quot;, &quot;go ahead&quot;, &quot;execute&quot;, &quot;run&quot;, &quot;start&quot;, &quot;thanks&quot;, &quot;would ya&quot;, &quot;please&quot;, &quot;okay?&quot;, &quot;proceed&quot;, &quot;continue&quot;, &quot;go on&quot;, &quot;do that&quot;, &quot;go it&quot;, &quot;do you understand?&quot;
</code></pre>
<h2>Config</h2>
<p>Esempio di configurazione:</p>
<pre><code>[MAIN]
is_local = True
provider_name = ollama
provider_model = deepseek-r1:32b
provider_server_address = http://127.0.0.1:11434 # Esempio per Ollama; usa http://127.0.0.1:1234 per LM-Studio
agent_name = Friday
recover_last_session = False
save_session = False
speak = False
listen = False

jarvis_personality = False
languages = en zh # Elenco delle lingue per TTS e potenzialmente per l’instradamento.
[BROWSER]
headless_browser = False
stealth_mode = False
</code></pre>
<p><strong>Spiegazione delle impostazioni di <code>config.ini</code>:</strong></p>
<ul>
<li><strong>Sezione <code>[MAIN]</code>:</strong>
<ul>
<li><code>is_local</code>: <code>True</code> se si utilizza un provider LLM locale (Ollama, LM-Studio, server locale compatibile OpenAI) o l’opzione server self-hosted. <code>False</code> se si utilizza un’API cloud (OpenAI, Google, ecc.).</li>
<li><code>provider_name</code>: Specifica il provider LLM.
<ul>
<li>Opzioni locali: <code>ollama</code>, <code>lm-studio</code>, <code>openai</code> (per server locali compatibili OpenAI), <code>server</code> (per il setup self-hosted).</li>
<li>Opzioni API: <code>openai</code>, <code>google</code>, <code>deepseek</code>, <code>huggingface</code>, <code>togetherAI</code>.</li>
</ul>
</li>
<li><code>provider_model</code>: Il nome o ID specifico del modello per il provider scelto (es. <code>deepseekcoder:6.7b</code> per Ollama, <code>gpt-3.5-turbo</code> per OpenAI API, <code>mistralai/Mixtral-8x7B-Instruct-v0.1</code> per TogetherAI).</li>
<li><code>provider_server_address</code>: L’indirizzo del tuo provider LLM.
<ul>
<li>Per provider locali: es. <code>http://127.0.0.1:11434</code> per Ollama, <code>http://127.0.0.1:1234</code> per LM-Studio.</li>
<li>Per il tipo <code>server</code>: l’indirizzo del tuo server LLM self-hosted (es. <code>http://your_server_ip:3333</code>).</li>
<li>Per API cloud (<code>is_local = False</code>): spesso viene ignorato o può essere lasciato vuoto, dato che l’endpoint API è solitamente gestito dalla libreria client.</li>
</ul>
</li>
<li><code>agent_name</code>: Nome dell’assistente AI (es. Friday). Utilizzato come parola di attivazione per il riconoscimento vocale se abilitato.</li>
<li><code>recover_last_session</code>: <code>True</code> per tentare di ripristinare lo stato della sessione precedente, <code>False</code> per iniziare da zero.</li>
<li><code>save_session</code>: <code>True</code> per salvare lo stato della sessione corrente per un eventuale recupero, <code>False</code> altrimenti.</li>
<li><code>speak</code>: <code>True</code> per abilitare l’output vocale tramite sintesi vocale, <code>False</code> per disabilitarlo.</li>
<li><code>listen</code>: <code>True</code> per abilitare l’input vocale tramite riconoscimento vocale (solo modalità CLI), <code>False</code> per disabilitarlo.</li>
<li><code>work_dir</code>: <strong>Cruciale:</strong> La directory dove AgenticSeek leggerà/scriverà i file. <strong>Assicurati che questo percorso sia valido e accessibile dal tuo sistema.</strong></li>
<li><code>jarvis_personality</code>: <code>True</code> per utilizzare un prompt di sistema più in stile &quot;Jarvis&quot; (sperimentale), <code>False</code> per il prompt standard.</li>
<li><code>languages</code>: Un elenco separato da virgole di lingue (es. <code>en, zh, fr</code>). Usato per la selezione della voce TTS (predefinita la prima) e può aiutare l’instradatore LLM. Evita troppe lingue o lingue molto simili per efficienza.</li>
</ul>
</li>
<li><strong>Sezione <code>[BROWSER]</code>:</strong>
<ul>
<li><code>headless_browser</code>: <code>True</code> per eseguire il browser automatizzato senza finestra visibile (consigliato per interfaccia web o uso non interattivo). <code>False</code> per mostrare la finestra del browser (utile in modalità CLI o per debug).</li>
<li><code>stealth_mode</code>: <code>True</code> per abilitare misure che rendono più difficile rilevare l’automazione del browser. Potrebbe richiedere l’installazione manuale di estensioni come anticaptcha.</li>
</ul>
</li>
</ul>
<p>Questa sezione riassume i tipi di provider LLM supportati. Configurali in <code>config.ini</code>.</p>
<p><strong>Provider Locali (In esecuzione sull’hardware locale):</strong></p>
<p>| Nome provider in <code>config.ini</code> | <code>is_local</code> | Descrizione                                                             | Sezione Setup                                                                 |
|-------------------------------|------------|-------------------------------------------------------------------------|-------------------------------------------------------------------------------|
| <code>ollama</code>                      | <code>True</code>     | Usa Ollama per servire LLM locali.                                      | <a href="#setup-for-running-llm-locally-on-your-machine">Setup per eseguire LLM localmente</a> |
| <code>lm-studio</code>                   | <code>True</code>     | Usa LM-Studio per servire LLM locali.                                   | <a href="#setup-for-running-llm-locally-on-your-machine">Setup per eseguire LLM localmente</a> |
| <code>openai</code> (per server locale)  | <code>True</code>     | Connette a un server locale che espone una API compatibile OpenAI (es. llama.cpp). | <a href="#setup-for-running-llm-locally-on-your-machine">Setup per eseguire LLM localmente</a> |
| <code>server</code>                      | <code>False</code>    | Connette al server LLM self-hosted AgenticSeek in esecuzione su un’altra macchina. | <a href="#setup-to-run-the-llm-on-your-own-server">Setup per eseguire LLM su proprio server</a> |</p>
<p><strong>Provider API (Cloud):</strong></p>
<p>| Nome provider in <code>config.ini</code> | <code>is_local</code> | Descrizione                                        | Sezione Setup                                      |
|-------------------------------|------------|----------------------------------------------------|----------------------------------------------------|
| <code>openai</code>                      | <code>False</code>    | Usa l’API ufficiale di OpenAI (es. GPT-3.5, GPT-4). | <a href="#setup-to-run-with-an-api">Setup per l'uso con API</a> |
| <code>google</code>                      | <code>False</code>    | Usa i modelli Gemini di Google tramite API.         | <a href="#setup-to-run-with-an-api">Setup per l'uso con API</a> |
| <code>deepseek</code>                    | <code>False</code>    | Usa l’API ufficiale Deepseek.                       | <a href="#setup-to-run-with-an-api">Setup per l'uso con API</a> |
| <code>huggingface</code>                 | <code>False</code>    | Usa l’Hugging Face Inference API.                   | <a href="#setup-to-run-with-an-api">Setup per l'uso con API</a> |
| <code>togetherAI</code>                  | <code>False</code>    | Usa l’API TogetherAI per vari modelli open.         | <a href="#setup-to-run-with-an-api">Setup per l'uso con API</a> |</p>
<hr />
<h2>Risoluzione dei problemi</h2>
<p>Se incontri problemi, questa sezione fornisce indicazioni.</p>
<h1>Problemi noti</h1>
<h2>Problemi con ChromeDriver</h2>
<p><strong>Esempio di errore:</strong> <code>SessionNotCreatedException: Message: session not created: This version of ChromeDriver only supports Chrome version XXX</code></p>
<ul>
<li><strong>Causa:</strong> La versione di ChromeDriver installata non è compatibile con la versione del browser Google Chrome.</li>
<li><strong>Soluzione:</strong>
<ol>
<li><strong>Verifica versione Chrome:</strong> Apri Google Chrome, vai su <code>Impostazioni &gt; Informazioni su Chrome</code> per trovare la tua versione (es. &quot;Versione 120.0.6099.110&quot;).</li>
<li><strong>Scarica la versione corrispondente di ChromeDriver:</strong>
<ul>
<li>Per versioni Chrome 115 e successive: Vai ai <a href="https://googlechromelabs.github.io/chrome-for-testing/">Chrome for Testing (CfT) JSON Endpoints</a>. Trova il canale &quot;stable&quot; e scarica ChromeDriver per il tuo sistema operativo che corrisponde alla major version di Chrome.</li>
<li>Per versioni precedenti (meno comuni): Potresti trovarle sulla pagina <a href="https://chromedriver.chromium.org/downloads">ChromeDriver - WebDriver for Chrome</a>.</li>
<li>L'immagine sotto mostra un esempio dalla pagina CfT:
<img src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/chromedriver_readme.png" alt="Scarica una versione specifica di Chromedriver dalla pagina Chrome for Testing" /></li>
</ul>
</li>
<li><strong>Installa ChromeDriver:</strong>
<ul>
<li>Assicurati che il file <code>chromedriver</code> scaricato (o <code>chromedriver.exe</code> su Windows) sia posizionato in una directory presente nella variabile di ambiente PATH del sistema (es. <code>/usr/local/bin</code> su Linux/macOS, o una cartella scripts personalizzata aggiunta a PATH su Windows).</li>
<li>In alternativa, posizionalo nella directory principale del progetto <code>agenticSeek</code>.</li>
<li>Assicurati che il driver sia eseguibile (es. <code>chmod +x chromedriver</code> su Linux/macOS).</li>
</ul>
</li>
<li>Consulta la sezione <a href="#chromedriver-installation">Installazione di ChromeDriver</a> nella guida di Installazione principale per maggiori dettagli.</li>
</ol>
</li>
</ul>
<p>Se questa sezione è incompleta o incontri altri problemi con ChromeDriver, valuta la possibilità di cercare tra le <a href="https://github.com/Fosowl/agenticSeek/issues">Issue GitHub esistenti</a> o aprirne una nuova.</p>
<p><code>Exception: Failed to initialize browser: Message: session not created: This version of ChromeDriver only supports Chrome version 113 Current browser version is 134.0.6998.89 with binary path</code></p>
<p>Questo accade se c’è una discrepanza tra la versione del browser e quella di chromedriver.</p>
<p>Devi andare a scaricare l’ultima versione:</p>
<p>https://developer.chrome.com/docs/chromedriver/downloads</p>
<p>Se usi Chrome versione 115 o superiore vai su:</p>
<p>https://googlechromelabs.github.io/chrome-for-testing/</p>
<p>E scarica la versione di chromedriver corrispondente al tuo sistema operativo.</p>
<p><img src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/chromedriver_readme.png" alt="alt text" /></p>
<p>Se questa sezione è incompleta ti preghiamo di aprire una issue.</p>
<h2>Problemi con connection adapters</h2>
<pre><code>Exception: Provider lm-studio failed: HTTP request failed: No connection adapters were found for '127.0.0.1:1234/v1/chat/completions'` (Nota: la porta può variare)
</code></pre>
<ul>
<li><strong>Causa:</strong> Il campo <code>provider_server_address</code> in <code>config.ini</code> per <code>lm-studio</code> (o altri server locali compatibili OpenAI) manca del prefisso <code>http://</code> o punta alla porta sbagliata.</li>
<li><strong>Soluzione:</strong>
<ul>
<li>Assicurati che l’indirizzo includa <code>http://</code>. LM-Studio di solito usa <code>http://127.0.0.1:1234</code>.</li>
<li>Correggi <code>config.ini</code>: <code>provider_server_address = http://127.0.0.1:1234</code> (o la porta effettiva del tuo server LM-Studio).</li>
</ul>
</li>
</ul>
<h2>Base URL di SearxNG non fornito</h2>
<pre><code>raise ValueError(&quot;SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.&quot;)
ValueError: SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.`
</code></pre>
<h2>FAQ</h2>
<p><strong>D: Che hardware mi serve?</strong></p>
<p>| Dimensione modello | GPU       | Commento                                                                                          |
|--------------------|-----------|---------------------------------------------------------------------------------------------------|
| 7B                 | 8GB Vram  | ⚠️ Sconsigliato. Prestazioni scarse, allucinazioni frequenti, agenti planner probabilmente falliranno. |
| 14B                | 12 GB VRAM (es. RTX 3060) | ✅ Usabile per compiti semplici. Può avere difficoltà con navigazione web e pianificazione.      |
| 32B                | 24+ GB VRAM (es. RTX 4090) | 🚀 Successo nella maggior parte dei task, può ancora avere problemi nel task planning           |
| 70B+               | 48+ GB Vram | 💪 Eccellente. Raccomandato per casi d’uso avanzati.                                             |</p>
<p><strong>D: Ottengo un errore, cosa devo fare?</strong></p>
<p>Assicurati che il locale sia in esecuzione (<code>ollama serve</code>), che il tuo <code>config.ini</code> corrisponda al provider e che le dipendenze siano installate. Se nulla funziona sentiti libero di aprire una issue.</p>
<p><strong>D: Può davvero funzionare 100% in locale?</strong></p>
<p>Sì, con i provider Ollama, lm-studio o server, tutto il riconoscimento vocale, LLM e sintesi vocale avvengono in locale. Le opzioni non locali (OpenAI o altre API) sono opzionali.</p>
<p><strong>D: Perché dovrei usare AgenticSeek quando ho Manus?</strong></p>
<p>A differenza di Manus, AgenticSeek dà priorità all’indipendenza dai sistemi esterni, offrendoti più controllo, privacy e risparmiando sui costi API.</p>
<p><strong>D: Chi c’è dietro il progetto?</strong></p>
<p>Il progetto è stato creato da me, insieme a due amici che sono manutentori e contributor dalla community open-source su GitHub. Siamo solo un gruppo di appassionati, non una startup né affiliati ad alcuna organizzazione.</p>
<p>Qualsiasi account AgenticSeek su X diverso dal mio personale (https://x.com/Martin993886460) è un’impersonificazione.</p>
<h2>Contribuisci</h2>
<p>Cerchiamo sviluppatori per migliorare AgenticSeek! Dai un’occhiata alle issue aperte o alle discussioni.</p>
<p><a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/docs/CONTRIBUTING.md">Guida al contributo</a></p>
<p><a href="https://www.star-history.com/#Fosowl/agenticSeek&amp;Date"><img src="https://api.star-history.com/svg?repos=Fosowl/agenticSeek&amp;type=Date" alt="Star History Chart" /></a></p>
<h2>Manutentori:</h2>
<blockquote>
<p><a href="https://github.com/Fosowl">Fosowl</a> | Fuso orario Parigi</p>
</blockquote>
<blockquote>
<p><a href="https://github.com/antoineVIVIES">antoineVIVIES</a> | Fuso orario Taipei</p>
</blockquote>
<blockquote>
<p><a href="https://github.com/steveh8758">steveh8758</a> | Fuso orario Taipei</p>
</blockquote>
<h2>Ringraziamenti speciali:</h2>
<blockquote>
<p><a href="https://github.com/tcsenpai">tcsenpai</a> e <a href="https://github.com/plitc">plitc</a> per l’aiuto con la dockerizzazione backend</p>
</blockquote>
<h2>Sponsor:</h2>
<p>Sponsor mensili da 5$ o più qui:</p>
<ul>
<li><strong>tatra-labs</strong>
Certainly! Please provide the content of Part 4 of 4 that you would like me to translate into Italian.</li>
</ul>
<hr />
<p>Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx">Open Ai Tx</a> | Last indexed: 2025-06-16</p>
<hr />

        </div>
    </div>
    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Default Statcounter code for openaitx
https://openaitx.github.io/ -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>
    <!-- End of Statcounter Code -->
</body>
</html>