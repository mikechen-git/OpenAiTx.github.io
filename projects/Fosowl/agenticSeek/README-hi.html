<!DOCTYPE html>
<html lang="hi">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>agenticSeek - Fosowl/agenticSeek</title>
    <meta name="title" content="agenticSeek - Fosowl/agenticSeek">
    <meta name="description" content="Fosowl/agenticSeek - GitHub repository hi documentation and informationAgenticSeek: निजी, स्थानीय Manus विकल्प English | 中文 | 繁體中文 | Français | 日本語 | Português (Brasil) | Español एक 100% स्थानीय Manus AI विकल्प, यह वॉयस-सक्षम AI सहायक स्वायत्त रूप से वेब ब्राउज़ करता है, कोड लिखता है, और कार्यों की योजना बनाता है, जबकि सभी डेटा आपके डिवाइस पर रहता है। स्थानीय रीजनिंग मॉडल्स के लिए अनुकूलित, यह पूरी तरह से आपके हार्डवेयर पर चलता है, जिससे पूर्ण गोपनीयता और शून्य क्लाउड निर्भरता सुनिश्चित होती है। AgenticSeek क्यों? 🔒 पूरी तरह स्थानीय और निजी - सब कुछ आपके कंप्यूटर पर चलता है — कोई क्लाउड नहीं, कोई डेटा साझा नहीं। आपकी फाइलें, बातचीत और खोजें निजी रहती हैं। 🌐 स्मार्ट वेब ब्राउज़िंग - AgenticSeek खुद से इंटरनेट ब्राउज़ कर सकता है — खोज, पढ़ना, जानकारी निकालना, वेब फॉर्म भरना — सब कुछ हैंड्स-फ्री। 💻 स्वायत्त कोडिंग सहायक - कोड चाहिए? यह Python, C, Go, Java और अन्य में...">
    <meta name="keywords" content="Fosowl, agenticSeek, GitHub, repository, hi documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/Fosowl/agenticSeek/README-hi.html">
    <meta property="og:title" content="agenticSeek - Fosowl/agenticSeek">
    <meta property="og:description" content="Fosowl/agenticSeek - GitHub repository hi documentation and information">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">
    <script src="/js/marked.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        body { display: flex; flex-direction: column; min-height: 100vh; }
        .main-container { margin: 0 auto; width: 100%; max-width: 980px; padding: 0 20px; }
        @media (max-width: 768px) { .main-container { padding: 0 15px; } }
        .markdown-body img { max-width: 100%; height: auto; }
        .header { text-align: center; margin-bottom: 30px; padding: 20px; background-color: #f6f8fa; border-bottom: 1px solid #e1e4e8; position: relative; }
        .header .links { margin-top: 10px; font-size: 16px; }
        .header .links a { color: #0366d6; text-decoration: none; margin-left: 5px; }
        .header .links a:hover { text-decoration: underline; }
        .language-badges { margin-top: 15px; text-align: center; }
        .language-badges a { display: inline-block; margin: 2px; text-decoration: none; }
        .language-badges img { height: 20px; border-radius: 3px; }
        .language-badges a:hover img { opacity: 0.8; }
    </style>
</head>
<body>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/Fosowl/agenticSeek" id="githubRepoLink" target="_blank">Fosowl/agenticSeek</a>
<br>
<h1 style="display: none;">AgenticSeek: निजी, स्थानीय Manus विकल्प English | 中文 | 繁體中文 | Français | 日本語 | Português (Brasil) | Español एक 100% स्थानीय Manus AI विकल्प, यह वॉयस-सक्षम AI सहायक स्वायत्त रूप से वेब ब्राउज़ करता है, कोड लिखता है, और कार्यों की योजना बनाता है, जबकि सभी डेटा आपके डिवाइस पर रहता है। स्थानीय रीजनिंग मॉडल्स के लिए अनुकूलित, यह पूरी तरह से आपके हार्डवेयर पर चलता है, जिससे पूर्ण गोपनीयता और शून्य क्लाउड निर्भरता सुनिश्चित होती है। AgenticSeek क्यों? 🔒 पूरी तरह स्थानीय और निजी - सब कुछ आपके कंप्यूटर पर चलता है — कोई क्लाउड नहीं, कोई डेटा साझा नहीं। आपकी फाइलें, बातचीत और खोजें निजी रहती हैं। 🌐 स्मार्ट वेब ब्राउज़िंग - AgenticSeek खुद से इंटरनेट ब्राउज़ कर सकता है — खोज, पढ़ना, जानकारी निकालना, वेब फॉर्म भरना — सब कुछ हैंड्स-फ्री। 💻 स्वायत्त कोडिंग सहायक - कोड चाहिए? यह Python, C, Go, Java और अन्य में...</h1>
        </div>
        <div class="language-badges" id="languageBadges">
            <!-- You can generate language badges here if needed -->
        </div>
    </div>
    <div class="main-container">
        <div class="markdown-body" id="content">
            <h1>AgenticSeek: निजी, स्थानीय Manus विकल्प</h1>
<p align="center">
<img align="center" src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/agentic_seek_logo.png" width="300" height="300" alt="Agentic Seek Logo">
<p>
<p>English | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_CHS.md">中文</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_CHT.md">繁體中文</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_FR.md">Français</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_JP.md">日本語</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_PTBR.md">Português (Brasil)</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_ES.md">Español</a></p>
<p><em>एक <strong>100% स्थानीय Manus AI विकल्प</strong>, यह वॉयस-सक्षम AI सहायक स्वायत्त रूप से वेब ब्राउज़ करता है, कोड लिखता है, और कार्यों की योजना बनाता है, जबकि सभी डेटा आपके डिवाइस पर रहता है। स्थानीय रीजनिंग मॉडल्स के लिए अनुकूलित, यह पूरी तरह से आपके हार्डवेयर पर चलता है, जिससे पूर्ण गोपनीयता और शून्य क्लाउड निर्भरता सुनिश्चित होती है।</em></p>
<p><a href="https://fosowl.github.io/agenticSeek.html"><img src="https://img.shields.io/static/v1?label=Website&amp;message=AgenticSeek&amp;color=blue&amp;style=flat-square" alt="Visit AgenticSeek" /></a> <img src="https://img.shields.io/badge/license-GPL--3.0-green" alt="License" /> <a href="https://discord.gg/8hGDaME3TC"><img src="https://img.shields.io/badge/Discord-Join%20Us-7289DA?logo=discord&amp;logoColor=white" alt="Discord" /></a> <a href="https://x.com/Martin993886460"><img src="https://img.shields.io/twitter/url/https/twitter.com/fosowl.svg?style=social&amp;label=Update%20%40Fosowl" alt="Twitter" /></a> <a href="https://github.com/Fosowl/agenticSeek/stargazers"><img src="https://img.shields.io/github/stars/Fosowl/agenticSeek?style=social" alt="GitHub stars" /></a></p>
<h3>AgenticSeek क्यों?</h3>
<ul>
<li><p>🔒 पूरी तरह स्थानीय और निजी - सब कुछ आपके कंप्यूटर पर चलता है — कोई क्लाउड नहीं, कोई डेटा साझा नहीं। आपकी फाइलें, बातचीत और खोजें निजी रहती हैं।</p>
</li>
<li><p>🌐 स्मार्ट वेब ब्राउज़िंग - AgenticSeek खुद से इंटरनेट ब्राउज़ कर सकता है — खोज, पढ़ना, जानकारी निकालना, वेब फॉर्म भरना — सब कुछ हैंड्स-फ्री।</p>
</li>
<li><p>💻 स्वायत्त कोडिंग सहायक - कोड चाहिए? यह Python, C, Go, Java और अन्य में प्रोग्राम लिख, डिबग और चला सकता है — बिना निगरानी के।</p>
</li>
<li><p>🧠 स्मार्ट एजेंट चयन - आप पूछते हैं, यह अपने आप सबसे उपयुक्त एजेंट चुनता है। जैसे विशेषज्ञों की टीम हमेशा मदद के लिए तैयार हो।</p>
</li>
<li><p>📋 जटिल कार्यों की योजना और निष्पादन - ट्रिप प्लानिंग से लेकर जटिल परियोजनाओं तक — यह बड़े कार्यों को चरणों में विभाजित कर कई AI एजेंट्स की मदद से पूरा कर सकता है।</p>
</li>
<li><p>🎙️ वॉयस-सक्षम - साफ, तेज, भविष्यवादी वॉयस और स्पीच-टू-टेक्स्ट जिससे आप इससे ऐसे बात कर सकते हैं जैसे यह आपकी पर्सनल साइ-फाई AI हो। (विकासाधीन)</p>
</li>
</ul>
<h3><strong>डेमो</strong></h3>
<blockquote>
<p><em>क्या आप agenticSeek प्रोजेक्ट को खोज सकते हैं, जान सकते हैं किन स्किल्स की जरूरत है, फिर CV_candidates.zip खोल सकते हैं और बता सकते हैं कि कौन सा सबसे अच्छा मेल खाता है प्रोजेक्ट के लिए</em></p>
</blockquote>
<p>https://github.com/user-attachments/assets/b8ca60e9-7b3b-4533-840e-08f9ac426316</p>
<p>डिस्क्लेमर: यह डेमो, जिसमें सभी फाइलें भी (जैसे: CV_candidates.zip), पूरी तरह काल्पनिक हैं। हम कोई कंपनी नहीं हैं, हम ओपन-सोर्स योगदानकर्ता ढूंढ रहे हैं, उम्मीदवार नहीं।</p>
<blockquote>
<p>🛠⚠️️ <strong>सक्रिय कार्य प्रगति पर है</strong></p>
</blockquote>
<blockquote>
<p>🙏 यह प्रोजेक्ट एक साइड-प्रोजेक्ट के रूप में शुरू हुआ था और इसका कोई रोडमैप या फंडिंग नहीं है। यह मेरी अपेक्षा से कहीं अधिक बढ़ गया है और GitHub Trending तक पहुँच गया है। योगदान, फीडबैक और धैर्य की अत्यंत सराहना की जाती है।</p>
</blockquote>
<h2>पूर्वापेक्षाएँ</h2>
<p>शुरू करने से पहले, सुनिश्चित करें कि आपके पास निम्नलिखित सॉफ़्टवेयर इंस्टॉल हैं:</p>
<ul>
<li><strong>Git:</strong> रिपॉजिटरी क्लोन करने के लिए। <a href="https://git-scm.com/downloads">Git डाउनलोड करें</a></li>
<li><strong>Python 3.10.x:</strong> हम दृढ़ता से सलाह देते हैं कि आप Python का 3.10.x संस्करण उपयोग करें। अन्य संस्करणों के उपयोग से डिपेंडेंसी त्रुटियाँ आ सकती हैं। <a href="https://www.python.org/downloads/release/python-3100/">Python 3.10 डाउनलोड करें</a> (कोई भी 3.10.x संस्करण चुनें)।</li>
<li><strong>Docker Engine &amp; Docker Compose:</strong> बंडल्ड सेवाओं जैसे SearxNG चलाने के लिए।
<ul>
<li>Docker Desktop इंस्टॉल करें (जिसमें Docker Compose V2 शामिल है): <a href="https://docs.docker.com/desktop/install/windows-install/">Windows</a> | <a href="https://docs.docker.com/desktop/install/mac-install/">Mac</a> | <a href="https://docs.docker.com/desktop/install/linux-install/">Linux</a></li>
<li>वैकल्पिक रूप से, Linux पर Docker Engine और Docker Compose अलग से इंस्टॉल करें: <a href="https://docs.docker.com/engine/install/">Docker Engine</a> | <a href="https://docs.docker.com/compose/install/">Docker Compose</a> (सुनिश्चित करें कि आप Compose V2 इंस्टॉल कर रहे हैं, जैसे, <code>sudo apt-get install docker-compose-plugin</code>)।</li>
</ul>
</li>
</ul>
<h3>1. <strong>रिपॉजिटरी क्लोन करें और सेटअप करें</strong></h3>
<pre><code class="language-sh">git clone https://github.com/Fosowl/agenticSeek.git
cd agenticSeek
mv .env.example .env
</code></pre>
<h3>2. .env फ़ाइल की सामग्री बदलें</h3>
<pre><code class="language-sh">SEARXNG_BASE_URL=&quot;http://127.0.0.1:8080&quot;
REDIS_BASE_URL=&quot;redis://redis:6379/0&quot;
WORK_DIR=&quot;/Users/mlg/Documents/workspace_for_ai&quot;
OLLAMA_PORT=&quot;11434&quot;
LM_STUDIO_PORT=&quot;1234&quot;
CUSTOM_ADDITIONAL_LLM_PORT=&quot;11435&quot;
OPENAI_API_KEY='optional'
DEEPSEEK_API_KEY='optional'
OPENROUTER_API_KEY='optional'
TOGETHER_API_KEY='optional'
GOOGLE_API_KEY='optional'
ANTHROPIC_API_KEY='optional'
</code></pre>
<p>आवश्यकतानुसार <code>.env</code> फ़ाइल को अपने मूल्यों के साथ अपडेट करें:</p>
<ul>
<li><strong>SEARXNG_BASE_URL</strong>: अपरिवर्तित छोड़ें</li>
<li><strong>REDIS_BASE_URL</strong>: अपरिवर्तित छोड़ें</li>
<li><strong>WORK_DIR</strong>: आपके लोकल मशीन पर कार्य निर्देशिका का पथ। AgenticSeek इन फाइलों को पढ़ और इंटरैक्ट कर सकेगा।</li>
<li><strong>OLLAMA_PORT</strong>: Ollama सेवा के लिए पोर्ट नंबर।</li>
<li><strong>LM_STUDIO_PORT</strong>: LM Studio सेवा के लिए पोर्ट नंबर।</li>
<li><strong>CUSTOM_ADDITIONAL_LLM_PORT</strong>: किसी भी अतिरिक्त कस्टम LLM सेवा के लिए पोर्ट।</li>
</ul>
<p><strong>API Key पूरी तरह वैकल्पिक हैं, यदि आप LLM को स्थानीय रूप से चलाना चुनते हैं, जो इस प्रोजेक्ट का मुख्य उद्देश्य है। यदि आपके पास पर्याप्त हार्डवेयर है तो खाली छोड़ दें।</strong></p>
<h3>3. <strong>Docker प्रारंभ करें</strong></h3>
<p>सुनिश्चित करें कि आपके सिस्टम पर Docker इंस्टॉल और चालू है। आप निम्नलिखित कमांड से Docker शुरू कर सकते हैं:</p>
<ul>
<li><p><strong>Linux/macOS पर:</strong><br />
एक टर्मिनल खोलें और चलाएँ:</p>
<pre><code class="language-sh">sudo systemctl start docker
</code></pre>
<p>या यदि इंस्टॉल है तो अपने एप्लिकेशन मेनू से Docker Desktop लॉन्च करें।</p>
</li>
<li><p><strong>Windows पर:</strong><br />
Start मेनू से Docker Desktop शुरू करें।</p>
</li>
</ul>
<p>आप यह सत्यापित कर सकते हैं कि Docker चल रहा है, निम्नलिखित चलाकर:</p>
<pre><code class="language-sh">docker info
</code></pre>
<p>यदि आपको अपने Docker इंस्टॉलेशन की जानकारी दिखती है, तो यह सही तरीके से चल रहा है।</p>
<p><a href="#list-of-local-providers">स्थानीय प्रदाताओं की तालिका</a> नीचे देखें।</p>
<p>अगला चरण: <a href="#start-services-and-run">AgenticSeek को स्थानीय रूप से चलाएँ</a></p>
<p><em>यदि आपको समस्याएँ आ रही हैं तो <a href="#troubleshooting">Troubleshooting</a> अनुभाग देखें।</em>
<em>यदि आपका हार्डवेयर LLMs को स्थानीय रूप से नहीं चला सकता, तो <a href="#setup-to-run-with-an-api">API के साथ चलाने का सेटअप</a> देखें।</em>
<em>विस्तृत <code>config.ini</code> स्पष्टीकरण के लिए <a href="#config">Config Section</a> देखें।</em></p>
<hr />
<h2>अपने कंप्यूटर पर LLM स्थानीय रूप से चलाने के लिए सेटअप</h2>
<p><strong>हार्डवेयर आवश्यकताएँ:</strong></p>
<p>स्थानीय रूप से LLMs चलाने के लिए, आपके पास पर्याप्त हार्डवेयर होना चाहिए। न्यूनतम रूप से, एक GPU चाहिए जो Magistral, Qwen या Deepseek 14B चला सके। विस्तृत मॉडल/प्रदर्शन अनुशंसाओं के लिए FAQ देखें।</p>
<p><strong>अपने स्थानीय प्रदाता को सेटअप करें</strong></p>
<p>अपने स्थानीय प्रदाता को शुरू करें, उदाहरण के लिए ollama के साथ:</p>
<pre><code class="language-sh">ollama serve
</code></pre>
<p>नीचे स्थानीय रूप से समर्थित प्रदाताओं की सूची देखें।</p>
<p><strong>config.ini अपडेट करें</strong></p>
<p>config.ini फ़ाइल को बदलें ताकि provider_name को समर्थित प्रदाता पर और provider_model को आपके प्रदाता द्वारा समर्थित LLM पर सेट करें। हम <em>Magistral</em> या <em>Deepseek</em> जैसे रीजनिंग मॉडल की सलाह देते हैं।</p>
<p>आवश्यक हार्डवेयर के लिए README के अंत में <strong>FAQ</strong> देखें।</p>
<pre><code class="language-sh">[MAIN]
is_local = True # जब आप स्थानीय रूप से या रिमोट प्रदाता के साथ चला रहे हों।
provider_name = ollama # या lm-studio, openai, आदि..
provider_model = deepseek-r1:14b # अपने हार्डवेयर के अनुसार मॉडल चुनें
provider_server_address = 127.0.0.1:11434
agent_name = Jarvis # अपने AI का नाम
recover_last_session = True # पिछला सत्र पुनः प्राप्त करना है या नहीं
save_session = True # वर्तमान सत्र याद रखना है या नहीं
speak = False # टेक्स्ट-टू-स्पीच
listen = False # स्पीच-टू-टेक्स्ट, केवल CLI के लिए, प्रयोगात्मक
jarvis_personality = False # &quot;Jarvis&quot; जैसी पर्सनैलिटी का उपयोग करना है या नहीं (प्रयोगात्मक)
languages = en zh # भाषाओं की सूची, टेक्स्ट-टू-स्पीच डिफ़ॉल्ट रूप से सूची की पहली भाषा चुनेगा
[BROWSER]
headless_browser = True # CLI पर होस्ट का उपयोग न कर रहे हों तो अपरिवर्तित छोड़ें।
stealth_mode = True # ब्राउज़र डिटेक्शन कम करने के लिए undetected selenium का उपयोग करें
</code></pre>
<p><strong>चेतावनी</strong>:</p>
<ul>
<li><p><code>config.ini</code> फ़ाइल फॉर्मेट टिप्पणियों (comments) का समर्थन नहीं करता।
उदाहरण कॉन्फ़िगरेशन को सीधे कॉपी-पेस्ट न करें, क्योंकि टिप्पणियाँ त्रुटियाँ पैदा करेंगी। इसके बजाय, अपनी इच्छित सेटिंग्स के साथ मैन्युअली <code>config.ini</code> फ़ाइल को संपादित करें, किसी भी टिप्पणी को हटाकर।</p>
</li>
<li><p>यदि आप LLMs चलाने के लिए LM-studio का उपयोग कर रहे हैं, तो provider_name को <code>openai</code> पर <em>सेट न करें</em>। इसे <code>lm-studio</code> पर सेट करें।</p>
</li>
<li><p>कुछ प्रदाता (जैसे: lm-studio) के लिए आपके IP के आगे <code>http://</code> होना आवश्यक है। उदाहरण: <code>http://127.0.0.1:1234</code></p>
</li>
</ul>
<p><strong>स्थानीय प्रदाताओं की सूची</strong></p>
<p>| प्रदाता      | स्थानीय? | विवरण                                                   |
|-------------|----------|---------------------------------------------------------|
| ollama      | हाँ      | ollama का उपयोग करके स्थानीय रूप से LLM चलाएँ           |
| lm-studio   | हाँ      | LM studio के साथ स्थानीय रूप से LLM चलाएँ (<code>provider_name</code> को <code>lm-studio</code> पर सेट करें)|
| openai      | हाँ      | OpenAI संगत API का उपयोग करें (जैसे: llama.cpp सर्वर)   |</p>
<p>अगला चरण: <a href="#Start-services-and-Run">AgenticSeek सेवाएँ शुरू करें और चलाएँ</a></p>
<p><em>यदि आपको समस्याएँ आ रही हैं तो <a href="#troubleshooting">Troubleshooting</a> अनुभाग देखें।</em>
<em>यदि आपका हार्डवेयर LLMs को स्थानीय रूप से नहीं चला सकता, तो <a href="#setup-to-run-with-an-api">API के साथ चलाने का सेटअप</a> देखें।</em>
<em>विस्तृत <code>config.ini</code> स्पष्टीकरण के लिए <a href="#config">Config Section</a> देखें।</em></p>
<h2>API के साथ चलाने के लिए सेटअप</h2>
<p>यह सेटअप बाहरी, क्लाउड-आधारित LLM प्रदाताओं का उपयोग करता है। आपको अपनी पसंदीदा सेवा से API कुंजी प्राप्त करनी होगी।</p>
<p><strong>1. API प्रदाता चुनें और API कुंजी प्राप्त करें:</strong></p>
<p>नीचे दी गई <a href="#list-of-api-providers">API प्रदाताओं की सूची</a> देखें। उनकी वेबसाइट पर जाकर साइन अप करें और API कुंजी प्राप्त करें।</p>
<p><strong>2. अपने API कुंजी को एनवायरनमेंट वेरिएबल के रूप में सेट करें:</strong></p>
<ul>
<li><strong>Linux/macOS:</strong>
अपनी टर्मिनल खोलें और <code>export</code> कमांड का उपयोग करें। इसे अपने शेल के प्रोफाइल फ़ाइल (जैसे, <code>~/.bashrc</code>, <code>~/.zshrc</code>) में जोड़ना सबसे अच्छा है ताकि यह स्थायी रहे।
<pre><code class="language-sh">export PROVIDER_API_KEY=&quot;your_api_key_here&quot; 
# PROVIDER_API_KEY को विशिष्ट वेरिएबल नाम से बदलें, जैसे OPENAI_API_KEY, GOOGLE_API_KEY
</code></pre>
TogetherAI के लिए उदाहरण:
<pre><code class="language-sh">export TOGETHER_API_KEY=&quot;xxxxxxxxxxxxxxxxxxxxxx&quot;
</code></pre>
</li>
<li><strong>Windows:</strong></li>
<li><strong>कमांड प्रॉम्प्ट (वर्तमान सत्र के लिए अस्थायी):</strong>
<pre><code class="language-cmd">set PROVIDER_API_KEY=your_api_key_here
</code></pre>
</li>
<li><strong>पावरशेल (वर्तमान सत्र के लिए अस्थायी):</strong>
<pre><code class="language-powershell">$env:PROVIDER_API_KEY=&quot;your_api_key_here&quot;
</code></pre>
</li>
<li><strong>स्थायी रूप से:</strong> विंडोज़ सर्च बार में &quot;environment variables&quot; खोजें, &quot;Edit the system environment variables&quot; पर क्लिक करें, फिर &quot;Environment Variables...&quot; बटन पर क्लिक करें। एक नया User variable उपयुक्त नाम (जैसे, <code>OPENAI_API_KEY</code>) और अपने key को value के रूप में जोड़ें।</li>
</ul>
<p><em>(अधिक जानकारी के लिए देखें FAQ: <a href="#how-do-i-set-api-keys">मैं API keys कैसे सेट करूं?</a>)</em></p>
<p><strong>3. <code>config.ini</code> अपडेट करें:</strong></p>
<pre><code class="language-ini">[MAIN]
is_local = False
provider_name = openai # या google, deepseek, togetherAI, huggingface
provider_model = gpt-3.5-turbo # या gemini-1.5-flash, deepseek-chat, mistralai/Mixtral-8x7B-Instruct-v0.1 आदि।
provider_server_address = # आमतौर पर is_local = False होने पर अधिकांश APIs के लिए अनदेखा या खाली छोड़ा जा सकता है
# ... अन्य सेटिंग्स ...
</code></pre>
<p><em>चेतावनी:</em> सुनिश्चित करें कि <code>config.ini</code> की values में कोई trailing spaces न हों।</p>
<p><strong>API प्रदाताओं की सूची</strong></p>
<p>| प्रदाता        | <code>provider_name</code>  | लोकल?   | विवरण                                               | API Key लिंक (उदाहरण)                         |
|---------------|------------------|---------|------------------------------------------------------|-----------------------------------------------|
| OpenAI        | <code>openai</code>         | नहीं    | OpenAI के API के माध्यम से ChatGPT मॉडल का उपयोग करें| <a href="https://platform.openai.com/signup">platform.openai.com/signup</a> |
| Google Gemini | <code>google</code>         | नहीं    | Google AI Studio के माध्यम से Google Gemini मॉडल का उपयोग करें | <a href="https://aistudio.google.com/keys">aistudio.google.com/keys</a> |
| Deepseek      | <code>deepseek</code>       | नहीं    | उनके API के माध्यम से Deepseek मॉडल का उपयोग करें   | <a href="https://platform.deepseek.com">platform.deepseek.com</a> |
| Hugging Face  | <code>huggingface</code>    | नहीं    | Hugging Face Inference API से मॉडल का उपयोग करें    | <a href="https://huggingface.co/settings/tokens">huggingface.co/settings/tokens</a> |
| TogetherAI    | <code>togetherAI</code>     | नहीं    | TogetherAI API के माध्यम से विभिन्न ओपन-सोर्स मॉडल का उपयोग करें | <a href="https://api.together.ai/settings/api-keys">api.together.ai/settings/api-keys</a> |</p>
<p><em>नोट:</em></p>
<ul>
<li>हम जटिल वेब ब्राउज़िंग और टास्क प्लानिंग के लिए <code>gpt-4o</code> या अन्य OpenAI मॉडल का उपयोग करने की सलाह नहीं देते हैं क्योंकि वर्तमान प्रॉम्प्ट ऑप्टिमाइजेशन Deepseek जैसे मॉडलों के लिए तैयार की गई है।</li>
<li>कोडिंग/बाश कार्यों में Gemini के साथ समस्या आ सकती है, क्योंकि यह Deepseek के लिए अनुकूलित फॉर्मेटिंग प्रॉम्प्ट का सख्ती से पालन नहीं कर सकता।</li>
<li><code>config.ini</code> की <code>provider_server_address</code> आमतौर पर तब उपयोग नहीं होती जब <code>is_local = False</code> होता है, क्योंकि API endpoint आमतौर पर संबंधित प्रदाता की लाइब्रेरी में हार्डकोडेड होती है।</li>
</ul>
<p>अगला चरण: <a href="#Start-services-and-Run">सेवाएं प्रारंभ करें और AgenticSeek चलाएं</a></p>
<p><em>यदि आपको समस्याएँ आ रही हैं तो <strong>Known issues</strong> अनुभाग देखें</em></p>
<p><em>विस्तृत config फ़ाइल व्याख्या के लिए <strong>Config</strong> अनुभाग देखें।</em></p>
<hr />
<h2>सेवाएं प्रारंभ करें और चलाएं</h2>
<p>डिफ़ॉल्ट रूप से AgenticSeek पूरी तरह से डॉकर में चलता है।</p>
<p>आवश्यक सेवाएं प्रारंभ करें। इससे docker-compose.yml से सभी सेवाएं शुरू हो जाएंगी, जिसमें शामिल हैं:
- searxng
- redis (searxng के लिए आवश्यक)
- frontend
- backend (<code>full</code> का उपयोग करने पर)</p>
<pre><code class="language-sh">./start_services.sh full # MacOS
start ./start_services.cmd full # Window
</code></pre>
<p><strong>चेतावनी:</strong> यह चरण सभी डॉकर इमेज डाउनलोड और लोड करेगा, जिसमें 30 मिनट तक लग सकते हैं। सेवाएं प्रारंभ करने के बाद, कृपया तब तक प्रतीक्षा करें जब तक backend सेवा पूरी तरह से चालू न हो जाए (आपको लॉग में <strong>backend: &quot;GET /health HTTP/1.1&quot; 200 OK</strong> दिखना चाहिए) किसी भी संदेश को भेजने से पहले। backend सेवाओं को पहली बार चलने पर 5 मिनट तक लग सकते हैं।</p>
<p><code>http://localhost:3000/</code> पर जाएं और आपको वेब इंटरफेस दिखाई देना चाहिए।</p>
<p><em>सेवा प्रारंभ करने में समस्या निवारण:</em> यदि ये स्क्रिप्ट विफल होती हैं, तो सुनिश्चित करें कि Docker Engine चल रहा है और Docker Compose (V2, <code>docker compose</code>) सही तरीके से इंस्टॉल है। टर्मिनल में आउटपुट में त्रुटि संदेश देखें। देखें <a href="#faq-troubleshooting">FAQ: Help! AgenticSeek या उसकी स्क्रिप्ट चलाते समय मुझे त्रुटि मिलती है।</a></p>
<p><strong>वैकल्पिक:</strong> होस्ट पर चलाएं (CLI मोड):</p>
<p>CLI इंटरफ़ेस के साथ चलाने के लिए आपको पैकेज होस्ट पर इंस्टॉल करना होगा:</p>
<pre><code class="language-sh">./install.sh
./install.bat # windows
</code></pre>
<p>सेवाएं प्रारंभ करें:</p>
<pre><code class="language-sh">./start_services.sh # MacOS
start ./start_services.cmd # Window
</code></pre>
<p>CLI का उपयोग करें: <code>python3 cli.py</code></p>
<hr />
<h2>उपयोग</h2>
<p>सुनिश्चित करें कि सेवाएं <code>./start_services.sh full</code> के साथ चालू हैं और वेब इंटरफेस के लिए <code>localhost:3000</code> पर जाएं।</p>
<p>आप स्पीच टू टेक्स्ट का उपयोग भी कर सकते हैं, इसके लिए config में <code>listen = True</code> सेट करें। केवल CLI मोड के लिए।</p>
<p>बाहर निकलने के लिए, बस <code>goodbye</code> बोलें/टाइप करें।</p>
<p>यहाँ कुछ उदाहरण उपयोग दिए गए हैं:</p>
<blockquote>
<p><em>python में एक सांप का खेल बनाओ!</em></p>
</blockquote>
<blockquote>
<p><em>Rennes, France में शीर्ष कैफे खोजो और तीन के पते सहित सूची को rennes_cafes.txt में सहेजें।</em></p>
</blockquote>
<blockquote>
<p><em>एक Go प्रोग्राम लिखो जो किसी संख्या का फैक्टोरियल निकाले, उसे factorial.go के रूप में अपने workspace में सहेजें।</em></p>
</blockquote>
<blockquote>
<p><em>मेरे summer_pictures फ़ोल्डर में सभी JPG फ़ाइलें खोजो, उन्हें आज की तारीख से रीनेम करो, और रीनेम की गई फ़ाइलों की सूची photos_list.txt में सहेजें।</em></p>
</blockquote>
<blockquote>
<p><em>ऑनलाइन खोजें 2024 की लोकप्रिय साइंस-फिक्शन फिल्मों के लिए और आज रात देखने के लिए तीन चुनें। सूची movie_night.txt में सहेजें।</em></p>
</blockquote>
<blockquote>
<p><em>वेब पर 2025 की नवीनतम AI समाचार लेख खोजें, तीन चुनें, और उनके शीर्षक और सारांश निकालने के लिए एक Python स्क्रिप्ट लिखें। स्क्रिप्ट को news_scraper.py और सारांश को ai_news.txt में /home/projects में सहेजें।</em></p>
</blockquote>
<blockquote>
<p><em>शुक्रवार को, वेब पर एक फ्री स्टॉक प्राइस API खोजो, supersuper7434567@gmail.com से रजिस्टर करें, फिर एक Python स्क्रिप्ट लिखें जो API से Tesla के दैनिक दाम निकाले और परिणामों को stock_prices.csv में सहेजें।</em></p>
</blockquote>
<p><em>ध्यान दें कि फॉर्म भरने की क्षमताएँ अभी भी प्रायोगिक हैं और विफल हो सकती हैं।</em></p>
<p>जब आप अपनी क्वेरी टाइप करेंगे, AgenticSeek उस कार्य के लिए सबसे उपयुक्त एजेंट आवंटित करेगा।</p>
<p>चूंकि यह एक प्रारंभिक प्रोटोटाइप है, एजेंट रूटिंग सिस्टम हमेशा आपकी क्वेरी के आधार पर सही एजेंट आवंटित नहीं कर सकता।</p>
<p>इसलिए, आपको बहुत स्पष्ट रूप से बताना चाहिए कि आप क्या चाहते हैं और AI किस प्रकार आगे बढ़े, उदाहरण के लिए यदि आप चाहते हैं कि वह वेब सर्च करे, तो न कहें:</p>
<p><code>क्या आपको सोलो-ट्रैवल के लिए अच्छे देश पता हैं?</code></p>
<p>इसके बजाय, पूछें:</p>
<p><code>वेब सर्च करें और पता करें कि सोलो-ट्रैवल के लिए सबसे अच्छे देश कौन से हैं</code></p>
<hr />
<h2><strong>अपने स्वयं के सर्वर पर LLM चलाने के लिए सेटअप</strong></h2>
<p>यदि आपके पास एक शक्तिशाली कंप्यूटर या सर्वर है जिसका आप उपयोग कर सकते हैं, लेकिन आप इसे अपने लैपटॉप से उपयोग करना चाहते हैं, तो आपके पास हमारे कस्टम llm सर्वर का उपयोग करके LLM को रिमोट सर्वर पर चलाने के विकल्प हैं।</p>
<p>अपने &quot;सर्वर&quot; पर जो AI मॉडल चलाएगा, IP पता प्राप्त करें</p>
<pre><code class="language-sh">ip a | grep &quot;inet &quot; | grep -v 127.0.0.1 | awk '{print $2}' | cut -d/ -f1 # local ip
curl https://ipinfo.io/ip # public ip
</code></pre>
<p>नोट: विंडोज़ या macOS के लिए, क्रमशः ipconfig या ifconfig का उपयोग करके IP पता प्राप्त करें।</p>
<p>रिपॉजिटरी क्लोन करें और <code>server/</code> फ़ोल्डर में जाएं।</p>
<pre><code class="language-sh">git clone --depth 1 https://github.com/Fosowl/agenticSeek.git
cd agenticSeek/llm_server/
</code></pre>
<p>सर्वर के लिए आवश्यक पैकेज इंस्टॉल करें:</p>
<pre><code class="language-sh">pip3 install -r requirements.txt
</code></pre>
<p>सर्वर स्क्रिप्ट चलाएँ।</p>
<pre><code class="language-sh">python3 app.py --provider ollama --port 3333
</code></pre>
<p>आपके पास LLM सेवा के रूप में <code>ollama</code> और <code>llamacpp</code> में से चुनने का विकल्प है।</p>
<p>अब अपने व्यक्तिगत कंप्यूटर पर:</p>
<p><code>config.ini</code> फ़ाइल बदलें और <code>provider_name</code> को <code>server</code> तथा <code>provider_model</code> को <code>deepseek-r1:xxb</code> पर सेट करें।
<code>provider_server_address</code> को उस मशीन के IP एड्रेस पर सेट करें, जो मॉडल चलाएगी।</p>
<pre><code class="language-sh">[MAIN]
is_local = False
provider_name = server
provider_model = deepseek-r1:70b
provider_server_address = x.x.x.x:3333
</code></pre>
<p>अगला चरण: <a href="#Start-services-and-Run">सेवाएं प्रारंभ करें और AgenticSeek चलाएं</a></p>
<hr />
<h2>स्पीच टू टेक्स्ट</h2>
<p>चेतावनी: स्पीच टू टेक्स्ट अभी केवल CLI मोड में काम करता है।</p>
<p>कृपया ध्यान दें कि फिलहाल स्पीच टू टेक्स्ट केवल अंग्रेज़ी में काम करता है।</p>
<p>स्पीच-टू-टेक्स्ट सुविधा डिफ़ॉल्ट रूप से अक्षम है। इसे सक्षम करने के लिए, config.ini फ़ाइल में listen विकल्प को True पर सेट करें:</p>
<pre><code>listen = True
</code></pre>
<p>सक्षम होने पर, स्पीच-टू-टेक्स्ट सुविधा एक ट्रिगर कीवर्ड (एजेंट का नाम) सुनती है, जिसके बाद ही यह आपका इनपुट प्रोसेस करना शुरू करेगी। आप एजेंट का नाम <em>config.ini</em> फ़ाइल में <code>agent_name</code> मान को अपडेट करके अनुकूलित कर सकते हैं:</p>
<pre><code>agent_name = Friday
</code></pre>
<p>सर्वोत्तम पहचान के लिए, हम अनुशंसा करते हैं कि एजेंट नाम के रूप में &quot;John&quot; या &quot;Emma&quot; जैसे सामान्य अंग्रेज़ी नाम का उपयोग करें।</p>
<p>एक बार जब आप ट्रांसक्रिप्ट दिखना शुरू हो जाए, तो एजेंट का नाम ज़ोर से बोलें ताकि वह जाग जाए (जैसे, &quot;Friday&quot;)।</p>
<p>अपना प्रश्न स्पष्ट रूप से बोलें।</p>
<p>अपने अनुरोध को एक पुष्टि वाक्यांश के साथ समाप्त करें ताकि सिस्टम आगे बढ़ सके। पुष्टि वाक्यांशों के उदाहरण:</p>
<pre><code>&quot;do it&quot;, &quot;go ahead&quot;, &quot;execute&quot;, &quot;run&quot;, &quot;start&quot;, &quot;thanks&quot;, &quot;would ya&quot;, &quot;please&quot;, &quot;okay?&quot;, &quot;proceed&quot;, &quot;continue&quot;, &quot;go on&quot;, &quot;do that&quot;, &quot;go it&quot;, &quot;do you understand?&quot;
</code></pre>
<h2>Config</h2>
<p>उदाहरण config:</p>
<pre><code>[MAIN]
is_local = True
provider_name = ollama
provider_model = deepseek-r1:32b
provider_server_address = http://127.0.0.1:11434 # Ollama के लिए उदाहरण; LM-Studio के लिए http://127.0.0.1:1234 का उपयोग करें
agent_name = Friday
recover_last_session = False
save_session = False
speak = False
listen = False

jarvis_personality = False
languages = en zh # TTS और संभवतः रूटिंग के लिए भाषाओं की सूची।
[BROWSER]
headless_browser = False
stealth_mode = False
</code></pre>
<p><strong><code>config.ini</code> सेटिंग्स का विवरण</strong>:</p>
<ul>
<li><strong><code>[MAIN]</code> सेक्शन:</strong>
<ul>
<li><code>is_local</code>: <code>True</code> यदि आप स्थानीय LLM प्रदाता (Ollama, LM-Studio, स्थानीय OpenAI-संगत सर्वर) या स्वयं-होस्टेड सर्वर विकल्प का उपयोग कर रहे हैं। <code>False</code> यदि आप क्लाउड-आधारित API (OpenAI, Google, आदि) का उपयोग कर रहे हैं।</li>
<li><code>provider_name</code>: LLM प्रदाता निर्दिष्ट करता है।
<ul>
<li>स्थानीय विकल्प: <code>ollama</code>, <code>lm-studio</code>, <code>openai</code> (स्थानीय OpenAI-संगत सर्वर के लिए), <code>server</code> (स्वयं-होस्टेड सर्वर सेटअप के लिए)।</li>
<li>API विकल्प: <code>openai</code>, <code>google</code>, <code>deepseek</code>, <code>huggingface</code>, <code>togetherAI</code>।</li>
</ul>
</li>
<li><code>provider_model</code>: चुने गए प्रदाता के लिए विशिष्ट मॉडल नाम या आईडी (जैसे, Ollama के लिए <code>deepseekcoder:6.7b</code>, OpenAI API के लिए <code>gpt-3.5-turbo</code>, TogetherAI के लिए <code>mistralai/Mixtral-8x7B-Instruct-v0.1</code>)।</li>
<li><code>provider_server_address</code>: आपके LLM प्रदाता का पता।
<ul>
<li>स्थानीय प्रदाताओं के लिए: जैसे, Ollama के लिए <code>http://127.0.0.1:11434</code>, LM-Studio के लिए <code>http://127.0.0.1:1234</code>।</li>
<li><code>server</code> प्रदाता प्रकार के लिए: आपके स्वयं-होस्टेड LLM सर्वर का पता (जैसे, <code>http://your_server_ip:3333</code>)।</li>
<li>क्लाउड API (<code>is_local = False</code>) के लिए: यह अक्सर अनदेखा किया जाता है या खाली छोड़ा जा सकता है, क्योंकि API एंडपॉइंट आमतौर पर क्लाइंट लाइब्रेरी द्वारा संभाला जाता है।</li>
</ul>
</li>
<li><code>agent_name</code>: एआई असिस्टेंट का नाम (जैसे, Friday)। यदि सक्षम किया गया हो तो स्पीच-टू-टेक्स्ट के लिए ट्रिगर वर्ड के रूप में उपयोग होता है।</li>
<li><code>recover_last_session</code>: <code>True</code> पिछले सत्र की स्थिति बहाल करने का प्रयास करने के लिए, <code>False</code> नया सत्र शुरू करने के लिए।</li>
<li><code>save_session</code>: <code>True</code> वर्तमान सत्र की स्थिति को संभावित पुनर्प्राप्ति के लिए सहेजने के लिए, अन्यथा <code>False</code>।</li>
<li><code>speak</code>: <code>True</code> टेक्स्ट-टू-स्पीच वॉयस आउटपुट सक्षम करने के लिए, अक्षम करने के लिए <code>False</code>।</li>
<li><code>listen</code>: <code>True</code> स्पीच-टू-टेक्स्ट वॉयस इनपुट सक्षम करने के लिए (केवल CLI मोड), अन्यथा <code>False</code>।</li>
<li><code>work_dir</code>: <strong>महत्वपूर्ण:</strong> वह निर्देशिका जहाँ AgenticSeek फाइलें पढ़ेगा/लिखेगा। <strong>सुनिश्चित करें कि यह पथ आपके सिस्टम पर मान्य और सुलभ है।</strong></li>
<li><code>jarvis_personality</code>: <code>True</code> अधिक &quot;Jarvis-जैसा&quot; सिस्टम प्रॉम्प्ट उपयोग करने के लिए (प्रायोगिक), सामान्य प्रॉम्प्ट के लिए <code>False</code>।</li>
<li><code>languages</code>: कॉमा-सेपरेटेड भाषाओं की सूची (जैसे, <code>en, zh, fr</code>)। TTS वॉयस चयन के लिए उपयोग होती है (डिफ़ॉल्ट पहले भाषा पर) और LLM राउटर में सहायक हो सकती है। राउटर दक्षता के लिए बहुत अधिक या बहुत समान भाषाओं से बचें।</li>
</ul>
</li>
<li><strong><code>[BROWSER]</code> सेक्शन:</strong>
<ul>
<li><code>headless_browser</code>: <code>True</code> स्वचालित ब्राउज़र को बिना विज़िबल विंडो के चलाने के लिए (वेब इंटरफेस या नॉन-इंटरैक्टिव उपयोग के लिए अनुशंसित)। ब्राउज़र विंडो दिखाने के लिए <code>False</code> (CLI मोड या डिबगिंग के लिए उपयोगी)।</li>
<li><code>stealth_mode</code>: <code>True</code> ब्राउज़र ऑटोमेशन को पहचानना कठिन बनाने के उपायों को सक्षम करने के लिए। इसके लिए anticaptcha जैसी ब्राउज़र एक्सटेंशन की मैन्युअल स्थापना की आवश्यकता हो सकती है।</li>
</ul>
</li>
</ul>
<p>यह अनुभाग समर्थित LLM प्रदाता प्रकारों का सारांश प्रस्तुत करता है। इन्हें <code>config.ini</code> में कॉन्फ़िगर करें।</p>
<p><strong>स्थानीय प्रदाता (अपने हार्डवेयर पर चलाएं):</strong></p>
<p>| <code>config.ini</code> में प्रदाता नाम     | <code>is_local</code> | विवरण                                                                   | सेटअप अनुभाग                                              |
|-------------------------------|------------|------------------------------------------------------------------------|-----------------------------------------------------------|
| <code>ollama</code>                      | <code>True</code>     | स्थानीय LLMs सर्व करने के लिए Ollama का उपयोग करें।                     | <a href="#setup-for-running-llm-locally-on-your-machine">अपने कंप्यूटर पर LLM चलाने के लिए सेटअप</a> |
| <code>lm-studio</code>                   | <code>True</code>     | स्थानीय LLMs सर्व करने के लिए LM-Studio का उपयोग करें।                  | <a href="#setup-for-running-llm-locally-on-your-machine">अपने कंप्यूटर पर LLM चलाने के लिए सेटअप</a> |
| <code>openai</code> (स्थानीय सर्वर के लिए) | <code>True</code>     | स्थानीय सर्वर से कनेक्ट करें जो OpenAI-संगत API एक्सपोज़ करता है (जैसे, llama.cpp)। | <a href="#setup-for-running-llm-locally-on-your-machine">अपने कंप्यूटर पर LLM चलाने के लिए सेटअप</a> |
| <code>server</code>                      | <code>False</code>    | दूसरे मशीन पर चल रहे AgenticSeek स्वयं-होस्टेड LLM सर्वर से कनेक्ट करें। | <a href="#setup-to-run-the-llm-on-your-own-server">अपने सर्वर पर LLM चलाने के लिए सेटअप</a> |</p>
<p><strong>API प्रदाता (क्लाउड-आधारित):</strong></p>
<p>| <code>config.ini</code> में प्रदाता नाम     | <code>is_local</code> | विवरण                                         | सेटअप अनुभाग                                   |
|-------------------------------|------------|-----------------------------------------------|------------------------------------------------|
| <code>openai</code>                      | <code>False</code>    | OpenAI के आधिकारिक API का उपयोग करें (जैसे, GPT-3.5, GPT-4)। | <a href="#setup-to-run-with-an-api">API के साथ चलाने के लिए सेटअप</a> |
| <code>google</code>                      | <code>False</code>    | API के माध्यम से Google के Gemini मॉडल का उपयोग करें।       | <a href="#setup-to-run-with-an-api">API के साथ चलाने के लिए सेटअप</a> |
| <code>deepseek</code>                    | <code>False</code>    | Deepseek के आधिकारिक API का उपयोग करें।                | <a href="#setup-to-run-with-an-api">API के साथ चलाने के लिए सेटअप</a> |
| <code>huggingface</code>                 | <code>False</code>    | Hugging Face Inference API का उपयोग करें।                | <a href="#setup-to-run-with-an-api">API के साथ चलाने के लिए सेटअप</a> |
| <code>togetherAI</code>                  | <code>False</code>    | विभिन्न खुले मॉडल्स के लिए TogetherAI का API उपयोग करें।    | <a href="#setup-to-run-with-an-api">API के साथ चलाने के लिए सेटअप</a> |</p>
<hr />
<h2>समस्या निवारण</h2>
<p>यदि आपको समस्या आती है, तो यह अनुभाग मार्गदर्शन प्रदान करता है।</p>
<h1>ज्ञात समस्याएँ</h1>
<h2>ChromeDriver समस्याएँ</h2>
<p><strong>त्रुटि उदाहरण:</strong> <code>SessionNotCreatedException: Message: session not created: This version of ChromeDriver only supports Chrome version XXX</code></p>
<ul>
<li><strong>कारण:</strong> आपके इंस्टॉल किए गए ChromeDriver का संस्करण आपके Google Chrome ब्राउज़र संस्करण के साथ संगत नहीं है।</li>
<li><strong>समाधान:</strong>
<ol>
<li><strong>Chrome संस्करण जांचें:</strong> Google Chrome खोलें, <code>Settings &gt; About Chrome</code> पर जाएं और अपना संस्करण देखें (जैसे, &quot;Version 120.0.6099.110&quot;)।</li>
<li><strong>मिलती-जुलती ChromeDriver डाउनलोड करें:</strong>
<ul>
<li>Chrome संस्करण 115 और नए के लिए: <a href="https://googlechromelabs.github.io/chrome-for-testing/">Chrome for Testing (CfT) JSON Endpoints</a> पर जाएं। &quot;stable&quot; चैनल देखें और अपने OS के लिए Chrome के मुख्य संस्करण से मेल खाने वाला ChromeDriver डाउनलोड करें।</li>
<li>पुराने संस्करणों के लिए (कम आम): आप इन्हें <a href="https://chromedriver.chromium.org/downloads">ChromeDriver - WebDriver for Chrome</a> पृष्ठ पर पा सकते हैं।</li>
<li>नीचे दी गई छवि CfT पृष्ठ से एक उदाहरण दिखाती है:
<img src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/chromedriver_readme.png" alt="Download Chromedriver specific version from Chrome for Testing page" /></li>
</ul>
</li>
<li><strong>ChromeDriver इंस्टॉल करें:</strong>
<ul>
<li>सुनिश्चित करें कि डाउनलोड किया गया <code>chromedriver</code> (या Windows पर <code>chromedriver.exe</code>) आपके सिस्टम के PATH एनवायरनमेंट वेरिएबल में सूचीबद्ध डायरेक्टरी में है (जैसे, Linux/macOS पर <code>/usr/local/bin</code>, या Windows पर PATH में जोड़ा गया कस्टम स्क्रिप्ट फोल्डर)।</li>
<li>वैकल्पिक रूप से, इसे <code>agenticSeek</code> प्रोजेक्ट की रूट डायरेक्टरी में रखें।</li>
<li>सुनिश्चित करें कि ड्राइवर निष्पादन योग्य है (जैसे, Linux/macOS पर <code>chmod +x chromedriver</code>)।</li>
</ul>
</li>
<li>अधिक जानकारी के लिए मुख्य इंस्टॉलेशन गाइड में <a href="#chromedriver-installation">ChromeDriver Installation</a> अनुभाग देखें।</li>
</ol>
</li>
</ul>
<p>यदि यह अनुभाग अधूरा है या आपको अन्य ChromeDriver समस्याएँ आती हैं, तो मौजूदा <a href="https://github.com/Fosowl/agenticSeek/issues">GitHub Issues</a> खोजें या नया मुद्दा बनाएं।</p>
<p><code>Exception: Failed to initialize browser: Message: session not created: This version of ChromeDriver only supports Chrome version 113 Current browser version is 134.0.6998.89 with binary path</code></p>
<p>यह तब होता है जब आपके ब्राउज़र और chromedriver के संस्करण में मेल नहीं होता है।</p>
<p>आपको नवीनतम संस्करण डाउनलोड करने के लिए जाना होगा:</p>
<p>https://developer.chrome.com/docs/chromedriver/downloads</p>
<p>यदि आप Chrome संस्करण 115 या नए का उपयोग कर रहे हैं तो जाएं:</p>
<p>https://googlechromelabs.github.io/chrome-for-testing/</p>
<p>और अपने OS से मेल खाने वाला chromedriver संस्करण डाउनलोड करें।</p>
<p><img src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/chromedriver_readme.png" alt="alt text" /></p>
<p>यदि यह अनुभाग अधूरा है तो कृपया एक मुद्दा बनाएं।</p>
<h2>connection adapters समस्याएँ</h2>
<pre><code>Exception: Provider lm-studio failed: HTTP request failed: No connection adapters were found for '127.0.0.1:1234/v1/chat/completions'` (ध्यान दें: पोर्ट भिन्न हो सकता है)
</code></pre>
<ul>
<li><strong>कारण:</strong> <code>config.ini</code> में <code>lm-studio</code> (या अन्य समान स्थानीय OpenAI-संगत सर्वरों) के लिए <code>provider_server_address</code> में <code>http://</code> उपसर्ग नहीं है या यह गलत पोर्ट पर इंगित कर रहा है।</li>
<li><strong>समाधान:</strong>
<ul>
<li>सुनिश्चित करें कि पते में <code>http://</code> शामिल है। LM-Studio आमतौर पर डिफ़ॉल्ट रूप से <code>http://127.0.0.1:1234</code> पर चलता है।</li>
<li>सही <code>config.ini</code>: <code>provider_server_address = http://127.0.0.1:1234</code> (या आपके वास्तविक LM-Studio सर्वर पोर्ट पर)।</li>
</ul>
</li>
</ul>
<h2>SearxNG बेस URL प्रदान नहीं किया गया</h2>
<pre><code>raise ValueError(&quot;SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.&quot;)
ValueError: SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.`
</code></pre>
<h2>अक्सर पूछे जाने वाले प्रश्न (FAQ)</h2>
<p><strong>प्रश्न: मुझे कौन सा हार्डवेयर चाहिए?</strong></p>
<p>| मॉडल आकार   | GPU           | टिप्पणी                                                                           |
|-----------|---------------|------------------------------------------------------------------------------------|
| 7B        | 8GB Vram       | ⚠️ अनुशंसित नहीं। प्रदर्शन खराब है, बार-बार भ्रम, और प्लानर एजेंट शायद असफल होंगे।   |
| 14B       | 12 GB VRAM (जैसे RTX 3060) | ✅ साधारण कार्यों के लिए उपयोगी। वेब ब्राउज़िंग और योजना कार्यों में दिक्कत हो सकती है। |
| 32B       | 24+ GB VRAM (जैसे RTX 4090) | 🚀 अधिकांश कार्यों में सफलता, फिर भी कार्य योजना में संघर्ष हो सकता है।               |
| 70B+      | 48+ GB Vram    | 💪 उत्कृष्ट। उन्नत उपयोग मामलों के लिए अनुशंसित।                                 |</p>
<p><strong>प्रश्न: मुझे त्रुटि मिलती है तो क्या करूँ?</strong></p>
<p>सुनिश्चित करें कि local चल रहा है (<code>ollama serve</code>), आपका <code>config.ini</code> आपके प्रदाता से मेल खाता है, और dependencies इंस्टॉल हैं। यदि कुछ भी काम न करे तो एक issue बना सकते हैं।</p>
<p><strong>प्रश्न: क्या यह सचमुच 100% स्थानीय रूप से चल सकता है?</strong></p>
<p>हाँ, Ollama, lm-studio या server प्रदाताओं के साथ, सभी स्पीच टू टेक्स्ट, LLM और टेक्स्ट टू स्पीच मॉडल स्थानीय रूप से चलते हैं। गैर-स्थानीय विकल्प (OpenAI या अन्य API) वैकल्पिक हैं।</p>
<p><strong>प्रश्न: मुझे Manus है तो AgenticSeek क्यों इस्तेमाल करूँ?</strong></p>
<p>Manus के विपरीत, AgenticSeek बाहरी प्रणालियों से स्वतंत्रता को प्राथमिकता देता है, जिससे आपको अधिक नियंत्रण, गोपनीयता और API लागत से बचाव मिलता है।</p>
<p><strong>प्रश्न: इस प्रोजेक्ट के पीछे कौन है?</strong></p>
<p>यह प्रोजेक्ट मैंने बनाया था, साथ ही दो दोस्त मेंटेनर और ओपन-सोर्स समुदाय के योगदानकर्ता हैं। हम सिर्फ उत्साही व्यक्तियों का समूह हैं, कोई स्टार्टअप या संगठन से संबद्ध नहीं हैं।</p>
<p>मेरे व्यक्तिगत खाते (https://x.com/Martin993886460) के अलावा X पर कोई भी AgenticSeek खाता प्रतिरूपण है।</p>
<h2>योगदान करें</h2>
<p>हम AgenticSeek को बेहतर बनाने के लिए डेवलपर्स की तलाश कर रहे हैं! खुले मुद्दे या चर्चा देखें।</p>
<p><a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/docs/CONTRIBUTING.md">योगदान गाइड</a></p>
<p><a href="https://www.star-history.com/#Fosowl/agenticSeek&amp;Date"><img src="https://api.star-history.com/svg?repos=Fosowl/agenticSeek&amp;type=Date" alt="Star History Chart" /></a></p>
<h2>मेंटेनर:</h2>
<blockquote>
<p><a href="https://github.com/Fosowl">Fosowl</a> | पेरिस समय</p>
</blockquote>
<blockquote>
<p><a href="https://github.com/antoineVIVIES">antoineVIVIES</a> | ताइपेई समय</p>
</blockquote>
<blockquote>
<p><a href="https://github.com/steveh8758">steveh8758</a> | ताइपेई समय</p>
</blockquote>
<h2>विशेष धन्यवाद:</h2>
<blockquote>
<p><a href="https://github.com/tcsenpai">tcsenpai</a> और <a href="https://github.com/plitc">plitc</a> बैकएंड डॉकराइजेशन में मदद के लिए</p>
</blockquote>
<h2>प्रायोजक:</h2>
<p>5$ या अधिक मासिक प्रायोजक यहाँ दिखेंगे:</p>
<ul>
<li><strong>tatra-labs</strong></li>
</ul>
<p>It seems that you have not provided the content of Part 4 of 4 that needs to be translated. Please paste the text of the technical document (Part 4 of 4), and I will translate it into Hindi while preserving the original Markdown format and relative paths as you requested.</p>
<hr />
<p>Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx">Open Ai Tx</a> | Last indexed: 2025-06-16</p>
<hr />

        </div>
    </div>
    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Statcounter and other scripts can be added here -->
</body>
</html>