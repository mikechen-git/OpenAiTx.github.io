<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>agenticSeek - Fosowl/agenticSeek</title>
    <meta name="title" content="agenticSeek - Fosowl/agenticSeek">
    <meta name="description" content="Fosowl/agenticSeek - GitHub repository ru documentation and informationAgenticSeek: Приватная локальная альтернатива Manus. English | 中文 | 繁體中文 | Français | 日本語 | Português (Brasil) | Español Полностью локальная альтернатива Manus ...">
    <meta name="keywords" content="Fosowl, agenticSeek, GitHub, repository, ru documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/Fosowl/agenticSeek/README-ru.html">
    <meta property="og:title" content="agenticSeek - Fosowl/agenticSeek">
    <meta property="og:description" content="Fosowl/agenticSeek - GitHub repository ru documentation and information">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">
    <script src="/js/marked.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        body { display: flex; flex-direction: column; min-height: 100vh; }
        .main-container { margin: 0 auto; width: 100%; max-width: 980px; padding: 0 20px; }
        @media (max-width: 768px) { .main-container { padding: 0 15px; } }
        .markdown-body img { max-width: 100%; height: auto; }
        .header { text-align: center; margin-bottom: 30px; padding: 20px; background-color: #f6f8fa; border-bottom: 1px solid #e1e4e8; position: relative; }
        .header .links { margin-top: 10px; font-size: 16px; }
        .header .links a { color: #0366d6; text-decoration: none; margin-left: 5px; }
        .header .links a:hover { text-decoration: underline; }
        .language-badges { margin-top: 15px; text-align: center; }
        .language-badges a { display: inline-block; margin: 2px; text-decoration: none; }
        .language-badges img { height: 20px; border-radius: 3px; }
        .language-badges a:hover img { opacity: 0.8; }
    </style>
</head>
<body>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/Fosowl/agenticSeek" id="githubRepoLink" target="_blank">Fosowl/agenticSeek</a>
<h1 style="display: none;">AgenticSeek: Приватная локальная альтернатива Manus. English | 中文 | 繁體中文 | Français | 日本語 | Português (Brasil) | Español Полностью локальная альтернатива Manus ...</h1>
        </div>
        <div class="language-badges" id="languageBadges">
            <!-- You can generate language badges here if needed -->
        </div>
    </div>
    <div class="main-container">
        <div class="markdown-body" id="content">
            <h1>AgenticSeek: Приватная локальная альтернатива Manus.</h1>
<p align="center">
<img align="center" src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/agentic_seek_logo.png" width="300" height="300" alt="Agentic Seek Logo">
<p>
<p>English | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_CHS.md">中文</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_CHT.md">繁體中文</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_FR.md">Français</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_JP.md">日本語</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_PTBR.md">Português (Brasil)</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_ES.md">Español</a></p>
<p><em>Полностью локальная альтернатива Manus AI</em>: этот голосовой AI-ассистент автономно просматривает веб, пишет код и планирует задачи, сохраняя все данные только на вашем устройстве. Оптимизирован для локальных моделей рассуждения, полностью работает на вашем оборудовании, обеспечивая полную приватность и отсутствие облачной зависимости.</p>
<p><a href="https://fosowl.github.io/agenticSeek.html"><img src="https://img.shields.io/static/v1?label=Website&amp;message=AgenticSeek&amp;color=blue&amp;style=flat-square" alt="Visit AgenticSeek" /></a> <img src="https://img.shields.io/badge/license-GPL--3.0-green" alt="License" /> <a href="https://discord.gg/8hGDaME3TC"><img src="https://img.shields.io/badge/Discord-Join%20Us-7289DA?logo=discord&amp;logoColor=white" alt="Discord" /></a> <a href="https://x.com/Martin993886460"><img src="https://img.shields.io/twitter/url/https/twitter.com/fosowl.svg?style=social&amp;label=Update%20%40Fosowl" alt="Twitter" /></a> <a href="https://github.com/Fosowl/agenticSeek/stargazers"><img src="https://img.shields.io/github/stars/Fosowl/agenticSeek?style=social" alt="GitHub stars" /></a></p>
<h3>Почему AgenticSeek?</h3>
<ul>
<li><p>🔒 Полностью локальный и приватный — Всё работает на вашем компьютере: никаких облаков, никакой передачи данных. Ваши файлы, диалоги и поисковые запросы остаются приватными.</p>
</li>
<li><p>🌐 Умный веб-браузер — AgenticSeek может самостоятельно просматривать интернет: искать, читать, извлекать информацию, заполнять веб-формы — полностью без рук.</p>
</li>
<li><p>💻 Автономный помощник по программированию — Нужен код? Он может писать, отлаживать и запускать программы на Python, C, Go, Java и других языках — полностью автономно.</p>
</li>
<li><p>🧠 Умный выбор агента — Вы задаёте вопрос, и он автоматически выбирает лучшего агента для задачи. Как команда экспертов, готовых помочь.</p>
</li>
<li><p>📋 Планирование и выполнение сложных задач — От планирования путешествий до реализации сложных проектов — может разбивать большие задачи на этапы и выполнять их с помощью нескольких AI-агентов.</p>
</li>
<li><p>🎙️ Голосовое управление — Современное, быстрое, футуристичное распознавание речи и синтез позволяет общаться с ним голосом, как с личным AI из фантастики. (В разработке)</p>
</li>
</ul>
<h3><strong>Демонстрация</strong></h3>
<blockquote>
<p><em>Можешь найти проект agenticSeek, узнать какие навыки требуются, затем открыть CV_candidates.zip и сказать, кто из кандидатов лучше всего подходит для проекта?</em></p>
</blockquote>
<p>https://github.com/user-attachments/assets/b8ca60e9-7b3b-4533-840e-08f9ac426316</p>
<p>Дисклеймер: Эта демонстрация, включая все показанные файлы (например, CV_candidates.zip), полностью вымышлена. Мы не корпорация, мы ищем open-source контрибьюторов, а не кандидатов.</p>
<blockquote>
<p>🛠⚠️️ <strong>Активная разработка</strong></p>
</blockquote>
<blockquote>
<p>🙏 Этот проект начался как хобби и не имеет ни дорожной карты, ни финансирования. Он превзошёл все ожидания, попав в GitHub Trending. Вклад, обратная связь и терпение очень ценятся.</p>
</blockquote>
<h2>Необходимое ПО</h2>
<p>Перед началом убедитесь, что у вас установлено следующее ПО:</p>
<ul>
<li><strong>Git:</strong> Для клонирования репозитория. <a href="https://git-scm.com/downloads">Скачать Git</a></li>
<li><strong>Python 3.10.x:</strong> Настоятельно рекомендуется использовать Python версии 3.10.x. Использование других версий может привести к ошибкам зависимостей. <a href="https://www.python.org/downloads/release/python-3100/">Скачать Python 3.10</a> (выберите версию 3.10.x).</li>
<li><strong>Docker Engine &amp; Docker Compose:</strong> Для запуска встроенных сервисов, таких как SearxNG.
<ul>
<li>Установите Docker Desktop (включает Docker Compose V2): <a href="https://docs.docker.com/desktop/install/windows-install/">Windows</a> | <a href="https://docs.docker.com/desktop/install/mac-install/">Mac</a> | <a href="https://docs.docker.com/desktop/install/linux-install/">Linux</a></li>
<li>Либо установите Docker Engine и Docker Compose отдельно для Linux: <a href="https://docs.docker.com/engine/install/">Docker Engine</a> | <a href="https://docs.docker.com/compose/install/">Docker Compose</a> (убедитесь, что установили Compose V2, например, <code>sudo apt-get install docker-compose-plugin</code>).</li>
</ul>
</li>
</ul>
<h3>1. <strong>Клонируйте репозиторий и настройте</strong></h3>
<pre><code class="language-sh">git clone https://github.com/Fosowl/agenticSeek.git
cd agenticSeek
mv .env.example .env
</code></pre>
<h3>2. Измените содержимое файла .env</h3>
<pre><code class="language-sh">SEARXNG_BASE_URL=&quot;http://127.0.0.1:8080&quot;
REDIS_BASE_URL=&quot;redis://redis:6379/0&quot;
WORK_DIR=&quot;/Users/mlg/Documents/workspace_for_ai&quot;
OLLAMA_PORT=&quot;11434&quot;
LM_STUDIO_PORT=&quot;1234&quot;
CUSTOM_ADDITIONAL_LLM_PORT=&quot;11435&quot;
OPENAI_API_KEY='optional'
DEEPSEEK_API_KEY='optional'
OPENROUTER_API_KEY='optional'
TOGETHER_API_KEY='optional'
GOOGLE_API_KEY='optional'
ANTHROPIC_API_KEY='optional'
</code></pre>
<p>Обновите файл <code>.env</code> своими значениями по мере необходимости:</p>
<ul>
<li><strong>SEARXNG_BASE_URL</strong>: Оставьте без изменений</li>
<li><strong>REDIS_BASE_URL</strong>: Оставьте без изменений</li>
<li><strong>WORK_DIR</strong>: Путь к вашей рабочей директории на локальном устройстве. AgenticSeek сможет читать и взаимодействовать с этими файлами.</li>
<li><strong>OLLAMA_PORT</strong>: Порт для сервиса Ollama.</li>
<li><strong>LM_STUDIO_PORT</strong>: Порт для сервиса LM Studio.</li>
<li><strong>CUSTOM_ADDITIONAL_LLM_PORT</strong>: Порт для дополнительного собственного LLM-сервиса.</li>
</ul>
<p><strong>API-ключи полностью опциональны для пользователей, которые запускают LLM локально. Это основное назначение проекта. Оставьте пустыми, если у вас достаточно ресурсов.</strong></p>
<h3>3. <strong>Запустите Docker</strong></h3>
<p>Убедитесь, что Docker установлен и запущен на вашей системе. Запустить Docker можно следующими командами:</p>
<ul>
<li><p><strong>Linux/macOS:</strong><br />
Откройте терминал и выполните:</p>
<pre><code class="language-sh">sudo systemctl start docker
</code></pre>
<p>Или запустите Docker Desktop из меню приложений, если он установлен.</p>
</li>
<li><p><strong>Windows:</strong><br />
Запустите Docker Desktop из меню &quot;Пуск&quot;.</p>
</li>
</ul>
<p>Проверьте, что Docker работает, выполнив:</p>
<pre><code class="language-sh">docker info
</code></pre>
<p>Если вы видите информацию о вашей установке Docker, значит всё работает корректно.</p>
<p>См. таблицу <a href="#list-of-local-providers">Локальных провайдеров</a> ниже для сводки.</p>
<p>Следующий шаг: <a href="#start-services-and-run">Запуск AgenticSeek локально</a></p>
<p><em>Обратитесь к разделу <a href="#troubleshooting">Устранение неполадок</a>, если возникнут проблемы.</em>
<em>Если ваше оборудование не поддерживает локальный запуск LLM, смотрите <a href="#setup-to-run-with-an-api">Настройка для работы с API</a>.</em>
<em>За подробным описанием <code>config.ini</code> смотрите <a href="#config">Раздел Конфигурации</a>.</em></p>
<hr />
<h2>Настройка для запуска LLM локально на вашем устройстве</h2>
<p><strong>Требования к оборудованию:</strong></p>
<p>Для локального запуска LLM необходимы соответствующие ресурсы. Минимум — GPU, способный запускать Magistral, Qwen или Deepseek 14B. См. раздел FAQ для рекомендаций по моделям и производительности.</p>
<p><strong>Настройте локального провайдера</strong></p>
<p>Запустите локального провайдера, например с помощью ollama:</p>
<pre><code class="language-sh">ollama serve
</code></pre>
<p>См. ниже список поддерживаемых локальных провайдеров.</p>
<p><strong>Обновите config.ini</strong></p>
<p>Измените файл config.ini, установив provider_name на поддерживаемого провайдера и provider_model на LLM, поддерживаемую вашим провайдером. Рекомендуем использовать модели рассуждения, такие как <em>Magistral</em> или <em>Deepseek</em>.</p>
<p>См. <strong>FAQ</strong> в конце README для требований к оборудованию.</p>
<pre><code class="language-sh">[MAIN]
is_local = True # Локальный запуск или через удалённого провайдера.
provider_name = ollama # или lm-studio, openai и т.д.
provider_model = deepseek-r1:14b # выберите модель, подходящую вашему оборудованию
provider_server_address = 127.0.0.1:11434
agent_name = Jarvis # имя вашего AI
recover_last_session = True # восстанавливать предыдущую сессию
save_session = True # сохранять текущую сессию
speak = False # синтез речи
listen = False # распознавание речи, только для CLI, экспериментально
jarvis_personality = False # использовать стиль личности &quot;Jarvis&quot; (экспериментально)
languages = en zh # список языков, синтез речи по умолчанию на первом в списке
[BROWSER]
headless_browser = True # не меняйте, если не используете CLI на хосте
stealth_mode = True # использовать undetected selenium для уменьшения обнаружения браузера
</code></pre>
<p><strong>Внимание</strong>:</p>
<ul>
<li><p>Формат файла <code>config.ini</code> не поддерживает комментарии.
Не копируйте и не вставляйте пример конфигурации напрямую: комментарии вызовут ошибки. Вместо этого вручную измените файл <code>config.ini</code>, исключив все комментарии.</p>
</li>
<li><p>Не указывайте provider_name как <code>openai</code>, если используете LM-studio для запуска LLM. Установите <code>lm-studio</code>.</p>
</li>
<li><p>Некоторые провайдеры (например, lm-studio) требуют наличие <code>http://</code> перед IP. Например, <code>http://127.0.0.1:1234</code></p>
</li>
</ul>
<p><strong>Список локальных провайдеров</strong></p>
<p>| Провайдер   | Локальный? | Описание                                                 |
|-------------|------------|----------------------------------------------------------|
| ollama      | Да         | Запуск LLM локально с помощью ollama как LLM-провайдера  |
| lm-studio   | Да         | Запуск LLM локально с помощью LM studio (установите <code>provider_name</code> как <code>lm-studio</code>)|
| openai      | Да         | Использование совместимого с openai API (например, сервер llama.cpp)  |</p>
<p>Следующий шаг: <a href="#Start-services-and-Run">Запуск сервисов и AgenticSeek</a></p>
<p><em>Смотрите раздел <a href="#troubleshooting">Устранение неполадок</a>, если возникли проблемы.</em>
<em>Если ваше оборудование не поддерживает локальный запуск LLM, смотрите <a href="#setup-to-run-with-an-api">Настройка для работы с API</a>.</em>
<em>Для подробных объяснений <code>config.ini</code> смотрите <a href="#config">Раздел Конфигурации</a>.</em></p>
<h2>Настройка для работы с API</h2>
<p>В этом режиме используются внешние облачные провайдеры LLM. Вам понадобится API-ключ от выбранного сервиса.</p>
<p><strong>1. Выберите API-провайдера и получите API-ключ:</strong></p>
<p>Смотрите <a href="#list-of-api-providers">Список API-провайдеров</a> ниже. Посетите их сайты, зарегистрируйтесь и получите API-ключ.</p>
<p><strong>2. Установите свой API-ключ как переменную окружения:</strong></p>
<ul>
<li><strong>Linux/macOS:</strong>
Откройте терминал и используйте команду <code>export</code>. Лучше добавить эту команду в профиль оболочки (например, <code>~/.bashrc</code>, <code>~/.zshrc</code>) для постоянства.
<pre><code class="language-sh">export PROVIDER_API_KEY=&quot;your_api_key_here&quot; 
# Замените PROVIDER_API_KEY на конкретное имя переменной, например, OPENAI_API_KEY, GOOGLE_API_KEY
</code></pre>
Пример для TogetherAI:
<pre><code class="language-sh">export TOGETHER_API_KEY=&quot;xxxxxxxxxxxxxxxxxxxxxx&quot;
</code></pre>
</li>
<li><strong>Windows:</strong></li>
<li><strong>Командная строка (Временно для текущей сессии):</strong>
<pre><code class="language-cmd">set PROVIDER_API_KEY=your_api_key_here
</code></pre>
</li>
<li><strong>PowerShell (Временно для текущей сессии):</strong>
<pre><code class="language-powershell">$env:PROVIDER_API_KEY=&quot;your_api_key_here&quot;
</code></pre>
</li>
<li><strong>Постоянно:</strong> Найдите &quot;переменные среды&quot; в поисковой строке Windows, нажмите &quot;Изменить системные переменные среды&quot;, затем нажмите кнопку &quot;Переменные среды...&quot;. Добавьте новую пользовательскую переменную с соответствующим именем (например, <code>OPENAI_API_KEY</code>) и вашим ключом в качестве значения.</li>
</ul>
<p><em>(См. FAQ: <a href="#how-do-i-set-api-keys">Как установить API ключи?</a> для получения подробностей).</em></p>
<p><strong>3. Обновите <code>config.ini</code>:</strong></p>
<pre><code class="language-ini">[MAIN]
is_local = False
provider_name = openai # Или google, deepseek, togetherAI, huggingface
provider_model = gpt-3.5-turbo # Или gemini-1.5-flash, deepseek-chat, mistralai/Mixtral-8x7B-Instruct-v0.1 и т.д.
provider_server_address = # Обычно игнорируется или может быть оставлен пустым, когда is_local = False для большинства API
# ... другие настройки ...
</code></pre>
<p><em>Внимание:</em> Убедитесь, что в значениях <code>config.ini</code> нет пробелов в конце строк.</p>
<p><strong>Список API провайдеров</strong></p>
<p>| Провайдер    | <code>provider_name</code> | Локальный? | Описание                                            | Ссылка для получения API ключа                |
|--------------|-----------------|------------|-----------------------------------------------------|-----------------------------------------------|
| OpenAI       | <code>openai</code>        | Нет        | Использовать модели ChatGPT через API OpenAI.       | <a href="https://platform.openai.com/signup">platform.openai.com/signup</a> |
| Google Gemini| <code>google</code>        | Нет        | Использовать модели Google Gemini через Google AI Studio. | <a href="https://aistudio.google.com/keys">aistudio.google.com/keys</a> |
| Deepseek     | <code>deepseek</code>      | Нет        | Использовать модели Deepseek через их API.          | <a href="https://platform.deepseek.com">platform.deepseek.com</a> |
| Hugging Face | <code>huggingface</code>   | Нет        | Использовать модели через Hugging Face Inference API.| <a href="https://huggingface.co/settings/tokens">huggingface.co/settings/tokens</a> |
| TogetherAI   | <code>togetherAI</code>    | Нет        | Использовать различные open-source модели через TogetherAI API.| <a href="https://api.together.ai/settings/api-keys">api.together.ai/settings/api-keys</a> |</p>
<p><em>Примечание:</em></p>
<ul>
<li>Мы не рекомендуем использовать <code>gpt-4o</code> или другие модели OpenAI для сложного веб-сёрфинга и планирования задач, так как текущие оптимизации подсказок ориентированы на модели вроде Deepseek.</li>
<li>Задания на программирование/bash могут сталкиваться с ошибками при использовании Gemini, поскольку он может не строго следовать форматированию подсказок, оптимизированных для Deepseek.</li>
<li>Параметр <code>provider_server_address</code> в <code>config.ini</code> обычно не используется, когда <code>is_local = False</code>, так как конечная точка API обычно зашита в соответствующей библиотеке провайдера.</li>
</ul>
<p>Следующий шаг: <a href="#Start-services-and-Run">Запустите сервисы и AgenticSeek</a></p>
<p><em>См. раздел <strong>Известные проблемы</strong>, если у вас возникают сложности.</em></p>
<p><em>См. раздел <strong>Config</strong> для подробного объяснения файла конфигурации.</em></p>
<hr />
<h2>Запуск сервисов и работа</h2>
<p>По умолчанию AgenticSeek полностью запускается в docker.</p>
<p>Запустите необходимые сервисы. Это запустит все сервисы из docker-compose.yml, включая:
- searxng
- redis (требуется для searxng)
- frontend
- backend (если используется <code>full</code>)</p>
<pre><code class="language-sh">./start_services.sh full # MacOS
start ./start_services.cmd full # Window
</code></pre>
<p><strong>Внимание:</strong> На этом этапе будут загружены и распакованы все образы Docker, что может занять до 30 минут. После запуска сервисов дождитесь полной загрузки backend-сервиса (вы должны увидеть <strong>backend: &quot;GET /health HTTP/1.1&quot; 200 OK</strong> в логе) перед отправкой каких-либо сообщений. При первом запуске backend-сервис может запускаться до 5 минут.</p>
<p>Перейдите на <code>http://localhost:3000/</code> — вы должны увидеть веб-интерфейс.</p>
<p><em>Устранение проблем с запуском сервисов:</em> Если эти скрипты не срабатывают, убедитесь, что Docker Engine запущен и Docker Compose (V2, <code>docker compose</code>) корректно установлен. Проверьте сообщения об ошибках в терминале. См. <a href="#faq-troubleshooting">FAQ: Помогите! Я получаю ошибку при запуске AgenticSeek или его скриптов.</a></p>
<p><strong>Необязательно:</strong> Запуск на хосте (CLI-режим):</p>
<p>Чтобы использовать CLI-интерфейс, необходимо установить пакет на хосте:</p>
<pre><code class="language-sh">./install.sh
./install.bat # windows
</code></pre>
<p>Запустите сервисы:</p>
<pre><code class="language-sh">./start_services.sh # MacOS
start ./start_services.cmd # Window
</code></pre>
<p>Используйте CLI: <code>python3 cli.py</code></p>
<hr />
<h2>Использование</h2>
<p>Убедитесь, что сервисы запущены командой <code>./start_services.sh full</code>, и перейдите на <code>localhost:3000</code> для доступа к веб-интерфейсу.</p>
<p>Вы также можете использовать преобразование речи в текст, установив <code>listen = True</code> в конфиге. Только для CLI-режима.</p>
<p>Для выхода просто скажите/напишите <code>goodbye</code>.</p>
<p>Вот несколько примеров использования:</p>
<blockquote>
<p><em>Сделай игру &quot;змейка&quot; на python!</em></p>
</blockquote>
<blockquote>
<p><em>Найди лучшие кафе в Ренне (Франция) через интернет и сохрани список из трёх с их адресами в rennes_cafes.txt.</em></p>
</blockquote>
<blockquote>
<p><em>Напиши программу на Go для вычисления факториала числа, сохрани как factorial.go в свою рабочую папку</em></p>
</blockquote>
<blockquote>
<p><em>Найди в папке summer_pictures все файлы JPG, переименуй их с сегодняшней датой и сохрани список переименованных файлов в photos_list.txt</em></p>
</blockquote>
<blockquote>
<p><em>Ищи онлайн популярные научно-фантастические фильмы 2024 года и выбери три для просмотра сегодня вечером. Сохрани список в movie_night.txt.</em></p>
</blockquote>
<blockquote>
<p><em>Найди последние новости об ИИ за 2025 год, выбери три статьи и напиши скрипт на Python для парсинга их заголовков и кратких описаний. Сохрани скрипт как news_scraper.py, а описания в ai_news.txt в /home/projects</em></p>
</blockquote>
<blockquote>
<p><em>В пятницу найди бесплатный API для котировок акций, зарегистрируйся с supersuper7434567@gmail.com, затем напиши скрипт на Python для ежедневного получения цен Tesla по этому API и сохрани результаты в stock_prices.csv</em></p>
</blockquote>
<p><em>Обратите внимание, что заполнение форм пока экспериментальное и может не работать.</em></p>
<p>После ввода вашего запроса AgenticSeek выделит для задачи подходящего агента.</p>
<p>Поскольку это ранний прототип, система маршрутизации агентов может не всегда выбирать правильного агента по вашему запросу.</p>
<p>Поэтому старайтесь максимально чётко формулировать, что вы хотите и как AI должен действовать. Например, если вы хотите, чтобы он провёл веб-поиск, не говорите:</p>
<p><code>Ты знаешь хорошие страны для одиночных путешествий?</code></p>
<p>Вместо этого спросите:</p>
<p><code>Сделай веб-поиск и выясни, какие страны лучше всего подходят для одиночных путешествий</code></p>
<hr />
<h2><strong>Настройка для запуска LLM на вашем сервере</strong></h2>
<p>Если у вас есть мощный компьютер или сервер, который вы хотите использовать удалённо со своего ноутбука, вы можете запустить LLM на удалённом сервере с помощью нашего собственного llm-сервера.</p>
<p>На вашем &quot;сервере&quot;, где будет запущена AI-модель, получите IP-адрес</p>
<pre><code class="language-sh">ip a | grep &quot;inet &quot; | grep -v 127.0.0.1 | awk '{print $2}' | cut -d/ -f1 # локальный ip
curl https://ipinfo.io/ip # публичный ip
</code></pre>
<p>Примечание: для Windows или macOS используйте соответственно ipconfig или ifconfig для определения IP-адреса.</p>
<p>Клонируйте репозиторий и перейдите в папку <code>server/</code>.</p>
<pre><code class="language-sh">git clone --depth 1 https://github.com/Fosowl/agenticSeek.git
cd agenticSeek/llm_server/
</code></pre>
<p>Установите специфические для сервера зависимости:</p>
<pre><code class="language-sh">pip3 install -r requirements.txt
</code></pre>
<p>Запустите серверный скрипт.</p>
<pre><code class="language-sh">python3 app.py --provider ollama --port 3333
</code></pre>
<p>Вы можете выбрать между использованием <code>ollama</code> и <code>llamacpp</code> как сервиса LLM.</p>
<p>Теперь на вашем персональном компьютере:</p>
<p>Измените файл <code>config.ini</code>, чтобы установить <code>provider_name</code> в <code>server</code> и <code>provider_model</code> в <code>deepseek-r1:xxb</code>.
Установите <code>provider_server_address</code> на IP-адрес машины, где будет запускаться модель.</p>
<pre><code class="language-sh">[MAIN]
is_local = False
provider_name = server
provider_model = deepseek-r1:70b
provider_server_address = x.x.x.x:3333
</code></pre>
<p>Следующий шаг: <a href="#Start-services-and-Run">Запустите сервисы и AgenticSeek</a></p>
<hr />
<h2>Преобразование речи в текст</h2>
<p>Внимание: функция распознавания речи работает только в CLI-режиме на данный момент.</p>
<p>Обратите внимание, что сейчас речь в текст работает только на английском языке.</p>
<p>Функция распознавания речи по умолчанию отключена. Чтобы включить её, установите опцию listen в True в файле config.ini:</p>
<pre><code>listen = True
</code></pre>
<p>Когда функция включена, перед началом обработки вашего ввода она будет слушать ключевое слово-триггер, которым является имя агента. Вы можете изменить имя агента, изменив значение <code>agent_name</code> в файле <em>config.ini</em>:</p>
<pre><code>agent_name = Friday
</code></pre>
<p>Для оптимального распознавания рекомендуется использовать распространённое английское имя, например &quot;John&quot; или &quot;Emma&quot;, в качестве имени агента.</p>
<p>Как только вы увидите, что транскрипция начала появляться, произнесите имя агента вслух, чтобы разбудить его (например, &quot;Friday&quot;).</p>
<p>Чётко озвучьте свой запрос.</p>
<p>Завершите свой запрос подтверждающей фразой, чтобы система приступила к выполнению. Примеры подтверждающих фраз:</p>
<pre><code>&quot;do it&quot;, &quot;go ahead&quot;, &quot;execute&quot;, &quot;run&quot;, &quot;start&quot;, &quot;thanks&quot;, &quot;would ya&quot;, &quot;please&quot;, &quot;okay?&quot;, &quot;proceed&quot;, &quot;continue&quot;, &quot;go on&quot;, &quot;do that&quot;, &quot;go it&quot;, &quot;do you understand?&quot;
</code></pre>
<h2>Config</h2>
<p>Пример конфигурации:</p>
<pre><code>[MAIN]
is_local = True
provider_name = ollama
provider_model = deepseek-r1:32b
provider_server_address = http://127.0.0.1:11434 # Пример для Ollama; используйте http://127.0.0.1:1234 для LM-Studio
agent_name = Friday
recover_last_session = False
save_session = False
speak = False
listen = False

jarvis_personality = False
languages = en zh # Список языков для TTS и, возможно, маршрутизации.
[BROWSER]
headless_browser = False
stealth_mode = False
</code></pre>
<p><strong>Пояснение к настройкам <code>config.ini</code></strong>:</p>
<ul>
<li><strong>Раздел <code>[MAIN]</code>:</strong>
<ul>
<li><code>is_local</code>: <code>True</code>, если используется локальный LLM-провайдер (Ollama, LM-Studio, локальный сервер совместимый с OpenAI) или опция самостоятельного размещения. <code>False</code>, если используется облачный API (OpenAI, Google и др.).</li>
<li><code>provider_name</code>: Указывает провайдера LLM.
<ul>
<li>Локальные варианты: <code>ollama</code>, <code>lm-studio</code>, <code>openai</code> (для локальных серверов, совместимых с OpenAI), <code>server</code> (для самостоятельного размещения).</li>
<li>Варианты API: <code>openai</code>, <code>google</code>, <code>deepseek</code>, <code>huggingface</code>, <code>togetherAI</code>.</li>
</ul>
</li>
<li><code>provider_model</code>: Конкретное название или ID модели для выбранного провайдера (например, <code>deepseekcoder:6.7b</code> для Ollama, <code>gpt-3.5-turbo</code> для OpenAI API, <code>mistralai/Mixtral-8x7B-Instruct-v0.1</code> для TogetherAI).</li>
<li><code>provider_server_address</code>: Адрес вашего LLM-провайдера.
<ul>
<li>Для локальных провайдеров: например, <code>http://127.0.0.1:11434</code> для Ollama, <code>http://127.0.0.1:1234</code> для LM-Studio.</li>
<li>Для типа провайдера <code>server</code>: адрес вашего собственного сервера LLM (например, <code>http://your_server_ip:3333</code>).</li>
<li>Для облачных API (<code>is_local = False</code>): часто игнорируется или может быть пустым, так как конечная точка API обычно обрабатывается клиентской библиотекой.</li>
</ul>
</li>
<li><code>agent_name</code>: Имя AI-ассистента (например, Friday). Используется как ключевое слово для активации распознавания речи, если оно включено.</li>
<li><code>recover_last_session</code>: <code>True</code> — попытка восстановить состояние предыдущей сессии, <code>False</code> — начать новую.</li>
<li><code>save_session</code>: <code>True</code> — сохранить состояние текущей сессии для возможного восстановления, иначе <code>False</code>.</li>
<li><code>speak</code>: <code>True</code> — включить голосовой вывод (TTS), <code>False</code> — отключить.</li>
<li><code>listen</code>: <code>True</code> — включить голосовой ввод (распознавание речи, только CLI режим), <code>False</code> — отключить.</li>
<li><code>work_dir</code>: <strong>Важное значение:</strong> Каталог, в котором AgenticSeek будет читать и записывать файлы. <strong>Убедитесь, что этот путь существует и доступен на вашей системе.</strong></li>
<li><code>jarvis_personality</code>: <code>True</code> — использовать более &quot;Jarvis-подобный&quot; системный prompt (экспериментально), <code>False</code> — стандартный prompt.</li>
<li><code>languages</code>: Перечисление языков через запятую (например, <code>en, zh, fr</code>). Используется для выбора голоса TTS (по умолчанию — первый язык) и может помочь маршрутизатору LLM. Не указывайте слишком много или очень похожих языков для эффективности.</li>
</ul>
</li>
<li><strong>Раздел <code>[BROWSER]</code>:</strong>
<ul>
<li><code>headless_browser</code>: <code>True</code> — запускать автоматизированный браузер без видимого окна (рекомендуется для веб-интерфейса или неинтерактивного использования). <code>False</code> — показать окно браузера (удобно для CLI или отладки).</li>
<li><code>stealth_mode</code>: <code>True</code> — включить меры против обнаружения автоматизации браузера. Может потребовать ручной установки расширений, например, anticaptcha.</li>
</ul>
</li>
</ul>
<p>В этом разделе приводится сводка поддерживаемых типов LLM-провайдеров. Настройте их в <code>config.ini</code>.</p>
<p><strong>Локальные провайдеры (на вашем оборудовании):</strong></p>
<p>| Имя провайдера в <code>config.ini</code> | <code>is_local</code> | Описание                                                                  | Раздел настройки                                                    |
|-------------------------------|------------|---------------------------------------------------------------------------|---------------------------------------------------------------------|
| <code>ollama</code>                      | <code>True</code>     | Использовать Ollama для локального запуска LLM.                           | <a href="#setup-for-running-llm-locally-on-your-machine">Настройка локального LLM</a> |
| <code>lm-studio</code>                   | <code>True</code>     | Использовать LM-Studio для локального запуска LLM.                        | <a href="#setup-for-running-llm-locally-on-your-machine">Настройка локального LLM</a> |
| <code>openai</code> (для локального сервера) | <code>True</code> | Подключение к локальному серверу с OpenAI-совместимым API (например, llama.cpp). | <a href="#setup-for-running-llm-locally-on-your-machine">Настройка локального LLM</a> |
| <code>server</code>                      | <code>False</code>    | Подключение к самостоятельному серверу AgenticSeek LLM на другом устройстве. | <a href="#setup-to-run-the-llm-on-your-own-server">Настройка собственного сервера LLM</a> |</p>
<p><strong>API-провайдеры (облачные):</strong></p>
<p>| Имя провайдера в <code>config.ini</code> | <code>is_local</code> | Описание                                        | Раздел настройки                                       |
|-------------------------------|------------|--------------------------------------------------|--------------------------------------------------------|
| <code>openai</code>                      | <code>False</code>    | Использовать официальный API OpenAI (например, GPT-3.5, GPT-4). | <a href="#setup-to-run-with-an-api">Настройка с API</a>           |
| <code>google</code>                      | <code>False</code>    | Использовать модели Gemini от Google через API.  | <a href="#setup-to-run-with-an-api">Настройка с API</a>           |
| <code>deepseek</code>                    | <code>False</code>    | Использовать официальный API Deepseek.           | <a href="#setup-to-run-with-an-api">Настройка с API</a>           |
| <code>huggingface</code>                 | <code>False</code>    | Использовать API Hugging Face Inference.         | <a href="#setup-to-run-with-an-api">Настройка с API</a>           |
| <code>togetherAI</code>                  | <code>False</code>    | Использовать API TogetherAI для различных open моделей. | <a href="#setup-to-run-with-an-api">Настройка с API</a>           |</p>
<hr />
<h2>Поиск и устранение неисправностей</h2>
<p>Если вы сталкиваетесь с проблемами, этот раздел поможет вам.</p>
<h1>Известные проблемы</h1>
<h2>Проблемы с ChromeDriver</h2>
<p><strong>Пример ошибки:</strong> <code>SessionNotCreatedException: Message: session not created: This version of ChromeDriver only supports Chrome version XXX</code></p>
<ul>
<li><strong>Причина:</strong> Установленная версия ChromeDriver не совместима с вашей версией браузера Google Chrome.</li>
<li><strong>Решение:</strong>
<ol>
<li><strong>Проверьте версию Chrome:</strong> Откройте Google Chrome, перейдите в <code>Настройки &gt; О Chrome</code>, чтобы узнать версию (например, &quot;Версия 120.0.6099.110&quot;).</li>
<li><strong>Скачайте подходящий ChromeDriver:</strong>
<ul>
<li>Для версий Chrome 115 и новее: Перейдите на страницу <a href="https://googlechromelabs.github.io/chrome-for-testing/">Chrome for Testing (CfT) JSON Endpoints</a>. Найдите канал &quot;stable&quot; и скачайте ChromeDriver для вашей ОС, соответствующий основной версии Chrome.</li>
<li>Для более старых версий (редко): Попробуйте найти их на странице <a href="https://chromedriver.chromium.org/downloads">ChromeDriver - WebDriver for Chrome</a>.</li>
<li>На изображении ниже показан пример CfT страницы:
<img src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/chromedriver_readme.png" alt="Download Chromedriver specific version from Chrome for Testing page" /></li>
</ul>
</li>
<li><strong>Установите ChromeDriver:</strong>
<ul>
<li>Убедитесь, что скачанный <code>chromedriver</code> (или <code>chromedriver.exe</code> на Windows) находится в каталоге, который прописан в переменной PATH вашей системы (например, <code>/usr/local/bin</code> на Linux/macOS или папка со скриптами, добавленная в PATH на Windows).</li>
<li>Либо поместите его в корневую папку проекта <code>agenticSeek</code>.</li>
<li>Проверьте, что драйвер исполняемый (например, выполните <code>chmod +x chromedriver</code> на Linux/macOS).</li>
</ul>
</li>
<li>Подробнее смотрите в разделе <a href="#chromedriver-installation">ChromeDriver Installation</a> в основном руководстве по установке.</li>
</ol>
</li>
</ul>
<p>Если этот раздел неполный или вы столкнулись с другими проблемами ChromeDriver, попробуйте поискать похожие <a href="https://github.com/Fosowl/agenticSeek/issues">GitHub Issues</a> или создайте новую заявку.</p>
<p><code>Exception: Failed to initialize browser: Message: session not created: This version of ChromeDriver only supports Chrome version 113 Current browser version is 134.0.6998.89 with binary path</code></p>
<p>Это возникает, если версии браузера и chromedriver не совпадают.</p>
<p>Вам нужно скачать последнюю версию по адресу:</p>
<p>https://developer.chrome.com/docs/chromedriver/downloads</p>
<p>Если вы используете Chrome версии 115 или новее, перейдите по адресу:</p>
<p>https://googlechromelabs.github.io/chrome-for-testing/</p>
<p>И скачайте chromedriver, соответствующий вашей ОС.</p>
<p><img src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/chromedriver_readme.png" alt="alt text" /></p>
<p>Если этот раздел неполный, пожалуйста, создайте issue.</p>
<h2>Проблемы connection adapters</h2>
<pre><code>Exception: Provider lm-studio failed: HTTP request failed: No connection adapters were found for '127.0.0.1:1234/v1/chat/completions'` (Примечание: порт может отличаться)
</code></pre>
<ul>
<li><strong>Причина:</strong> В <code>provider_server_address</code> в <code>config.ini</code> для <code>lm-studio</code> (или других локальных серверов, совместимых с OpenAI) отсутствует префикс <code>http://</code> или указан неверный порт.</li>
<li><strong>Решение:</strong>
<ul>
<li>Убедитесь, что адрес начинается с <code>http://</code>. По умолчанию для LM-Studio: <code>http://127.0.0.1:1234</code>.</li>
<li>Исправьте <code>config.ini</code>: <code>provider_server_address = http://127.0.0.1:1234</code> (или ваш реальный порт LM-Studio).</li>
</ul>
</li>
</ul>
<h2>Не указан базовый URL SearxNG</h2>
<pre><code>raise ValueError(&quot;SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.&quot;)
ValueError: SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.`
</code></pre>
<h2>FAQ</h2>
<p><strong>В: Какое железо мне нужно?</strong></p>
<p>| Размер модели | GPU  | Комментарий                                               |
|--------------|------|-----------------------------------------------------------|
| 7B           | 8ГБ VRAM | ⚠️ Не рекомендуется. Низкая производительность, частые галлюцинации, агенты-планировщики, скорее всего, не будут работать. |
| 14B          | 12ГБ VRAM (например, RTX 3060) | ✅ Подходит для простых задач. Может испытывать трудности с веб-браузингом и планированием. |
| 32B          | 24+ ГБ VRAM (например, RTX 4090) | 🚀 Большинство задач успешно, могут быть сложности с планированием задач. |
| 70B+         | 48+ ГБ VRAM | 💪 Отлично. Рекомендуется для продвинутых сценариев. |</p>
<p><strong>В: Я получаю ошибку — что делать?</strong></p>
<p>Убедитесь, что локальный сервер запущен (<code>ollama serve</code>), ваш <code>config.ini</code> соответствует провайдеру, и все зависимости установлены. Если ничего не помогает — создайте issue.</p>
<p><strong>В: Может ли всё работать полностью локально?</strong></p>
<p>Да, с провайдерами Ollama, lm-studio или server, все компоненты — распознавание речи, LLM и синтез речи — работают локально. Не локальные варианты (OpenAI или другие API) — опциональны.</p>
<p><strong>В: Почему стоит использовать AgenticSeek, если есть Manus?</strong></p>
<p>В отличие от Manus, AgenticSeek ориентирован на независимость от внешних систем, что даёт больше контроля, приватности и позволяет избежать затрат на API.</p>
<p><strong>В: Кто стоит за проектом?</strong></p>
<p>Проект создан мной и двумя друзьями, которые также являются мейнтейнерами и контрибьюторами из open-source сообщества на GitHub. Мы — просто группа энтузиастов, не стартап и не связаны с какими-либо организациями.</p>
<p>Любой аккаунт AgenticSeek на X, кроме моего личного (https://x.com/Martin993886460), — это подделка.</p>
<h2>Вклад</h2>
<p>Мы ищем разработчиков для улучшения AgenticSeek! Смотрите открытые issue или обсуждения.</p>
<p><a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/docs/CONTRIBUTING.md">Руководство по вкладу</a></p>
<p><a href="https://www.star-history.com/#Fosowl/agenticSeek&amp;Date"><img src="https://api.star-history.com/svg?repos=Fosowl/agenticSeek&amp;type=Date" alt="Star History Chart" /></a></p>
<h2>Мейнтейнеры:</h2>
<blockquote>
<p><a href="https://github.com/Fosowl">Fosowl</a> | Парижское время</p>
</blockquote>
<blockquote>
<p><a href="https://github.com/antoineVIVIES">antoineVIVIES</a> | Тайбэйское время</p>
</blockquote>
<blockquote>
<p><a href="https://github.com/steveh8758">steveh8758</a> | Тайбэйское время</p>
</blockquote>
<h2>Особая благодарность:</h2>
<blockquote>
<p><a href="https://github.com/tcsenpai">tcsenpai</a> и <a href="https://github.com/plitc">plitc</a> за помощь с docker-изация backend</p>
</blockquote>
<h2>Спонсоры:</h2>
<p>5$ и более — здесь отображаются ежемесячные спонсоры:</p>
<ul>
<li><strong>tatra-labs</strong></li>
</ul>
<p>Certainly! Please provide the content for Part 4 of 4 that you would like translated.</p>
<hr />
<p>Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx">Open Ai Tx</a> | Last indexed: 2025-06-16</p>
<hr />

        </div>
    </div>
    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Statcounter and other scripts can be added here -->
</body>
</html>