<!DOCTYPE html>
<html lang="nl">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>agenticSeek - Fosowl/agenticSeek nl</title>
    <meta name="title" content="agenticSeek - Fosowl/agenticSeek nl | AgenticSeek: Privé, Lokaal Manus-alternatief. Engels | 中文 | 繁體中文 | Français | 日本語 | Portugees (Brazilië) | Spaans Een 100% lokaal alternatief voor Manus AI, dez...">
    <meta name="description" content="Fosowl/agenticSeek - GitHub repository nl documentation and information | AgenticSeek: Privé, Lokaal Manus-alternatief. Engels | 中文 | 繁體中文 | Français | 日本語 | Portugees (Brazilië) | Spaans Een 100% lokaal alternatief voor Manus AI, dez...">
    <meta name="keywords" content="Fosowl, agenticSeek, GitHub, repository, nl documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/Fosowl/agenticSeek/README-nl.html">
    <meta property="og:title" content="agenticSeek - Fosowl/agenticSeek nl | AgenticSeek: Privé, Lokaal Manus-alternatief. Engels | 中文 | 繁體中文 | Français | 日本語 | Portugees (Brazilië) | Spaans Een 100% lokaal alternatief voor Manus AI, dez...">
    <meta property="og:description" content="Fosowl/agenticSeek - GitHub repository nl documentation and information | AgenticSeek: Privé, Lokaal Manus-alternatief. Engels | 中文 | 繁體中文 | Français | 日本語 | Portugees (Brazilië) | Spaans Een 100% lokaal alternatief voor Manus AI, dez...">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">
    <script src="/js/marked.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        body { display: flex; flex-direction: column; min-height: 100vh; }
        .main-container { margin: 0 auto; width: 100%; max-width: 980px; padding: 0 20px; }
        @media (max-width: 768px) { .main-container { padding: 0 15px; } }
        .markdown-body img { max-width: 100%; height: auto; }
        .header { text-align: center; margin-bottom: 30px; padding: 20px; background-color: #f6f8fa; border-bottom: 1px solid #e1e4e8; position: relative; }
        .header .links { margin-top: 10px; font-size: 16px; }
        .header .links a { color: #0366d6; text-decoration: none; margin-left: 5px; }
        .header .links a:hover { text-decoration: underline; }
        .language-badges { margin-top: 15px; text-align: center; }
        .language-badges a { display: inline-block; margin: 2px; text-decoration: none; }
        .language-badges img { height: 20px; border-radius: 3px; }
        .language-badges a:hover img { opacity: 0.8; }
    </style>
</head>
<body>
    <div style="position: fixed; top: 2px; left: 2px; z-index: 2000; background: rgba(255,255,255,0.95); border-radius: 6px; box-shadow: 0 2px 8px rgba(0,0,0,0.06); padding: 4px 14px; font-size: 15px; color: #222; font-family: 'Segoe UI', Arial, sans-serif; font-weight: 500; letter-spacing: 0.5px;">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" style="color: #0366d6; text-decoration: none; font-weight: 600;">Open AI Tx</a>
    </div>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/Fosowl/agenticSeek" id="githubRepoLink" target="_blank">Fosowl/agenticSeek</a>
<h1 style="display: none;">AgenticSeek: Privé, Lokaal Manus-alternatief. Engels | 中文 | 繁體中文 | Français | 日本語 | Portugees (Brazilië) | Spaans Een 100% lokaal alternatief voor Manus AI, dez...</h1>
        </div>
        <div class="language-badges" id="languageBadges">
            <!-- You can generate language badges here if needed -->
        </div>
    </div>
    <div class="main-container">
        <div class="markdown-body" id="content">
            <h1>AgenticSeek: Privé, Lokaal Manus-alternatief.</h1>
<p align="center">
<img align="center" src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/agentic_seek_logo.png" width="300" height="300" alt="Agentic Seek Logo">
<p>
<p>Engels | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_CHS.md">中文</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_CHT.md">繁體中文</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_FR.md">Français</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_JP.md">日本語</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_PTBR.md">Portugees (Brazilië)</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_ES.md">Spaans</a></p>
<p><em>Een <strong>100% lokaal alternatief voor Manus AI</strong>, deze stemgestuurde AI-assistent doorzoekt autonoom het web, schrijft code en plant taken, terwijl alle data op je apparaat blijft. Speciaal ontworpen voor lokale redeneermodellen, draait volledig op jouw hardware, wat volledige privacy en nul cloud-afhankelijkheid garandeert.</em></p>
<p><a href="https://fosowl.github.io/agenticSeek.html"><img src="https://img.shields.io/static/v1?label=Website&amp;message=AgenticSeek&amp;color=blue&amp;style=flat-square" alt="Bezoek AgenticSeek" /></a> <img src="https://img.shields.io/badge/license-GPL--3.0-green" alt="Licentie" /> <a href="https://discord.gg/8hGDaME3TC"><img src="https://img.shields.io/badge/Discord-Join%20Us-7289DA?logo=discord&amp;logoColor=white" alt="Discord" /></a> <a href="https://x.com/Martin993886460"><img src="https://img.shields.io/twitter/url/https/twitter.com/fosowl.svg?style=social&amp;label=Update%20%40Fosowl" alt="Twitter" /></a> <a href="https://github.com/Fosowl/agenticSeek/stargazers"><img src="https://img.shields.io/github/stars/Fosowl/agenticSeek?style=social" alt="GitHub stars" /></a></p>
<h3>Waarom AgenticSeek?</h3>
<ul>
<li><p>🔒 Volledig Lokaal &amp; Privé – Alles draait op jouw machine — geen cloud, geen datadeling. Jouw bestanden, gesprekken en zoekopdrachten blijven privé.</p>
</li>
<li><p>🌐 Slimme Webbrowser – AgenticSeek kan zelfstandig internetten — zoeken, lezen, info extraheren, webformulieren invullen — volledig handsfree.</p>
</li>
<li><p>💻 Autonome Code-assistent – Code nodig? Schrijft, debugt en voert programma's uit in Python, C, Go, Java en meer — geheel zonder toezicht.</p>
</li>
<li><p>🧠 Slimme Agentselectie – Jij vraagt, het kiest automatisch de beste agent voor de taak. Net alsof je een team van experts tot je beschikking hebt.</p>
</li>
<li><p>📋 Plant &amp; Voert Complexe Taken Uit – Van reisplanning tot complexe projecten — het splitst grote taken op in stappen en voert deze uit met meerdere AI-agents.</p>
</li>
<li><p>🎙️ Stemgestuurd – Schone, snelle, futuristische spraak-naar-tekst en tekst-naar-spraak, zodat je kunt praten alsof het je persoonlijke AI uit een sciencefictionfilm is. (In ontwikkeling)</p>
</li>
</ul>
<h3><strong>Demo</strong></h3>
<blockquote>
<p><em>Kun je zoeken naar het agenticSeek-project, uitzoeken welke vaardigheden vereist zijn, vervolgens het bestand CV_candidates.zip openen en me vertellen welke kandidaten het beste bij het project passen?</em></p>
</blockquote>
<p>https://github.com/user-attachments/assets/b8ca60e9-7b3b-4533-840e-08f9ac426316</p>
<p>Disclaimer: Deze demo, inclusief alle getoonde bestanden (bijv: CV_candidates.zip), is volledig fictief. Wij zijn geen bedrijf, we zoeken open-source bijdragers, geen kandidaten.</p>
<blockquote>
<p>🛠⚠️️ <strong>Actief Werk in Uitvoering</strong></p>
</blockquote>
<blockquote>
<p>🙏 Dit project begon als een zijproject en heeft geen roadmap of financiering. Het is uitgegroeid tot meer dan verwacht en eindigde zelfs in GitHub Trending. Bijdragen, feedback en geduld worden zeer gewaardeerd.</p>
</blockquote>
<h2>Vereisten</h2>
<p>Voordat je begint, zorg ervoor dat je de volgende software hebt geïnstalleerd:</p>
<ul>
<li><strong>Git:</strong> Voor het klonen van de repository. <a href="https://git-scm.com/downloads">Download Git</a></li>
<li><strong>Python 3.10.x:</strong> We raden sterk aan om Python versie 3.10.x te gebruiken. Andere versies kunnen afhankelijkheidsfouten veroorzaken. <a href="https://www.python.org/downloads/release/python-3100/">Download Python 3.10</a> (kies een 3.10.x versie).</li>
<li><strong>Docker Engine &amp; Docker Compose:</strong> Voor het draaien van gebundelde diensten zoals SearxNG.
<ul>
<li>Installeer Docker Desktop (inclusief Docker Compose V2): <a href="https://docs.docker.com/desktop/install/windows-install/">Windows</a> | <a href="https://docs.docker.com/desktop/install/mac-install/">Mac</a> | <a href="https://docs.docker.com/desktop/install/linux-install/">Linux</a></li>
<li>Of installeer Docker Engine en Docker Compose apart op Linux: <a href="https://docs.docker.com/engine/install/">Docker Engine</a> | <a href="https://docs.docker.com/compose/install/">Docker Compose</a> (zorg dat je Compose V2 installeert, bijv. <code>sudo apt-get install docker-compose-plugin</code>).</li>
</ul>
</li>
</ul>
<h3>1. <strong>Kloon de repository en stel in</strong></h3>
<pre><code class="language-sh">git clone https://github.com/Fosowl/agenticSeek.git
cd agenticSeek
mv .env.example .env
</code></pre>
<h3>2. Wijzig de inhoud van het .env-bestand</h3>
<pre><code class="language-sh">SEARXNG_BASE_URL=&quot;http://127.0.0.1:8080&quot;
REDIS_BASE_URL=&quot;redis://redis:6379/0&quot;
WORK_DIR=&quot;/Users/mlg/Documents/workspace_for_ai&quot;
OLLAMA_PORT=&quot;11434&quot;
LM_STUDIO_PORT=&quot;1234&quot;
CUSTOM_ADDITIONAL_LLM_PORT=&quot;11435&quot;
OPENAI_API_KEY='optioneel'
DEEPSEEK_API_KEY='optioneel'
OPENROUTER_API_KEY='optioneel'
TOGETHER_API_KEY='optioneel'
GOOGLE_API_KEY='optioneel'
ANTHROPIC_API_KEY='optioneel'
</code></pre>
<p>Werk het <code>.env</code>-bestand bij met je eigen waarden waar nodig:</p>
<ul>
<li><strong>SEARXNG_BASE_URL</strong>: Ongewijzigd laten</li>
<li><strong>REDIS_BASE_URL</strong>: Ongewijzigd laten</li>
<li><strong>WORK_DIR</strong>: Pad naar je werkmap op je lokale machine. AgenticSeek kan deze bestanden lezen en ermee werken.</li>
<li><strong>OLLAMA_PORT</strong>: Poortnummer voor de Ollama-service.</li>
<li><strong>LM_STUDIO_PORT</strong>: Poortnummer voor de LM Studio-service.</li>
<li><strong>CUSTOM_ADDITIONAL_LLM_PORT</strong>: Poort voor elke extra custom LLM-service.</li>
</ul>
<p><strong>API-sleutels zijn volledig optioneel voor gebruikers die ervoor kiezen om LLM lokaal uit te voeren. Dit is het primaire doel van dit project. Laat leeg als je over voldoende hardware beschikt</strong></p>
<h3>3. <strong>Start Docker</strong></h3>
<p>Zorg dat Docker is geïnstalleerd en actief op je systeem. Je kunt Docker starten met de volgende commando's:</p>
<ul>
<li><p><strong>Op Linux/macOS:</strong><br />
Open een terminal en voer uit:</p>
<pre><code class="language-sh">sudo systemctl start docker
</code></pre>
<p>Of start Docker Desktop vanuit je applicatiemenu als deze is geïnstalleerd.</p>
</li>
<li><p><strong>Op Windows:</strong><br />
Start Docker Desktop via het Startmenu.</p>
</li>
</ul>
<p>Je kunt controleren of Docker draait door het volgende uit te voeren:</p>
<pre><code class="language-sh">docker info
</code></pre>
<p>Zie je informatie over je Docker-installatie, dan werkt Docker correct.</p>
<p>Bekijk de tabel met <a href="#list-of-local-providers">Lokale Providers</a> hieronder voor een overzicht.</p>
<p>Volgende stap: <a href="#start-services-and-run">Voer AgenticSeek lokaal uit</a></p>
<p><em>Kijk in de sectie <a href="#troubleshooting">Probleemoplossing</a> als je problemen ondervindt.</em>
<em>Kan jouw hardware LLM's niet lokaal draaien, zie <a href="#setup-to-run-with-an-api">Installatie voor gebruik met een API</a>.</em>
<em>Voor gedetailleerde uitleg over <code>config.ini</code>, zie <a href="#config">Config Sectie</a>.</em></p>
<hr />
<h2>Installatie voor lokaal draaien van LLM op je eigen machine</h2>
<p><strong>Hardwarevereisten:</strong></p>
<p>Om LLM's lokaal te draaien heb je voldoende hardware nodig. Minimaal is een GPU vereist die Magistral, Qwen of Deepseek 14B kan draaien. Zie de FAQ voor gedetailleerde model- en prestatieaanbevelingen.</p>
<p><strong>Stel je lokale provider in</strong></p>
<p>Start je lokale provider, bijvoorbeeld met ollama:</p>
<pre><code class="language-sh">ollama serve
</code></pre>
<p>Zie hieronder voor een lijst van lokaal ondersteunde providers.</p>
<p><strong>Werk de config.ini bij</strong></p>
<p>Wijzig het config.ini-bestand om provider_name in te stellen op een ondersteunde provider en provider_model op een LLM die door jouw provider wordt ondersteund. We raden redeneermodellen aan zoals <em>Magistral</em> of <em>Deepseek</em>.</p>
<p>Zie de <strong>FAQ</strong> aan het einde van de README voor benodigde hardware.</p>
<pre><code class="language-sh">[MAIN]
is_local = True # Of je lokaal of met een externe provider draait.
provider_name = ollama # of lm-studio, openai, etc..
provider_model = deepseek-r1:14b # kies een model dat past bij je hardware
provider_server_address = 127.0.0.1:11434
agent_name = Jarvis # naam van je AI
recover_last_session = True # of je de vorige sessie herstelt
save_session = True # of je de huidige sessie onthoudt
speak = False # tekst-naar-spraak
listen = False # spraak-naar-tekst, alleen voor CLI, experimenteel
jarvis_personality = False # Of je een meer &quot;Jarvis&quot;-achtige persoonlijkheid gebruikt (experimenteel)
languages = en zh # De lijst van talen, tekst-naar-spraak gebruikt standaard de eerste taal in de lijst
[BROWSER]
headless_browser = True # laat ongewijzigd tenzij je CLI op host gebruikt.
stealth_mode = True # Gebruik undetected selenium om browserdetectie te verminderen
</code></pre>
<p><strong>Waarschuwing</strong>:</p>
<ul>
<li><p>Het <code>config.ini</code>-bestand ondersteunt geen opmerkingen.
Kopieer en plak de voorbeeldconfiguratie niet direct, omdat opmerkingen fouten zullen veroorzaken. Wijzig het <code>config.ini</code>-bestand handmatig met je gewenste instellingen, zonder opmerkingen.</p>
</li>
<li><p>Stel <em>NIET</em> provider_name in op <code>openai</code> als je LM-studio gebruikt voor het draaien van LLM's. Stel in op <code>lm-studio</code>.</p>
</li>
<li><p>Sommige providers (bijv. lm-studio) vereisen dat je <code>http://</code> voor het IP-adres plaatst. Bijvoorbeeld <code>http://127.0.0.1:1234</code></p>
</li>
</ul>
<p><strong>Lijst van lokale providers</strong></p>
<p>| Provider  | Lokaal? | Beschrijving                                               |
|-----------|--------|-----------------------------------------------------------|
| ollama    | Ja     | Draai LLM's lokaal met ollama als LLM-provider            |
| lm-studio | Ja     | Draai LLM lokaal met LM studio (zet <code>provider_name</code> op <code>lm-studio</code>)|
| openai    | Ja     | Gebruik openai-compatibele API (bijv: llama.cpp server)   |</p>
<p>Volgende stap: <a href="#Start-services-and-Run">Start services en voer AgenticSeek uit</a></p>
<p><em>Kijk in de sectie <a href="#troubleshooting">Probleemoplossing</a> als je problemen ondervindt.</em>
<em>Kan jouw hardware LLM's niet lokaal draaien, zie <a href="#setup-to-run-with-an-api">Installatie voor gebruik met een API</a>.</em>
<em>Voor gedetailleerde uitleg over <code>config.ini</code>, zie <a href="#config">Config Sectie</a>.</em></p>
<h2>Installatie voor gebruik met een API</h2>
<p>Deze installatie gebruikt externe, cloudgebaseerde LLM-providers. Je hebt een API-sleutel nodig van de gekozen dienst.</p>
<p><strong>1. Kies een API-provider en verkrijg een API-sleutel:</strong></p>
<p>Raadpleeg de <a href="#list-of-api-providers">Lijst van API-providers</a> hieronder. Bezoek hun websites om je aan te melden en een API-sleutel te verkrijgen.</p>
<p><strong>2. Stel je API-sleutel in als omgevingsvariabele:</strong></p>
<ul>
<li><strong>Linux/macOS:</strong>
Open je terminal en gebruik het <code>export</code>-commando. Voeg dit het beste toe aan je shell-profielbestand (bijv. <code>~/.bashrc</code>, <code>~/.zshrc</code>) voor blijvende werking.
<pre><code class="language-sh">export PROVIDER_API_KEY=&quot;jouw_api_key_hier&quot; 
# Vervang PROVIDER_API_KEY door de specifieke variabelenaam, bijv. OPENAI_API_KEY, GOOGLE_API_KEY
</code></pre>
Voorbeeld voor TogetherAI:
<pre><code class="language-sh">export TOGETHER_API_KEY=&quot;xxxxxxxxxxxxxxxxxxxxxx&quot;
</code></pre>
</li>
<li><strong>Windows:</strong></li>
<li><strong>Command Prompt (Tijdelijk voor huidige sessie):</strong>
<pre><code class="language-cmd">set PROVIDER_API_KEY=your_api_key_here
</code></pre>
</li>
<li><strong>PowerShell (Tijdelijk voor huidige sessie):</strong>
<pre><code class="language-powershell">$env:PROVIDER_API_KEY=&quot;your_api_key_here&quot;
</code></pre>
</li>
<li><strong>Permanent:</strong> Zoek naar &quot;omgevingsvariabelen&quot; in de Windows zoekbalk, klik op &quot;Systeemomgevingsvariabelen bewerken,&quot; en klik vervolgens op de knop &quot;Omgevingsvariabelen...&quot;. Voeg een nieuwe gebruikersvariabele toe met de juiste naam (bijv. <code>OPENAI_API_KEY</code>) en je sleutel als waarde.</li>
</ul>
<p><em>(Zie FAQ: <a href="#how-do-i-set-api-keys">Hoe stel ik API-sleutels in?</a> voor meer details).</em></p>
<p><strong>3. Werk <code>config.ini</code> bij:</strong></p>
<pre><code class="language-ini">[MAIN]
is_local = False
provider_name = openai # Of google, deepseek, togetherAI, huggingface
provider_model = gpt-3.5-turbo # Of gemini-1.5-flash, deepseek-chat, mistralai/Mixtral-8x7B-Instruct-v0.1 etc.
provider_server_address = # Wordt meestal genegeerd of kan leeg worden gelaten wanneer is_local = False voor de meeste APIs
# ... overige instellingen ...
</code></pre>
<p><em>Waarschuwing:</em> Zorg ervoor dat er geen spaties aan het einde van de waarden in <code>config.ini</code> staan.</p>
<p><strong>Lijst van API-aanbieders</strong></p>
<p>| Provider     | <code>provider_name</code> | Lokaal? | Beschrijving                                        | API Key Link (Voorbeelden)                       |
|--------------|-----------------|---------|-----------------------------------------------------|--------------------------------------------------|
| OpenAI       | <code>openai</code>        | Nee     | Gebruik ChatGPT-modellen via de API van OpenAI.     | <a href="https://platform.openai.com/signup">platform.openai.com/signup</a> |
| Google Gemini| <code>google</code>        | Nee     | Gebruik Google Gemini-modellen via Google AI Studio.| <a href="https://aistudio.google.com/keys">aistudio.google.com/keys</a> |
| Deepseek     | <code>deepseek</code>      | Nee     | Gebruik Deepseek-modellen via hun API.              | <a href="https://platform.deepseek.com">platform.deepseek.com</a> |
| Hugging Face | <code>huggingface</code>   | Nee     | Gebruik modellen van de Hugging Face Inference API. | <a href="https://huggingface.co/settings/tokens">huggingface.co/settings/tokens</a> |
| TogetherAI   | <code>togetherAI</code>    | Nee     | Gebruik diverse open-source modellen via TogetherAI API.| <a href="https://api.together.ai/settings/api-keys">api.together.ai/settings/api-keys</a> |</p>
<p><em>Opmerking:</em></p>
<ul>
<li>We raden af om <code>gpt-4o</code> of andere OpenAI-modellen te gebruiken voor complexe web-browsing en taakplanning, omdat de huidige prompt-optimalisaties zijn afgestemd op modellen zoals Deepseek.</li>
<li>Programmeer/bash-taken kunnen problemen ondervinden met Gemini, omdat deze mogelijk niet strikt de prompt-formattering volgt die is geoptimaliseerd voor Deepseek.</li>
<li>Het veld <code>provider_server_address</code> in <code>config.ini</code> wordt meestal niet gebruikt als <code>is_local = False</code>, omdat het API-eindpunt doorgaans is vastgelegd in de bibliotheek van de betreffende provider.</li>
</ul>
<p>Volgende stap: <a href="#Start-services-and-Run">Start services en voer AgenticSeek uit</a></p>
<p><em>Zie de sectie <strong>Bekende problemen</strong> als je problemen ondervindt</em></p>
<p><em>Zie de sectie <strong>Config</strong> voor een gedetailleerde uitleg van het configuratiebestand.</em></p>
<hr />
<h2>Start services en voer uit</h2>
<p>Standaard wordt AgenticSeek volledig in Docker uitgevoerd.</p>
<p>Start de vereiste services. Dit start alle services uit het docker-compose.yml bestand, waaronder:
- searxng
- redis (vereist door searxng)
- frontend
- backend (indien <code>full</code> gebruikt)</p>
<pre><code class="language-sh">./start_services.sh full # MacOS
start ./start_services.cmd full # Windows
</code></pre>
<p><strong>Waarschuwing:</strong> Deze stap downloadt en laadt alle Docker-images, wat tot 30 minuten kan duren. Na het starten van de services, wacht tot de backend service volledig draait (je zou <strong>backend: &quot;GET /health HTTP/1.1&quot; 200 OK</strong> in de log moeten zien) voordat je berichten verstuurt. De backend services kunnen bij de eerste keer starten tot 5 minuten nodig hebben.</p>
<p>Ga naar <code>http://localhost:3000/</code> en je zou de webinterface moeten zien.</p>
<p><em>Problemen bij het starten van services:</em> Als deze scripts falen, controleer dan of Docker Engine draait en Docker Compose (V2, <code>docker compose</code>) correct is geïnstalleerd. Controleer de uitvoer in de terminal op foutmeldingen. Zie <a href="#faq-troubleshooting">FAQ: Help! Ik krijg een foutmelding bij het uitvoeren van AgenticSeek of de scripts.</a></p>
<p><strong>Optioneel:</strong> Uitvoeren op host (CLI modus):</p>
<p>Om de CLI-interface te gebruiken moet je het pakket op de host installeren:</p>
<pre><code class="language-sh">./install.sh
./install.bat # windows
</code></pre>
<p>Start de services:</p>
<pre><code class="language-sh">./start_services.sh # MacOS
start ./start_services.cmd # Windows
</code></pre>
<p>Gebruik de CLI: <code>python3 cli.py</code></p>
<hr />
<h2>Gebruik</h2>
<p>Zorg ervoor dat de services draaien met <code>./start_services.sh full</code> en ga naar <code>localhost:3000</code> voor de webinterface.</p>
<p>Je kunt ook spraak naar tekst gebruiken door <code>listen = True</code> in te stellen in de config. Alleen voor CLI-modus.</p>
<p>Om af te sluiten, zeg of typ je gewoon <code>goodbye</code>.</p>
<p>Hier zijn enkele voorbeelden van gebruik:</p>
<blockquote>
<p><em>Maak een slangenspel in python!</em></p>
</blockquote>
<blockquote>
<p><em>Zoek op internet naar de beste cafés in Rennes, Frankrijk, en sla een lijst van drie op met hun adressen in rennes_cafes.txt.</em></p>
</blockquote>
<blockquote>
<p><em>Schrijf een Go-programma om de faculteit van een getal te berekenen, sla het op als factorial.go in je werkmap</em></p>
</blockquote>
<blockquote>
<p><em>Zoek in mijn map summer_pictures naar alle JPG-bestanden, hernoem ze met de datum van vandaag, en sla een lijst van hernoemde bestanden op in photos_list.txt</em></p>
</blockquote>
<blockquote>
<p><em>Zoek online naar populaire sci-fi films uit 2024 en kies er drie om vanavond te kijken. Sla de lijst op in movie_night.txt.</em></p>
</blockquote>
<blockquote>
<p><em>Zoek op internet naar de laatste AI-nieuwsartikelen uit 2025, selecteer er drie, en schrijf een Python-script om hun titels en samenvattingen te scrapen. Sla het script op als news_scraper.py en de samenvattingen in ai_news.txt in /home/projects</em></p>
</blockquote>
<blockquote>
<p><em>Vrijdag, zoek op internet naar een gratis API voor aandelenkoersen, registreer met supersuper7434567@gmail.com, schrijf dan een Python-script om met de API dagelijks prijzen voor Tesla op te halen, en sla de resultaten op in stock_prices.csv</em></p>
</blockquote>
<p><em>Let op dat formuliervul-mogelijkheden nog experimenteel zijn en kunnen falen.</em></p>
<p>Nadat je je vraag hebt ingevoerd, zal AgenticSeek de beste agent voor de taak toewijzen.</p>
<p>Omdat dit een vroeg prototype is, zal het routeringssysteem van de agent mogelijk niet altijd de juiste agent toewijzen op basis van je vraag.</p>
<p>Wees daarom heel expliciet in wat je wilt en hoe de AI te werk zou moeten gaan; wil je bijvoorbeeld dat het een webzoekopdracht uitvoert, zeg dan niet:</p>
<p><code>Ken je goede landen voor solo-reizen?</code></p>
<p>Vraag in plaats daarvan:</p>
<p><code>Doe een webzoekopdracht en zoek uit wat de beste landen zijn voor solo-reizen</code></p>
<hr />
<h2><strong>Setup om de LLM op je eigen server te draaien</strong></h2>
<p>Als je een krachtige computer of server hebt die je kunt gebruiken, maar je wilt deze vanaf je laptop gebruiken, heb je de mogelijkheid om de LLM op een externe server te draaien met onze aangepaste llm-server.</p>
<p>Op je &quot;server&quot; die het AI-model zal draaien, haal het ip-adres op</p>
<pre><code class="language-sh">ip a | grep &quot;inet &quot; | grep -v 127.0.0.1 | awk '{print $2}' | cut -d/ -f1 # lokaal ip
curl https://ipinfo.io/ip # publiek ip
</code></pre>
<p>Opmerking: Voor Windows of macOS gebruik respectievelijk ipconfig of ifconfig om het IP-adres te vinden.</p>
<p>Kloon de repository en ga naar de map <code>server/</code>.</p>
<pre><code class="language-sh">git clone --depth 1 https://github.com/Fosowl/agenticSeek.git
cd agenticSeek/llm_server/
</code></pre>
<p>Installeer de server-specifieke vereisten:</p>
<pre><code class="language-sh">pip3 install -r requirements.txt
</code></pre>
<p>Voer het serverscript uit.</p>
<pre><code class="language-sh">python3 app.py --provider ollama --port 3333
</code></pre>
<p>Je kunt kiezen tussen <code>ollama</code> en <code>llamacpp</code> als LLM-service.</p>
<p>Nu op je persoonlijke computer:</p>
<p>Wijzig het <code>config.ini</code>-bestand door <code>provider_name</code> op <code>server</code> te zetten en <code>provider_model</code> op <code>deepseek-r1:xxb</code>.
Stel <code>provider_server_address</code> in op het ip-adres van de machine die het model draait.</p>
<pre><code class="language-sh">[MAIN]
is_local = False
provider_name = server
provider_model = deepseek-r1:70b
provider_server_address = x.x.x.x:3333
</code></pre>
<p>Volgende stap: <a href="#Start-services-and-Run">Start services en voer AgenticSeek uit</a></p>
<hr />
<h2>Spraak naar Tekst</h2>
<p>Waarschuwing: spraak naar tekst werkt momenteel alleen in CLI-modus.</p>
<p>Let op: spraak naar tekst werkt momenteel alleen in het Engels.</p>
<p>De spraak-naar-tekst functionaliteit is standaard uitgeschakeld. Om deze in te schakelen, stel je de optie listen in op True in het config.ini-bestand:</p>
<pre><code>listen = True
</code></pre>
<p>Wanneer ingeschakeld, luistert de spraak-naar-tekst functie naar een triggerwoord, namelijk de naam van de agent, voordat je invoer wordt verwerkt. Je kunt de naam van de agent aanpassen door de waarde van <code>agent_name</code> bij te werken in het <em>config.ini</em> bestand:</p>
<pre><code>agent_name = Friday
</code></pre>
<p>Voor optimale herkenning raden we aan een gangbare Engelse naam te gebruiken zoals &quot;John&quot; of &quot;Emma&quot; als agentnaam.</p>
<p>Zodra u het transcript ziet verschijnen, zeg de naam van de agent hardop om deze te activeren (bijv. &quot;Friday&quot;).</p>
<p>Spreek uw vraag duidelijk uit.</p>
<p>Beëindig uw verzoek met een bevestigingszin om het systeem te laten doorgaan. Voorbeelden van bevestigingszinnen zijn:</p>
<pre><code>&quot;doe het&quot;, &quot;ga je gang&quot;, &quot;uitvoeren&quot;, &quot;start&quot;, &quot;begin&quot;, &quot;dank je&quot;, &quot;zou je&quot;, &quot;alstublieft&quot;, &quot;oké?&quot;, &quot;doorgaan&quot;, &quot;verder&quot;, &quot;doe dat&quot;, &quot;doe het&quot;, &quot;begrijp je?&quot;
</code></pre>
<h2>Config</h2>
<p>Voorbeeldconfiguratie:</p>
<pre><code>[MAIN]
is_local = True
provider_name = ollama
provider_model = deepseek-r1:32b
provider_server_address = http://127.0.0.1:11434 # Voorbeeld voor Ollama; gebruik http://127.0.0.1:1234 voor LM-Studio
agent_name = Friday
recover_last_session = False
save_session = False
speak = False
listen = False

jarvis_personality = False
languages = en zh # Lijst van talen voor TTS en eventueel routering.
[BROWSER]
headless_browser = False
stealth_mode = False
</code></pre>
<p><strong>Uitleg van <code>config.ini</code> Instellingen</strong>:</p>
<ul>
<li><strong><code>[MAIN]</code> Sectie:</strong>
<ul>
<li><code>is_local</code>: <code>True</code> als u een lokale LLM-provider gebruikt (Ollama, LM-Studio, lokale OpenAI-compatibele server) of de self-hosted serveroptie. <code>False</code> als u een cloudgebaseerde API gebruikt (OpenAI, Google, enz.).</li>
<li><code>provider_name</code>: Specificeert de LLM-provider.
<ul>
<li>Lokale opties: <code>ollama</code>, <code>lm-studio</code>, <code>openai</code> (voor lokale OpenAI-compatibele servers), <code>server</code> (voor de zelfgehoste server).</li>
<li>API-opties: <code>openai</code>, <code>google</code>, <code>deepseek</code>, <code>huggingface</code>, <code>togetherAI</code>.</li>
</ul>
</li>
<li><code>provider_model</code>: De specifieke modelnaam of ID voor de gekozen provider (bijv. <code>deepseekcoder:6.7b</code> voor Ollama, <code>gpt-3.5-turbo</code> voor OpenAI API, <code>mistralai/Mixtral-8x7B-Instruct-v0.1</code> voor TogetherAI).</li>
<li><code>provider_server_address</code>: Het adres van uw LLM-provider.
<ul>
<li>Voor lokale providers: bijv. <code>http://127.0.0.1:11434</code> voor Ollama, <code>http://127.0.0.1:1234</code> voor LM-Studio.</li>
<li>Voor de <code>server</code>-provider: Het adres van uw zelfgehoste LLM-server (bijv. <code>http://your_server_ip:3333</code>).</li>
<li>Voor cloud-API's (<code>is_local = False</code>): Dit wordt vaak genegeerd of kan leeg gelaten worden, omdat het API-eindpunt meestal door de clientbibliotheek wordt afgehandeld.</li>
</ul>
</li>
<li><code>agent_name</code>: Naam van de AI-assistent (bijv. Friday). Wordt gebruikt als triggerwoord voor spraak-naar-tekst als dit is ingeschakeld.</li>
<li><code>recover_last_session</code>: <code>True</code> om te proberen de vorige sessiestatus te herstellen, <code>False</code> om opnieuw te beginnen.</li>
<li><code>save_session</code>: <code>True</code> om de huidige sessiestatus op te slaan voor mogelijk herstel, <code>False</code> anders.</li>
<li><code>speak</code>: <code>True</code> om spraakuitvoer (text-to-speech) in te schakelen, <code>False</code> om uit te schakelen.</li>
<li><code>listen</code>: <code>True</code> om spraak-naar-tekst-invoer (alleen CLI-modus) in te schakelen, <code>False</code> om uit te schakelen.</li>
<li><code>work_dir</code>: <strong>Cruciaal:</strong> De map waarin AgenticSeek bestanden zal lezen/schrijven. <strong>Zorg ervoor dat dit pad geldig en toegankelijk is op uw systeem.</strong></li>
<li><code>jarvis_personality</code>: <code>True</code> voor een meer &quot;Jarvis-achtige&quot; systeemprompt (experimenteel), <code>False</code> voor de standaardprompt.</li>
<li><code>languages</code>: Een komma-gescheiden lijst met talen (bijv. <code>en, zh, fr</code>). Wordt gebruikt voor TTS-stemselectie (standaard de eerste) en kan de LLM-router helpen. Vermijd te veel of zeer vergelijkbare talen voor routerefficiëntie.</li>
</ul>
</li>
<li><strong><code>[BROWSER]</code> Sectie:</strong>
<ul>
<li><code>headless_browser</code>: <code>True</code> om de geautomatiseerde browser zonder zichtbaar venster uit te voeren (aanbevolen voor webinterface of niet-interactief gebruik). <code>False</code> om het browservenster te tonen (handig voor CLI-modus of debugging).</li>
<li><code>stealth_mode</code>: <code>True</code> om maatregelen te activeren die browserautomatisering moeilijker te detecteren maken. Kan vereisen dat u handmatig browserextensies zoals anticaptcha installeert.</li>
</ul>
</li>
</ul>
<p>Deze sectie geeft een overzicht van de ondersteunde LLM-providertypes. Stel ze in via <code>config.ini</code>.</p>
<p><strong>Lokale Providers (Draaiend op eigen hardware):</strong></p>
<p>| Providernaam in <code>config.ini</code> | <code>is_local</code> | Beschrijving                                                                 | Setup Sectie                                                    |
|-------------------------------|------------|-----------------------------------------------------------------------------|------------------------------------------------------------------|
| <code>ollama</code>                      | <code>True</code>     | Gebruik Ollama om lokale LLM's te draaien.                                  | <a href="#setup-for-running-llm-locally-on-your-machine">Setup voor lokaal draaien van LLM</a> |
| <code>lm-studio</code>                   | <code>True</code>     | Gebruik LM-Studio om lokale LLM's te draaien.                               | <a href="#setup-for-running-llm-locally-on-your-machine">Setup voor lokaal draaien van LLM</a> |
| <code>openai</code> (voor lokale server) | <code>True</code>     | Verbind met een lokale server die een OpenAI-compatibele API aanbiedt (bijv. llama.cpp). | <a href="#setup-for-running-llm-locally-on-your-machine">Setup voor lokaal draaien van LLM</a> |
| <code>server</code>                      | <code>False</code>    | Verbind met de zelfgehoste AgenticSeek LLM-server op een andere machine.    | <a href="#setup-to-run-the-llm-on-your-own-server">Setup om de LLM op uw eigen server te draaien</a> |</p>
<p><strong>API Providers (Cloud-Based):</strong></p>
<p>| Providernaam in <code>config.ini</code> | <code>is_local</code> | Beschrijving                                     | Setup Sectie                                        |
|-------------------------------|------------|--------------------------------------------------|-----------------------------------------------------|
| <code>openai</code>                      | <code>False</code>    | Gebruik de officiële API van OpenAI (bijv. GPT-3.5, GPT-4). | <a href="#setup-to-run-with-an-api">Setup om te draaien met een API</a> |
| <code>google</code>                      | <code>False</code>    | Gebruik Google's Gemini-modellen via API.        | <a href="#setup-to-run-with-an-api">Setup om te draaien met een API</a> |
| <code>deepseek</code>                    | <code>False</code>    | Gebruik de officiële API van Deepseek.           | <a href="#setup-to-run-with-an-api">Setup om te draaien met een API</a> |
| <code>huggingface</code>                 | <code>False</code>    | Gebruik de Hugging Face Inference API.           | <a href="#setup-to-run-with-an-api">Setup om te draaien met een API</a> |
| <code>togetherAI</code>                  | <code>False</code>    | Gebruik TogetherAI's API voor diverse open modellen. | <a href="#setup-to-run-with-an-api">Setup om te draaien met een API</a> |</p>
<hr />
<h2>Problemen oplossen</h2>
<p>Als u problemen ondervindt, biedt deze sectie hulp.</p>
<h1>Bekende Problemen</h1>
<h2>ChromeDriver Problemen</h2>
<p><strong>Foutvoorbeeld:</strong> <code>SessionNotCreatedException: Message: session not created: This version of ChromeDriver only supports Chrome version XXX</code></p>
<ul>
<li><strong>Oorzaak:</strong> De geïnstalleerde ChromeDriver-versie is niet compatibel met de versie van uw Google Chrome-browser.</li>
<li><strong>Oplossing:</strong>
<ol>
<li><strong>Controleer Chrome-versie:</strong> Open Google Chrome, ga naar <code>Instellingen &gt; Over Chrome</code> om uw versie te vinden (bijv. &quot;Versie 120.0.6099.110&quot;).</li>
<li><strong>Download de bijpassende ChromeDriver:</strong>
<ul>
<li>Voor Chrome versies 115 en nieuwer: Ga naar de <a href="https://googlechromelabs.github.io/chrome-for-testing/">Chrome for Testing (CfT) JSON Endpoints</a>. Zoek het &quot;stable&quot; kanaal en download de ChromeDriver voor uw besturingssysteem die overeenkomt met uw Chrome-hoofdversie.</li>
<li>Voor oudere versies (minder voorkomend): Mogelijk vindt u deze op de <a href="https://chromedriver.chromium.org/downloads">ChromeDriver - WebDriver for Chrome</a> pagina.</li>
<li>De onderstaande afbeelding toont een voorbeeld van de CfT-pagina:
<img src="./media/chromedriver_readme.png" alt="Download Chromedriver specifieke versie van de Chrome for Testing pagina" /></li>
</ul>
</li>
<li><strong>Installeer ChromeDriver:</strong>
<ul>
<li>Zorg ervoor dat de gedownloade <code>chromedriver</code> (of <code>chromedriver.exe</code> op Windows) in een map staat die is opgenomen in het PATH-omgevingsvariabele van uw systeem (bijv. <code>/usr/local/bin</code> op Linux/macOS, of een aangepaste scriptsmap toegevoegd aan PATH op Windows).</li>
<li>Of plaats hem in de hoofdmap van het <code>agenticSeek</code>-project.</li>
<li>Zorg ervoor dat de driver uitvoerbaar is (bijv. <code>chmod +x chromedriver</code> op Linux/macOS).</li>
</ul>
</li>
<li>Raadpleeg de sectie <a href="#chromedriver-installation">ChromeDriver Installatie</a> in de hoofdinstallatiehandleiding voor meer details.</li>
</ol>
</li>
</ul>
<p>Als deze sectie onvolledig is of u andere ChromeDriver-problemen ondervindt, zoek dan bestaande <a href="https://github.com/Fosowl/agenticSeek/issues">GitHub Issues</a> of maak een nieuw issue aan.</p>
<p><code>Exception: Failed to initialize browser: Message: session not created: This version of ChromeDriver only supports Chrome version 113 Current browser version is 134.0.6998.89 with binary path</code></p>
<p>Dit gebeurt als er een mismatch is tussen uw browser- en chromedriver-versie.</p>
<p>U dient de nieuwste versie te downloaden:</p>
<p>https://developer.chrome.com/docs/chromedriver/downloads</p>
<p>Als u Chrome versie 115 of nieuwer gebruikt, ga naar:</p>
<p>https://googlechromelabs.github.io/chrome-for-testing/</p>
<p>En download de chromedriver-versie die overeenkomt met uw OS.</p>
<p><img src="./media/chromedriver_readme.png" alt="alt text" /></p>
<p>Als deze sectie onvolledig is, maak dan een issue aan.</p>
<h2>connection adapters Problemen</h2>
<pre><code>Exception: Provider lm-studio failed: HTTP request failed: No connection adapters were found for '127.0.0.1:1234/v1/chat/completions'` (Let op: poort kan verschillen)
</code></pre>
<ul>
<li><strong>Oorzaak:</strong> Het <code>provider_server_address</code> in <code>config.ini</code> voor <code>lm-studio</code> (of andere vergelijkbare lokale OpenAI-compatibele servers) mist het <code>http://</code>-voorvoegsel of wijst naar de verkeerde poort.</li>
<li><strong>Oplossing:</strong>
<ul>
<li>Zorg ervoor dat het adres <code>http://</code> bevat. LM-Studio gebruikt typisch <code>http://127.0.0.1:1234</code>.</li>
<li>Corrigeer <code>config.ini</code>: <code>provider_server_address = http://127.0.0.1:1234</code> (of uw daadwerkelijke LM-Studio serverpoort).</li>
</ul>
</li>
</ul>
<h2>SearxNG Base URL Niet Opgegeven</h2>
<pre><code>raise ValueError(&quot;SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.&quot;)
ValueError: SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.`
</code></pre>
<h2>FAQ</h2>
<p><strong>Q: Welke hardware heb ik nodig?</strong></p>
<p>| Modelgrootte  | GPU  | Opmerking                                               |
|-----------|--------|-------------------------------------------------------------|
| 7B        | 8GB Vram | ⚠️ Niet aanbevolen. Slechte prestaties, frequente hallucinaties en planner agents zullen waarschijnlijk falen. |
| 14B        | 12 GB VRAM (bijv. RTX 3060) | ✅ Bruikbaar voor eenvoudige taken. Kan moeite hebben met web browsing en planningstaken. |
| 32B        | 24+ GB VRAM (bijv. RTX 4090) | 🚀 Succesvol bij de meeste taken, kan nog steeds moeite hebben met taakplanning |
| 70B+        | 48+ GB Vram | 💪 Uitstekend. Aanbevolen voor geavanceerde toepassingen. |</p>
<p><strong>Q: Ik krijg een foutmelding, wat moet ik doen?</strong></p>
<p>Controleer of lokaal draait (<code>ollama serve</code>), uw <code>config.ini</code> overeenkomt met uw provider en alle afhankelijkheden geïnstalleerd zijn. Werkt het dan nog niet, maak gerust een issue aan.</p>
<p><strong>Q: Kan het echt 100% lokaal draaien?</strong></p>
<p>Ja, met Ollama, lm-studio of serverproviders draaien alle spraak-naar-tekst, LLM en tekst-naar-spraak-modellen lokaal. Niet-lokale opties (OpenAI of andere API’s) zijn optioneel.</p>
<p><strong>Q: Waarom zou ik AgenticSeek gebruiken als ik Manus heb?</strong></p>
<p>In tegenstelling tot Manus, hecht AgenticSeek veel waarde aan onafhankelijkheid van externe systemen, wat u meer controle, privacy en geen API-kosten biedt.</p>
<p><strong>Q: Wie zit er achter het project?</strong></p>
<p>Het project is opgezet door mij samen met twee vrienden die als maintainers en bijdragers uit de open-source gemeenschap op GitHub fungeren. We zijn gewoon een groep gepassioneerde individuen, geen startup of onderdeel van een organisatie.</p>
<p>Elk AgenticSeek-account op X behalve mijn persoonlijke account (https://x.com/Martin993886460) is een imitatie.</p>
<h2>Bijdragen</h2>
<p>We zoeken ontwikkelaars om AgenticSeek te verbeteren! Bekijk open issues of discussies.</p>
<p><a href="./docs/CONTRIBUTING.md">Bijdragegids</a></p>
<p><a href="https://www.star-history.com/#Fosowl/agenticSeek&amp;Date"><img src="https://api.star-history.com/svg?repos=Fosowl/agenticSeek&amp;type=Date" alt="Star History Chart" /></a></p>
<h2>Onderhouders:</h2>
<blockquote>
<p><a href="https://github.com/Fosowl">Fosowl</a> | Parijs Tijd</p>
</blockquote>
<blockquote>
<p><a href="https://github.com/antoineVIVIES">antoineVIVIES</a> | Taipei Tijd</p>
</blockquote>
<blockquote>
<p><a href="https://github.com/steveh8758">steveh8758</a> | Taipei Tijd</p>
</blockquote>
<h2>Speciale Dank:</h2>
<blockquote>
<p><a href="https://github.com/tcsenpai">tcsenpai</a> en <a href="https://github.com/plitc">plitc</a> Voor hulp bij backend-dockerization</p>
</blockquote>
<h2>Sponsors:</h2>
<p>5$ of meer Maandelijkse sponsor verschijnt hier:</p>
<ul>
<li><strong>tatra-labs</strong></li>
</ul>
<p>Sorry, but you didn't provide the content you want translated. Please provide the text of Part 4 of 4, and I'll translate it into Dutch for you, preserving the formatting and Markdown as requested.</p>
<hr />
<p>Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx">Open Ai Tx</a> | Last indexed: 2025-06-16</p>
<hr />

        </div>
    </div>
    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Default Statcounter code for openaitx
https://openaitx.github.io/ -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>
    <!-- End of Statcounter Code -->
</body>
</html>