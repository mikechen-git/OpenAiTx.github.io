<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>agenticSeek - Fosowl/agenticSeek</title>
    <meta name="title" content="agenticSeek - Fosowl/agenticSeek">
    <meta name="description" content="Fosowl/agenticSeek - GitHub repository es documentation and informationAgenticSeek: Alternativa privada y local a Manus English | 中文 | 繁體中文 | Français | 日本語 | Português (Brasil) | Español Una alternativa 100% local a Manus AI, este...">
    <meta name="keywords" content="Fosowl, agenticSeek, GitHub, repository, es documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/Fosowl/agenticSeek/README-es.html">
    <meta property="og:title" content="agenticSeek - Fosowl/agenticSeek">
    <meta property="og:description" content="Fosowl/agenticSeek - GitHub repository es documentation and information">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">
    <script src="/js/marked.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        body { display: flex; flex-direction: column; min-height: 100vh; }
        .main-container { margin: 0 auto; width: 100%; max-width: 980px; padding: 0 20px; }
        @media (max-width: 768px) { .main-container { padding: 0 15px; } }
        .markdown-body img { max-width: 100%; height: auto; }
        .header { text-align: center; margin-bottom: 30px; padding: 20px; background-color: #f6f8fa; border-bottom: 1px solid #e1e4e8; position: relative; }
        .header .links { margin-top: 10px; font-size: 16px; }
        .header .links a { color: #0366d6; text-decoration: none; margin-left: 5px; }
        .header .links a:hover { text-decoration: underline; }
        .language-badges { margin-top: 15px; text-align: center; }
        .language-badges a { display: inline-block; margin: 2px; text-decoration: none; }
        .language-badges img { height: 20px; border-radius: 3px; }
        .language-badges a:hover img { opacity: 0.8; }
    </style>
</head>
<body>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/Fosowl/agenticSeek" id="githubRepoLink" target="_blank">Fosowl/agenticSeek</a>
<h1 style="display: none;">AgenticSeek: Alternativa privada y local a Manus English | 中文 | 繁體中文 | Français | 日本語 | Português (Brasil) | Español Una alternativa 100% local a Manus AI, este...</h1>
        </div>
        <div class="language-badges" id="languageBadges">
            <!-- You can generate language badges here if needed -->
        </div>
    </div>
    <div class="main-container">
        <div class="markdown-body" id="content">
            <h1>AgenticSeek: Alternativa privada y local a Manus</h1>
<p align="center">
<img align="center" src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/agentic_seek_logo.png" width="300" height="300" alt="Agentic Seek Logo">
<p>
<p>English | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_CHS.md">中文</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_CHT.md">繁體中文</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_FR.md">Français</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_JP.md">日本語</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_PTBR.md">Português (Brasil)</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_ES.md">Español</a></p>
<p><em>Una <strong>alternativa 100% local a Manus AI</strong>, este asistente de IA habilitado por voz navega autónomamente por la web, escribe código y planifica tareas, manteniendo todos los datos en tu dispositivo. Adaptado para modelos de razonamiento locales, se ejecuta completamente en tu hardware, garantizando total privacidad y cero dependencia de la nube.</em></p>
<p><a href="https://fosowl.github.io/agenticSeek.html"><img src="https://img.shields.io/static/v1?label=Website&amp;message=AgenticSeek&amp;color=blue&amp;style=flat-square" alt="Visita AgenticSeek" /></a> <img src="https://img.shields.io/badge/license-GPL--3.0-green" alt="Licencia" /> <a href="https://discord.gg/8hGDaME3TC"><img src="https://img.shields.io/badge/Discord-Join%20Us-7289DA?logo=discord&amp;logoColor=white" alt="Discord" /></a> <a href="https://x.com/Martin993886460"><img src="https://img.shields.io/twitter/url/https/twitter.com/fosowl.svg?style=social&amp;label=Update%20%40Fosowl" alt="Twitter" /></a> <a href="https://github.com/Fosowl/agenticSeek/stargazers"><img src="https://img.shields.io/github/stars/Fosowl/agenticSeek?style=social" alt="GitHub stars" /></a></p>
<h3>¿Por qué AgenticSeek?</h3>
<ul>
<li><p>🔒 Totalmente local y privado - Todo funciona en tu máquina — sin nube, sin compartir datos. Tus archivos, conversaciones y búsquedas permanecen privadas.</p>
</li>
<li><p>🌐 Navegación web inteligente - AgenticSeek puede navegar por Internet de forma autónoma — buscar, leer, extraer información, rellenar formularios web — todo manos libres.</p>
</li>
<li><p>💻 Asistente de codificación autónomo - ¿Necesitas código? Puede escribir, depurar y ejecutar programas en Python, C, Go, Java y más — todo sin supervisión.</p>
</li>
<li><p>🧠 Selección inteligente de agentes - Tú preguntas, él determina automáticamente el mejor agente para el trabajo. Como tener un equipo de expertos listo para ayudar.</p>
</li>
<li><p>📋 Planifica y ejecuta tareas complejas - Desde planificar viajes hasta proyectos complejos — puede dividir grandes tareas en pasos y realizarlas usando múltiples agentes de IA.</p>
</li>
<li><p>🎙️ Habilitado por voz - Voz limpia, rápida y futurista, y conversión de voz a texto que te permite hablarle como si fuera tu IA personal de una película de ciencia ficción. (En progreso)</p>
</li>
</ul>
<h3><strong>Demostración</strong></h3>
<blockquote>
<p><em>¿Puedes buscar el proyecto agenticSeek, aprender qué habilidades se requieren, luego abrir el archivo CV_candidates.zip y decirme cuáles coinciden mejor con el proyecto?</em></p>
</blockquote>
<p>https://github.com/user-attachments/assets/b8ca60e9-7b3b-4533-840e-08f9ac426316</p>
<p>Aviso: Esta demostración, incluidos todos los archivos que aparecen (por ejemplo: CV_candidates.zip), es completamente ficticia. No somos una corporación, buscamos colaboradores de código abierto, no candidatos.</p>
<blockquote>
<p>🛠⚠️️ <strong>Trabajo activo en progreso</strong></p>
</blockquote>
<blockquote>
<p>🙏 Este proyecto comenzó como un proyecto paralelo y no tiene hoja de ruta ni financiación. Ha crecido mucho más de lo que esperaba al llegar a GitHub Trending. Se agradecen profundamente las contribuciones, comentarios y paciencia.</p>
</blockquote>
<h2>Prerrequisitos</h2>
<p>Antes de comenzar, asegúrate de tener instalado el siguiente software:</p>
<ul>
<li><strong>Git:</strong> Para clonar el repositorio. <a href="https://git-scm.com/downloads">Descargar Git</a></li>
<li><strong>Python 3.10.x:</strong> Recomendamos encarecidamente utilizar la versión 3.10.x de Python. Usar otras versiones podría causar errores de dependencias. <a href="https://www.python.org/downloads/release/python-3100/">Descargar Python 3.10</a> (elige una versión 3.10.x).</li>
<li><strong>Docker Engine &amp; Docker Compose:</strong> Para ejecutar servicios integrados como SearxNG.
<ul>
<li>Instala Docker Desktop (que incluye Docker Compose V2): <a href="https://docs.docker.com/desktop/install/windows-install/">Windows</a> | <a href="https://docs.docker.com/desktop/install/mac-install/">Mac</a> | <a href="https://docs.docker.com/desktop/install/linux-install/">Linux</a></li>
<li>Alternativamente, instala Docker Engine y Docker Compose por separado en Linux: <a href="https://docs.docker.com/engine/install/">Docker Engine</a> | <a href="https://docs.docker.com/compose/install/">Docker Compose</a> (asegúrate de instalar Compose V2, por ejemplo, <code>sudo apt-get install docker-compose-plugin</code>).</li>
</ul>
</li>
</ul>
<h3>1. <strong>Clona el repositorio y haz la configuración inicial</strong></h3>
<pre><code class="language-sh">git clone https://github.com/Fosowl/agenticSeek.git
cd agenticSeek
mv .env.example .env
</code></pre>
<h3>2. Cambia el contenido del archivo .env</h3>
<pre><code class="language-sh">SEARXNG_BASE_URL=&quot;http://127.0.0.1:8080&quot;
REDIS_BASE_URL=&quot;redis://redis:6379/0&quot;
WORK_DIR=&quot;/Users/mlg/Documents/workspace_for_ai&quot;
OLLAMA_PORT=&quot;11434&quot;
LM_STUDIO_PORT=&quot;1234&quot;
CUSTOM_ADDITIONAL_LLM_PORT=&quot;11435&quot;
OPENAI_API_KEY='optional'
DEEPSEEK_API_KEY='optional'
OPENROUTER_API_KEY='optional'
TOGETHER_API_KEY='optional'
GOOGLE_API_KEY='optional'
ANTHROPIC_API_KEY='optional'
</code></pre>
<p>Actualiza el archivo <code>.env</code> con tus propios valores según sea necesario:</p>
<ul>
<li><strong>SEARXNG_BASE_URL</strong>: Déjalo sin cambios</li>
<li><strong>REDIS_BASE_URL</strong>: Déjalo sin cambios</li>
<li><strong>WORK_DIR</strong>: Ruta a tu directorio de trabajo en tu máquina local. AgenticSeek podrá leer e interactuar con estos archivos.</li>
<li><strong>OLLAMA_PORT</strong>: Número de puerto para el servicio Ollama.</li>
<li><strong>LM_STUDIO_PORT</strong>: Número de puerto para el servicio LM Studio.</li>
<li><strong>CUSTOM_ADDITIONAL_LLM_PORT</strong>: Puerto para cualquier servicio LLM adicional personalizado.</li>
</ul>
<p><strong>Las claves API son totalmente opcionales para el usuario que elija ejecutar LLM localmente, que es el propósito principal de este proyecto. Déjalas vacías si tienes hardware suficiente</strong></p>
<h3>3. <strong>Inicia Docker</strong></h3>
<p>Asegúrate de que Docker esté instalado y en funcionamiento en tu sistema. Puedes iniciar Docker usando los siguientes comandos:</p>
<ul>
<li><p><strong>En Linux/macOS:</strong><br />
Abre una terminal y ejecuta:</p>
<pre><code class="language-sh">sudo systemctl start docker
</code></pre>
<p>O lanza Docker Desktop desde el menú de aplicaciones si está instalado.</p>
</li>
<li><p><strong>En Windows:</strong><br />
Inicia Docker Desktop desde el menú Inicio.</p>
</li>
</ul>
<p>Puedes verificar que Docker está funcionando ejecutando:</p>
<pre><code class="language-sh">docker info
</code></pre>
<p>Si ves información sobre tu instalación de Docker, está funcionando correctamente.</p>
<p>Consulta la tabla de <a href="#list-of-local-providers">Proveedores Locales</a> a continuación para un resumen.</p>
<p>Siguiente paso: <a href="#start-services-and-run">Ejecutar AgenticSeek localmente</a></p>
<p><em>Consulta la sección de <a href="#troubleshooting">Solución de Problemas</a> si tienes inconvenientes.</em>
<em>Si tu hardware no puede ejecutar LLMs localmente, consulta <a href="#setup-to-run-with-an-api">Configuración para usar con una API</a>.</em>
<em>Para explicaciones detalladas del <code>config.ini</code>, consulta la <a href="#config">Sección de Configuración</a>.</em></p>
<hr />
<h2>Configuración para ejecutar LLM localmente en tu máquina</h2>
<p><strong>Requisitos de hardware:</strong></p>
<p>Para ejecutar LLMs localmente, necesitarás hardware suficiente. Como mínimo, se requiere una GPU capaz de ejecutar Magistral, Qwen o Deepseek 14B. Consulta las recomendaciones de modelos/rendimiento en las preguntas frecuentes.</p>
<p><strong>Configura tu proveedor local</strong></p>
<p>Inicia tu proveedor local, por ejemplo con ollama:</p>
<pre><code class="language-sh">ollama serve
</code></pre>
<p>Consulta abajo la lista de proveedores locales soportados.</p>
<p><strong>Actualiza el config.ini</strong></p>
<p>Cambia el archivo config.ini para establecer provider_name a un proveedor soportado y provider_model a un LLM soportado por tu proveedor. Recomendamos modelos de razonamiento como <em>Magistral</em> o <em>Deepseek</em>.</p>
<p>Consulta las <strong>FAQ</strong> al final del README para el hardware requerido.</p>
<pre><code class="language-sh">[MAIN]
is_local = True # Indica si estás ejecutando localmente o con proveedor remoto.
provider_name = ollama # o lm-studio, openai, etc.
provider_model = deepseek-r1:14b # elige un modelo que se ajuste a tu hardware
provider_server_address = 127.0.0.1:11434
agent_name = Jarvis # nombre de tu IA
recover_last_session = True # si se recupera la sesión previa
save_session = True # si se recuerda la sesión actual
speak = False # texto a voz
listen = False # voz a texto, solo para CLI, experimental
jarvis_personality = False # si usar una personalidad más tipo &quot;Jarvis&quot; (experimental)
languages = en zh # Lista de idiomas, texto a voz usará el primero por defecto
[BROWSER]
headless_browser = True # déjalo sin cambios salvo que uses CLI en el host.
stealth_mode = True # Usa selenium indetectable para reducir detección de navegador
</code></pre>
<p><strong>Advertencia</strong>:</p>
<ul>
<li><p>El formato del archivo <code>config.ini</code> no admite comentarios.
No copies y pegues la configuración de ejemplo directamente, ya que los comentarios causarán errores. En su lugar, modifica manualmente el archivo <code>config.ini</code> con tus ajustes deseados, excluyendo cualquier comentario.</p>
</li>
<li><p><em>NO</em> establezcas provider_name en <code>openai</code> si usas LM-studio para ejecutar LLMs. Debe ser <code>lm-studio</code>.</p>
</li>
<li><p>Algunos proveedores (ej: lm-studio) requieren que incluyas <code>http://</code> delante de la IP. Por ejemplo <code>http://127.0.0.1:1234</code></p>
</li>
</ul>
<p><strong>Lista de proveedores locales</strong></p>
<p>| Proveedor    | ¿Local? | Descripción                                                           |
|--------------|---------|-----------------------------------------------------------------------|
| ollama       | Sí      | Ejecuta LLMs localmente fácilmente usando ollama como proveedor LLM   |
| lm-studio    | Sí      | Ejecuta LLM localmente con LM studio (establece <code>provider_name</code> en <code>lm-studio</code>)|
| openai       | Sí      | Usa una API compatible con openai (ej: servidor llama.cpp)            |</p>
<p>Siguiente paso: <a href="#Start-services-and-Run">Iniciar servicios y ejecutar AgenticSeek</a></p>
<p><em>Consulta la sección de <a href="#troubleshooting">Solución de Problemas</a> si tienes inconvenientes.</em>
<em>Si tu hardware no puede ejecutar LLMs localmente, consulta <a href="#setup-to-run-with-an-api">Configuración para usar con una API</a>.</em>
<em>Para explicaciones detalladas del <code>config.ini</code>, consulta la <a href="#config">Sección de Configuración</a>.</em></p>
<h2>Configuración para usar con una API</h2>
<p>Esta configuración utiliza proveedores LLM externos en la nube. Necesitarás una clave API del servicio elegido.</p>
<p><strong>1. Elige un proveedor de API y obtén una clave API:</strong></p>
<p>Consulta la <a href="#list-of-api-providers">Lista de proveedores de API</a> a continuación. Visita sus sitios web para registrarte y obtener una clave API.</p>
<p><strong>2. Establece tu clave API como variable de entorno:</strong></p>
<ul>
<li><strong>Linux/macOS:</strong>
Abre tu terminal y usa el comando <code>export</code>. Es mejor añadir esto al archivo de perfil de tu shell (por ejemplo, <code>~/.bashrc</code>, <code>~/.zshrc</code>) para que sea persistente.
<pre><code class="language-sh">export PROVIDER_API_KEY=&quot;tu_clave_api_aquí&quot; 
# Reemplaza PROVIDER_API_KEY por el nombre específico de la variable, por ejemplo, OPENAI_API_KEY, GOOGLE_API_KEY
</code></pre>
Ejemplo para TogetherAI:
<pre><code class="language-sh">export TOGETHER_API_KEY=&quot;xxxxxxxxxxxxxxxxxxxxxx&quot;
</code></pre>
</li>
<li><strong>Windows:</strong></li>
<li><strong>Command Prompt (Temporal para la sesión actual):</strong>
<pre><code class="language-cmd">set PROVIDER_API_KEY=tu_clave_api_aquí
</code></pre>
</li>
<li><strong>PowerShell (Temporal para la sesión actual):</strong>
<pre><code class="language-powershell">$env:PROVIDER_API_KEY=&quot;tu_clave_api_aquí&quot;
</code></pre>
</li>
<li><strong>Permanentemente:</strong> Busca &quot;variables de entorno&quot; en la barra de búsqueda de Windows, haz clic en &quot;Editar las variables de entorno del sistema&quot; y luego haz clic en el botón &quot;Variables de entorno...&quot;. Añade una nueva variable de usuario con el nombre apropiado (por ejemplo, <code>OPENAI_API_KEY</code>) y tu clave como valor.</li>
</ul>
<p><em>(Consulta la FAQ: <a href="#how-do-i-set-api-keys">¿Cómo configuro las claves API?</a> para más detalles).</em></p>
<p><strong>3. Actualiza <code>config.ini</code>:</strong></p>
<pre><code class="language-ini">[MAIN]
is_local = False
provider_name = openai # O google, deepseek, togetherAI, huggingface
provider_model = gpt-3.5-turbo # O gemini-1.5-flash, deepseek-chat, mistralai/Mixtral-8x7B-Instruct-v0.1 etc.
provider_server_address = # Normalmente se ignora o puede dejarse en blanco cuando is_local = False para la mayoría de APIs
# ... otros ajustes ...
</code></pre>
<p><em>Advertencia:</em> Asegúrate de que no haya espacios al final de los valores en <code>config.ini</code>.</p>
<p><strong>Lista de Proveedores de API</strong></p>
<p>| Proveedor    | <code>provider_name</code> | ¿Local? | Descripción                                        | Enlace de Clave API (Ejemplos)                    |
|--------------|-----------------|---------|----------------------------------------------------|---------------------------------------------------|
| OpenAI       | <code>openai</code>        | No      | Usa modelos ChatGPT vía la API de OpenAI.           | <a href="https://platform.openai.com/signup">platform.openai.com/signup</a> |
| Google Gemini| <code>google</code>        | No      | Usa modelos Google Gemini vía Google AI Studio.     | <a href="https://aistudio.google.com/keys">aistudio.google.com/keys</a> |
| Deepseek     | <code>deepseek</code>      | No      | Usa modelos Deepseek vía su API.                    | <a href="https://platform.deepseek.com">platform.deepseek.com</a> |
| Hugging Face | <code>huggingface</code>   | No      | Usa modelos de la Hugging Face Inference API.       | <a href="https://huggingface.co/settings/tokens">huggingface.co/settings/tokens</a> |
| TogetherAI   | <code>togetherAI</code>    | No      | Usa varios modelos open-source vía la API de TogetherAI.| <a href="https://api.together.ai/settings/api-keys">api.together.ai/settings/api-keys</a> |</p>
<p><em>Nota:</em></p>
<ul>
<li>Recomendamos no usar <code>gpt-4o</code> u otros modelos de OpenAI para tareas complejas de navegación web y planificación, ya que las optimizaciones actuales de prompts están diseñadas para modelos como Deepseek.</li>
<li>Las tareas de codificación/bash pueden presentar problemas con Gemini, ya que puede que no siga estrictamente los prompts de formato optimizados para Deepseek.</li>
<li>El campo <code>provider_server_address</code> en <code>config.ini</code> generalmente no se usa cuando <code>is_local = False</code>, ya que el endpoint de la API suele estar predefinido en la librería del proveedor correspondiente.</li>
</ul>
<p>Próximo paso: <a href="#Start-services-and-Run">Iniciar servicios y ejecutar AgenticSeek</a></p>
<p><em>Consulta la sección <strong>Problemas conocidos</strong> si tienes inconvenientes</em></p>
<p><em>Consulta la sección <strong>Config</strong> para una explicación detallada del archivo de configuración.</em></p>
<hr />
<h2>Iniciar servicios y ejecutar</h2>
<p>Por defecto AgenticSeek se ejecuta completamente en docker.</p>
<p>Inicia los servicios necesarios. Esto iniciará todos los servicios definidos en docker-compose.yml, incluyendo:
- searxng
- redis (requerido por searxng)
- frontend
- backend (si usas <code>full</code>)</p>
<pre><code class="language-sh">./start_services.sh full # MacOS
start ./start_services.cmd full # Windows
</code></pre>
<p><strong>Advertencia:</strong> Este paso descargará y cargará todas las imágenes de Docker, lo cual puede tardar hasta 30 minutos. Después de iniciar los servicios, espera hasta que el servicio backend esté completamente funcionando (deberías ver <strong>backend: &quot;GET /health HTTP/1.1&quot; 200 OK</strong> en el log) antes de enviar cualquier mensaje. El backend puede tardar unos 5 minutos en iniciar en la primera ejecución.</p>
<p>Ve a <code>http://localhost:3000/</code> y deberías ver la interfaz web.</p>
<p><em>Solución de problemas al iniciar servicios:</em> Si estos scripts fallan, asegúrate de que Docker Engine esté en ejecución y que Docker Compose (V2, <code>docker compose</code>) esté correctamente instalado. Revisa la salida en la terminal para mensajes de error. Consulta <a href="#faq-troubleshooting">FAQ: ¡Ayuda! Obtengo un error al ejecutar AgenticSeek o sus scripts.</a></p>
<p><strong>Opcional:</strong> Ejecutar en el host (modo CLI):</p>
<p>Para ejecutar con interfaz CLI deberás instalar el paquete en el host:</p>
<pre><code class="language-sh">./install.sh
./install.bat # windows
</code></pre>
<p>Inicia los servicios:</p>
<pre><code class="language-sh">./start_services.sh # MacOS
start ./start_services.cmd # Windows
</code></pre>
<p>Usa la CLI: <code>python3 cli.py</code></p>
<hr />
<h2>Uso</h2>
<p>Asegúrate de que los servicios estén activos con <code>./start_services.sh full</code> y accede a <code>localhost:3000</code> para la interfaz web.</p>
<p>También puedes usar reconocimiento de voz configurando <code>listen = True</code> en el config. Solo para modo CLI.</p>
<p>Para salir, simplemente di/escribe <code>goodbye</code>.</p>
<p>Algunos ejemplos de uso:</p>
<blockquote>
<p><em>¡Crea un juego de la serpiente en python!</em></p>
</blockquote>
<blockquote>
<p><em>Busca en la web los mejores cafés en Rennes, Francia, y guarda una lista de tres con sus direcciones en rennes_cafes.txt.</em></p>
</blockquote>
<blockquote>
<p><em>Escribe un programa en Go que calcule el factorial de un número, guárdalo como factorial.go en tu espacio de trabajo</em></p>
</blockquote>
<blockquote>
<p><em>Busca en mi carpeta summer_pictures todos los archivos JPG, renómbralos con la fecha de hoy y guarda una lista de los archivos renombrados en photos_list.txt</em></p>
</blockquote>
<blockquote>
<p><em>Busca en línea películas populares de ciencia ficción del 2024 y elige tres para ver esta noche. Guarda la lista en movie_night.txt.</em></p>
</blockquote>
<blockquote>
<p><em>Busca en la web los últimos artículos de noticias de IA de 2025, selecciona tres y escribe un script en Python para extraer sus títulos y resúmenes. Guarda el script como news_scraper.py y los resúmenes en ai_news.txt en /home/projects</em></p>
</blockquote>
<blockquote>
<p><em>El viernes, busca en la web una API gratuita de precios de acciones, regístrate con supersuper7434567@gmail.com y escribe un script en Python para obtener los precios diarios de Tesla usando la API, guardando los resultados en stock_prices.csv</em></p>
</blockquote>
<p><em>Ten en cuenta que las capacidades de llenado de formularios aún son experimentales y podrían fallar.</em></p>
<p>Después de escribir tu consulta, AgenticSeek asignará el mejor agente para la tarea.</p>
<p>Dado que este es un prototipo temprano, el sistema de enrutamiento de agentes puede no asignar siempre el agente adecuado según tu consulta.</p>
<p>Por lo tanto, debes ser muy explícito en lo que deseas y cómo la IA debe proceder; por ejemplo, si quieres que realice una búsqueda web, no digas:</p>
<p><code>¿Conoces algunos buenos países para viajar solo?</code></p>
<p>En su lugar, pide:</p>
<p><code>Haz una búsqueda web y encuentra cuáles son los mejores países para viajar solo</code></p>
<hr />
<h2><strong>Configuración para ejecutar el LLM en tu propio servidor</strong></h2>
<p>Si tienes un ordenador potente o un servidor que puedas usar, pero quieres utilizarlo desde tu laptop, tienes la opción de ejecutar el LLM en un servidor remoto usando nuestro servidor LLM personalizado.</p>
<p>En tu &quot;servidor&quot; que ejecutará el modelo de IA, obtén la dirección IP</p>
<pre><code class="language-sh">ip a | grep &quot;inet &quot; | grep -v 127.0.0.1 | awk '{print $2}' | cut -d/ -f1 # ip local
curl https://ipinfo.io/ip # ip pública
</code></pre>
<p>Nota: Para Windows o macOS, utiliza ipconfig o ifconfig respectivamente para encontrar la dirección IP.</p>
<p>Clona el repositorio y entra en la carpeta <code>server/</code>.</p>
<pre><code class="language-sh">git clone --depth 1 https://github.com/Fosowl/agenticSeek.git
cd agenticSeek/llm_server/
</code></pre>
<p>Instala los requisitos específicos del servidor:</p>
<pre><code class="language-sh">pip3 install -r requirements.txt
</code></pre>
<p>Ejecuta el script del servidor.</p>
<pre><code class="language-sh">python3 app.py --provider ollama --port 3333
</code></pre>
<p>Puedes elegir entre usar <code>ollama</code> y <code>llamacpp</code> como servicio LLM.</p>
<p>Ahora en tu ordenador personal:</p>
<p>Cambia el archivo <code>config.ini</code> para establecer <code>provider_name</code> a <code>server</code> y <code>provider_model</code> a <code>deepseek-r1:xxb</code>.
Configura <code>provider_server_address</code> con la dirección IP de la máquina que ejecutará el modelo.</p>
<pre><code class="language-sh">[MAIN]
is_local = False
provider_name = server
provider_model = deepseek-r1:70b
provider_server_address = x.x.x.x:3333
</code></pre>
<p>Próximo paso: <a href="#Start-services-and-Run">Iniciar servicios y ejecutar AgenticSeek</a></p>
<hr />
<h2>Reconocimiento de Voz a Texto</h2>
<p>Advertencia: el reconocimiento de voz a texto solo funciona en modo CLI por el momento.</p>
<p>Ten en cuenta que actualmente el reconocimiento de voz a texto solo funciona en inglés.</p>
<p>La funcionalidad de reconocimiento de voz a texto está deshabilitada por defecto. Para habilitarla, establece la opción listen a True en el archivo config.ini:</p>
<pre><code>listen = True
</code></pre>
<p>Cuando está habilitada, la función de voz a texto escucha una palabra clave de activación, que es el nombre del agente, antes de comenzar a procesar tu entrada. Puedes personalizar el nombre del agente actualizando el valor <code>agent_name</code> en el archivo <em>config.ini</em>:</p>
<pre><code>agent_name = Friday
</code></pre>
<p>Para un reconocimiento óptimo, recomendamos usar un nombre inglés común como &quot;John&quot; o &quot;Emma&quot; como nombre del agente.</p>
<p>Una vez que vea que la transcripción comienza a aparecer, diga el nombre del agente en voz alta para activarlo (por ejemplo, &quot;Friday&quot;).</p>
<p>Hable su consulta claramente.</p>
<p>Finalice su solicitud con una frase de confirmación para indicar al sistema que debe proceder. Ejemplos de frases de confirmación incluyen:</p>
<pre><code>&quot;hazlo&quot;, &quot;adelante&quot;, &quot;ejecuta&quot;, &quot;corre&quot;, &quot;inicia&quot;, &quot;gracias&quot;, &quot;harías eso&quot;, &quot;por favor&quot;, &quot;¿vale?&quot;, &quot;proceder&quot;, &quot;continuar&quot;, &quot;sigue&quot;, &quot;haz eso&quot;, &quot;hazlo&quot;, &quot;¿entiendes?&quot;
</code></pre>
<h2>Configuración</h2>
<p>Ejemplo de configuración:</p>
<pre><code>[MAIN]
is_local = True
provider_name = ollama
provider_model = deepseek-r1:32b
provider_server_address = http://127.0.0.1:11434 # Ejemplo para Ollama; use http://127.0.0.1:1234 para LM-Studio
agent_name = Friday
recover_last_session = False
save_session = False
speak = False
listen = False

jarvis_personality = False
languages = en zh # Lista de idiomas para TTS y potencialmente enrutamiento.
[BROWSER]
headless_browser = False
stealth_mode = False
</code></pre>
<p><strong>Explicación de los ajustes en <code>config.ini</code></strong>:</p>
<ul>
<li><strong>Sección <code>[MAIN]</code>:</strong>
<ul>
<li><code>is_local</code>: <code>True</code> si utiliza un proveedor LLM local (Ollama, LM-Studio, servidor compatible con OpenAI local) o la opción de servidor autohospedado. <code>False</code> si usa una API en la nube (OpenAI, Google, etc.).</li>
<li><code>provider_name</code>: Especifica el proveedor LLM.
<ul>
<li>Opciones locales: <code>ollama</code>, <code>lm-studio</code>, <code>openai</code> (para servidores locales compatibles con OpenAI), <code>server</code> (para configuración de servidor autohospedado).</li>
<li>Opciones de API: <code>openai</code>, <code>google</code>, <code>deepseek</code>, <code>huggingface</code>, <code>togetherAI</code>.</li>
</ul>
</li>
<li><code>provider_model</code>: El nombre o ID del modelo específico para el proveedor elegido (por ejemplo, <code>deepseekcoder:6.7b</code> para Ollama, <code>gpt-3.5-turbo</code> para OpenAI API, <code>mistralai/Mixtral-8x7B-Instruct-v0.1</code> para TogetherAI).</li>
<li><code>provider_server_address</code>: La dirección de su proveedor LLM.
<ul>
<li>Para proveedores locales: por ejemplo, <code>http://127.0.0.1:11434</code> para Ollama, <code>http://127.0.0.1:1234</code> para LM-Studio.</li>
<li>Para el tipo de proveedor <code>server</code>: La dirección de su servidor LLM autohospedado (por ejemplo, <code>http://your_server_ip:3333</code>).</li>
<li>Para APIs en la nube (<code>is_local = False</code>): Normalmente se ignora o puede dejarse en blanco, ya que el endpoint API suele ser gestionado por la librería cliente.</li>
</ul>
</li>
<li><code>agent_name</code>: Nombre del asistente de IA (por ejemplo, Friday). Se utiliza como palabra clave para activación por voz si está habilitado.</li>
<li><code>recover_last_session</code>: <code>True</code> para intentar restaurar el estado de la sesión anterior, <code>False</code> para comenzar de nuevo.</li>
<li><code>save_session</code>: <code>True</code> para guardar el estado de la sesión actual para una posible recuperación, <code>False</code> en caso contrario.</li>
<li><code>speak</code>: <code>True</code> para habilitar la salida de voz por texto a voz, <code>False</code> para deshabilitarla.</li>
<li><code>listen</code>: <code>True</code> para habilitar la entrada de voz a texto (solo en modo CLI), <code>False</code> para deshabilitarla.</li>
<li><code>work_dir</code>: <strong>Crucial:</strong> El directorio donde AgenticSeek leerá/escribirá archivos. <strong>Asegúrese de que esta ruta sea válida y accesible en su sistema.</strong></li>
<li><code>jarvis_personality</code>: <code>True</code> para usar un prompt de sistema más tipo &quot;Jarvis&quot; (experimental), <code>False</code> para el prompt estándar.</li>
<li><code>languages</code>: Una lista separada por comas de idiomas (por ejemplo, <code>en, zh, fr</code>). Se usa para la selección de voz TTS (por defecto el primero) y puede ayudar al enrutador LLM. Evite demasiados idiomas o muy similares para la eficiencia del enrutador.</li>
</ul>
</li>
<li><strong>Sección <code>[BROWSER]</code>:</strong>
<ul>
<li><code>headless_browser</code>: <code>True</code> para ejecutar el navegador automatizado sin ventana visible (recomendado para interfaz web o uso no interactivo). <code>False</code> para mostrar la ventana del navegador (útil para modo CLI o depuración).</li>
<li><code>stealth_mode</code>: <code>True</code> para habilitar medidas que dificultan la detección de la automatización del navegador. Puede requerir la instalación manual de extensiones como anticaptcha.</li>
</ul>
</li>
</ul>
<p>Esta sección resume los tipos de proveedores LLM soportados. Configúrelos en <code>config.ini</code>.</p>
<p><strong>Proveedores locales (Ejecutados en su propio hardware):</strong></p>
<p>| Nombre del proveedor en <code>config.ini</code> | <code>is_local</code> | Descripción                                                                 | Sección de configuración                                                    |
|--------------------------------------|------------|-----------------------------------------------------------------------------|----------------------------------------------------------------------------|
| <code>ollama</code>                            | <code>True</code>     | Use Ollama para servir LLMs locales.                                        | <a href="#setup-for-running-llm-locally-on-your-machine">Configuración para ejecutar LLM localmente</a> |
| <code>lm-studio</code>                         | <code>True</code>     | Use LM-Studio para servir LLMs locales.                                     | <a href="#setup-for-running-llm-locally-on-your-machine">Configuración para ejecutar LLM localmente</a> |
| <code>openai</code> (para servidor local)       | <code>True</code>     | Conéctese a un servidor local que exponga una API compatible con OpenAI (por ejemplo, llama.cpp). | <a href="#setup-for-running-llm-locally-on-your-machine">Configuración para ejecutar LLM localmente</a> |
| <code>server</code>                            | <code>False</code>    | Conéctese al servidor LLM autohospedado de AgenticSeek ejecutándose en otra máquina. | <a href="#setup-to-run-the-llm-on-your-own-server">Configuración para ejecutar el LLM en su propio servidor</a> |</p>
<p><strong>Proveedores de API (En la nube):</strong></p>
<p>| Nombre del proveedor en <code>config.ini</code> | <code>is_local</code> | Descripción                                      | Sección de configuración                              |
|--------------------------------------|------------|--------------------------------------------------|-------------------------------------------------------|
| <code>openai</code>                            | <code>False</code>    | Use la API oficial de OpenAI (por ejemplo, GPT-3.5, GPT-4). | <a href="#setup-to-run-with-an-api">Configuración para usar una API</a> |
| <code>google</code>                            | <code>False</code>    | Use los modelos Gemini de Google vía API.         | <a href="#setup-to-run-with-an-api">Configuración para usar una API</a> |
| <code>deepseek</code>                          | <code>False</code>    | Use la API oficial de Deepseek.                   | <a href="#setup-to-run-with-an-api">Configuración para usar una API</a> |
| <code>huggingface</code>                       | <code>False</code>    | Use la API de Inferencia de Hugging Face.         | <a href="#setup-to-run-with-an-api">Configuración para usar una API</a> |
| <code>togetherAI</code>                        | <code>False</code>    | Use la API de TogetherAI para varios modelos open.| <a href="#setup-to-run-with-an-api">Configuración para usar una API</a> |</p>
<hr />
<h2>Solución de problemas</h2>
<p>Si encuentra problemas, esta sección proporciona orientación.</p>
<h1>Problemas conocidos</h1>
<h2>Problemas con ChromeDriver</h2>
<p><strong>Ejemplo de error:</strong> <code>SessionNotCreatedException: Message: session not created: This version of ChromeDriver only supports Chrome version XXX</code></p>
<ul>
<li><strong>Causa:</strong> La versión de ChromeDriver instalada no es compatible con la versión de su navegador Google Chrome.</li>
<li><strong>Solución:</strong>
<ol>
<li><strong>Verifique la versión de Chrome:</strong> Abra Google Chrome, vaya a <code>Configuración &gt; Acerca de Chrome</code> para encontrar la versión (por ejemplo, &quot;Versión 120.0.6099.110&quot;).</li>
<li><strong>Descargue el ChromeDriver correspondiente:</strong>
<ul>
<li>Para versiones de Chrome 115 y superiores: Vaya a los <a href="https://googlechromelabs.github.io/chrome-for-testing/">Chrome for Testing (CfT) JSON Endpoints</a>. Busque el canal &quot;stable&quot; y descargue el ChromeDriver para su SO que coincida con la versión principal de su Chrome.</li>
<li>Para versiones antiguas (menos común): Puede encontrarlas en la página de <a href="https://chromedriver.chromium.org/downloads">ChromeDriver - WebDriver for Chrome</a>.</li>
<li>La imagen a continuación muestra un ejemplo de la página CfT:
<img src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/chromedriver_readme.png" alt="Descargue una versión específica de Chromedriver desde la página Chrome for Testing" /></li>
</ul>
</li>
<li><strong>Instale ChromeDriver:</strong>
<ul>
<li>Asegúrese de que el <code>chromedriver</code> descargado (o <code>chromedriver.exe</code> en Windows) esté en un directorio incluido en la variable de entorno PATH de su sistema (por ejemplo, <code>/usr/local/bin</code> en Linux/macOS, o una carpeta de scripts personalizada añadida a PATH en Windows).</li>
<li>Alternativamente, colóquelo en el directorio raíz del proyecto <code>agenticSeek</code>.</li>
<li>Asegúrese de que el driver sea ejecutable (por ejemplo, <code>chmod +x chromedriver</code> en Linux/macOS).</li>
</ul>
</li>
<li>Consulte la sección <a href="#chromedriver-installation">Instalación de ChromeDriver</a> en la guía principal de instalación para más detalles.</li>
</ol>
</li>
</ul>
<p>Si esta sección está incompleta o encuentra otros problemas con ChromeDriver, considere buscar en los <a href="https://github.com/Fosowl/agenticSeek/issues">Issues de GitHub</a> existentes o crear uno nuevo.</p>
<p><code>Exception: Failed to initialize browser: Message: session not created: This version of ChromeDriver only supports Chrome version 113 Current browser version is 134.0.6998.89 with binary path</code></p>
<p>Esto ocurre si hay un desajuste entre su versión de navegador y la versión de chromedriver.</p>
<p>Debe ir a descargar la versión más reciente:</p>
<p>https://developer.chrome.com/docs/chromedriver/downloads</p>
<p>Si está usando Chrome versión 115 o superior vaya a:</p>
<p>https://googlechromelabs.github.io/chrome-for-testing/</p>
<p>Y descargue la versión de chromedriver que coincida con su SO.</p>
<p><img src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/chromedriver_readme.png" alt="texto alternativo" /></p>
<p>Si esta sección está incompleta por favor cree un issue.</p>
<h2>Problemas de adaptadores de conexión</h2>
<pre><code>Exception: Provider lm-studio failed: HTTP request failed: No connection adapters were found for '127.0.0.1:1234/v1/chat/completions'` (Nota: el puerto puede variar)
</code></pre>
<ul>
<li><strong>Causa:</strong> La variable <code>provider_server_address</code> en <code>config.ini</code> para <code>lm-studio</code> (u otros servidores similares compatibles con OpenAI locales) no tiene el prefijo <code>http://</code> o apunta al puerto incorrecto.</li>
<li><strong>Solución:</strong>
<ul>
<li>Asegúrese de que la dirección incluya <code>http://</code>. LM-Studio normalmente usa por defecto <code>http://127.0.0.1:1234</code>.</li>
<li>Corrija en <code>config.ini</code>: <code>provider_server_address = http://127.0.0.1:1234</code> (o el puerto real de su servidor LM-Studio).</li>
</ul>
</li>
</ul>
<h2>URL base de SearxNG no proporcionada</h2>
<pre><code>raise ValueError(&quot;SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.&quot;)
ValueError: SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.`
</code></pre>
<h2>FAQ</h2>
<p><strong>P: ¿Qué hardware necesito?</strong></p>
<p>| Tamaño del modelo | GPU  | Comentario                                               |
|-------------------|------|---------------------------------------------------------|
| 7B                | 8GB Vram | ⚠️ No recomendado. Bajo rendimiento, frecuentes alucinaciones, y los agentes planificadores probablemente fallarán. |
| 14B               | 12 GB VRAM (ej. RTX 3060) | ✅ Usable para tareas simples. Puede tener problemas con navegación web y tareas de planificación. |
| 32B               | 24+ GB VRAM (ej. RTX 4090) | 🚀 Éxito en la mayoría de tareas, aún puede tener dificultades en planificación de tareas |
| 70B+              | 48+ GB Vram | 💪 Excelente. Recomendado para casos de uso avanzados. |</p>
<p><strong>P: Me aparece un error, ¿qué hago?</strong></p>
<p>Asegúrese de que el local esté en ejecución (<code>ollama serve</code>), que su <code>config.ini</code> coincida con su proveedor, y que las dependencias estén instaladas. Si nada funciona, siéntase libre de crear un issue.</p>
<p><strong>P: ¿Realmente puede funcionar 100% localmente?</strong></p>
<p>Sí, con Ollama, lm-studio o servidores, todos los modelos de voz a texto, LLM y texto a voz se ejecutan localmente. Las opciones no locales (OpenAI u otras APIs) son opcionales.</p>
<p><strong>P: ¿Por qué debería usar AgenticSeek si ya tengo Manus?</strong></p>
<p>A diferencia de Manus, AgenticSeek prioriza la independencia de sistemas externos, dándole más control, privacidad y evitando costos de API.</p>
<p><strong>P: ¿Quién está detrás del proyecto?</strong></p>
<p>El proyecto fue creado por mí, junto a dos amigos que actúan como mantenedores y colaboradores de la comunidad open-source en GitHub. Solo somos un grupo de personas apasionadas, no una startup ni afiliados a ninguna organización.</p>
<p>Cualquier cuenta de AgenticSeek en X que no sea mi cuenta personal (https://x.com/Martin993886460) es una suplantación.</p>
<h2>Contribuir</h2>
<p>¡Buscamos desarrolladores para mejorar AgenticSeek! Consulte los issues abiertos o las discusiones.</p>
<p><a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/docs/CONTRIBUTING.md">Guía de contribución</a></p>
<p><a href="https://www.star-history.com/#Fosowl/agenticSeek&amp;Date"><img src="https://api.star-history.com/svg?repos=Fosowl/agenticSeek&amp;type=Date" alt="Gráfico de estrellas históricas" /></a></p>
<h2>Mantenedores:</h2>
<blockquote>
<p><a href="https://github.com/Fosowl">Fosowl</a> | Hora de París</p>
</blockquote>
<blockquote>
<p><a href="https://github.com/antoineVIVIES">antoineVIVIES</a> | Hora de Taipéi</p>
</blockquote>
<blockquote>
<p><a href="https://github.com/steveh8758">steveh8758</a> | Hora de Taipéi</p>
</blockquote>
<h2>Agradecimientos especiales:</h2>
<blockquote>
<p><a href="https://github.com/tcsenpai">tcsenpai</a> y <a href="https://github.com/plitc">plitc</a> por ayudar con la dockerización del backend</p>
</blockquote>
<h2>Patrocinadores:</h2>
<p>Patrocinadores mensuales de 5$ o más aparecen aquí:</p>
<ul>
<li><strong>tatra-labs</strong></li>
</ul>
<p>Certainly! Please provide the text for Part 4 of 4 so I can translate it as requested.</p>
<hr />
<p>Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx">Open Ai Tx</a> | Last indexed: 2025-06-16</p>
<hr />

        </div>
    </div>
    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Statcounter and other scripts can be added here -->
</body>
</html>