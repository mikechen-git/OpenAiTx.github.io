<!DOCTYPE html><html lang="en"><head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AgenticSeek: Alternativa privada y local a Manus - Fosowl/agenticSeek</title>

    <!-- Primary Meta Tags -->
    <meta name="title" content="AgenticSeek: Alternativa privada y local a Manus - Fosowl/agenticSeek">
    <meta name="description" content="Fosowl/agenticSeek - GitHub repository es documentation and information">
    <meta name="keywords" content="Fosowl, agenticSeek, GitHub, repository, es documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">

    <!-- Open Graph -->
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/Fosowl/agenticSeek/README-es.html">
    <meta property="og:title" content="AgenticSeek: Alternativa privada y local a Manus - Fosowl/agenticSeek">
    <meta property="og:description" content="Fosowl/agenticSeek - GitHub repository es documentation and information">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">

    <!-- Favicon -->
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">

    <!-- Marked.js for Markdown rendering -->
    <script type="text/javascript" async="" src="https://www.statcounter.com/counter/recorder.js"></script><script src="/js/marked.min.js?v=20250613"></script>
    <!-- Highlight.js for code syntax highlighting -->
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <!-- Custom CSS -->
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        /* Layout */
        body {
            display: flex;
            flex-direction: column;
            min-height: 100vh;
        }

        .main-container {
            margin: 0 auto;
            width: 100%;
            max-width: 980px;
            padding: 0 20px;
        }

        @media (max-width: 768px) {
            .main-container {
                padding: 0 15px;
            }
        }

        /* Image size restrictions */
        .markdown-body img {
            max-width: 100%;
            height: auto;
        }

        /* Existing styles */
        .header {
            text-align: center;
            margin-bottom: 30px;
            padding: 20px;
            background-color: #f6f8fa;
            border-bottom: 1px solid #e1e4e8;
            position: relative;
        }

        .back-button {
            position: absolute;
            left: 20px;
            top: 50%;
            transform: translateY(-50%);
            color: #0366d6;
            text-decoration: none;
            display: flex;
            align-items: center;
            font-size: 14px;
            padding: 5px 10px;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            background-color: #fff;
        }

        .back-button:hover {
            background-color: #f6f8fa;
            border-color: #0366d6;
        }

        .back-button::before {
            content: "←";
            margin-right: 5px;
            font-size: 16px;
        }

        .header .links {
            margin-top: 10px;
            font-size: 16px;
        }

        .header .links a {
            color: #0366d6;
            text-decoration: none;
            margin-left: 5px;
        }

        .header .links a:hover {
            text-decoration: underline;
        }
        
        /* Language badges styles */
        .language-badges {
            margin-top: 15px;
            text-align: center;
        }
        .language-badges a {
            display: inline-block;
            margin: 2px;
            text-decoration: none;
        }
        .language-badges img {
            height: 20px;
            border-radius: 3px;
        }
        .language-badges a:hover img {
            opacity: 0.8;
        }
    </style>
</head>

<body>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/Fosowl/agenticSeek" id="githubRepoLink" target="_blank">Fosowl/agenticSeek</a>
        </div>
        <div class="language-badges" id="languageBadges"><a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=en"><img src="https://img.shields.io/badge/EN-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=zh-CN"><img src="https://img.shields.io/badge/简中-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=zh-TW"><img src="https://img.shields.io/badge/繁中-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=ja"><img src="https://img.shields.io/badge/日本語-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=ko"><img src="https://img.shields.io/badge/한국어-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=th"><img src="https://img.shields.io/badge/ไทย-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=fr"><img src="https://img.shields.io/badge/Français-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=de"><img src="https://img.shields.io/badge/Deutsch-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=es"><img src="https://img.shields.io/badge/Español-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=it"><img src="https://img.shields.io/badge/Italiano-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=ru"><img src="https://img.shields.io/badge/Русский-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=pt"><img src="https://img.shields.io/badge/Português-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=nl"><img src="https://img.shields.io/badge/Nederlands-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=pl"><img src="https://img.shields.io/badge/Polski-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=ar"><img src="https://img.shields.io/badge/العربية-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=tr"><img src="https://img.shields.io/badge/Türkçe-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=vi"><img src="https://img.shields.io/badge/Tiếng Việt-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=hi"><img src="https://img.shields.io/badge/हिंदी-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=fa"><img src="https://img.shields.io/badge/فارسی-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=Fosowl&amp;project=agenticSeek&amp;lang=id"><img src="https://img.shields.io/badge/Bahasa Indonesia-white" alt="version"></a></div>
    </div>

    <div class="main-container">
        <div class="markdown-body" id="content"><h1>AgenticSeek: Alternativa privada y local a Manus</h1>
<p align="center">
<img align="center" src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/agentic_seek_logo.png" width="300" height="300" alt="Agentic Seek Logo">
</p><p>

</p><p>  English | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_CHS.md">中文</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_CHT.md">繁體中文</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_FR.md">Français</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_JP.md">日本語</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_PTBR.md">Português (Brasil)</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_ES.md">Español</a></p>
<p><em>Una <strong>alternativa 100% local a Manus AI</strong>, este asistente de IA habilitado por voz navega autónomamente por la web, escribe código y planifica tareas, manteniendo todos los datos en tu dispositivo. Adaptado para modelos de razonamiento locales, se ejecuta completamente en tu hardware, garantizando total privacidad y cero dependencia de la nube.</em></p>
<p><a href="https://fosowl.github.io/agenticSeek.html"><img src="https://img.shields.io/static/v1?label=Website&amp;message=AgenticSeek&amp;color=blue&amp;style=flat-square" alt="Visita AgenticSeek"></a> <img src="https://img.shields.io/badge/license-GPL--3.0-green" alt="Licencia"> <a href="https://discord.gg/8hGDaME3TC"><img src="https://img.shields.io/badge/Discord-Join%20Us-7289DA?logo=discord&amp;logoColor=white" alt="Discord"></a> <a href="https://x.com/Martin993886460"><img src="https://img.shields.io/twitter/url/https/twitter.com/fosowl.svg?style=social&amp;label=Update%20%40Fosowl" alt="Twitter"></a> <a href="https://github.com/Fosowl/agenticSeek/stargazers"><img src="https://img.shields.io/github/stars/Fosowl/agenticSeek?style=social" alt="GitHub stars"></a></p>
<h3>¿Por qué AgenticSeek?</h3>
<ul>
<li><p>🔒 Totalmente local y privado - Todo funciona en tu máquina — sin nube, sin compartir datos. Tus archivos, conversaciones y búsquedas permanecen privadas.</p>
</li>
<li><p>🌐 Navegación web inteligente - AgenticSeek puede navegar por Internet de forma autónoma — buscar, leer, extraer información, rellenar formularios web — todo manos libres.</p>
</li>
<li><p>💻 Asistente de codificación autónomo - ¿Necesitas código? Puede escribir, depurar y ejecutar programas en Python, C, Go, Java y más — todo sin supervisión.</p>
</li>
<li><p>🧠 Selección inteligente de agentes - Tú preguntas, él determina automáticamente el mejor agente para el trabajo. Como tener un equipo de expertos listo para ayudar.</p>
</li>
<li><p>📋 Planifica y ejecuta tareas complejas - Desde planificar viajes hasta proyectos complejos — puede dividir grandes tareas en pasos y realizarlas usando múltiples agentes de IA.</p>
</li>
<li><p>🎙️ Habilitado por voz - Voz limpia, rápida y futurista, y conversión de voz a texto que te permite hablarle como si fuera tu IA personal de una película de ciencia ficción. (En progreso)</p>
</li>
</ul>
<h3><strong>Demostración</strong></h3>
<blockquote>
<p><em>¿Puedes buscar el proyecto agenticSeek, aprender qué habilidades se requieren, luego abrir el archivo CV_candidates.zip y decirme cuáles coinciden mejor con el proyecto?</em></p>
</blockquote>
<p><a href="https://github.com/user-attachments/assets/b8ca60e9-7b3b-4533-840e-08f9ac426316">https://github.com/user-attachments/assets/b8ca60e9-7b3b-4533-840e-08f9ac426316</a></p>
<p>Aviso: Esta demostración, incluidos todos los archivos que aparecen (por ejemplo: CV_candidates.zip), es completamente ficticia. No somos una corporación, buscamos colaboradores de código abierto, no candidatos.</p>
<blockquote>
<p>🛠⚠️️ <strong>Trabajo activo en progreso</strong></p>
</blockquote>
<blockquote>
<p>🙏 Este proyecto comenzó como un proyecto paralelo y no tiene hoja de ruta ni financiación. Ha crecido mucho más de lo que esperaba al llegar a GitHub Trending. Se agradecen profundamente las contribuciones, comentarios y paciencia.</p>
</blockquote>
<h2>Prerrequisitos</h2>
<p>Antes de comenzar, asegúrate de tener instalado el siguiente software:</p>
<ul>
<li><strong>Git:</strong> Para clonar el repositorio. <a href="https://git-scm.com/downloads">Descargar Git</a></li>
<li><strong>Python 3.10.x:</strong> Recomendamos encarecidamente utilizar la versión 3.10.x de Python. Usar otras versiones podría causar errores de dependencias. <a href="https://www.python.org/downloads/release/python-3100/">Descargar Python 3.10</a> (elige una versión 3.10.x).</li>
<li><strong>Docker Engine &amp; Docker Compose:</strong> Para ejecutar servicios integrados como SearxNG.<ul>
<li>Instala Docker Desktop (que incluye Docker Compose V2): <a href="https://docs.docker.com/desktop/install/windows-install/">Windows</a> | <a href="https://docs.docker.com/desktop/install/mac-install/">Mac</a> | <a href="https://docs.docker.com/desktop/install/linux-install/">Linux</a></li>
<li>Alternativamente, instala Docker Engine y Docker Compose por separado en Linux: <a href="https://docs.docker.com/engine/install/">Docker Engine</a> | <a href="https://docs.docker.com/compose/install/">Docker Compose</a> (asegúrate de instalar Compose V2, por ejemplo, <code>sudo apt-get install docker-compose-plugin</code>).</li>
</ul>
</li>
</ul>
<h3>1. <strong>Clona el repositorio y haz la configuración inicial</strong></h3>
<pre><code class="language-sh hljs language-bash">git <span class="hljs-built_in">clone</span> https://github.com/Fosowl/agenticSeek.git
<span class="hljs-built_in">cd</span> agenticSeek
<span class="hljs-built_in">mv</span> .env.example .<span class="hljs-built_in">env</span>
</code></pre>
<h3>2. Cambia el contenido del archivo .env</h3>
<pre><code class="language-sh hljs language-bash">SEARXNG_BASE_URL=<span class="hljs-string">"http://127.0.0.1:8080"</span>
REDIS_BASE_URL=<span class="hljs-string">"redis://redis:6379/0"</span>
WORK_DIR=<span class="hljs-string">"/Users/mlg/Documents/workspace_for_ai"</span>
OLLAMA_PORT=<span class="hljs-string">"11434"</span>
LM_STUDIO_PORT=<span class="hljs-string">"1234"</span>
CUSTOM_ADDITIONAL_LLM_PORT=<span class="hljs-string">"11435"</span>
OPENAI_API_KEY=<span class="hljs-string">'optional'</span>
DEEPSEEK_API_KEY=<span class="hljs-string">'optional'</span>
OPENROUTER_API_KEY=<span class="hljs-string">'optional'</span>
TOGETHER_API_KEY=<span class="hljs-string">'optional'</span>
GOOGLE_API_KEY=<span class="hljs-string">'optional'</span>
ANTHROPIC_API_KEY=<span class="hljs-string">'optional'</span>
</code></pre>
<p>Actualiza el archivo <code>.env</code> con tus propios valores según sea necesario:</p>
<ul>
<li><strong>SEARXNG_BASE_URL</strong>: Déjalo sin cambios</li>
<li><strong>REDIS_BASE_URL</strong>: Déjalo sin cambios</li>
<li><strong>WORK_DIR</strong>: Ruta a tu directorio de trabajo en tu máquina local. AgenticSeek podrá leer e interactuar con estos archivos.</li>
<li><strong>OLLAMA_PORT</strong>: Número de puerto para el servicio Ollama.</li>
<li><strong>LM_STUDIO_PORT</strong>: Número de puerto para el servicio LM Studio.</li>
<li><strong>CUSTOM_ADDITIONAL_LLM_PORT</strong>: Puerto para cualquier servicio LLM adicional personalizado.</li>
</ul>
<p><strong>Las claves API son totalmente opcionales para el usuario que elija ejecutar LLM localmente, que es el propósito principal de este proyecto. Déjalas vacías si tienes hardware suficiente</strong></p>
<h3>3. <strong>Inicia Docker</strong></h3>
<p>Asegúrate de que Docker esté instalado y en funcionamiento en tu sistema. Puedes iniciar Docker usando los siguientes comandos:</p>
<ul>
<li><p><strong>En Linux/macOS:</strong><br>  Abre una terminal y ejecuta:</p>
<pre><code class="language-sh hljs language-bash">sudo systemctl start docker
</code></pre>
<p>  O lanza Docker Desktop desde el menú de aplicaciones si está instalado.</p>
</li>
<li><p><strong>En Windows:</strong><br>  Inicia Docker Desktop desde el menú Inicio.</p>
</li>
</ul>
<p>Puedes verificar que Docker está funcionando ejecutando:</p>
<pre><code class="language-sh hljs language-bash">docker info
</code></pre>
<p>Si ves información sobre tu instalación de Docker, está funcionando correctamente.</p>
<p>Consulta la tabla de <a href="#list-of-local-providers">Proveedores Locales</a> a continuación para un resumen.</p>
<p>Siguiente paso: <a href="#start-services-and-run">Ejecutar AgenticSeek localmente</a></p>
<p><em>Consulta la sección de <a href="#troubleshooting">Solución de Problemas</a> si tienes inconvenientes.</em><br><em>Si tu hardware no puede ejecutar LLMs localmente, consulta <a href="#setup-to-run-with-an-api">Configuración para usar con una API</a>.</em><br><em>Para explicaciones detalladas del <code>config.ini</code>, consulta la <a href="#config">Sección de Configuración</a>.</em></p>
<hr>
<h2>Configuración para ejecutar LLM localmente en tu máquina</h2>
<p><strong>Requisitos de hardware:</strong></p>
<p>Para ejecutar LLMs localmente, necesitarás hardware suficiente. Como mínimo, se requiere una GPU capaz de ejecutar Magistral, Qwen o Deepseek 14B. Consulta las recomendaciones de modelos/rendimiento en las preguntas frecuentes.</p>
<p><strong>Configura tu proveedor local</strong></p>
<p>Inicia tu proveedor local, por ejemplo con ollama:</p>
<pre><code class="language-sh hljs language-bash">ollama serve
</code></pre>
<p>Consulta abajo la lista de proveedores locales soportados.</p>
<p><strong>Actualiza el config.ini</strong></p>
<p>Cambia el archivo config.ini para establecer provider_name a un proveedor soportado y provider_model a un LLM soportado por tu proveedor. Recomendamos modelos de razonamiento como <em>Magistral</em> o <em>Deepseek</em>.</p>
<p>Consulta las <strong>FAQ</strong> al final del README para el hardware requerido.</p>
<pre><code class="language-sh hljs language-bash">[MAIN]
is_local = True <span class="hljs-comment"># Indica si estás ejecutando localmente o con proveedor remoto.</span>
provider_name = ollama <span class="hljs-comment"># o lm-studio, openai, etc.</span>
provider_model = deepseek-r1:14b <span class="hljs-comment"># elige un modelo que se ajuste a tu hardware</span>
provider_server_address = 127.0.0.1:11434
agent_name = Jarvis <span class="hljs-comment"># nombre de tu IA</span>
recover_last_session = True <span class="hljs-comment"># si se recupera la sesión previa</span>
save_session = True <span class="hljs-comment"># si se recuerda la sesión actual</span>
speak = False <span class="hljs-comment"># texto a voz</span>
listen = False <span class="hljs-comment"># voz a texto, solo para CLI, experimental</span>
jarvis_personality = False <span class="hljs-comment"># si usar una personalidad más tipo "Jarvis" (experimental)</span>
languages = en zh <span class="hljs-comment"># Lista de idiomas, texto a voz usará el primero por defecto</span>
[BROWSER]
headless_browser = True <span class="hljs-comment"># déjalo sin cambios salvo que uses CLI en el host.</span>
stealth_mode = True <span class="hljs-comment"># Usa selenium indetectable para reducir detección de navegador</span>
</code></pre>
<p><strong>Advertencia</strong>:</p>
<ul>
<li><p>El formato del archivo <code>config.ini</code> no admite comentarios.<br>No copies y pegues la configuración de ejemplo directamente, ya que los comentarios causarán errores. En su lugar, modifica manualmente el archivo <code>config.ini</code> con tus ajustes deseados, excluyendo cualquier comentario.</p>
</li>
<li><p><em>NO</em> establezcas provider_name en <code>openai</code> si usas LM-studio para ejecutar LLMs. Debe ser <code>lm-studio</code>.</p>
</li>
<li><p>Algunos proveedores (ej: lm-studio) requieren que incluyas <code>http://</code> delante de la IP. Por ejemplo <code>http://127.0.0.1:1234</code></p>
</li>
</ul>
<p><strong>Lista de proveedores locales</strong></p>
<table>
<thead>
<tr>
<th>Proveedor</th>
<th>¿Local?</th>
<th>Descripción</th>
</tr>
</thead>
<tbody><tr>
<td>ollama</td>
<td>Sí</td>
<td>Ejecuta LLMs localmente fácilmente usando ollama como proveedor LLM</td>
</tr>
<tr>
<td>lm-studio</td>
<td>Sí</td>
<td>Ejecuta LLM localmente con LM studio (establece <code>provider_name</code> en <code>lm-studio</code>)</td>
</tr>
<tr>
<td>openai</td>
<td>Sí</td>
<td>Usa una API compatible con openai (ej: servidor llama.cpp)</td>
</tr>
</tbody></table>
<p>Siguiente paso: <a href="#Start-services-and-Run">Iniciar servicios y ejecutar AgenticSeek</a>  </p>
<p><em>Consulta la sección de <a href="#troubleshooting">Solución de Problemas</a> si tienes inconvenientes.</em><br><em>Si tu hardware no puede ejecutar LLMs localmente, consulta <a href="#setup-to-run-with-an-api">Configuración para usar con una API</a>.</em><br><em>Para explicaciones detalladas del <code>config.ini</code>, consulta la <a href="#config">Sección de Configuración</a>.</em></p>
<h2>Configuración para usar con una API</h2>
<p>Esta configuración utiliza proveedores LLM externos en la nube. Necesitarás una clave API del servicio elegido.</p>
<p><strong>1. Elige un proveedor de API y obtén una clave API:</strong></p>
<p>Consulta la <a href="#list-of-api-providers">Lista de proveedores de API</a> a continuación. Visita sus sitios web para registrarte y obtener una clave API.</p>
<p><strong>2. Establece tu clave API como variable de entorno:</strong></p>
<ul>
<li><strong>Linux/macOS:</strong><br>Abre tu terminal y usa el comando <code>export</code>. Es mejor añadir esto al archivo de perfil de tu shell (por ejemplo, <code>~/.bashrc</code>, <code>~/.zshrc</code>) para que sea persistente.<pre><code class="language-sh hljs language-bash"><span class="hljs-built_in">export</span> PROVIDER_API_KEY=<span class="hljs-string">"tu_clave_api_aquí"</span> 
<span class="hljs-comment"># Reemplaza PROVIDER_API_KEY por el nombre específico de la variable, por ejemplo, OPENAI_API_KEY, GOOGLE_API_KEY</span>
</code></pre>
Ejemplo para TogetherAI:<pre><code class="language-sh hljs language-bash"><span class="hljs-built_in">export</span> TOGETHER_API_KEY=<span class="hljs-string">"xxxxxxxxxxxxxxxxxxxxxx"</span>
</code></pre>
</li>
<li><strong>Windows:</strong></li>
<li><strong>Command Prompt (Temporal para la sesión actual):</strong><pre><code class="language-cmd">set PROVIDER_API_KEY=tu_clave_api_aquí
</code></pre>
</li>
<li><strong>PowerShell (Temporal para la sesión actual):</strong><pre><code class="language-powershell">$env:PROVIDER_API_KEY="tu_clave_api_aquí"
</code></pre>
</li>
<li><strong>Permanentemente:</strong> Busca "variables de entorno" en la barra de búsqueda de Windows, haz clic en "Editar las variables de entorno del sistema" y luego haz clic en el botón "Variables de entorno...". Añade una nueva variable de usuario con el nombre apropiado (por ejemplo, <code>OPENAI_API_KEY</code>) y tu clave como valor.</li>
</ul>
<p><em>(Consulta la FAQ: <a href="#how-do-i-set-api-keys">¿Cómo configuro las claves API?</a> para más detalles).</em></p>
<p><strong>3. Actualiza <code>config.ini</code>:</strong></p>
<pre><code class="language-ini hljs"><span class="hljs-section">[MAIN]</span>
<span class="hljs-attr">is_local</span> = <span class="hljs-literal">False</span>
<span class="hljs-attr">provider_name</span> = openai <span class="hljs-comment"># O google, deepseek, togetherAI, huggingface</span>
<span class="hljs-attr">provider_model</span> = gpt-<span class="hljs-number">3.5</span>-turbo <span class="hljs-comment"># O gemini-1.5-flash, deepseek-chat, mistralai/Mixtral-8x7B-Instruct-v0.1 etc.</span>
provider_server_address = <span class="hljs-comment"># Normalmente se ignora o puede dejarse en blanco cuando is_local = False para la mayoría de APIs</span>
<span class="hljs-comment"># ... otros ajustes ...</span>
</code></pre>
<p><em>Advertencia:</em> Asegúrate de que no haya espacios al final de los valores en <code>config.ini</code>.</p>
<p><strong>Lista de Proveedores de API</strong></p>
<table>
<thead>
<tr>
<th>Proveedor</th>
<th><code>provider_name</code></th>
<th>¿Local?</th>
<th>Descripción</th>
<th>Enlace de Clave API (Ejemplos)</th>
</tr>
</thead>
<tbody><tr>
<td>OpenAI</td>
<td><code>openai</code></td>
<td>No</td>
<td>Usa modelos ChatGPT vía la API de OpenAI.</td>
<td><a href="https://platform.openai.com/signup">platform.openai.com/signup</a></td>
</tr>
<tr>
<td>Google Gemini</td>
<td><code>google</code></td>
<td>No</td>
<td>Usa modelos Google Gemini vía Google AI Studio.</td>
<td><a href="https://aistudio.google.com/keys">aistudio.google.com/keys</a></td>
</tr>
<tr>
<td>Deepseek</td>
<td><code>deepseek</code></td>
<td>No</td>
<td>Usa modelos Deepseek vía su API.</td>
<td><a href="https://platform.deepseek.com">platform.deepseek.com</a></td>
</tr>
<tr>
<td>Hugging Face</td>
<td><code>huggingface</code></td>
<td>No</td>
<td>Usa modelos de la Hugging Face Inference API.</td>
<td><a href="https://huggingface.co/settings/tokens">huggingface.co/settings/tokens</a></td>
</tr>
<tr>
<td>TogetherAI</td>
<td><code>togetherAI</code></td>
<td>No</td>
<td>Usa varios modelos open-source vía la API de TogetherAI.</td>
<td><a href="https://api.together.ai/settings/api-keys">api.together.ai/settings/api-keys</a></td>
</tr>
</tbody></table>
<p><em>Nota:</em></p>
<ul>
<li>Recomendamos no usar <code>gpt-4o</code> u otros modelos de OpenAI para tareas complejas de navegación web y planificación, ya que las optimizaciones actuales de prompts están diseñadas para modelos como Deepseek.</li>
<li>Las tareas de codificación/bash pueden presentar problemas con Gemini, ya que puede que no siga estrictamente los prompts de formato optimizados para Deepseek.</li>
<li>El campo <code>provider_server_address</code> en <code>config.ini</code> generalmente no se usa cuando <code>is_local = False</code>, ya que el endpoint de la API suele estar predefinido en la librería del proveedor correspondiente.</li>
</ul>
<p>Próximo paso: <a href="#Start-services-and-Run">Iniciar servicios y ejecutar AgenticSeek</a></p>
<p><em>Consulta la sección <strong>Problemas conocidos</strong> si tienes inconvenientes</em></p>
<p><em>Consulta la sección <strong>Config</strong> para una explicación detallada del archivo de configuración.</em></p>
<hr>
<h2>Iniciar servicios y ejecutar</h2>
<p>Por defecto AgenticSeek se ejecuta completamente en docker.</p>
<p>Inicia los servicios necesarios. Esto iniciará todos los servicios definidos en docker-compose.yml, incluyendo:<br>    - searxng<br>    - redis (requerido por searxng)<br>    - frontend<br>    - backend (si usas <code>full</code>)</p>
<pre><code class="language-sh hljs language-bash">./start_services.sh full <span class="hljs-comment"># MacOS</span>
start ./start_services.cmd full <span class="hljs-comment"># Windows</span>
</code></pre>
<p><strong>Advertencia:</strong> Este paso descargará y cargará todas las imágenes de Docker, lo cual puede tardar hasta 30 minutos. Después de iniciar los servicios, espera hasta que el servicio backend esté completamente funcionando (deberías ver <strong>backend: "GET /health HTTP/1.1" 200 OK</strong> en el log) antes de enviar cualquier mensaje. El backend puede tardar unos 5 minutos en iniciar en la primera ejecución.</p>
<p>Ve a <code>http://localhost:3000/</code> y deberías ver la interfaz web.</p>
<p><em>Solución de problemas al iniciar servicios:</em> Si estos scripts fallan, asegúrate de que Docker Engine esté en ejecución y que Docker Compose (V2, <code>docker compose</code>) esté correctamente instalado. Revisa la salida en la terminal para mensajes de error. Consulta <a href="#faq-troubleshooting">FAQ: ¡Ayuda! Obtengo un error al ejecutar AgenticSeek o sus scripts.</a></p>
<p><strong>Opcional:</strong> Ejecutar en el host (modo CLI):</p>
<p>Para ejecutar con interfaz CLI deberás instalar el paquete en el host:</p>
<pre><code class="language-sh hljs language-bash">./install.sh
./install.bat <span class="hljs-comment"># windows</span>
</code></pre>
<p>Inicia los servicios:</p>
<pre><code class="language-sh hljs language-bash">./start_services.sh <span class="hljs-comment"># MacOS</span>
start ./start_services.cmd <span class="hljs-comment"># Windows</span>
</code></pre>
<p>Usa la CLI: <code>python3 cli.py</code></p>
<hr>
<h2>Uso</h2>
<p>Asegúrate de que los servicios estén activos con <code>./start_services.sh full</code> y accede a <code>localhost:3000</code> para la interfaz web.</p>
<p>También puedes usar reconocimiento de voz configurando <code>listen = True</code> en el config. Solo para modo CLI.</p>
<p>Para salir, simplemente di/escribe <code>goodbye</code>.</p>
<p>Algunos ejemplos de uso:</p>
<blockquote>
<p><em>¡Crea un juego de la serpiente en python!</em></p>
</blockquote>
<blockquote>
<p><em>Busca en la web los mejores cafés en Rennes, Francia, y guarda una lista de tres con sus direcciones en rennes_cafes.txt.</em></p>
</blockquote>
<blockquote>
<p><em>Escribe un programa en Go que calcule el factorial de un número, guárdalo como factorial.go en tu espacio de trabajo</em></p>
</blockquote>
<blockquote>
<p><em>Busca en mi carpeta summer_pictures todos los archivos JPG, renómbralos con la fecha de hoy y guarda una lista de los archivos renombrados en photos_list.txt</em></p>
</blockquote>
<blockquote>
<p><em>Busca en línea películas populares de ciencia ficción del 2024 y elige tres para ver esta noche. Guarda la lista en movie_night.txt.</em></p>
</blockquote>
<blockquote>
<p><em>Busca en la web los últimos artículos de noticias de IA de 2025, selecciona tres y escribe un script en Python para extraer sus títulos y resúmenes. Guarda el script como news_scraper.py y los resúmenes en ai_news.txt en /home/projects</em></p>
</blockquote>
<blockquote>
<p><em>El viernes, busca en la web una API gratuita de precios de acciones, regístrate con <a href="mailto:supersuper7434567@gmail.com">supersuper7434567@gmail.com</a> y escribe un script en Python para obtener los precios diarios de Tesla usando la API, guardando los resultados en stock_prices.csv</em></p>
</blockquote>
<p><em>Ten en cuenta que las capacidades de llenado de formularios aún son experimentales y podrían fallar.</em></p>
<p>Después de escribir tu consulta, AgenticSeek asignará el mejor agente para la tarea.</p>
<p>Dado que este es un prototipo temprano, el sistema de enrutamiento de agentes puede no asignar siempre el agente adecuado según tu consulta.</p>
<p>Por lo tanto, debes ser muy explícito en lo que deseas y cómo la IA debe proceder; por ejemplo, si quieres que realice una búsqueda web, no digas:</p>
<p><code>¿Conoces algunos buenos países para viajar solo?</code></p>
<p>En su lugar, pide:</p>
<p><code>Haz una búsqueda web y encuentra cuáles son los mejores países para viajar solo</code></p>
<hr>
<h2><strong>Configuración para ejecutar el LLM en tu propio servidor</strong></h2>
<p>Si tienes un ordenador potente o un servidor que puedas usar, pero quieres utilizarlo desde tu laptop, tienes la opción de ejecutar el LLM en un servidor remoto usando nuestro servidor LLM personalizado.</p>
<p>En tu "servidor" que ejecutará el modelo de IA, obtén la dirección IP</p>
<pre><code class="language-sh hljs language-bash">ip a | grep <span class="hljs-string">"inet "</span> | grep -v 127.0.0.1 | awk <span class="hljs-string">'{print $2}'</span> | <span class="hljs-built_in">cut</span> -d/ -f1 <span class="hljs-comment"># ip local</span>
curl https://ipinfo.io/ip <span class="hljs-comment"># ip pública</span>
</code></pre>
<p>Nota: Para Windows o macOS, utiliza ipconfig o ifconfig respectivamente para encontrar la dirección IP.</p>
<p>Clona el repositorio y entra en la carpeta <code>server/</code>.</p>
<pre><code class="language-sh hljs language-bash">git <span class="hljs-built_in">clone</span> --depth 1 https://github.com/Fosowl/agenticSeek.git
<span class="hljs-built_in">cd</span> agenticSeek/llm_server/
</code></pre>
<p>Instala los requisitos específicos del servidor:</p>
<pre><code class="language-sh hljs language-bash">pip3 install -r requirements.txt
</code></pre>
<p>Ejecuta el script del servidor.</p>
<pre><code class="language-sh hljs language-bash">python3 app.py --provider ollama --port 3333
</code></pre>
<p>Puedes elegir entre usar <code>ollama</code> y <code>llamacpp</code> como servicio LLM.</p>
<p>Ahora en tu ordenador personal:</p>
<p>Cambia el archivo <code>config.ini</code> para establecer <code>provider_name</code> a <code>server</code> y <code>provider_model</code> a <code>deepseek-r1:xxb</code>.<br>Configura <code>provider_server_address</code> con la dirección IP de la máquina que ejecutará el modelo.</p>
<pre><code class="language-sh hljs language-bash">[MAIN]
is_local = False
provider_name = server
provider_model = deepseek-r1:70b
provider_server_address = x.x.x.x:3333
</code></pre>
<p>Próximo paso: <a href="#Start-services-and-Run">Iniciar servicios y ejecutar AgenticSeek</a>  </p>
<hr>
<h2>Reconocimiento de Voz a Texto</h2>
<p>Advertencia: el reconocimiento de voz a texto solo funciona en modo CLI por el momento.</p>
<p>Ten en cuenta que actualmente el reconocimiento de voz a texto solo funciona en inglés.</p>
<p>La funcionalidad de reconocimiento de voz a texto está deshabilitada por defecto. Para habilitarla, establece la opción listen a True en el archivo config.ini:</p>
<pre><code class="hljs language-ini"><span class="hljs-attr">listen</span> = <span class="hljs-literal">True</span>
</code></pre>
<p>Cuando está habilitada, la función de voz a texto escucha una palabra clave de activación, que es el nombre del agente, antes de comenzar a procesar tu entrada. Puedes personalizar el nombre del agente actualizando el valor <code>agent_name</code> en el archivo <em>config.ini</em>:</p>
<pre><code class="hljs language-ini"><span class="hljs-attr">agent_name</span> = Friday
</code></pre>
<p>Para un reconocimiento óptimo, recomendamos usar un nombre inglés común como "John" o "Emma" como nombre del agente.</p>
<p>Una vez que vea que la transcripción comienza a aparecer, diga el nombre del agente en voz alta para activarlo (por ejemplo, "Friday").</p>
<p>Hable su consulta claramente.</p>
<p>Finalice su solicitud con una frase de confirmación para indicar al sistema que debe proceder. Ejemplos de frases de confirmación incluyen:</p>
<pre><code class="hljs language-bash"><span class="hljs-string">"hazlo"</span>, <span class="hljs-string">"adelante"</span>, <span class="hljs-string">"ejecuta"</span>, <span class="hljs-string">"corre"</span>, <span class="hljs-string">"inicia"</span>, <span class="hljs-string">"gracias"</span>, <span class="hljs-string">"harías eso"</span>, <span class="hljs-string">"por favor"</span>, <span class="hljs-string">"¿vale?"</span>, <span class="hljs-string">"proceder"</span>, <span class="hljs-string">"continuar"</span>, <span class="hljs-string">"sigue"</span>, <span class="hljs-string">"haz eso"</span>, <span class="hljs-string">"hazlo"</span>, <span class="hljs-string">"¿entiendes?"</span>
</code></pre>
<h2>Configuración</h2>
<p>Ejemplo de configuración:</p>
<pre><code class="hljs language-ini"><span class="hljs-section">[MAIN]</span>
<span class="hljs-attr">is_local</span> = <span class="hljs-literal">True</span>
<span class="hljs-attr">provider_name</span> = ollama
<span class="hljs-attr">provider_model</span> = deepseek-r1:<span class="hljs-number">32</span>b
<span class="hljs-attr">provider_server_address</span> = http://<span class="hljs-number">127.0</span>.<span class="hljs-number">0.1</span>:<span class="hljs-number">11434</span> <span class="hljs-comment"># Ejemplo para Ollama; use http://127.0.0.1:1234 para LM-Studio</span>
<span class="hljs-attr">agent_name</span> = Friday
<span class="hljs-attr">recover_last_session</span> = <span class="hljs-literal">False</span>
<span class="hljs-attr">save_session</span> = <span class="hljs-literal">False</span>
<span class="hljs-attr">speak</span> = <span class="hljs-literal">False</span>
<span class="hljs-attr">listen</span> = <span class="hljs-literal">False</span>

<span class="hljs-attr">jarvis_personality</span> = <span class="hljs-literal">False</span>
<span class="hljs-attr">languages</span> = en zh <span class="hljs-comment"># Lista de idiomas para TTS y potencialmente enrutamiento.</span>
<span class="hljs-section">[BROWSER]</span>
<span class="hljs-attr">headless_browser</span> = <span class="hljs-literal">False</span>
<span class="hljs-attr">stealth_mode</span> = <span class="hljs-literal">False</span>
</code></pre>
<p><strong>Explicación de los ajustes en <code>config.ini</code></strong>:</p>
<ul>
<li><strong>Sección <code>[MAIN]</code>:</strong><ul>
<li><code>is_local</code>: <code>True</code> si utiliza un proveedor LLM local (Ollama, LM-Studio, servidor compatible con OpenAI local) o la opción de servidor autohospedado. <code>False</code> si usa una API en la nube (OpenAI, Google, etc.).</li>
<li><code>provider_name</code>: Especifica el proveedor LLM.<ul>
<li>Opciones locales: <code>ollama</code>, <code>lm-studio</code>, <code>openai</code> (para servidores locales compatibles con OpenAI), <code>server</code> (para configuración de servidor autohospedado).</li>
<li>Opciones de API: <code>openai</code>, <code>google</code>, <code>deepseek</code>, <code>huggingface</code>, <code>togetherAI</code>.</li>
</ul>
</li>
<li><code>provider_model</code>: El nombre o ID del modelo específico para el proveedor elegido (por ejemplo, <code>deepseekcoder:6.7b</code> para Ollama, <code>gpt-3.5-turbo</code> para OpenAI API, <code>mistralai/Mixtral-8x7B-Instruct-v0.1</code> para TogetherAI).</li>
<li><code>provider_server_address</code>: La dirección de su proveedor LLM.<ul>
<li>Para proveedores locales: por ejemplo, <code>http://127.0.0.1:11434</code> para Ollama, <code>http://127.0.0.1:1234</code> para LM-Studio.</li>
<li>Para el tipo de proveedor <code>server</code>: La dirección de su servidor LLM autohospedado (por ejemplo, <code>http://your_server_ip:3333</code>).</li>
<li>Para APIs en la nube (<code>is_local = False</code>): Normalmente se ignora o puede dejarse en blanco, ya que el endpoint API suele ser gestionado por la librería cliente.</li>
</ul>
</li>
<li><code>agent_name</code>: Nombre del asistente de IA (por ejemplo, Friday). Se utiliza como palabra clave para activación por voz si está habilitado.</li>
<li><code>recover_last_session</code>: <code>True</code> para intentar restaurar el estado de la sesión anterior, <code>False</code> para comenzar de nuevo.</li>
<li><code>save_session</code>: <code>True</code> para guardar el estado de la sesión actual para una posible recuperación, <code>False</code> en caso contrario.</li>
<li><code>speak</code>: <code>True</code> para habilitar la salida de voz por texto a voz, <code>False</code> para deshabilitarla.</li>
<li><code>listen</code>: <code>True</code> para habilitar la entrada de voz a texto (solo en modo CLI), <code>False</code> para deshabilitarla.</li>
<li><code>work_dir</code>: <strong>Crucial:</strong> El directorio donde AgenticSeek leerá/escribirá archivos. <strong>Asegúrese de que esta ruta sea válida y accesible en su sistema.</strong></li>
<li><code>jarvis_personality</code>: <code>True</code> para usar un prompt de sistema más tipo "Jarvis" (experimental), <code>False</code> para el prompt estándar.</li>
<li><code>languages</code>: Una lista separada por comas de idiomas (por ejemplo, <code>en, zh, fr</code>). Se usa para la selección de voz TTS (por defecto el primero) y puede ayudar al enrutador LLM. Evite demasiados idiomas o muy similares para la eficiencia del enrutador.</li>
</ul>
</li>
<li><strong>Sección <code>[BROWSER]</code>:</strong><ul>
<li><code>headless_browser</code>: <code>True</code> para ejecutar el navegador automatizado sin ventana visible (recomendado para interfaz web o uso no interactivo). <code>False</code> para mostrar la ventana del navegador (útil para modo CLI o depuración).</li>
<li><code>stealth_mode</code>: <code>True</code> para habilitar medidas que dificultan la detección de la automatización del navegador. Puede requerir la instalación manual de extensiones como anticaptcha.</li>
</ul>
</li>
</ul>
<p>Esta sección resume los tipos de proveedores LLM soportados. Configúrelos en <code>config.ini</code>.</p>
<p><strong>Proveedores locales (Ejecutados en su propio hardware):</strong></p>
<table>
<thead>
<tr>
<th>Nombre del proveedor en <code>config.ini</code></th>
<th><code>is_local</code></th>
<th>Descripción</th>
<th>Sección de configuración</th>
</tr>
</thead>
<tbody><tr>
<td><code>ollama</code></td>
<td><code>True</code></td>
<td>Use Ollama para servir LLMs locales.</td>
<td><a href="#setup-for-running-llm-locally-on-your-machine">Configuración para ejecutar LLM localmente</a></td>
</tr>
<tr>
<td><code>lm-studio</code></td>
<td><code>True</code></td>
<td>Use LM-Studio para servir LLMs locales.</td>
<td><a href="#setup-for-running-llm-locally-on-your-machine">Configuración para ejecutar LLM localmente</a></td>
</tr>
<tr>
<td><code>openai</code> (para servidor local)</td>
<td><code>True</code></td>
<td>Conéctese a un servidor local que exponga una API compatible con OpenAI (por ejemplo, llama.cpp).</td>
<td><a href="#setup-for-running-llm-locally-on-your-machine">Configuración para ejecutar LLM localmente</a></td>
</tr>
<tr>
<td><code>server</code></td>
<td><code>False</code></td>
<td>Conéctese al servidor LLM autohospedado de AgenticSeek ejecutándose en otra máquina.</td>
<td><a href="#setup-to-run-the-llm-on-your-own-server">Configuración para ejecutar el LLM en su propio servidor</a></td>
</tr>
</tbody></table>
<p><strong>Proveedores de API (En la nube):</strong></p>
<table>
<thead>
<tr>
<th>Nombre del proveedor en <code>config.ini</code></th>
<th><code>is_local</code></th>
<th>Descripción</th>
<th>Sección de configuración</th>
</tr>
</thead>
<tbody><tr>
<td><code>openai</code></td>
<td><code>False</code></td>
<td>Use la API oficial de OpenAI (por ejemplo, GPT-3.5, GPT-4).</td>
<td><a href="#setup-to-run-with-an-api">Configuración para usar una API</a></td>
</tr>
<tr>
<td><code>google</code></td>
<td><code>False</code></td>
<td>Use los modelos Gemini de Google vía API.</td>
<td><a href="#setup-to-run-with-an-api">Configuración para usar una API</a></td>
</tr>
<tr>
<td><code>deepseek</code></td>
<td><code>False</code></td>
<td>Use la API oficial de Deepseek.</td>
<td><a href="#setup-to-run-with-an-api">Configuración para usar una API</a></td>
</tr>
<tr>
<td><code>huggingface</code></td>
<td><code>False</code></td>
<td>Use la API de Inferencia de Hugging Face.</td>
<td><a href="#setup-to-run-with-an-api">Configuración para usar una API</a></td>
</tr>
<tr>
<td><code>togetherAI</code></td>
<td><code>False</code></td>
<td>Use la API de TogetherAI para varios modelos open.</td>
<td><a href="#setup-to-run-with-an-api">Configuración para usar una API</a></td>
</tr>
</tbody></table>
<hr>
<h2>Solución de problemas</h2>
<p>Si encuentra problemas, esta sección proporciona orientación.</p>
<h1>Problemas conocidos</h1>
<h2>Problemas con ChromeDriver</h2>
<p><strong>Ejemplo de error:</strong> <code>SessionNotCreatedException: Message: session not created: This version of ChromeDriver only supports Chrome version XXX</code></p>
<ul>
<li><strong>Causa:</strong> La versión de ChromeDriver instalada no es compatible con la versión de su navegador Google Chrome.</li>
<li><strong>Solución:</strong><ol>
<li><strong>Verifique la versión de Chrome:</strong> Abra Google Chrome, vaya a <code>Configuración &gt; Acerca de Chrome</code> para encontrar la versión (por ejemplo, "Versión 120.0.6099.110").</li>
<li><strong>Descargue el ChromeDriver correspondiente:</strong><ul>
<li>Para versiones de Chrome 115 y superiores: Vaya a los <a href="https://googlechromelabs.github.io/chrome-for-testing/">Chrome for Testing (CfT) JSON Endpoints</a>. Busque el canal "stable" y descargue el ChromeDriver para su SO que coincida con la versión principal de su Chrome.</li>
<li>Para versiones antiguas (menos común): Puede encontrarlas en la página de <a href="https://chromedriver.chromium.org/downloads">ChromeDriver - WebDriver for Chrome</a>.</li>
<li>La imagen a continuación muestra un ejemplo de la página CfT:<br><img src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/chromedriver_readme.png" alt="Descargue una versión específica de Chromedriver desde la página Chrome for Testing"></li>
</ul>
</li>
<li><strong>Instale ChromeDriver:</strong><ul>
<li>Asegúrese de que el <code>chromedriver</code> descargado (o <code>chromedriver.exe</code> en Windows) esté en un directorio incluido en la variable de entorno PATH de su sistema (por ejemplo, <code>/usr/local/bin</code> en Linux/macOS, o una carpeta de scripts personalizada añadida a PATH en Windows).</li>
<li>Alternativamente, colóquelo en el directorio raíz del proyecto <code>agenticSeek</code>.</li>
<li>Asegúrese de que el driver sea ejecutable (por ejemplo, <code>chmod +x chromedriver</code> en Linux/macOS).</li>
</ul>
</li>
<li>Consulte la sección <a href="#chromedriver-installation">Instalación de ChromeDriver</a> en la guía principal de instalación para más detalles.</li>
</ol>
</li>
</ul>
<p>Si esta sección está incompleta o encuentra otros problemas con ChromeDriver, considere buscar en los <a href="https://github.com/Fosowl/agenticSeek/issues">Issues de GitHub</a> existentes o crear uno nuevo.</p>
<p><code>Exception: Failed to initialize browser: Message: session not created: This version of ChromeDriver only supports Chrome version 113 Current browser version is 134.0.6998.89 with binary path</code></p>
<p>Esto ocurre si hay un desajuste entre su versión de navegador y la versión de chromedriver.</p>
<p>Debe ir a descargar la versión más reciente:</p>
<p><a href="https://developer.chrome.com/docs/chromedriver/downloads">https://developer.chrome.com/docs/chromedriver/downloads</a></p>
<p>Si está usando Chrome versión 115 o superior vaya a:</p>
<p><a href="https://googlechromelabs.github.io/chrome-for-testing/">https://googlechromelabs.github.io/chrome-for-testing/</a></p>
<p>Y descargue la versión de chromedriver que coincida con su SO.</p>
<p><img src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/chromedriver_readme.png" alt="texto alternativo"></p>
<p>Si esta sección está incompleta por favor cree un issue.</p>
<h2>Problemas de adaptadores de conexión</h2>
<pre><code class="hljs language-php"><span class="hljs-built_in">Exception</span>: Provider lm-studio failed: HTTP request failed: No connection adapters were found <span class="hljs-keyword">for</span> <span class="hljs-string">'127.0.0.1:1234/v1/chat/completions'</span>` (Nota: el puerto puede variar)
</code></pre>
<ul>
<li><strong>Causa:</strong> La variable <code>provider_server_address</code> en <code>config.ini</code> para <code>lm-studio</code> (u otros servidores similares compatibles con OpenAI locales) no tiene el prefijo <code>http://</code> o apunta al puerto incorrecto.</li>
<li><strong>Solución:</strong><ul>
<li>Asegúrese de que la dirección incluya <code>http://</code>. LM-Studio normalmente usa por defecto <code>http://127.0.0.1:1234</code>.</li>
<li>Corrija en <code>config.ini</code>: <code>provider_server_address = http://127.0.0.1:1234</code> (o el puerto real de su servidor LM-Studio).</li>
</ul>
</li>
</ul>
<h2>URL base de SearxNG no proporcionada</h2>
<pre><code class="hljs language-csharp"><span class="hljs-function">raise <span class="hljs-title">ValueError</span>(<span class="hljs-params"><span class="hljs-string">"SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable."</span></span>)
ValueError: SearxNG <span class="hljs-keyword">base</span> URL must be provided either <span class="hljs-keyword">as</span> an argument <span class="hljs-keyword">or</span> via the SEARXNG_BASE_URL environment variable.`
</span></code></pre>
<h2>FAQ</h2>
<p><strong>P: ¿Qué hardware necesito?</strong>  </p>
<table>
<thead>
<tr>
<th>Tamaño del modelo</th>
<th>GPU</th>
<th>Comentario</th>
</tr>
</thead>
<tbody><tr>
<td>7B</td>
<td>8GB Vram</td>
<td>⚠️ No recomendado. Bajo rendimiento, frecuentes alucinaciones, y los agentes planificadores probablemente fallarán.</td>
</tr>
<tr>
<td>14B</td>
<td>12 GB VRAM (ej. RTX 3060)</td>
<td>✅ Usable para tareas simples. Puede tener problemas con navegación web y tareas de planificación.</td>
</tr>
<tr>
<td>32B</td>
<td>24+ GB VRAM (ej. RTX 4090)</td>
<td>🚀 Éxito en la mayoría de tareas, aún puede tener dificultades en planificación de tareas</td>
</tr>
<tr>
<td>70B+</td>
<td>48+ GB Vram</td>
<td>💪 Excelente. Recomendado para casos de uso avanzados.</td>
</tr>
</tbody></table>
<p><strong>P: Me aparece un error, ¿qué hago?</strong>  </p>
<p>Asegúrese de que el local esté en ejecución (<code>ollama serve</code>), que su <code>config.ini</code> coincida con su proveedor, y que las dependencias estén instaladas. Si nada funciona, siéntase libre de crear un issue.</p>
<p><strong>P: ¿Realmente puede funcionar 100% localmente?</strong>  </p>
<p>Sí, con Ollama, lm-studio o servidores, todos los modelos de voz a texto, LLM y texto a voz se ejecutan localmente. Las opciones no locales (OpenAI u otras APIs) son opcionales.</p>
<p><strong>P: ¿Por qué debería usar AgenticSeek si ya tengo Manus?</strong></p>
<p>A diferencia de Manus, AgenticSeek prioriza la independencia de sistemas externos, dándole más control, privacidad y evitando costos de API.</p>
<p><strong>P: ¿Quién está detrás del proyecto?</strong></p>
<p>El proyecto fue creado por mí, junto a dos amigos que actúan como mantenedores y colaboradores de la comunidad open-source en GitHub. Solo somos un grupo de personas apasionadas, no una startup ni afiliados a ninguna organización.</p>
<p>Cualquier cuenta de AgenticSeek en X que no sea mi cuenta personal (<a href="https://x.com/Martin993886460">https://x.com/Martin993886460</a>) es una suplantación.</p>
<h2>Contribuir</h2>
<p>¡Buscamos desarrolladores para mejorar AgenticSeek! Consulte los issues abiertos o las discusiones.</p>
<p><a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/docs/CONTRIBUTING.md">Guía de contribución</a></p>
<p><a href="https://www.star-history.com/#Fosowl/agenticSeek&amp;Date"><img src="https://api.star-history.com/svg?repos=Fosowl/agenticSeek&amp;type=Date" alt="Gráfico de estrellas históricas"></a></p>
<h2>Mantenedores:</h2>
<blockquote>
<p><a href="https://github.com/Fosowl">Fosowl</a> | Hora de París </p>
</blockquote>
<blockquote>
<p><a href="https://github.com/antoineVIVIES">antoineVIVIES</a> | Hora de Taipéi </p>
</blockquote>
<blockquote>
<p><a href="https://github.com/steveh8758">steveh8758</a> | Hora de Taipéi </p>
</blockquote>
<h2>Agradecimientos especiales:</h2>
<blockquote>
<p><a href="https://github.com/tcsenpai">tcsenpai</a> y <a href="https://github.com/plitc">plitc</a> por ayudar con la dockerización del backend</p>
</blockquote>
<h2>Patrocinadores:</h2>
<p>Patrocinadores mensuales de 5$ o más aparecen aquí:</p>
<ul>
<li><strong>tatra-labs</strong></li>
</ul>
<p>Certainly! Please provide the text for Part 4 of 4 so I can translate it as requested.</p>
<hr>
<p>Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx">Open Ai Tx</a> | Last indexed: 2025-06-16</p>
<hr>
</div>
    </div>

    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Default Statcounter code for openaitx
https://openaitx.github.io/ -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async=""></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>
    <!-- End of Statcounter Code -->
    


</body></html>