<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>agenticSeek - Fosowl/agenticSeek</title>
    <meta name="title" content="agenticSeek - Fosowl/agenticSeek">
    <meta name="description" content="Fosowl/agenticSeek - GitHub repository zh-CN documentation and informationAgenticSeek：本地私有的 Manus 替代方案 English | 中文 | 繁體中文 | Français | 日本語 | Português (Brasil) | Español 一个100% 本地化的 Manus AI 替代方案，此语音驱动的 AI 助手可自主浏览网页、编写代码、规划任务，所有数据都保存...">
    <meta name="keywords" content="Fosowl, agenticSeek, GitHub, repository, zh-CN documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/Fosowl/agenticSeek/README-zh-CN.html">
    <meta property="og:title" content="agenticSeek - Fosowl/agenticSeek">
    <meta property="og:description" content="Fosowl/agenticSeek - GitHub repository zh-CN documentation and information">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">
    <script src="/js/marked.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        body { display: flex; flex-direction: column; min-height: 100vh; }
        .main-container { margin: 0 auto; width: 100%; max-width: 980px; padding: 0 20px; }
        @media (max-width: 768px) { .main-container { padding: 0 15px; } }
        .markdown-body img { max-width: 100%; height: auto; }
        .header { text-align: center; margin-bottom: 30px; padding: 20px; background-color: #f6f8fa; border-bottom: 1px solid #e1e4e8; position: relative; }
        .header .links { margin-top: 10px; font-size: 16px; }
        .header .links a { color: #0366d6; text-decoration: none; margin-left: 5px; }
        .header .links a:hover { text-decoration: underline; }
        .language-badges { margin-top: 15px; text-align: center; }
        .language-badges a { display: inline-block; margin: 2px; text-decoration: none; }
        .language-badges img { height: 20px; border-radius: 3px; }
        .language-badges a:hover img { opacity: 0.8; }
    </style>
</head>
<body>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/Fosowl/agenticSeek" id="githubRepoLink" target="_blank">Fosowl/agenticSeek</a>
<h1 style="display: none;">AgenticSeek：本地私有的 Manus 替代方案 English | 中文 | 繁體中文 | Français | 日本語 | Português (Brasil) | Español 一个100% 本地化的 Manus AI 替代方案，此语音驱动的 AI 助手可自主浏览网页、编写代码、规划任务，所有数据都保存...</h1>
        </div>
        <div class="language-badges" id="languageBadges">
            <!-- You can generate language badges here if needed -->
        </div>
    </div>
    <div class="main-container">
        <div class="markdown-body" id="content">
            <h1>AgenticSeek：本地私有的 Manus 替代方案</h1>
<p align="center">
<img align="center" src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/agentic_seek_logo.png" width="300" height="300" alt="Agentic Seek Logo">
<p>
<p>English | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_CHS.md">中文</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_CHT.md">繁體中文</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_FR.md">Français</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_JP.md">日本語</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_PTBR.md">Português (Brasil)</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_ES.md">Español</a></p>
<p><em>一个<strong>100% 本地化的 Manus AI 替代方案</strong>，此语音驱动的 AI 助手可自主浏览网页、编写代码、规划任务，所有数据都保存在您的设备上。为本地推理模型量身定制，完全在您的硬件上运行，确保数据隐私，无需依赖云端。</em></p>
<p><a href="https://fosowl.github.io/agenticSeek.html"><img src="https://img.shields.io/static/v1?label=Website&amp;message=AgenticSeek&amp;color=blue&amp;style=flat-square" alt="访问 AgenticSeek" /></a> <img src="https://img.shields.io/badge/license-GPL--3.0-green" alt="License" /> <a href="https://discord.gg/8hGDaME3TC"><img src="https://img.shields.io/badge/Discord-Join%20Us-7289DA?logo=discord&amp;logoColor=white" alt="Discord" /></a> <a href="https://x.com/Martin993886460"><img src="https://img.shields.io/twitter/url/https/twitter.com/fosowl.svg?style=social&amp;label=Update%20%40Fosowl" alt="Twitter" /></a> <a href="https://github.com/Fosowl/agenticSeek/stargazers"><img src="https://img.shields.io/github/stars/Fosowl/agenticSeek?style=social" alt="GitHub stars" /></a></p>
<h3>为什么选择 AgenticSeek？</h3>
<ul>
<li><p>🔒 完全本地&amp;私有 —— 所有内容在您的计算机上运行，无云端、无数据共享。您的文件、对话和搜索都保持私密。</p>
</li>
<li><p>🌐 智能网页浏览 —— AgenticSeek 能自主上网搜索、阅读、提取信息、填写网页表单，全程免手动操作。</p>
</li>
<li><p>💻 自主编程助手 —— 需要代码？它能编写、调试并运行 Python、C、Go、Java 等多种程序，无需人工干预。</p>
</li>
<li><p>🧠 智能代理选择 —— 您只需提问，它会自动选择最适合的 AI 代理处理任务，就像有一支专家团队随时待命。</p>
</li>
<li><p>📋 规划并执行复杂任务 —— 从行程规划到复杂项目，能将大任务拆分为步骤，利用多智能体高效完成。</p>
</li>
<li><p>🎙️ 语音驱动 —— 干净、快速、未来感十足的语音与语音转文字功能，让您像在科幻电影中那样与个人 AI 对话。（开发中）</p>
</li>
</ul>
<h3><strong>演示</strong></h3>
<blockquote>
<p><em>你能搜索 agenticSeek 项目，了解需要哪些技能，然后打开 CV_candidates.zip，告诉我哪个最匹配该项目吗</em></p>
</blockquote>
<p>https://github.com/user-attachments/assets/b8ca60e9-7b3b-4533-840e-08f9ac426316</p>
<p>免责声明：该演示及所有出现的文件（如：CV_candidates.zip）均为虚构。我们不是公司，仅寻求开源贡献者而非候选人。</p>
<blockquote>
<p>🛠⚠️️ <strong>项目正在积极开发中</strong></p>
</blockquote>
<blockquote>
<p>🙏 本项目起初只是一个副业项目，没有明确的路线图和资金支持。它超出了我的预期并登上了 GitHub Trending。非常感谢您的贡献、反馈与耐心。</p>
</blockquote>
<h2>前置条件</h2>
<p>在开始之前，请确保已安装以下软件：</p>
<ul>
<li><strong>Git：</strong> 用于克隆仓库。<a href="https://git-scm.com/downloads">下载 Git</a></li>
<li><strong>Python 3.10.x：</strong> 强烈建议使用 Python 3.10.x 版本。其他版本可能会导致依赖错误。<a href="https://www.python.org/downloads/release/python-3100/">下载 Python 3.10</a>（请选择 3.10.x 版本）。</li>
<li><strong>Docker Engine &amp; Docker Compose：</strong> 用于运行如 SearxNG 等捆绑服务。
<ul>
<li>安装 Docker Desktop（包含 Docker Compose V2）：<a href="https://docs.docker.com/desktop/install/windows-install/">Windows</a> | <a href="https://docs.docker.com/desktop/install/mac-install/">Mac</a> | <a href="https://docs.docker.com/desktop/install/linux-install/">Linux</a></li>
<li>或者在 Linux 上分别安装 Docker Engine 和 Docker Compose：<a href="https://docs.docker.com/engine/install/">Docker Engine</a> | <a href="https://docs.docker.com/compose/install/">Docker Compose</a>（请确保安装 Compose V2，例如 <code>sudo apt-get install docker-compose-plugin</code>）。</li>
</ul>
</li>
</ul>
<h3>1. <strong>克隆仓库并进行设置</strong></h3>
<pre><code class="language-sh">git clone https://github.com/Fosowl/agenticSeek.git
cd agenticSeek
mv .env.example .env
</code></pre>
<h3>2. 修改 .env 文件内容</h3>
<pre><code class="language-sh">SEARXNG_BASE_URL=&quot;http://127.0.0.1:8080&quot;
REDIS_BASE_URL=&quot;redis://redis:6379/0&quot;
WORK_DIR=&quot;/Users/mlg/Documents/workspace_for_ai&quot;
OLLAMA_PORT=&quot;11434&quot;
LM_STUDIO_PORT=&quot;1234&quot;
CUSTOM_ADDITIONAL_LLM_PORT=&quot;11435&quot;
OPENAI_API_KEY='optional'
DEEPSEEK_API_KEY='optional'
OPENROUTER_API_KEY='optional'
TOGETHER_API_KEY='optional'
GOOGLE_API_KEY='optional'
ANTHROPIC_API_KEY='optional'
</code></pre>
<p>根据需要用您自己的值更新 <code>.env</code> 文件：</p>
<ul>
<li><strong>SEARXNG_BASE_URL</strong>：保持不变</li>
<li><strong>REDIS_BASE_URL</strong>：保持不变</li>
<li><strong>WORK_DIR</strong>：本地机器上的工作目录路径。AgenticSeek 可以读取和操作这些文件。</li>
<li><strong>OLLAMA_PORT</strong>：Ollama 服务端口号。</li>
<li><strong>LM_STUDIO_PORT</strong>：LM Studio 服务端口号。</li>
<li><strong>CUSTOM_ADDITIONAL_LLM_PORT</strong>：任何自定义 LLM 服务的端口号。</li>
</ul>
<p><strong>API Key 完全可选，针对选择本地运行 LLM 的用户。该项目的主要目标即为此。如果硬件足够请留空。</strong></p>
<h3>3. <strong>启动 Docker</strong></h3>
<p>确保您的系统已安装并运行 Docker。可用以下命令启动 Docker：</p>
<ul>
<li><p><strong>在 Linux/macOS 上：</strong>
打开终端并运行：</p>
<pre><code class="language-sh">sudo systemctl start docker
</code></pre>
<p>或者在应用程序菜单中启动 Docker Desktop（如已安装）。</p>
</li>
<li><p><strong>在 Windows 上：</strong>
从开始菜单启动 Docker Desktop。</p>
</li>
</ul>
<p>可通过以下命令验证 Docker 是否运行：</p>
<pre><code class="language-sh">docker info
</code></pre>
<p>如果显示有关 Docker 安装的信息，则说明运行正常。</p>
<p>本地提供商的汇总见下表 <a href="#list-of-local-providers">Local Providers</a>。</p>
<p>下一步：<a href="#start-services-and-run">本地运行 AgenticSeek</a></p>
<p><em>如遇问题，请参阅 <a href="#troubleshooting">故障排查</a> 部分。</em>
<em>如您的硬件无法本地运行 LLM，请参考 <a href="#setup-to-run-with-an-api">API 运行设置</a>。</em>
<em>如需详细 <code>config.ini</code> 配置说明，请参见 <a href="#config">Config Section</a>。</em></p>
<hr />
<h2>本地运行 LLM 设置</h2>
<p><strong>硬件要求：</strong></p>
<p>如需本地运行 LLM，需具备足够的硬件资源。最低需有可运行 Magistral、Qwen 或 Deepseek 14B 的 GPU。详细的模型/性能建议请参见 FAQ。</p>
<p><strong>设置本地提供商</strong></p>
<p>启动本地提供商，例如使用 ollama：</p>
<pre><code class="language-sh">ollama serve
</code></pre>
<p>下方列出本地支持的提供商。</p>
<p><strong>更新 config.ini</strong></p>
<p>修改 config.ini，将 provider_name 设置为受支持的提供商，将 provider_model 设置为该提供商支持的 LLM。推荐推理模型如 <em>Magistral</em> 或 <em>Deepseek</em>。</p>
<p>所需硬件请参考 README 最后的 <strong>FAQ</strong>。</p>
<pre><code class="language-sh">[MAIN]
is_local = True # 是否本地运行，或使用远程提供商
provider_name = ollama # 或 lm-studio、openai 等
provider_model = deepseek-r1:14b # 选择适合硬件的模型
provider_server_address = 127.0.0.1:11434
agent_name = Jarvis # AI 名称
recover_last_session = True # 是否恢复上次会话
save_session = True # 是否记住当前会话
speak = False # 文本转语音
listen = False # 语音转文本，仅 CLI，实验性
jarvis_personality = False # 是否使用更“Jarvis”风格的个性（实验性）
languages = en zh # 支持的语言列表，TTS 默认使用第一个
[BROWSER]
headless_browser = True # 除非在主机 CLI 使用，否则保持不变
stealth_mode = True # 使用 undetected selenium 降低被浏览器检测的概率
</code></pre>
<p><strong>注意事项</strong>：</p>
<ul>
<li><p><code>config.ini</code> 文件格式不支持注释。
请勿直接复制示例配置中的注释，否则会报错。请手动根据需求修改 <code>config.ini</code>，并去除所有注释。</p>
</li>
<li><p>如使用 LM-studio 本地运行 LLM，不要将 provider_name 设置为 <code>openai</code>，请设为 <code>lm-studio</code>。</p>
</li>
<li><p>某些提供商（如 lm-studio）需要在 IP 前加 <code>http://</code>，如 <code>http://127.0.0.1:1234</code></p>
</li>
</ul>
<p><strong>本地提供商列表</strong></p>
<p>| 提供商    | 本地？ | 描述                                                    |
|-----------|--------|---------------------------------------------------------|
| ollama    | 是     | 通过 ollama 作为 LLM 提供商，轻松本地运行 LLM           |
| lm-studio | 是     | 使用 LM studio 本地运行 LLM（<code>provider_name</code> 设为 <code>lm-studio</code>）|
| openai    | 是     | 使用 openai 兼容 API（如 llama.cpp server）             |</p>
<p>下一步：<a href="#Start-services-and-Run">启动服务并运行 AgenticSeek</a></p>
<p><em>如遇问题，请参阅 <a href="#troubleshooting">故障排查</a> 部分。</em>
<em>如您的硬件无法本地运行 LLM，请参考 <a href="#setup-to-run-with-an-api">API 运行设置</a>。</em>
<em>如需详细 <code>config.ini</code> 配置说明，请参见 <a href="#config">Config Section</a>。</em></p>
<h2>使用 API 运行设置</h2>
<p>此方式使用外部云端 LLM 提供商。您需要从所选服务获取 API key。</p>
<p><strong>1. 选择 API 提供商并获取 API Key：</strong></p>
<p>参见下方 <a href="#list-of-api-providers">API 提供商列表</a>。访问其官网注册并获得 API key。</p>
<p><strong>2. 将 API Key 设为环境变量：</strong></p>
<ul>
<li><strong>Linux/macOS:</strong>
打开终端，使用 <code>export</code> 命令。建议添加到 shell 配置文件（如 <code>~/.bashrc</code>、<code>~/.zshrc</code>）以便持久生效。
<pre><code class="language-sh">export PROVIDER_API_KEY=&quot;your_api_key_here&quot;
# 将 PROVIDER_API_KEY 替换为具体变量名，如 OPENAI_API_KEY、GOOGLE_API_KEY
</code></pre>
TogetherAI 示例：
<pre><code class="language-sh">export TOGETHER_API_KEY=&quot;xxxxxxxxxxxxxxxxxxxxxx&quot;
</code></pre>
</li>
<li><strong>Windows:</strong></li>
<li><strong>命令提示符（当前会话临时生效）：</strong>
<pre><code class="language-cmd">set PROVIDER_API_KEY=your_api_key_here
</code></pre>
</li>
<li><strong>PowerShell（当前会话临时生效）：</strong>
<pre><code class="language-powershell">$env:PROVIDER_API_KEY=&quot;your_api_key_here&quot;
</code></pre>
</li>
<li><strong>永久生效：</strong> 在 Windows 搜索栏中搜索“环境变量”，点击“编辑系统环境变量”，然后点击“环境变量...”按钮。添加一个新的用户变量，名称为相应的变量名（如 <code>OPENAI_API_KEY</code>），值为你的密钥。</li>
</ul>
<p><em>（详见常见问题：<a href="#how-do-i-set-api-keys">如何设置 API 密钥？</a>）</em></p>
<p><strong>3. 更新 <code>config.ini</code>：</strong></p>
<pre><code class="language-ini">[MAIN]
is_local = False
provider_name = openai # 或 google, deepseek, togetherAI, huggingface
provider_model = gpt-3.5-turbo # 或 gemini-1.5-flash, deepseek-chat, mistralai/Mixtral-8x7B-Instruct-v0.1 等
provider_server_address = # 当 is_local = False 时通常忽略或可留空，大多数 API 均如此
# ... 其他设置 ...
</code></pre>
<p><em>警告：</em> 请确保 <code>config.ini</code> 的值中没有尾随空格。</p>
<p><strong>API 提供商列表</strong></p>
<p>| 提供商        | <code>provider_name</code> | 本地？ | 描述                                             | API 密钥链接（示例）                           |
|---------------|-----------------|--------|--------------------------------------------------|-----------------------------------------------|
| OpenAI        | <code>openai</code>        | 否     | 通过 OpenAI 的 API 使用 ChatGPT 模型。            | <a href="https://platform.openai.com/signup">platform.openai.com/signup</a> |
| Google Gemini | <code>google</code>        | 否     | 通过 Google AI Studio 使用 Google Gemini 模型。   | <a href="https://aistudio.google.com/keys">aistudio.google.com/keys</a> |
| Deepseek      | <code>deepseek</code>      | 否     | 通过 Deepseek 的 API 使用 Deepseek 模型。         | <a href="https://platform.deepseek.com">platform.deepseek.com</a> |
| Hugging Face  | <code>huggingface</code>   | 否     | 通过 Hugging Face Inference API 使用模型。        | <a href="https://huggingface.co/settings/tokens">huggingface.co/settings/tokens</a> |
| TogetherAI    | <code>togetherAI</code>    | 否     | 通过 TogetherAI API 使用多种开源模型。            | <a href="https://api.together.ai/settings/api-keys">api.together.ai/settings/api-keys</a> |</p>
<p><em>注意：</em></p>
<ul>
<li>我们不建议在复杂网页浏览和任务规划场景下使用 <code>gpt-4o</code> 或其他 OpenAI 模型，因为当前的提示词优化主要面向 Deepseek 等模型。</li>
<li>在编码/bash 任务中，Gemini 可能无法严格遵循为 Deepseek 优化的格式化提示，可能会出现问题。</li>
<li>当 <code>is_local = False</code> 时，<code>config.ini</code> 中的 <code>provider_server_address</code> 一般不会被使用，因为各提供商库中通常已硬编码了 API 端点。</li>
</ul>
<p>下一步：<a href="#Start-services-and-Run">启动服务并运行 AgenticSeek</a></p>
<p><em>如遇问题请参见 <strong>已知问题</strong> 部分</em></p>
<p><em>详细配置文件说明请参见 <strong>Config</strong> 部分。</em></p>
<hr />
<h2>启动服务并运行</h2>
<p>默认情况下，AgenticSeek 完全在 Docker 中运行。</p>
<p>启动所需服务。这将启动 docker-compose.yml 中的所有服务，包括：</p>
<ul>
<li>searxng</li>
<li>redis（searxng 需要）</li>
<li>前端</li>
<li>后端（如使用 <code>full</code>）</li>
</ul>
<pre><code class="language-sh">./start_services.sh full # MacOS
start ./start_services.cmd full # Window
</code></pre>
<p><strong>警告：</strong> 此步骤将下载并加载所有 Docker 镜像，可能需要长达 30 分钟。启动服务后，请等待后端服务完全运行（你应该在日志中看到 <strong>backend: &quot;GET /health HTTP/1.1&quot; 200 OK</strong>）再发送任何消息。首次运行时后端服务可能需要 5 分钟启动。</p>
<p>访问 <code>http://localhost:3000/</code>，你应该可以看到 Web 界面。</p>
<p><em>服务启动故障排查：</em> 如脚本启动失败，请确保 Docker Engine 已运行且 Docker Compose（V2，<code>docker compose</code>）已正确安装。检查终端输出的错误信息。参见 <a href="#faq-troubleshooting">常见问题：运行 AgenticSeek 或其脚本时报错怎么办？</a></p>
<p><strong>可选：</strong> 在主机上运行（CLI 模式）：</p>
<p>如需用 CLI 界面运行，需要在主机上安装依赖包：</p>
<pre><code class="language-sh">./install.sh
./install.bat # windows
</code></pre>
<p>启动服务：</p>
<pre><code class="language-sh">./start_services.sh # MacOS
start ./start_services.cmd # Window
</code></pre>
<p>使用 CLI：<code>python3 cli.py</code></p>
<hr />
<h2>使用方法</h2>
<p>请确保服务已通过 <code>./start_services.sh full</code> 启动，并访问 <code>localhost:3000</code> 打开 Web 界面。</p>
<p>你还可以通过在配置中设置 <code>listen = True</code> 启用语音转文字功能，仅限 CLI 模式。</p>
<p>退出时，只需说或输入 <code>goodbye</code>。</p>
<p>以下是一些使用示例：</p>
<blockquote>
<p><em>用 python 制作一个贪吃蛇游戏！</em></p>
</blockquote>
<blockquote>
<p><em>在网上搜索法国雷恩（Rennes）排名前列的咖啡馆，并保存其中三家的名称和地址到 rennes_cafes.txt 文件。</em></p>
</blockquote>
<blockquote>
<p><em>写一个 Go 程序计算一个数的阶乘，将其保存为 workspace 目录下的 factorial.go</em></p>
</blockquote>
<blockquote>
<p><em>在 summer_pictures 文件夹中查找所有 JPG 文件，用今天的日期重命名，并将重命名后的文件列表保存到 photos_list.txt</em></p>
</blockquote>
<blockquote>
<p><em>在线搜索 2024 年流行的科幻电影，挑选三部今晚观看。将名单保存到 movie_night.txt。</em></p>
</blockquote>
<blockquote>
<p><em>在网上搜索 2025 年最新 AI 新闻文章，选择三篇，用 Python 脚本抓取它们的标题和摘要。将脚本保存为 news_scraper.py，摘要保存到 /home/projects 下的 ai_news.txt</em></p>
</blockquote>
<blockquote>
<p><em>周五，在网上搜索免费股票价格 API，用 supersuper7434567@gmail.com 注册账号，然后写一个 Python 脚本，利用该 API 获取特斯拉每天的股价，并将结果保存为 stock_prices.csv</em></p>
</blockquote>
<p><em>请注意，自动表单填写功能仍处于实验阶段，可能会失败。</em></p>
<p>当你输入查询后，AgenticSeek 会为任务分配最佳代理。</p>
<p>由于目前为早期原型，代理路由系统可能不会始终根据你的查询分配最合适的代理。</p>
<p>因此，你应尽量明确表达需求，并说明 AI 应如何操作。例如，如果希望它进行网页搜索，不要说：</p>
<p><code>你知道哪些适合单人旅行的好国家吗？</code></p>
<p>而应这样问：</p>
<p><code>请进行网页搜索，找出最适合单人旅行的国家</code></p>
<hr />
<h2><strong>配置在你自己的服务器上运行 LLM</strong></h2>
<p>如果你有一台高性能电脑或服务器，希望从笔记本远程使用它，可以选择用我们的自定义 llm server 在远程服务器上运行 LLM。</p>
<p>在将运行 AI 模型的“服务器”上，获取 IP 地址</p>
<pre><code class="language-sh">ip a | grep &quot;inet &quot; | grep -v 127.0.0.1 | awk '{print $2}' | cut -d/ -f1 # 本地 IP
curl https://ipinfo.io/ip # 公网 IP
</code></pre>
<p>注意：Windows 或 macOS 请分别用 ipconfig 或 ifconfig 查询 IP 地址。</p>
<p>克隆仓库并进入 <code>server/</code> 文件夹。</p>
<pre><code class="language-sh">git clone --depth 1 https://github.com/Fosowl/agenticSeek.git
cd agenticSeek/llm_server/
</code></pre>
<p>安装服务器端依赖：</p>
<pre><code class="language-sh">pip3 install -r requirements.txt
</code></pre>
<p>运行服务器脚本。</p>
<pre><code class="language-sh">python3 app.py --provider ollama --port 3333
</code></pre>
<p>你可以选择用 <code>ollama</code> 或 <code>llamacpp</code> 作为 LLM 服务。</p>
<p>现在在你的个人电脑上：</p>
<p>修改 <code>config.ini</code>，将 <code>provider_name</code> 设为 <code>server</code>，<code>provider_model</code> 设为 <code>deepseek-r1:xxb</code>。
把 <code>provider_server_address</code> 设置为运行模型机器的 IP 地址。</p>
<pre><code class="language-sh">[MAIN]
is_local = False
provider_name = server
provider_model = deepseek-r1:70b
provider_server_address = x.x.x.x:3333
</code></pre>
<p>下一步：<a href="#Start-services-and-Run">启动服务并运行 AgenticSeek</a></p>
<hr />
<h2>语音转文字</h2>
<p>警告：语音转文字目前仅支持 CLI 模式。</p>
<p>请注意，语音转文字目前仅支持英文。</p>
<p>语音转文字功能默认关闭。要启用，请在 config.ini 文件中设置 listen 选项为 True：</p>
<pre><code>listen = True
</code></pre>
<p>启用后，语音转文字功能会在你说出触发关键词（即代理名称）后开始处理输入。你可以通过修改 <em>config.ini</em> 文件中的 <code>agent_name</code> 值来自定义代理名称：</p>
<pre><code>agent_name = Friday
</code></pre>
<p>为了获得最佳识别效果，我们建议使用常见的英文名字作为代理名称，如 &quot;John&quot; 或 &quot;Emma&quot;。</p>
<p>当你看到转录文本开始出现时，大声说出代理的名字以唤醒它（例如，“Friday”）。</p>
<p>请清晰地表达你的请求。</p>
<p>在请求结束时，用确认短语作为信号提示系统继续。确认短语示例包括：</p>
<pre><code>&quot;do it&quot;, &quot;go ahead&quot;, &quot;execute&quot;, &quot;run&quot;, &quot;start&quot;, &quot;thanks&quot;, &quot;would ya&quot;, &quot;please&quot;, &quot;okay?&quot;, &quot;proceed&quot;, &quot;continue&quot;, &quot;go on&quot;, &quot;do that&quot;, &quot;go it&quot;, &quot;do you understand?&quot;
</code></pre>
<h2>配置</h2>
<p>配置示例:</p>
<pre><code>[MAIN]
is_local = True
provider_name = ollama
provider_model = deepseek-r1:32b
provider_server_address = http://127.0.0.1:11434 # Ollama 示例；LM-Studio 请用 http://127.0.0.1:1234
agent_name = Friday
recover_last_session = False
save_session = False
speak = False
listen = False

jarvis_personality = False
languages = en zh # 用于TTS及可能的路由的语言列表。
[BROWSER]
headless_browser = False
stealth_mode = False
</code></pre>
<p><strong><code>config.ini</code> 配置项说明</strong>:</p>
<ul>
<li><strong><code>[MAIN]</code> 部分:</strong>
<ul>
<li><code>is_local</code>: 若使用本地LLM服务（Ollama、LM-Studio、本地OpenAI兼容服务器）或自托管服务器选项，设为 <code>True</code>。若使用云端API（OpenAI、Google等），设为 <code>False</code>。</li>
<li><code>provider_name</code>: 指定LLM服务提供商。
<ul>
<li>本地选项：<code>ollama</code>、<code>lm-studio</code>、<code>openai</code>（本地OpenAI兼容服务器）、<code>server</code>（自托管服务器）。</li>
<li>API选项：<code>openai</code>、<code>google</code>、<code>deepseek</code>、<code>huggingface</code>、<code>togetherAI</code>。</li>
</ul>
</li>
<li><code>provider_model</code>: 选定服务商的具体模型名称或ID（如 Ollama 的 <code>deepseekcoder:6.7b</code>，OpenAI API 的 <code>gpt-3.5-turbo</code>，TogetherAI 的 <code>mistralai/Mixtral-8x7B-Instruct-v0.1</code>）。</li>
<li><code>provider_server_address</code>: 你的LLM服务提供商地址。
<ul>
<li>本地服务示例：Ollama 用 <code>http://127.0.0.1:11434</code>，LM-Studio 用 <code>http://127.0.0.1:1234</code>。</li>
<li><code>server</code> 类型：自托管LLM服务器地址（如 <code>http://your_server_ip:3333</code>）。</li>
<li>云端API（<code>is_local = False</code>）：通常忽略或留空，因为API端点通常由客户端库处理。</li>
</ul>
</li>
<li><code>agent_name</code>: AI助手名称（如 Friday）。在启用语音转文本时用作唤醒词。</li>
<li><code>recover_last_session</code>: 设为 <code>True</code> 时尝试恢复上次会话状态，设为 <code>False</code> 时重新开始。</li>
<li><code>save_session</code>: 设为 <code>True</code> 保存当前会话以便恢复，否则为 <code>False</code>。</li>
<li><code>speak</code>: 设为 <code>True</code> 启用文字转语音输出，设为 <code>False</code> 关闭。</li>
<li><code>listen</code>: 设为 <code>True</code> 启用语音转文本输入（仅CLI模式），设为 <code>False</code> 关闭。</li>
<li><code>work_dir</code>: <strong>重要：</strong> AgenticSeek 读写文件的目录。<strong>请确保该路径在你的系统中有效且可访问。</strong></li>
<li><code>jarvis_personality</code>: 设为 <code>True</code> 使用更“Jarvis风格”的系统提示（实验性），设为 <code>False</code> 使用标准提示。</li>
<li><code>languages</code>: 逗号分隔的语言列表（如 <code>en, zh, fr</code>）。用于TTS语音选择（默认第一项），也可辅助LLM路由。为提高效率，请勿设置过多或过于相似的语言。</li>
</ul>
</li>
<li><strong><code>[BROWSER]</code> 部分:</strong>
<ul>
<li><code>headless_browser</code>: 设为 <code>True</code> 以无界面方式运行自动化浏览器（推荐用于网页界面或非交互场景）；设为 <code>False</code> 显示浏览器窗口（适合CLI模式或调试）。</li>
<li><code>stealth_mode</code>: 设为 <code>True</code> 启用反检测措施，使浏览器自动化更难被检测。可能需要手动安装诸如 anticaptcha 等浏览器扩展。</li>
</ul>
</li>
</ul>
<p>本节总结了支持的LLM服务类型。请在 <code>config.ini</code> 中进行配置。</p>
<p><strong>本地服务（在你自己的硬件上运行）:</strong></p>
<p>| <code>config.ini</code> 配置名称          | <code>is_local</code> | 描述                                                                         | 配置章节                                                        |
|-------------------------------|------------|------------------------------------------------------------------------------|------------------------------------------------------------------|
| <code>ollama</code>                      | <code>True</code>     | 使用 Ollama 提供本地LLM服务。                                                | <a href="#setup-for-running-llm-locally-on-your-machine">本地LLM运行配置</a> |
| <code>lm-studio</code>                   | <code>True</code>     | 使用 LM-Studio 提供本地LLM服务。                                             | <a href="#setup-for-running-llm-locally-on-your-machine">本地LLM运行配置</a> |
| <code>openai</code> (本地服务器)         | <code>True</code>     | 连接本地OpenAI兼容API服务器（如 llama.cpp）。                                 | <a href="#setup-for-running-llm-locally-on-your-machine">本地LLM运行配置</a> |
| <code>server</code>                      | <code>False</code>    | 连接运行在其他机器上的 AgenticSeek 自托管LLM服务器。                         | <a href="#setup-to-run-the-llm-on-your-own-server">自托管LLM服务器配置</a>   |</p>
<p><strong>API服务（云端）:</strong></p>
<p>| <code>config.ini</code> 配置名称          | <code>is_local</code> | 描述                                       | 配置章节                                        |
|-------------------------------|------------|---------------------------------------------|-------------------------------------------------|
| <code>openai</code>                      | <code>False</code>    | 使用OpenAI官方API（如 GPT-3.5、GPT-4）。    | <a href="#setup-to-run-with-an-api">API运行配置</a>         |
| <code>google</code>                      | <code>False</code>    | 通过API使用Google Gemini模型。              | <a href="#setup-to-run-with-an-api">API运行配置</a>         |
| <code>deepseek</code>                    | <code>False</code>    | 使用Deepseek官方API。                       | <a href="#setup-to-run-with-an-api">API运行配置</a>         |
| <code>huggingface</code>                 | <code>False</code>    | 使用 Hugging Face 推理API。                 | <a href="#setup-to-run-with-an-api">API运行配置</a>         |
| <code>togetherAI</code>                  | <code>False</code>    | 使用 TogetherAI 的多种开源模型API。         | <a href="#setup-to-run-with-an-api">API运行配置</a>         |</p>
<hr />
<h2>故障排查</h2>
<p>遇到问题时，本节将为你提供指导。</p>
<h1>已知问题</h1>
<h2>ChromeDriver 问题</h2>
<p><strong>错误示例:</strong> <code>SessionNotCreatedException: Message: session not created: This version of ChromeDriver only supports Chrome version XXX</code></p>
<ul>
<li><strong>原因:</strong> 你安装的 ChromeDriver 版本与 Google Chrome 浏览器版本不兼容。</li>
<li><strong>解决办法:</strong>
<ol>
<li><strong>检查 Chrome 版本:</strong> 打开 Google Chrome，进入 <code>设置 &gt; 关于 Chrome</code> 查看版本号（如 &quot;Version 120.0.6099.110&quot;）。</li>
<li><strong>下载匹配的 ChromeDriver:</strong>
<ul>
<li>Chrome 115及以上版本：访问 <a href="https://googlechromelabs.github.io/chrome-for-testing/">Chrome for Testing (CfT) JSON Endpoints</a>。找到 “stable” 渠道，下载与你的Chrome主版本号对应的ChromeDriver。</li>
<li>旧版本（较少见）：可在 <a href="https://chromedriver.chromium.org/downloads">ChromeDriver - WebDriver for Chrome</a> 页面查找。</li>
<li>下图为 CfT 页面示例：
<img src="./media/chromedriver_readme.png" alt="从Chrome for Testing页面下载特定版本Chromedriver" /></li>
</ul>
</li>
<li><strong>安装 ChromeDriver:</strong>
<ul>
<li>确保下载的 <code>chromedriver</code>（Windows 下为 <code>chromedriver.exe</code>）放在系统PATH环境变量指定的目录下（如 Linux/macOS的 <code>/usr/local/bin</code>，或Windows下已加入PATH的脚本文件夹）。</li>
<li>或将其放在 <code>agenticSeek</code> 项目的根目录下。</li>
<li>确保驱动为可执行文件（Linux/macOS下可用 <code>chmod +x chromedriver</code>）。</li>
</ul>
</li>
<li>更多信息请参考主安装指南的 <a href="#chromedriver-installation">ChromeDriver 安装</a> 部分。</li>
</ol>
</li>
</ul>
<p>如果本节内容不完整或遇到其他ChromeDriver问题，请搜索现有 <a href="https://github.com/Fosowl/agenticSeek/issues">GitHub Issues</a> 或新建Issue反馈。</p>
<p><code>Exception: Failed to initialize browser: Message: session not created: This version of ChromeDriver only supports Chrome version 113 Current browser version is 134.0.6998.89 with binary path</code></p>
<p>如浏览器与chromedriver版本不匹配，会出现此类问题。</p>
<p>你需要前往以下链接下载最新版：</p>
<p>https://developer.chrome.com/docs/chromedriver/downloads</p>
<p>如使用的是 Chrome 115及以上版本，请访问：</p>
<p>https://googlechromelabs.github.io/chrome-for-testing/</p>
<p>并下载与你操作系统匹配的chromedriver版本。</p>
<p><img src="./media/chromedriver_readme.png" alt="alt text" /></p>
<p>如本节内容不完整请提交Issue。</p>
<h2>连接适配器问题</h2>
<pre><code>Exception: Provider lm-studio failed: HTTP request failed: No connection adapters were found for '127.0.0.1:1234/v1/chat/completions'` (注意：端口可能不同)
</code></pre>
<ul>
<li><strong>原因:</strong> <code>config.ini</code> 中 <code>lm-studio</code>（或其他本地OpenAI兼容服务器）的 <code>provider_server_address</code> 缺少 <code>http://</code> 前缀或指向了错误的端口。</li>
<li><strong>解决办法:</strong>
<ul>
<li>确保地址包含 <code>http://</code>。LM-Studio 通常默认为 <code>http://127.0.0.1:1234</code>。</li>
<li>正确的 <code>config.ini</code> 配置：<code>provider_server_address = http://127.0.0.1:1234</code>（或你实际的LM-Studio服务器端口）。</li>
</ul>
</li>
</ul>
<h2>未提供 SearxNG 基础URL</h2>
<pre><code>raise ValueError(&quot;SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.&quot;)
ValueError: SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.`
</code></pre>
<h2>常见问题</h2>
<p><strong>Q: 我需要什么硬件？</strong></p>
<p>| 模型规模  | GPU  | 说明                                                |
|-----------|--------|------------------------------------------------------|
| 7B        | 8GB 显存 | ⚠️ 不推荐。性能差，易产生幻觉，规划型Agent可能失败。 |
| 14B       | 12GB 显存（如RTX 3060） | ✅ 可用于简单任务。网页浏览和计划任务可能有困难。|
| 32B       | 24GB+ 显存（如RTX 4090） | 🚀 大部分任务表现良好，但复杂规划仍有挑战。    |
| 70B+      | 48GB+ 显存 | 💪 优秀，推荐用于高阶场景。                       |</p>
<p><strong>Q: 出现错误怎么办？</strong></p>
<p>确保本地服务已启动（如执行 <code>ollama serve</code>），<code>config.ini</code> 配置与你的服务商一致，且依赖项已安装。如仍有问题欢迎提交Issue反馈。</p>
<p><strong>Q: 真能100%本地运行吗？</strong></p>
<p>是的。配合 Ollama、lm-studio 或 server 服务，所有语音转文本、LLM与TTS模型都可本地运行。非本地选项（如OpenAI等API）为可选。</p>
<p><strong>Q: 我有 Manus，为什么还要用 AgenticSeek？</strong></p>
<p>AgenticSeek 以不依赖外部系统为设计重点，给予你更多控制权、隐私和避免API费用。</p>
<p><strong>Q: 项目背后是谁？</strong></p>
<p>本项目由我和两位朋友共同发起与维护，均来自GitHub开源社区。我们只是热爱技术的个人，并非创业公司或任何组织成员。</p>
<p>除我的个人账号（https://x.com/Martin993886460）外，任何自称AgenticSeek的X账号都是冒充。</p>
<h2>参与贡献</h2>
<p>欢迎开发者参与改进AgenticSeek！请查看开放Issue或讨论区。</p>
<p><a href="./docs/CONTRIBUTING.md">贡献指南</a></p>
<p><a href="https://www.star-history.com/#Fosowl/agenticSeek&amp;Date"><img src="https://api.star-history.com/svg?repos=Fosowl/agenticSeek&amp;type=Date" alt="Star History Chart" /></a></p>
<h2>维护者:</h2>
<blockquote>
<p><a href="https://github.com/Fosowl">Fosowl</a> | 巴黎时间</p>
</blockquote>
<blockquote>
<p><a href="https://github.com/antoineVIVIES">antoineVIVIES</a> | 台北时间</p>
</blockquote>
<blockquote>
<p><a href="https://github.com/steveh8758">steveh8758</a> | 台北时间</p>
</blockquote>
<h2>特别感谢:</h2>
<blockquote>
<p><a href="https://github.com/tcsenpai">tcsenpai</a> 和 <a href="https://github.com/plitc">plitc</a> 协助后端docker化</p>
</blockquote>
<h2>赞助者:</h2>
<p>每月赞助5美元及以上将在此处展示：</p>
<ul>
<li><strong>tatra-labs</strong></li>
</ul>
<p>Certainly! Please provide the content of Part 4 of 4 for translation.</p>
<hr />
<p>Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx">Open Ai Tx</a> | Last indexed: 2025-06-16</p>
<hr />

        </div>
    </div>
    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Statcounter and other scripts can be added here -->
</body>
</html>