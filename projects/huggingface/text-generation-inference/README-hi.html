<!DOCTYPE html>
<html lang="hi">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>text-generation-inference - huggingface/text-generation-inference hi</title>
    <meta name="title" content="text-generation-inference - huggingface/text-generation-inference hi | टेक्स्ट जनरेशन इन्फेरेंस टेक्स्ट जनरेशन इन्फेरेंस के लिए एक रस्ट, पायथन और gRPC सर्वर। Hugging Face में उत्पादन में उपयोग किया जाता है Hugging Chat, Inference A...">
    <meta name="description" content="huggingface/text-generation-inference - GitHub repository hi documentation and information | टेक्स्ट जनरेशन इन्फेरेंस टेक्स्ट जनरेशन इन्फेरेंस के लिए एक रस्ट, पायथन और gRPC सर्वर। Hugging Face में उत्पादन में उपयोग किया जाता है Hugging Chat, Inference A...">
    <meta name="keywords" content="huggingface, text-generation-inference, GitHub, repository, hi documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/huggingface/text-generation-inference/README-hi.html">
    <meta property="og:title" content="text-generation-inference - huggingface/text-generation-inference hi | टेक्स्ट जनरेशन इन्फेरेंस टेक्स्ट जनरेशन इन्फेरेंस के लिए एक रस्ट, पायथन और gRPC सर्वर। Hugging Face में उत्पादन में उपयोग किया जाता है Hugging Chat, Inference A...">
    <meta property="og:description" content="huggingface/text-generation-inference - GitHub repository hi documentation and information | टेक्स्ट जनरेशन इन्फेरेंस टेक्स्ट जनरेशन इन्फेरेंस के लिए एक रस्ट, पायथन और gRPC सर्वर। Hugging Face में उत्पादन में उपयोग किया जाता है Hugging Chat, Inference A...">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">
    <script src="/js/marked.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        body { display: flex; flex-direction: column; min-height: 100vh; }
        .main-container { margin: 0 auto; width: 100%; max-width: 980px; padding: 0 20px; }
        @media (max-width: 768px) { .main-container { padding: 0 15px; } }
        .markdown-body img { max-width: 100%; height: auto; }
        .header { text-align: center; margin-bottom: 30px; padding: 20px; background-color: #f6f8fa; border-bottom: 1px solid #e1e4e8; position: relative; }
        .header .links { margin-top: 10px; font-size: 16px; }
        .header .links a { color: #0366d6; text-decoration: none; margin-left: 5px; }
        .header .links a:hover { text-decoration: underline; }
        .language-badges { margin-top: 15px; text-align: center; }
        .language-badges a { display: inline-block; margin: 2px; text-decoration: none; }
        .language-badges img { height: 20px; border-radius: 3px; }
        .language-badges a:hover img { opacity: 0.8; }
    </style>
</head>
<body>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/huggingface/text-generation-inference" id="githubRepoLink" target="_blank">huggingface/text-generation-inference</a>
<h1 style="display: none;">टेक्स्ट जनरेशन इन्फेरेंस टेक्स्ट जनरेशन इन्फेरेंस के लिए एक रस्ट, पायथन और gRPC सर्वर। Hugging Face में उत्पादन में उपयोग किया जाता है Hugging Chat, Inference A...</h1>
        </div>
        <div class="language-badges" id="languageBadges">
            <!-- You can generate language badges here if needed -->
        </div>
    </div>
    <div class="main-container">
        <div class="markdown-body" id="content">
            <div align="center">
<a href="https://www.youtube.com/watch?v=jlMAX2Oaht0">
  <img width=560 alt="Making TGI deployment optimal" src="https://raw.githubusercontent.com/huggingface/text-generation-inference/main/assets/thumbnail.png">
</a>
<h1>टेक्स्ट जनरेशन इन्फेरेंस</h1>
<a href="https://github.com/huggingface/text-generation-inference">
  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/huggingface/text-generation-inference?style=social">
</a>
<a href="https://huggingface.github.io/text-generation-inference">
  <img alt="Swagger API documentation" src="https://img.shields.io/badge/API-Swagger-informational">
</a>
<p>टेक्स्ट जनरेशन इन्फेरेंस के लिए एक रस्ट, पायथन और gRPC सर्वर। <a href="https://huggingface.co">Hugging Face</a> में उत्पादन में उपयोग किया जाता है
Hugging Chat, Inference API और Inference Endpoints को पावर देने के लिए।</p>
</div>
<h2>सामग्री तालिका</h2>
<ul>
<li><a href="#get-started">शुरू करें</a>
<ul>
<li><a href="#docker">डॉकर</a></li>
<li><a href="#api-documentation">API दस्तावेज़</a></li>
<li><a href="#using-a-private-or-gated-model">प्राइवेट या गेटेड मॉडल का उपयोग</a></li>
<li><a href="#a-note-on-shared-memory-shm">शेयरड मेमोरी (shm) पर एक नोट</a></li>
<li><a href="#distributed-tracing">डिस्ट्रिब्यूटेड ट्रेसिंग</a></li>
<li><a href="#architecture">आर्किटेक्चर</a></li>
<li><a href="#local-install">लोकल इंस्टॉल</a></li>
<li><a href="#local-install-nix">लोकल इंस्टॉल (Nix)</a></li>
</ul>
</li>
<li><a href="#optimized-architectures">ऑप्टिमाइज़्ड आर्किटेक्चर</a></li>
<li><a href="#run-locally">लोकल रूप से चलाएं</a>
<ul>
<li><a href="#run">चलाएं</a></li>
<li><a href="#quantization">क्वांटाइजेशन</a></li>
</ul>
</li>
<li><a href="#develop">डेवलप करें</a></li>
<li><a href="#testing">टेस्टिंग</a></li>
</ul>
<p>टेक्स्ट जनरेशन इन्फेरेंस (TGI) बड़े भाषा मॉडल (LLMs) को डिप्लॉय और सर्व करने के लिए एक टूलकिट है। TGI लोकप्रिय ओपन-सोर्स LLMs के लिए उच्च प्रदर्शन वाले टेक्स्ट जनरेशन को सक्षम करता है, जिसमें Llama, Falcon, StarCoder, BLOOM, GPT-NeoX, और <a href="https://huggingface.co/docs/text-generation-inference/supported_models">और भी</a> शामिल हैं। TGI कई फीचर्स को लागू करता है, जैसे:</p>
<ul>
<li>सबसे लोकप्रिय LLMs को सर्व करने के लिए सरल लॉन्चर</li>
<li>प्रोडक्शन रेडी (Open Telemetry के साथ डिस्ट्रिब्यूटेड ट्रेसिंग, Prometheus मेट्रिक्स)</li>
<li>कई GPUs पर तेज़ इन्फेरेंस के लिए टेन्सर पैरेललिज्म</li>
<li>सर्वर-सेंट इवेंट्स (SSE) का उपयोग करते हुए टोकन स्ट्रीमिंग</li>
<li>कुल थ्रूपुट बढ़ाने के लिए आने वाले अनुरोधों का सतत बैचिंग</li>
<li><a href="https://huggingface.co/docs/text-generation-inference/en/messages_api">Messages API</a> जो Open AI Chat Completion API के साथ संगत है</li>
<li>सबसे लोकप्रिय आर्किटेक्चर पर <a href="https://github.com/HazyResearch/flash-attention">Flash Attention</a> और <a href="https://github.com/vllm-project/vllm">Paged Attention</a> का उपयोग करके इन्फेरेंस के लिए ऑप्टिमाइज़्ड ट्रांसफॉर्मर्स कोड</li>
<li>क्वांटाइजेशन :
<ul>
<li><a href="https://github.com/TimDettmers/bitsandbytes">bitsandbytes</a></li>
<li><a href="https://arxiv.org/abs/2210.17323">GPT-Q</a></li>
<li><a href="https://github.com/NetEase-FuXi/EETQ">EETQ</a></li>
<li><a href="https://github.com/casper-hansen/AutoAWQ">AWQ</a></li>
<li><a href="https://github.com/IST-DASLab/marlin">Marlin</a></li>
<li><a href="https://developer.nvidia.com/blog/nvidia-arm-and-intel-publish-fp8-specification-for-standardization-as-an-interchange-format-for-ai/">fp8</a></li>
</ul>
</li>
<li><a href="https://github.com/huggingface/safetensors">Safetensors</a> वेट लोडिंग</li>
<li><a href="https://arxiv.org/abs/2301.10226">A Watermark for Large Language Models</a> के साथ वॉटरमार्किंग</li>
<li>लॉजिट्स वार्पर (टेम्परेचर स्केलिंग, टॉप-p, टॉप-k, रिपीटेशन पेनल्टी, अधिक विवरण देखें <a href="https://huggingface.co/docs/transformers/internal/generation_utils#transformers.LogitsProcessor">transformers.LogitsProcessor</a>)</li>
<li>स्टॉप सीक्वेंस</li>
<li>लॉग प्रॉबेबिलिटीज़</li>
<li><a href="https://huggingface.co/docs/text-generation-inference/conceptual/speculation">स्पेकुलेशन</a> ~2x लैटेंसी</li>
<li><a href="https://huggingface.co/docs/text-generation-inference/conceptual/guidance">Guidance/JSON</a>। आउटपुट फॉर्मेट निर्दिष्ट करें ताकि इन्फेरेंस तेज़ हो और आउटपुट कुछ स्पेक्स के अनुसार वैध हो।</li>
<li>कस्टम प्रॉम्प्ट जनरेशन: मॉडल के आउटपुट को निर्देशित करने के लिए कस्टम प्रॉम्प्ट प्रदान करके आसानी से टेक्स्ट जनरेट करें</li>
<li>फाइन-ट्यूनिंग सपोर्ट: विशिष्ट कार्यों के लिए फाइन-ट्यून किए गए मॉडल का उपयोग करके उच्च सटीकता और प्रदर्शन प्राप्त करें</li>
</ul>
<h3>हार्डवेयर सपोर्ट</h3>
<ul>
<li><a href="https://github.com/huggingface/text-generation-inference/pkgs/container/text-generation-inference">Nvidia</a></li>
<li><a href="https://github.com/huggingface/text-generation-inference/pkgs/container/text-generation-inference">AMD</a> (-rocm)</li>
<li><a href="https://github.com/huggingface/optimum-neuron/tree/main/text-generation-inference">Inferentia</a></li>
<li><a href="https://github.com/huggingface/text-generation-inference/pull/1475">Intel GPU</a></li>
<li><a href="https://github.com/huggingface/tgi-gaudi">Gaudi</a></li>
<li><a href="https://huggingface.co/docs/optimum-tpu/howto/serving">Google TPU</a></li>
</ul>
<h2>शुरू करें</h2>
<h3>डॉकर</h3>
<p>विस्तृत प्रारंभिक मार्गदर्शिका के लिए कृपया <a href="https://huggingface.co/docs/text-generation-inference/quicktour">Quick Tour</a> देखें। शुरू करने का सबसे आसान तरीका आधिकारिक डॉकर कंटेनर का उपयोग करना है:</p>
<pre><code class="language-shell">model=HuggingFaceH4/zephyr-7b-beta
# प्रत्येक रन में वेट्स डाउनलोड करने से बचने के लिए डॉकर कंटेनर के साथ एक वॉल्यूम साझा करें
volume=$PWD/data

docker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data \
    ghcr.io/huggingface/text-generation-inference:3.3.2 --model-id $model
</code></pre>
<p>और फिर आप ऐसे अनुरोध कर सकते हैं</p>
<pre><code class="language-bash">curl 127.0.0.1:8080/generate_stream \
    -X POST \
    -d '{&quot;inputs&quot;:&quot;डीप लर्निंग क्या है?&quot;,&quot;parameters&quot;:{&quot;max_new_tokens&quot;:20}}' \
    -H 'Content-Type: application/json'
</code></pre>
<p>आप Open AI Chat Completion API संगत प्रतिक्रियाएं प्राप्त करने के लिए <a href="https://huggingface.co/docs/text-generation-inference/en/messages_api">TGI के Messages API</a> का भी उपयोग कर सकते हैं।</p>
<pre><code class="language-bash">curl localhost:8080/v1/chat/completions \
    -X POST \
    -d '{
  &quot;model&quot;: &quot;tgi&quot;,
  &quot;messages&quot;: [
    {
      &quot;role&quot;: &quot;system&quot;,
      &quot;content&quot;: &quot;आप एक सहायक सहायक हैं।&quot;
    },
    {
      &quot;role&quot;: &quot;user&quot;,
      &quot;content&quot;: &quot;डीप लर्निंग क्या है?&quot;
    }
  ],
  &quot;stream&quot;: true,
  &quot;max_tokens&quot;: 20
}' \
    -H 'Content-Type: application/json'
</code></pre>
<p><strong>नोट:</strong> NVIDIA GPUs का उपयोग करने के लिए, आपको <a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html">NVIDIA Container Toolkit</a> स्थापित करना होगा। हम CUDA संस्करण 12.2 या उससे उच्च के साथ NVIDIA ड्राइवरों का उपयोग करने की भी सलाह देते हैं। बिना GPU या CUDA समर्थन वाले मशीन पर Docker कंटेनर चलाने के लिए, <code>--gpus all</code> फ्लैग हटाना और <code>--disable-custom-kernels</code> जोड़ना पर्याप्त है, कृपया ध्यान दें कि CPU इस प्रोजेक्ट के लिए इरादे वाला प्लेटफ़ॉर्म नहीं है, इसलिए प्रदर्शन कम हो सकता है।</p>
<p><strong>नोट:</strong> TGI AMD Instinct MI210 और MI250 GPUs का समर्थन करता है। विवरण <a href="https://huggingface.co/docs/text-generation-inference/installation_amd#using-tgi-with-amd-gpus">Supported Hardware documentation</a> में पाया जा सकता है। AMD GPUs का उपयोग करने के लिए, कृपया ऊपर के कमांड के बजाय <code>docker run --device /dev/kfd --device /dev/dri --shm-size 1g -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:3.3.2-rocm --model-id $model</code> उपयोग करें।</p>
<p>अपने मॉडलों को सर्व करने के सभी विकल्प देखने के लिए (कोड में <a href="https://github.com/huggingface/text-generation-inference/blob/main/launcher/src/main.rs">यहाँ</a> या CLI में):</p>
<pre><code>text-generation-launcher --help
</code></pre>
<h3>API दस्तावेज़</h3>
<p>आप <code>/docs</code> मार्ग का उपयोग करके <code>text-generation-inference</code> REST API का OpenAPI दस्तावेज़ देख सकते हैं।
Swagger UI भी उपलब्ध है: <a href="https://huggingface.github.io/text-generation-inference">https://huggingface.github.io/text-generation-inference</a>।</p>
<h3>प्राइवेट या गेटेड मॉडल का उपयोग</h3>
<p><code>text-generation-inference</code> द्वारा उपयोग किए जाने वाले टोकन को कॉन्फ़िगर करने के लिए आपके पास <code>HF_TOKEN</code> पर्यावरण चर का उपयोग करने का विकल्प है। यह आपको संरक्षित संसाधनों तक पहुंच प्रदान करता है।</p>
<p>उदाहरण के लिए, यदि आप गेटेड Llama V2 मॉडल वेरिएंट सर्व करना चाहते हैं:</p>
<ol>
<li>https://huggingface.co/settings/tokens पर जाएँ</li>
<li>अपनी CLI READ टोकन कॉपी करें</li>
<li><code>HF_TOKEN=&lt;आपकी CLI READ टोकन&gt;</code> निर्यात करें</li>
</ol>
<p>या डॉकर के साथ:</p>
<pre><code class="language-shell">model=meta-llama/Meta-Llama-3.1-8B-Instruct
volume=$PWD/data # प्रत्येक रन में वेट्स डाउनलोड करने से बचने के लिए डॉकर कंटेनर के साथ वॉल्यूम साझा करें
token=&lt;आपकी cli READ टोकन&gt;

docker run --gpus all --shm-size 1g -e HF_TOKEN=$token -p 8080:80 -v $volume:/data \
    ghcr.io/huggingface/text-generation-inference:3.3.2 --model-id $model
</code></pre>
<h3>शेयरड मेमोरी (shm) पर एक नोट</h3>
<p><a href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/index.html"><code>NCCL</code></a> एक संचार फ्रेमवर्क है जिसका उपयोग
<code>PyTorch</code> वितरित प्रशिक्षण/इन्फेरेंस के लिए करता है। <code>text-generation-inference</code>
<code>NCCL</code> का उपयोग टेन्सर पैरेललिज्म को सक्षम करने के लिए करता है ताकि बड़े भाषा मॉडलों के लिए इन्फेरेंस को नाटकीय रूप से तेज़ किया जा सके।</p>
<p><code>NCCL</code> समूह के विभिन्न उपकरणों के बीच डेटा साझा करने के लिए, यदि NVLink या PCI का उपयोग करते हुए पीयर-टू-पीयर संभव नहीं है, तो <code>NCCL</code> होस्ट मेमोरी का उपयोग कर सकता है।</p>
<p>कंटेनर को 1G शेयरड मेमोरी का उपयोग करने की अनुमति देने और SHM शेयरिंग का समर्थन करने के लिए, ऊपर के कमांड में <code>--shm-size 1g</code> जोड़ा गया है।</p>
<p>यदि आप <code>text-generation-inference</code> को <code>Kubernetes</code> के अंदर चला रहे हैं, तो आप कंटेनर में शेयरड मेमोरी जोड़ सकते हैं
एक वॉल्यूम बनाकर:</p>
<pre><code class="language-yaml">- name: shm
  emptyDir:
   medium: Memory
   sizeLimit: 1Gi
</code></pre>
<p>और इसे <code>/dev/shm</code> पर माउंट करें।</p>
<p>अंत में, आप <code>NCCL_SHM_DISABLE=1</code> पर्यावरण चर का उपयोग करके SHM शेयरिंग को भी अक्षम कर सकते हैं। हालांकि, ध्यान दें कि इससे प्रदर्शन प्रभावित होगा।</p>
<h3>डिस्ट्रिब्यूटेड ट्रेसिंग</h3>
<p><code>text-generation-inference</code> OpenTelemetry का उपयोग करके डिस्ट्रिब्यूटेड ट्रेसिंग के साथ इंस्ट्रूमेंट किया गया है। आप इस फीचर का उपयोग <code>--otlp-endpoint</code> आर्गुमेंट के साथ OTLP कलेक्टर के पते को सेट करके कर सकते हैं। डिफ़ॉल्ट सेवा नाम को <code>--otlp-service-name</code> आर्गुमेंट के साथ ओवरराइड किया जा सकता है।</p>
<h3>आर्किटेक्चर</h3>
<p><img src="https://raw.githubusercontent.com/huggingface/text-generation-inference/main/docs/images/TGI.png" alt="TGI architecture" /></p>
<p>TGI के अंदरूनी कामकाज पर Adyen द्वारा विस्तृत ब्लॉगपोस्ट: <a href="https://www.adyen.com/knowledge-hub/llm-inference-at-scale-with-tgi">LLM inference at scale with TGI (Martin Iglesias Goyanes - Adyen, 2024)</a></p>
<h3>लोकल इंस्टॉल</h3>
<p>आप <code>text-generation-inference</code> को लोकल रूप से भी इंस्टॉल कर सकते हैं।</p>
<p>सबसे पहले रिपॉजिटरी क्लोन करें और उसमें डिरेक्टरी बदलें:</p>
<pre><code class="language-shell">git clone https://github.com/huggingface/text-generation-inference
cd text-generation-inference
</code></pre>
<p>फिर <a href="https://rustup.rs/">Rust इंस्टॉल करें</a> और कम से कम Python 3.9 के साथ एक Python वर्चुअल वातावरण बनाएं, जैसे <code>conda</code> या <code>python venv</code> का उपयोग करके:</p>
<pre><code class="language-shell">curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh

# conda का उपयोग करते हुए
conda create -n text-generation-inference python=3.11
conda activate text-generation-inference

# python venv का उपयोग करते हुए
python3 -m venv .venv
source .venv/bin/activate
</code></pre>
<p>आपको Protoc भी इंस्टॉल करना पड़ सकता है।</p>
<p>Linux पर:</p>
<pre><code class="language-shell">PROTOC_ZIP=protoc-21.12-linux-x86_64.zip
curl -OL https://github.com/protocolbuffers/protobuf/releases/download/v21.12/$PROTOC_ZIP
sudo unzip -o $PROTOC_ZIP -d /usr/local bin/protoc
sudo unzip -o $PROTOC_ZIP -d /usr/local 'include/*'
rm -f $PROTOC_ZIP
</code></pre>
<p>MacOS पर, Homebrew का उपयोग करते हुए:</p>
<pre><code class="language-shell">brew install protobuf
</code></pre>
<p>फिर चलाएं:</p>
<pre><code class="language-shell">BUILD_EXTENSIONS=True make install # रिपॉजिटरी और HF/transformer फोर्क को CUDA कर्नेल्स के साथ इंस्टॉल करें
text-generation-launcher --model-id mistralai/Mistral-7B-Instruct-v0.2
</code></pre>
<p><strong>नोट:</strong> कुछ मशीनों पर आपको OpenSSL लाइब्रेरी और gcc की आवश्यकता हो सकती है। Linux मशीनों पर चलाएं:</p>
<pre><code class="language-shell">sudo apt-get install libssl-dev gcc -y
</code></pre>
<h3>स्थानीय इंस्टाल (Nix)</h3>
<p>एक अन्य विकल्प है <code>text-generation-inference</code> को स्थानीय रूप से <a href="https://nixos.org">Nix</a> का उपयोग करके इंस्टॉल करना। वर्तमान में,
हम केवल x86_64 Linux पर CUDA GPUs के साथ Nix का समर्थन करते हैं। Nix का उपयोग करते समय, सभी निर्भरताएँ
बाइनरी कैश से खींची जा सकती हैं, जिससे उन्हें स्थानीय रूप से बनाने की आवश्यकता समाप्त हो जाती है।</p>
<p>सबसे पहले <a href="https://app.cachix.org/cache/huggingface">Cachix इंस्टॉल करें और Hugging Face कैश सक्षम करें</a>।
कैश सेटअप करना महत्वपूर्ण है, अन्यथा Nix कई निर्भरताओं को स्थानीय रूप से बनाएगा,
जिसमें कई घंटे लग सकते हैं।</p>
<p>इसके बाद आप TGI को <code>nix run</code> के साथ चला सकते हैं:</p>
<pre><code class="language-shell">cd text-generation-inference
nix run --extra-experimental-features nix-command --extra-experimental-features flakes . -- --model-id meta-llama/Llama-3.1-8B-Instruct
</code></pre>
<p><strong>नोट:</strong> जब आप गैर-NixOS सिस्टम पर Nix का उपयोग कर रहे हों, तो आपको CUDA ड्राइवर लाइब्रेरीज़ को
Nix पैकेजों के लिए दिखाई देने योग्य बनाने के लिए <a href="https://danieldk.eu/Nix-CUDA-on-non-NixOS-systems#make-runopengl-driverlib-and-symlink-the-driver-library">कुछ सिमलिंक्स बनानी होती हैं</a>।</p>
<p>TGI विकास के लिए, आप <code>impure</code> dev शेल का उपयोग कर सकते हैं:</p>
<pre><code class="language-shell">nix develop .#impure

# केवल पहली बार devshell शुरू करने पर या protobuf अपडेट करने के बाद आवश्यक है।
(
cd server
mkdir text_generation_server/pb || true
python -m grpc_tools.protoc -I../proto/v3 --python_out=text_generation_server/pb \
       --grpc_python_out=text_generation_server/pb --mypy_out=text_generation_server/pb ../proto/v3/generate.proto
find text_generation_server/pb/ -type f -name &quot;*.py&quot; -print0 -exec sed -i -e 's/^\(import.*pb2\)/from . \1/g' {} \;
touch text_generation_server/pb/__init__.py
)
</code></pre>
<p>सभी विकास निर्भरताएँ (cargo, Python, Torch), आदि इस
dev शेल में उपलब्ध हैं।</p>
<h2>अनुकूलित आर्किटेक्चर</h2>
<p>TGI सभी आधुनिक मॉडलों के लिए अनुकूलित मॉडल सेवा प्रदान करने के लिए तुरंत काम करता है। ये <a href="https://huggingface.co/docs/text-generation-inference/supported_models">इस सूची</a> में पाए जा सकते हैं।</p>
<p>अन्य आर्किटेक्चर को सर्वोत्तम प्रयास के आधार पर समर्थन दिया जाता है:</p>
<p><code>AutoModelForCausalLM.from_pretrained(&lt;model&gt;, device_map=&quot;auto&quot;)</code></p>
<p>या</p>
<p><code>AutoModelForSeq2SeqLM.from_pretrained(&lt;model&gt;, device_map=&quot;auto&quot;)</code></p>
<h2>स्थानीय रूप से चलाएँ</h2>
<h3>चलाएँ</h3>
<pre><code class="language-shell">text-generation-launcher --model-id mistralai/Mistral-7B-Instruct-v0.2
</code></pre>
<h3>क्वांटाइजेशन</h3>
<p>आप पूर्व-क्वांटाइज्ड वेट्स (AWQ, GPTQ, Marlin) भी चला सकते हैं या bitsandbytes, EETQ, fp8 के साथ ऑन-द-फ्लाई क्वांटाइजेशन कर सकते हैं, ताकि VRAM की आवश्यकता कम हो:</p>
<pre><code class="language-shell">text-generation-launcher --model-id mistralai/Mistral-7B-Instruct-v0.2 --quantize
</code></pre>
<p>4bit क्वांटाइजेशन <a href="https://arxiv.org/pdf/2305.14314.pdf">bitsandbytes के NF4 और FP4 डेटा टाइप</a> का उपयोग करके उपलब्ध है। इसे <code>text-generation-launcher</code> को कमांड लाइन आर्गुमेंट के रूप में <code>--quantize bitsandbytes-nf4</code> या <code>--quantize bitsandbytes-fp4</code> देकर सक्षम किया जा सकता है।</p>
<p>क्वांटाइजेशन के बारे में अधिक पढ़ें <a href="https://huggingface.co/docs/text-generation-inference/en/conceptual/quantization">क्वांटाइजेशन दस्तावेज़</a> में।</p>
<h2>विकास करें</h2>
<pre><code class="language-shell">make server-dev
make router-dev
</code></pre>
<h2>परीक्षण</h2>
<pre><code class="language-shell"># python
make python-server-tests
make python-client-tests
# या सर्वर और क्लाइंट दोनों के टेस्ट
make python-tests
# rust cargo टेस्ट
make rust-tests
# इंटीग्रेशन टेस्ट
make integration-tests
</code></pre>
<hr />
<p>Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx">Open Ai Tx</a> | Last indexed: 2025-06-11</p>
<hr />

        </div>
    </div>
    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Default Statcounter code for openaitx
https://openaitx.github.io/ -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>
    <!-- End of Statcounter Code -->
</body>
</html>