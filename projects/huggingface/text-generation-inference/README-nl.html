<!DOCTYPE html><html lang="en"><head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Tekstgeneratie Inference - huggingface/text-generation-inference</title>

    <!-- Primary Meta Tags -->
    <meta name="title" content="Tekstgeneratie Inference - huggingface/text-generation-inference">
    <meta name="description" content="huggingface/text-generation-inference - GitHub repository nl documentation and information">
    <meta name="keywords" content="huggingface, text-generation-inference, GitHub, repository, nl documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">

    <!-- Open Graph -->
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/huggingface/text-generation-inference/README-nl.html">
    <meta property="og:title" content="Tekstgeneratie Inference - huggingface/text-generation-inference">
    <meta property="og:description" content="huggingface/text-generation-inference - GitHub repository nl documentation and information">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">

    <!-- Favicon -->
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">

    <!-- Marked.js for Markdown rendering -->
    <script type="text/javascript" async="" src="https://www.statcounter.com/counter/recorder.js"></script><script src="/js/marked.min.js?v=20250613"></script>
    <!-- Highlight.js for code syntax highlighting -->
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <!-- Custom CSS -->
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        /* Layout */
        body {
            display: flex;
            flex-direction: column;
            min-height: 100vh;
        }

        .main-container {
            margin: 0 auto;
            width: 100%;
            max-width: 980px;
            padding: 0 20px;
        }

        @media (max-width: 768px) {
            .main-container {
                padding: 0 15px;
            }
        }

        /* Image size restrictions */
        .markdown-body img {
            max-width: 100%;
            height: auto;
        }

        /* Existing styles */
        .header {
            text-align: center;
            margin-bottom: 30px;
            padding: 20px;
            background-color: #f6f8fa;
            border-bottom: 1px solid #e1e4e8;
            position: relative;
        }

        .back-button {
            position: absolute;
            left: 20px;
            top: 50%;
            transform: translateY(-50%);
            color: #0366d6;
            text-decoration: none;
            display: flex;
            align-items: center;
            font-size: 14px;
            padding: 5px 10px;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            background-color: #fff;
        }

        .back-button:hover {
            background-color: #f6f8fa;
            border-color: #0366d6;
        }

        .back-button::before {
            content: "←";
            margin-right: 5px;
            font-size: 16px;
        }

        .header .links {
            margin-top: 10px;
            font-size: 16px;
        }

        .header .links a {
            color: #0366d6;
            text-decoration: none;
            margin-left: 5px;
        }

        .header .links a:hover {
            text-decoration: underline;
        }
        
        /* Language badges styles */
        .language-badges {
            margin-top: 15px;
            text-align: center;
        }
        .language-badges a {
            display: inline-block;
            margin: 2px;
            text-decoration: none;
        }
        .language-badges img {
            height: 20px;
            border-radius: 3px;
        }
        .language-badges a:hover img {
            opacity: 0.8;
        }
    </style>
</head>

<body>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/huggingface/text-generation-inference" id="githubRepoLink" target="_blank">huggingface/text-generation-inference</a>
        </div>
        <div class="language-badges" id="languageBadges"><a href="https://openaitx.github.io/view.html?user=huggingface&amp;project=text-generation-inference&amp;lang=en"><img src="https://img.shields.io/badge/EN-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=huggingface&amp;project=text-generation-inference&amp;lang=zh-CN"><img src="https://img.shields.io/badge/简中-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=huggingface&amp;project=text-generation-inference&amp;lang=zh-TW"><img src="https://img.shields.io/badge/繁中-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=huggingface&amp;project=text-generation-inference&amp;lang=ja"><img src="https://img.shields.io/badge/日本語-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=huggingface&amp;project=text-generation-inference&amp;lang=ko"><img src="https://img.shields.io/badge/한국어-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=huggingface&amp;project=text-generation-inference&amp;lang=th"><img src="https://img.shields.io/badge/ไทย-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=huggingface&amp;project=text-generation-inference&amp;lang=fr"><img src="https://img.shields.io/badge/Français-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=huggingface&amp;project=text-generation-inference&amp;lang=de"><img src="https://img.shields.io/badge/Deutsch-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=huggingface&amp;project=text-generation-inference&amp;lang=es"><img src="https://img.shields.io/badge/Español-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=huggingface&amp;project=text-generation-inference&amp;lang=it"><img src="https://img.shields.io/badge/Italiano-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=huggingface&amp;project=text-generation-inference&amp;lang=ru"><img src="https://img.shields.io/badge/Русский-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=huggingface&amp;project=text-generation-inference&amp;lang=pt"><img src="https://img.shields.io/badge/Português-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=huggingface&amp;project=text-generation-inference&amp;lang=nl"><img src="https://img.shields.io/badge/Nederlands-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=huggingface&amp;project=text-generation-inference&amp;lang=pl"><img src="https://img.shields.io/badge/Polski-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=huggingface&amp;project=text-generation-inference&amp;lang=ar"><img src="https://img.shields.io/badge/العربية-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=huggingface&amp;project=text-generation-inference&amp;lang=tr"><img src="https://img.shields.io/badge/Türkçe-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=huggingface&amp;project=text-generation-inference&amp;lang=vi"><img src="https://img.shields.io/badge/Tiếng Việt-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=huggingface&amp;project=text-generation-inference&amp;lang=hi"><img src="https://img.shields.io/badge/हिंदी-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=huggingface&amp;project=text-generation-inference&amp;lang=fa"><img src="https://img.shields.io/badge/فارسی-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=huggingface&amp;project=text-generation-inference&amp;lang=id"><img src="https://img.shields.io/badge/Bahasa Indonesia-white" alt="version"></a></div>
    </div>

    <div class="main-container">
        <div class="markdown-body" id="content"><div align="center">

<a href="https://www.youtube.com/watch?v=jlMAX2Oaht0">
  <img width="560" alt="Making TGI deployment optimal" src="https://huggingface.co/datasets/Narsil/tgi_assets/resolve/main/thumbnail.png">
</a>

<h1>Tekstgeneratie Inference</h1>
<a href="https://github.com/huggingface/text-generation-inference">
  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/huggingface/text-generation-inference?style=social">
</a>
<a href="https://huggingface.github.io/text-generation-inference">
  <img alt="Swagger API-documentatie" src="https://img.shields.io/badge/API-Swagger-informational">
</a>

<p>Een Rust-, Python- en gRPC-server voor tekstgeneratie-inference. Wordt in productie gebruikt bij <a href="https://huggingface.co">Hugging Face</a><br>om Hugging Chat, de Inference API en Inference Endpoints aan te drijven.</p>
</div>

<h2>Inhoudsopgave</h2>
<ul>
<li><a href="#get-started">Aan de slag</a><ul>
<li><a href="#docker">Docker</a></li>
<li><a href="#api-documentation">API-documentatie</a></li>
<li><a href="#using-a-private-or-gated-model">Een privé- of afgeschermd model gebruiken</a></li>
<li><a href="#a-note-on-shared-memory-shm">Een opmerking over gedeeld geheugen (shm)</a></li>
<li><a href="#distributed-tracing">Gedistribueerde tracing</a></li>
<li><a href="#architecture">Architectuur</a></li>
<li><a href="#local-install">Lokale installatie</a></li>
<li><a href="#local-install-nix">Lokale installatie (Nix)</a></li>
</ul>
</li>
<li><a href="#optimized-architectures">Geoptimaliseerde architecturen</a></li>
<li><a href="#run-locally">Lokaal uitvoeren</a><ul>
<li><a href="#run">Uitvoeren</a></li>
<li><a href="#quantization">Quantisatie</a></li>
</ul>
</li>
<li><a href="#develop">Ontwikkelen</a></li>
<li><a href="#testing">Testen</a></li>
</ul>
<p>Tekstgeneratie Inference (TGI) is een toolkit voor het implementeren en bedienen van Large Language Models (LLM's). TGI maakt het mogelijk om tekstgeneratie met hoge prestaties uit te voeren voor de populairste open-source LLM's, waaronder Llama, Falcon, StarCoder, BLOOM, GPT-NeoX en <a href="https://huggingface.co/docs/text-generation-inference/supported_models">meer</a>. TGI implementeert vele functies, zoals:</p>
<ul>
<li>Eenvoudige launcher om de populairste LLM's te bedienen</li>
<li>Productieklaar (gedistribueerde tracing met Open Telemetry, Prometheus-metrics)</li>
<li>Tensor Parallelisme voor snellere inference op meerdere GPU's</li>
<li>Token-streaming met Server-Sent Events (SSE)</li>
<li>Continue batching van binnenkomende verzoeken voor verhoogde totale doorvoer</li>
<li><a href="https://huggingface.co/docs/text-generation-inference/en/messages_api">Messages API</a> compatibel met Open AI Chat Completion API</li>
<li>Geoptimaliseerde transformers-code voor inference met <a href="https://github.com/HazyResearch/flash-attention">Flash Attention</a> en <a href="https://github.com/vllm-project/vllm">Paged Attention</a> op de populairste architecturen</li>
<li>Quantisatie met:<ul>
<li><a href="https://github.com/TimDettmers/bitsandbytes">bitsandbytes</a></li>
<li><a href="https://arxiv.org/abs/2210.17323">GPT-Q</a></li>
<li><a href="https://github.com/NetEase-FuXi/EETQ">EETQ</a></li>
<li><a href="https://github.com/casper-hansen/AutoAWQ">AWQ</a></li>
<li><a href="https://github.com/IST-DASLab/marlin">Marlin</a></li>
<li><a href="https://developer.nvidia.com/blog/nvidia-arm-and-intel-publish-fp8-specification-for-standardization-as-an-interchange-format-for-ai/">fp8</a></li>
</ul>
</li>
<li><a href="https://github.com/huggingface/safetensors">Safetensors</a> gewichtsladen</li>
<li>Watermerken met <a href="https://arxiv.org/abs/2301.10226">A Watermark for Large Language Models</a></li>
<li>Logits warper (temperatuurschaling, top-p, top-k, herhalingsstraf, meer details zie <a href="https://huggingface.co/docs/transformers/internal/generation_utils#transformers.LogitsProcessor">transformers.LogitsProcessor</a>)</li>
<li>Stopreeksen</li>
<li>Log waarschijnlijkheden</li>
<li><a href="https://huggingface.co/docs/text-generation-inference/conceptual/speculation">Speculatie</a> ~2x latentie</li>
<li><a href="https://huggingface.co/docs/text-generation-inference/conceptual/guidance">Guidance/JSON</a>. Specificeer uitvoerformaat om inference te versnellen en zorg ervoor dat de uitvoer geldig is volgens bepaalde specificaties.</li>
<li>Aangepaste Prompt Generatie: genereer eenvoudig tekst door aangepaste prompts te gebruiken om de output van het model te sturen</li>
<li>Fine-tuning ondersteuning: gebruik fijn-afgestelde modellen voor specifieke taken om hogere nauwkeurigheid en prestaties te bereiken</li>
</ul>
<h3>Hardware ondersteuning</h3>
<ul>
<li><a href="https://github.com/huggingface/text-generation-inference/pkgs/container/text-generation-inference">Nvidia</a></li>
<li><a href="https://github.com/huggingface/text-generation-inference/pkgs/container/text-generation-inference">AMD</a> (-rocm)</li>
<li><a href="https://github.com/huggingface/optimum-neuron/tree/main/text-generation-inference">Inferentia</a></li>
<li><a href="https://github.com/huggingface/text-generation-inference/pull/1475">Intel GPU</a></li>
<li><a href="https://github.com/huggingface/tgi-gaudi">Gaudi</a></li>
<li><a href="https://huggingface.co/docs/optimum-tpu/howto/serving">Google TPU</a></li>
</ul>
<h2>Aan de slag</h2>
<h3>Docker</h3>
<p>Voor een gedetailleerde startgids, zie de <a href="https://huggingface.co/docs/text-generation-inference/quicktour">Quick Tour</a>. De gemakkelijkste manier om te beginnen is met de officiële Docker-container:</p>
<pre><code class="language-shell hljs">model=HuggingFaceH4/zephyr-7b-beta
<span class="hljs-meta prompt_"># </span><span class="language-bash">deel een volume met de Docker-container om te voorkomen dat gewichten bij elke run worden gedownload</span>
volume=$PWD/data

docker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data \
    ghcr.io/huggingface/text-generation-inference:3.3.2 --model-id $model
</code></pre>
<p>En dan kun je verzoeken maken zoals</p>
<pre><code class="language-bash hljs">curl 127.0.0.1:8080/generate_stream \
    -X POST \
    -d <span class="hljs-string">'{"inputs":"Wat is Deep Learning?","parameters":{"max_new_tokens":20}}'</span> \
    -H <span class="hljs-string">'Content-Type: application/json'</span>
</code></pre>
<p>Je kunt ook gebruikmaken van de <a href="https://huggingface.co/docs/text-generation-inference/en/messages_api">Messages API van TGI</a> om antwoorden te krijgen die compatibel zijn met de Open AI Chat Completion API.</p>
<pre><code class="language-bash hljs">curl localhost:8080/v1/chat/completions \
    -X POST \
    -d <span class="hljs-string">'{
  "model": "tgi",
  "messages": [
    {
      "role": "system",
      "content": "Je bent een behulpzame assistent."
    },
    {
      "role": "user",
      "content": "Wat is deep learning?"
    }
  ],
  "stream": true,
  "max_tokens": 20
}'</span> \
    -H <span class="hljs-string">'Content-Type: application/json'</span>
</code></pre>
<p><strong>Opmerking:</strong> Om NVIDIA GPU's te gebruiken, moet je de <a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html">NVIDIA Container Toolkit</a> installeren. We raden ook aan NVIDIA-drivers met CUDA versie 12.2 of hoger te gebruiken. Voor het draaien van de Docker-container op een machine zonder GPU's of CUDA-ondersteuning is het voldoende om de <code>--gpus all</code> vlag te verwijderen en <code>--disable-custom-kernels</code> toe te voegen. Houd er rekening mee dat CPU niet het beoogde platform is voor dit project, dus de prestaties kunnen ondermaats zijn.</p>
<p><strong>Opmerking:</strong> TGI ondersteunt AMD Instinct MI210 en MI250 GPU's. Details zijn te vinden in de <a href="https://huggingface.co/docs/text-generation-inference/installation_amd#using-tgi-with-amd-gpus">Supported Hardware-documentatie</a>. Om AMD GPU's te gebruiken, gebruik in plaats van bovenstaande commando:</p>
<pre><code class="language-shell hljs">docker run --device /dev/kfd --device /dev/dri --shm-size 1g -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:3.3.2-rocm --model-id $model
</code></pre>
<p>Om alle opties te zien om je modellen te bedienen (in de <a href="https://raw.githubusercontent.com/huggingface/text-generation-inference/main/launcher/src/main.rs">code</a> of via de cli):</p>
<pre><code class="hljs language-scss">text-generation-launcher <span class="hljs-attr">--help</span>
</code></pre>
<h3>API-documentatie</h3>
<p>Je kunt de OpenAPI-documentatie van de <code>text-generation-inference</code> REST API raadplegen via de <code>/docs</code> route.<br>De Swagger UI is ook beschikbaar op: <a href="https://huggingface.github.io/text-generation-inference">https://huggingface.github.io/text-generation-inference</a>.</p>
<h3>Een privé- of afgeschermd model gebruiken</h3>
<p>Je hebt de optie om de omgevingsvariabele <code>HF_TOKEN</code> te gebruiken voor het configureren van de token die<br>door <code>text-generation-inference</code> wordt gebruikt. Dit stelt je in staat om toegang te krijgen tot beveiligde bronnen.</p>
<p>Als je bijvoorbeeld de afgeschermde Llama V2 modelvarianten wilt bedienen:</p>
<ol>
<li>Ga naar <a href="https://huggingface.co/settings/tokens">https://huggingface.co/settings/tokens</a></li>
<li>Kopieer je CLI READ-token</li>
<li>Exporteer <code>HF_TOKEN=&lt;jouw CLI READ-token&gt;</code></li>
</ol>
<p>of met Docker:</p>
<pre><code class="language-shell hljs">model=meta-llama/Meta-Llama-3.1-8B-Instruct
volume=$PWD/data # deel een volume met de Docker-container om te voorkomen dat gewichten bij elke run worden gedownload
token=&lt;jouw cli READ-token&gt;

docker run --gpus all --shm-size 1g -e HF_TOKEN=$token -p 8080:80 -v $volume:/data \
    ghcr.io/huggingface/text-generation-inference:3.3.2 --model-id $model
</code></pre>
<h3>Een opmerking over gedeeld geheugen (shm)</h3>
<p><a href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/index.html"><code>NCCL</code></a> is een communicatieframework dat door<br><code>PyTorch</code> wordt gebruikt voor gedistribueerde training/inference. <code>text-generation-inference</code> maakt<br>gebruik van <code>NCCL</code> om Tensor Parallelisme mogelijk te maken, waardoor inference voor grote taalmodellen aanzienlijk wordt versneld.</p>
<p>Om data te delen tussen de verschillende apparaten van een <code>NCCL</code> groep, kan <code>NCCL</code> terugvallen op het gebruik van het hostgeheugen als<br>peer-to-peer via NVLink of PCI niet mogelijk is.</p>
<p>Om de container 1G gedeeld geheugen te laten gebruiken en SHM-sharing te ondersteunen, voegen we <code>--shm-size 1g</code> toe aan het bovenstaande commando.</p>
<p>Als je <code>text-generation-inference</code> binnen <code>Kubernetes</code> draait, kun je gedeeld geheugen aan de container toevoegen door<br>een volume te maken met:</p>
<pre><code class="language-yaml hljs"><span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">shm</span>
  <span class="hljs-attr">emptyDir:</span>
   <span class="hljs-attr">medium:</span> <span class="hljs-string">Memory</span>
   <span class="hljs-attr">sizeLimit:</span> <span class="hljs-string">1Gi</span>
</code></pre>
<p>en dit te mounten op <code>/dev/shm</code>.</p>
<p>Tot slot kun je SHM-sharing ook uitschakelen door de omgevingsvariabele <code>NCCL_SHM_DISABLE=1</code> te gebruiken. Houd er echter rekening mee dat<br>dit de prestaties zal beïnvloeden.</p>
<h3>Gedistribueerde tracing</h3>
<p><code>text-generation-inference</code> is geïnstrumenteerd met gedistribueerde tracing via OpenTelemetry. Je kunt deze functie<br>gebruiken door het adres van een OTLP-collector in te stellen met het argument <code>--otlp-endpoint</code>. De standaard servicenaam kan worden<br>overschreven met het argument <code>--otlp-service-name</code>.</p>
<h3>Architectuur</h3>
<p><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/TGI.png" alt="TGI architectuur"></p>
<p>Uitgebreide blogpost door Adyen over de interne werking van TGI: <a href="https://www.adyen.com/knowledge-hub/llm-inference-at-scale-with-tgi">LLM inference at scale with TGI (Martin Iglesias Goyanes - Adyen, 2024)</a></p>
<h3>Lokale installatie</h3>
<p>Je kunt er ook voor kiezen om <code>text-generation-inference</code> lokaal te installeren.</p>
<p>Clone eerst de repository en ga ernaartoe:</p>
<pre><code class="language-shell hljs">git clone https://github.com/huggingface/text-generation-inference
cd text-generation-inference
</code></pre>
<p>Installeer vervolgens <a href="https://rustup.rs/">Rust</a> en maak een Python virtuele omgeving aan met ten minste<br>Python 3.9, bijvoorbeeld met <code>conda</code> of <code>python venv</code>:</p>
<pre><code class="language-shell hljs">curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
<span class="hljs-meta prompt_">
# </span><span class="language-bash">met conda</span>
conda create -n text-generation-inference python=3.11
conda activate text-generation-inference
<span class="hljs-meta prompt_">
# </span><span class="language-bash">met python venv</span>
python3 -m venv .venv
source .venv/bin/activate
</code></pre>
<p>Mogelijk moet je ook Protoc installeren.</p>
<p>Op Linux:</p>
<pre><code class="language-shell hljs">PROTOC_ZIP=protoc-21.12-linux-x86_64.zip
curl -OL https://github.com/protocolbuffers/protobuf/releases/download/v21.12/$PROTOC_ZIP
sudo unzip -o $PROTOC_ZIP -d /usr/local bin/protoc
sudo unzip -o $PROTOC_ZIP -d /usr/local 'include/*'
rm -f $PROTOC_ZIP
</code></pre>
<p>Op MacOS, met Homebrew:</p>
<pre><code class="language-shell hljs">brew install protobuf
</code></pre>
<p>Voer daarna uit:</p>
<pre><code class="language-shell hljs">BUILD_EXTENSIONS=True make install # Installeer repository en HF/transformers fork met CUDA kernels
text-generation-launcher --model-id mistralai/Mistral-7B-Instruct-v0.2
</code></pre>
<p><strong>Opmerking:</strong> op sommige systemen heb je mogelijk ook de OpenSSL libraries en gcc nodig. Op Linux systemen, voer uit:</p>
<pre><code class="language-shell hljs">sudo apt-get install libssl-dev gcc -y
</code></pre>
<h3>Lokale installatie (Nix)</h3>
<p>Een andere optie is om <code>text-generation-inference</code> lokaal te installeren met <a href="https://nixos.org">Nix</a>. Momenteel<br>ondersteunen we alleen Nix op x86_64 Linux met CUDA GPU's. Bij gebruik van Nix kunnen alle dependencies<br>worden gedownload uit een binaire cache, waardoor lokaal bouwen niet nodig is.</p>
<p>Volg eerst de instructies om <a href="https://app.cachix.org/cache/huggingface">Cachix te installeren en de Hugging Face cache in te schakelen</a>.<br>Het instellen van de cache is belangrijk, anders zal Nix veel dependencies lokaal bouwen,<br>wat uren kan duren.</p>
<p>Daarna kun je TGI starten met <code>nix run</code>:</p>
<pre><code class="language-shell hljs">cd text-generation-inference
nix run --extra-experimental-features nix-command --extra-experimental-features flakes . -- --model-id meta-llama/Llama-3.1-8B-Instruct
</code></pre>
<p><strong>Opmerking:</strong> wanneer je Nix gebruikt op een niet-NixOS systeem, moet je <a href="https://danieldk.eu/Nix-CUDA-on-non-NixOS-systems#make-runopengl-driverlib-and-symlink-the-driver-library">enkele symlinks aanmaken</a><br>om de CUDA driver libraries zichtbaar te maken voor Nix pakketten.</p>
<p>Voor TGI ontwikkeling kun je de <code>impure</code> dev shell gebruiken:</p>
<pre><code class="language-shell hljs">nix develop .#impure
<span class="hljs-meta prompt_">
# </span><span class="language-bash">Alleen nodig de eerste keer dat de devshell wordt gestart of na update van protobuf.</span>
(
cd server
mkdir text_generation_server/pb || true
python -m grpc_tools.protoc -I../proto/v3 --python_out=text_generation_server/pb \
       --grpc_python_out=text_generation_server/pb --mypy_out=text_generation_server/pb ../proto/v3/generate.proto
find text_generation_server/pb/ -type f -name "*.py" -print0 -exec sed -i -e 's/^\(import.*pb2\)/from . \1/g' {} \;
touch text_generation_server/pb/__init__.py
)
</code></pre>
<p>Alle ontwikkel-dependencies (cargo, Python, Torch), etc. zijn beschikbaar in deze<br>dev shell.</p>
<h2>Geoptimaliseerde architecturen</h2>
<p>TGI werkt direct out-of-the-box om geoptimaliseerde modellen te serveren voor alle moderne modellen. Deze zijn te vinden in <a href="https://huggingface.co/docs/text-generation-inference/supported_models">deze lijst</a>.</p>
<p>Andere architecturen worden op een best-effort basis ondersteund met:</p>
<p><code>AutoModelForCausalLM.from_pretrained(&lt;model&gt;, device_map="auto")</code></p>
<p>of</p>
<p><code>AutoModelForSeq2SeqLM.from_pretrained(&lt;model&gt;, device_map="auto")</code></p>
<h2>Lokale uitvoering</h2>
<h3>Run</h3>
<pre><code class="language-shell hljs">text-generation-launcher --model-id mistralai/Mistral-7B-Instruct-v0.2
</code></pre>
<h3>Kwantisatie</h3>
<p>Je kunt ook vooraf gekwantiseerde gewichten draaien (AWQ, GPTQ, Marlin) of gewichten on-the-fly kwantiseren met bitsandbytes, EETQ, fp8, om het VRAM-verbruik te verlagen:</p>
<pre><code class="language-shell hljs">text-generation-launcher --model-id mistralai/Mistral-7B-Instruct-v0.2 --quantize
</code></pre>
<p>4bit kwantisatie is beschikbaar met de <a href="https://arxiv.org/pdf/2305.14314.pdf">NF4 en FP4 datatypes van bitsandbytes</a>. Dit kan worden ingeschakeld door <code>--quantize bitsandbytes-nf4</code> of <code>--quantize bitsandbytes-fp4</code> mee te geven als commandoregelargument aan <code>text-generation-launcher</code>.</p>
<p>Lees meer over kwantisatie in de <a href="https://huggingface.co/docs/text-generation-inference/en/conceptual/quantization">Kwantisatie documentatie</a>.</p>
<h2>Ontwikkeling</h2>
<pre><code class="language-shell hljs">make server-dev
make router-dev
</code></pre>
<h2>Testen</h2>
<pre><code class="language-shell hljs"><span class="hljs-meta prompt_"># </span><span class="language-bash">python</span>
make python-server-tests
make python-client-tests
<span class="hljs-meta prompt_"># </span><span class="language-bash">of zowel server- als client-tests</span>
make python-tests
<span class="hljs-meta prompt_"># </span><span class="language-bash">rust cargo tests</span>
make rust-tests
<span class="hljs-meta prompt_"># </span><span class="language-bash">integratietests</span>
make integration-tests
</code></pre>
<hr>
<p>Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx">Open Ai Tx</a> | Last indexed: 2025-06-11</p>
<hr>
</div>
    </div>

    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Default Statcounter code for openaitx
https://openaitx.github.io/ -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async=""></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>
    <!-- End of Statcounter Code -->
    


</body></html>