<!DOCTYPE html>
<html lang="tr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>text-generation-inference - huggingface/text-generation-inference</title>
    <meta name="title" content="text-generation-inference - huggingface/text-generation-inference">
    <meta name="description" content="huggingface/text-generation-inference - GitHub repository tr documentation and informationMetin Üretimi Çıkarımı Rust, Python ve gRPC sunucusu olan metin üretimi çıkarımı için bir araçtır. Hugging Face üretiminde kullanılır Hugging Chat, Inference API ve Inference Endpoints'i desteklemek için. İçindekiler Başlarken Docker API dokümantasyonu Özel veya sınırlı model kullanımı Paylaşılan Bellek (shm) hakkında not Dağıtık İzleme Mimari Yerel kurulum Yerel kurulum (Nix) Optimizasyonlu mimariler Yerel çalıştırma Çalıştırma Kuantizasyon Geliştirme Test Metin Üretimi Çıkarımı (TGI), Büyük Dil Modelleri (LLM) dağıtımı ve sunumu için bir araç setidir. TGI, Llama, Falcon, StarCoder, BLOOM, GPT-NeoX ve daha fazlası dahil en popüler açık kaynak LLM'ler için yüksek performanslı metin üretimi sağlar. TGI şu özellikleri uygular: En popüler LLM'leri servis etmek için basit başlatıcı Üretim hazır (Open Telemetry ile dağıtık izleme, Prometheus metrikleri) Birden fazla GPU üzerinde daha hızlı çıkarım için Tensör Paralelliği Server-Sent Events (SSE) kullanarak token akışı Artan toplam verimlilik için gelen isteklerin sürekli toplu işlenmesi Open AI Chat Completion API ile uyumlu Mesajlar API'si...">
    <meta name="keywords" content="huggingface, text-generation-inference, GitHub, repository, tr documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/huggingface/text-generation-inference/README-tr.html">
    <meta property="og:title" content="text-generation-inference - huggingface/text-generation-inference">
    <meta property="og:description" content="huggingface/text-generation-inference - GitHub repository tr documentation and information">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">
    <script src="/js/marked.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        body { display: flex; flex-direction: column; min-height: 100vh; }
        .main-container { margin: 0 auto; width: 100%; max-width: 980px; padding: 0 20px; }
        @media (max-width: 768px) { .main-container { padding: 0 15px; } }
        .markdown-body img { max-width: 100%; height: auto; }
        .header { text-align: center; margin-bottom: 30px; padding: 20px; background-color: #f6f8fa; border-bottom: 1px solid #e1e4e8; position: relative; }
        .header .links { margin-top: 10px; font-size: 16px; }
        .header .links a { color: #0366d6; text-decoration: none; margin-left: 5px; }
        .header .links a:hover { text-decoration: underline; }
        .language-badges { margin-top: 15px; text-align: center; }
        .language-badges a { display: inline-block; margin: 2px; text-decoration: none; }
        .language-badges img { height: 20px; border-radius: 3px; }
        .language-badges a:hover img { opacity: 0.8; }
    </style>
</head>
<body>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/huggingface/text-generation-inference" id="githubRepoLink" target="_blank">huggingface/text-generation-inference</a>
<br>
<h1 style="display: none;">Metin Üretimi Çıkarımı Rust, Python ve gRPC sunucusu olan metin üretimi çıkarımı için bir araçtır. Hugging Face üretiminde kullanılır Hugging Chat, Inference API ve Inference Endpoints'i desteklemek için. İçindekiler Başlarken Docker API dokümantasyonu Özel veya sınırlı model kullanımı Paylaşılan Bellek (shm) hakkında not Dağıtık İzleme Mimari Yerel kurulum Yerel kurulum (Nix) Optimizasyonlu mimariler Yerel çalıştırma Çalıştırma Kuantizasyon Geliştirme Test Metin Üretimi Çıkarımı (TGI), Büyük Dil Modelleri (LLM) dağıtımı ve sunumu için bir araç setidir. TGI, Llama, Falcon, StarCoder, BLOOM, GPT-NeoX ve daha fazlası dahil en popüler açık kaynak LLM'ler için yüksek performanslı metin üretimi sağlar. TGI şu özellikleri uygular: En popüler LLM'leri servis etmek için basit başlatıcı Üretim hazır (Open Telemetry ile dağıtık izleme, Prometheus metrikleri) Birden fazla GPU üzerinde daha hızlı çıkarım için Tensör Paralelliği Server-Sent Events (SSE) kullanarak token akışı Artan toplam verimlilik için gelen isteklerin sürekli toplu işlenmesi Open AI Chat Completion API ile uyumlu Mesajlar API'si...</h1>
        </div>
        <div class="language-badges" id="languageBadges">
            <!-- You can generate language badges here if needed -->
        </div>
    </div>
    <div class="main-container">
        <div class="markdown-body" id="content">
            <div align="center">
<a href="https://www.youtube.com/watch?v=jlMAX2Oaht0">
  <img width=560 alt="Making TGI deployment optimal" src="https://raw.githubusercontent.com/huggingface/text-generation-inference/main/datasets/Narsil/tgi_assets/resolve/main/thumbnail.png">
</a>
<h1>Metin Üretimi Çıkarımı</h1>
<a href="https://github.com/huggingface/text-generation-inference">
  <img alt="GitHub Repo yıldızları" src="https://img.shields.io/github/stars/huggingface/text-generation-inference?style=social">
</a>
<a href="https://huggingface.github.io/text-generation-inference">
  <img alt="Swagger API dokümantasyonu" src="https://img.shields.io/badge/API-Swagger-informational">
</a>
<p>Rust, Python ve gRPC sunucusu olan metin üretimi çıkarımı için bir araçtır. <a href="https://huggingface.co">Hugging Face</a> üretiminde kullanılır
Hugging Chat, Inference API ve Inference Endpoints'i desteklemek için.</p>
</div>
<h2>İçindekiler</h2>
<ul>
<li><a href="#get-started">Başlarken</a>
<ul>
<li><a href="#docker">Docker</a></li>
<li><a href="#api-documentation">API dokümantasyonu</a></li>
<li><a href="#using-a-private-or-gated-model">Özel veya sınırlı model kullanımı</a></li>
<li><a href="#a-note-on-shared-memory-shm">Paylaşılan Bellek (shm) hakkında not</a></li>
<li><a href="#distributed-tracing">Dağıtık İzleme</a></li>
<li><a href="#architecture">Mimari</a></li>
<li><a href="#local-install">Yerel kurulum</a></li>
<li><a href="#local-install-nix">Yerel kurulum (Nix)</a></li>
</ul>
</li>
<li><a href="#optimized-architectures">Optimizasyonlu mimariler</a></li>
<li><a href="#run-locally">Yerel çalıştırma</a>
<ul>
<li><a href="#run">Çalıştırma</a></li>
<li><a href="#quantization">Kuantizasyon</a></li>
</ul>
</li>
<li><a href="#develop">Geliştirme</a></li>
<li><a href="#testing">Test</a></li>
</ul>
<p>Metin Üretimi Çıkarımı (TGI), Büyük Dil Modelleri (LLM) dağıtımı ve sunumu için bir araç setidir. TGI, Llama, Falcon, StarCoder, BLOOM, GPT-NeoX ve <a href="https://huggingface.co/docs/text-generation-inference/supported_models">daha fazlası</a> dahil en popüler açık kaynak LLM'ler için yüksek performanslı metin üretimi sağlar. TGI şu özellikleri uygular:</p>
<ul>
<li>En popüler LLM'leri servis etmek için basit başlatıcı</li>
<li>Üretim hazır (Open Telemetry ile dağıtık izleme, Prometheus metrikleri)</li>
<li>Birden fazla GPU üzerinde daha hızlı çıkarım için Tensör Paralelliği</li>
<li>Server-Sent Events (SSE) kullanarak token akışı</li>
<li>Artan toplam verimlilik için gelen isteklerin sürekli toplu işlenmesi</li>
<li>Open AI Chat Completion API ile uyumlu <a href="https://huggingface.co/docs/text-generation-inference/en/messages_api">Mesajlar API'si</a></li>
<li>En popüler mimarilerde çıkarım için <a href="https://github.com/HazyResearch/flash-attention">Flash Attention</a> ve <a href="https://github.com/vllm-project/vllm">Paged Attention</a> kullanarak optimize edilmiş transformers kodu</li>
<li>Kuantizasyon:
<ul>
<li><a href="https://github.com/TimDettmers/bitsandbytes">bitsandbytes</a></li>
<li><a href="https://arxiv.org/abs/2210.17323">GPT-Q</a></li>
<li><a href="https://github.com/NetEase-FuXi/EETQ">EETQ</a></li>
<li><a href="https://github.com/casper-hansen/AutoAWQ">AWQ</a></li>
<li><a href="https://github.com/IST-DASLab/marlin">Marlin</a></li>
<li><a href="https://developer.nvidia.com/blog/nvidia-arm-and-intel-publish-fp8-specification-for-standardization-as-an-interchange-format-for-ai/">fp8</a></li>
</ul>
</li>
<li><a href="https://github.com/huggingface/safetensors">Safetensors</a> ağırlık yükleme</li>
<li><a href="https://arxiv.org/abs/2301.10226">Büyük Dil Modelleri için Su İşareti</a> ile watermarking</li>
<li>Logits düzenleyici (sıcaklık ölçeklendirme, top-p, top-k, tekrar cezası, daha fazla detay için <a href="https://huggingface.co/docs/transformers/internal/generation_utils#transformers.LogitsProcessor">transformers.LogitsProcessor</a>)</li>
<li>Durdurma dizileri</li>
<li>Log olasılıkları</li>
<li><a href="https://huggingface.co/docs/text-generation-inference/conceptual/speculation">Spekülasyon</a> ~2 kat gecikme</li>
<li><a href="https://huggingface.co/docs/text-generation-inference/conceptual/guidance">Yönlendirme/JSON</a>. Çıktı formatını belirleyerek çıkarımı hızlandırır ve çıktının belirli kurallara uygun olmasını sağlar.</li>
<li>Özel İstek Oluşturma: Model çıktısını yönlendirmek için özel istemler kullanarak kolayca metin üretimi</li>
<li>İnce ayar desteği: Belirli görevler için ince ayarlanmış modelleri kullanarak daha yüksek doğruluk ve performans elde etme</li>
</ul>
<h3>Donanım desteği</h3>
<ul>
<li><a href="https://github.com/huggingface/text-generation-inference/pkgs/container/text-generation-inference">Nvidia</a></li>
<li><a href="https://github.com/huggingface/text-generation-inference/pkgs/container/text-generation-inference">AMD</a> (-rocm)</li>
<li><a href="https://github.com/huggingface/optimum-neuron/tree/main/text-generation-inference">Inferentia</a></li>
<li><a href="https://github.com/huggingface/text-generation-inference/pull/1475">Intel GPU</a></li>
<li><a href="https://github.com/huggingface/tgi-gaudi">Gaudi</a></li>
<li><a href="https://huggingface.co/docs/optimum-tpu/howto/serving">Google TPU</a></li>
</ul>
<h2>Başlarken</h2>
<h3>Docker</h3>
<p>Detaylı başlangıç rehberi için lütfen <a href="https://huggingface.co/docs/text-generation-inference/quicktour">Hızlı Tur</a> sayfasına bakınız. Başlamanın en kolay yolu resmi Docker konteynerini kullanmaktır:</p>
<pre><code class="language-shell">model=HuggingFaceH4/zephyr-7b-beta
# Docker konteyneri ile bir klasörü paylaşarak her çalıştırmada ağırlıkları indirmenizi önleyin
volume=$PWD/data

docker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data \
    ghcr.io/huggingface/text-generation-inference:3.3.2 --model-id $model
</code></pre>
<p>Daha sonra şu şekilde istek yapabilirsiniz:</p>
<pre><code class="language-bash">curl 127.0.0.1:8080/generate_stream \
    -X POST \
    -d '{&quot;inputs&quot;:&quot;Derin Öğrenme nedir?&quot;,&quot;parameters&quot;:{&quot;max_new_tokens&quot;:20}}' \
    -H 'Content-Type: application/json'
</code></pre>
<p>Ayrıca <a href="https://huggingface.co/docs/text-generation-inference/en/messages_api">TGI'nin Mesajlar API'sini</a> kullanarak Open AI Chat Completion API uyumlu yanıtlar alabilirsiniz.</p>
<pre><code class="language-bash">curl localhost:8080/v1/chat/completions \
    -X POST \
    -d '{
  &quot;model&quot;: &quot;tgi&quot;,
  &quot;messages&quot;: [
    {
      &quot;role&quot;: &quot;system&quot;,
      &quot;content&quot;: &quot;Yardımcı bir asistansınız.&quot;
    },
    {
      &quot;role&quot;: &quot;user&quot;,
      &quot;content&quot;: &quot;Derin öğrenme nedir?&quot;
    }
  ],
  &quot;stream&quot;: true,
  &quot;max_tokens&quot;: 20
}' \
    -H 'Content-Type: application/json'
</code></pre>
<p><strong>Not:</strong> NVIDIA GPU'ları kullanmak için <a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html">NVIDIA Container Toolkit</a> yüklemeniz gerekmektedir. Ayrıca CUDA sürümü 12.2 veya daha yüksek olan NVIDIA sürücülerini kullanmanızı öneririz. GPU veya CUDA desteği olmayan bir makinede Docker konteynerini çalıştırmak için <code>--gpus all</code> bayrağını kaldırıp <code>--disable-custom-kernels</code> eklemeniz yeterlidir, ancak CPU bu proje için önerilen platform olmadığından performans düşük olabilir.</p>
<p><strong>Not:</strong> TGI, AMD Instinct MI210 ve MI250 GPU'ları destekler. Detaylar için <a href="https://huggingface.co/docs/text-generation-inference/installation_amd#using-tgi-with-amd-gpus">Desteklenen Donanım dokümantasyonuna</a> bakınız. AMD GPU'ları kullanmak için yukarıdaki komut yerine şu komutu kullanınız: <code>docker run --device /dev/kfd --device /dev/dri --shm-size 1g -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:3.3.2-rocm --model-id $model</code>.</p>
<p>Modellerinizi servise sunmak için tüm seçenekleri görmek isterseniz (kodda <a href="https://github.com/huggingface/text-generation-inference/blob/main/launcher/src/main.rs">burada</a> veya CLI'da):</p>
<pre><code>text-generation-launcher --help
</code></pre>
<h3>API dokümantasyonu</h3>
<p><code>text-generation-inference</code> REST API'sinin OpenAPI dokümantasyonunu <code>/docs</code> rotası üzerinden inceleyebilirsiniz.
Swagger UI şu adreste de mevcuttur: <a href="https://huggingface.github.io/text-generation-inference">https://huggingface.github.io/text-generation-inference</a>.</p>
<h3>Özel veya sınırlı model kullanımı</h3>
<p><code>text-generation-inference</code> tarafından kullanılan tokenı yapılandırmak için <code>HF_TOKEN</code> ortam değişkenini kullanabilirsiniz. Bu sayede korumalı kaynaklara erişim sağlayabilirsiniz.</p>
<p>Örneğin, sınırlı Llama V2 model varyantlarını sunmak istiyorsanız:</p>
<ol>
<li>https://huggingface.co/settings/tokens adresine gidin</li>
<li>CLI READ tokenınızı kopyalayın</li>
<li><code>HF_TOKEN=&lt;tokenınız&gt;</code> olarak dışa aktarın</li>
</ol>
<p>veya Docker ile:</p>
<pre><code class="language-shell">model=meta-llama/Meta-Llama-3.1-8B-Instruct
volume=$PWD/data # Docker konteyneri ile bir klasörü paylaşarak her çalıştırmada ağırlıkları indirmenizi önleyin
token=&lt;CLI READ tokenınız&gt;

docker run --gpus all --shm-size 1g -e HF_TOKEN=$token -p 8080:80 -v $volume:/data \
    ghcr.io/huggingface/text-generation-inference:3.3.2 --model-id $model
</code></pre>
<h3>Paylaşılan Bellek (shm) hakkında not</h3>
<p><a href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/index.html"><code>NCCL</code></a>, <code>PyTorch</code> tarafından dağıtık eğitim/çıkarım için kullanılan bir iletişim çerçevesidir. <code>text-generation-inference</code>, büyük dil modelleri için çıkarımı hızlandırmak amacıyla Tensör Paralelliği'ni etkinleştirmek için <code>NCCL</code> kullanır.</p>
<p>Bir <code>NCCL</code> grubundaki farklı cihazlar arasında veri paylaşmak için, NVLink veya PCI üzerinden peer-to-peer mümkün değilse, <code>NCCL</code> host belleğine geri dönebilir.</p>
<p>Konteynerin 1G Paylaşılan Bellek kullanmasına izin vermek ve SHM paylaşımını desteklemek için yukarıdaki komutta <code>--shm-size 1g</code> eklenmiştir.</p>
<p>Eğer <code>text-generation-inference</code>'i <code>Kubernetes</code> içinde çalıştırıyorsanız, aşağıdaki gibi bir hacim oluşturarak konteynere Paylaşılan Bellek ekleyebilirsiniz:</p>
<pre><code class="language-yaml">- name: shm
  emptyDir:
   medium: Memory
   sizeLimit: 1Gi
</code></pre>
<p>ve bunu <code>/dev/shm</code> olarak bağlayabilirsiniz.</p>
<p>Son olarak, <code>NCCL_SHM_DISABLE=1</code> ortam değişkenini kullanarak SHM paylaşımını devre dışı bırakabilirsiniz. Ancak bu performansı olumsuz etkiler.</p>
<h3>Dağıtık İzleme</h3>
<p><code>text-generation-inference</code>, OpenTelemetry kullanarak dağıtık izleme ile donatılmıştır. Bu özelliği kullanmak için <code>--otlp-endpoint</code> argümanı ile OTLP toplayıcı adresini belirtebilirsiniz. Varsayılan servis adı <code>--otlp-service-name</code> argümanı ile değiştirilebilir.</p>
<h3>Mimari</h3>
<p><img src="https://raw.githubusercontent.com/huggingface/text-generation-inference/main/datasets/huggingface/documentation-images/resolve/main/TGI.png" alt="TGI mimarisi" /></p>
<p>TGI iç işleyişi hakkında Adyen tarafından detaylı blog yazısı: <a href="https://www.adyen.com/knowledge-hub/llm-inference-at-scale-with-tgi">TGI ile Ölçekli LLM çıkarımı (Martin Iglesias Goyanes - Adyen, 2024)</a></p>
<h3>Yerel kurulum</h3>
<p><code>text-generation-inference</code>'i yerel olarak da kurabilirsiniz.</p>
<p>İlk olarak repoyu klonlayın ve içine geçin:</p>
<pre><code class="language-shell">git clone https://github.com/huggingface/text-generation-inference
cd text-generation-inference
</code></pre>
<p>Sonra <a href="https://rustup.rs/">Rust yükleyin</a> ve en az Python 3.9 ile bir Python sanal ortamı oluşturun, örneğin <code>conda</code> veya <code>python venv</code> kullanarak:</p>
<pre><code class="language-shell">curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh

# conda kullanarak
conda create -n text-generation-inference python=3.11
conda activate text-generation-inference

# python venv kullanarak
python3 -m venv .venv
source .venv/bin/activate
</code></pre>
<p>Ayrıca Protoc'u yüklemeniz gerekebilir.</p>
<p>Linux üzerinde:</p>
<pre><code class="language-shell">PROTOC_ZIP=protoc-21.12-linux-x86_64.zip
curl -OL https://github.com/protocolbuffers/protobuf/releases/download/v21.12/$PROTOC_ZIP
sudo unzip -o $PROTOC_ZIP -d /usr/local bin/protoc
sudo unzip -o $PROTOC_ZIP -d /usr/local 'include/*'
rm -f $PROTOC_ZIP
</code></pre>
<p>MacOS üzerinde, Homebrew kullanarak:</p>
<pre><code class="language-shell">brew install protobuf
</code></pre>
<p>Sonra çalıştırın:</p>
<pre><code class="language-shell">BUILD_EXTENSIONS=True make install # Depoyu ve CUDA çekirdekleriyle HF/transformer çatallanmasını kur
text-generation-launcher --model-id mistralai/Mistral-7B-Instruct-v0.2
</code></pre>
<p><strong>Not:</strong> bazı makinelerde OpenSSL kütüphaneleri ve gcc de gerekebilir. Linux makinelerde şunu çalıştırın:</p>
<pre><code class="language-shell">sudo apt-get install libssl-dev gcc -y
</code></pre>
<h3>Yerel kurulum (Nix)</h3>
<p>Bir diğer seçenek de <code>text-generation-inference</code>'ı yerel olarak <a href="https://nixos.org">Nix</a> kullanarak kurmaktır. Şu anda,
sadece CUDA GPU'lu x86_64 Linux üzerinde Nix desteklenmektedir. Nix kullanırken tüm bağımlılıklar
ikili önbellekten çekilebilir, böylece yerel olarak derleme gereksinimi ortadan kalkar.</p>
<p>İlk olarak <a href="https://app.cachix.org/cache/huggingface">Cachix'i kurun ve Hugging Face önbelleğini etkinleştirin</a>.
Önbellek kurulumu önemlidir, aksi halde Nix birçok bağımlılığı yerel olarak derleyecektir,
bu saatler sürebilir.</p>
<p>Bundan sonra TGI'yi <code>nix run</code> ile çalıştırabilirsiniz:</p>
<pre><code class="language-shell">cd text-generation-inference
nix run --extra-experimental-features nix-command --extra-experimental-features flakes . -- --model-id meta-llama/Llama-3.1-8B-Instruct
</code></pre>
<p><strong>Not:</strong> NixOS olmayan bir sistemde Nix kullanıyorsanız, CUDA sürücü kütüphanelerinin Nix paketleri tarafından görünür olması için <a href="https://danieldk.eu/Nix-CUDA-on-non-NixOS-systems#make-runopengl-driverlib-and-symlink-the-driver-library">bazı sembolik bağlantılar oluşturmanız gerekir</a>.</p>
<p>TGI geliştirme için <code>impure</code> geliştirme kabuğunu kullanabilirsiniz:</p>
<pre><code class="language-shell">nix develop .#impure

# Geliştirme kabuğu ilk kez başlatıldığında veya protobuf güncellendikten sonra sadece bir kez gereklidir.
(
cd server
mkdir text_generation_server/pb || true
python -m grpc_tools.protoc -I../proto/v3 --python_out=text_generation_server/pb \
       --grpc_python_out=text_generation_server/pb --mypy_out=text_generation_server/pb ../proto/v3/generate.proto
find text_generation_server/pb/ -type f -name &quot;*.py&quot; -print0 -exec sed -i -e 's/^\(import.*pb2\)/from . \1/g' {} \;
touch text_generation_server/pb/__init__.py
)
</code></pre>
<p>Tüm geliştirme bağımlılıkları (cargo, Python, Torch vb.) bu geliştirme kabuğunda mevcuttur.</p>
<h2>Optimize Edilmiş Mimari</h2>
<p>TGI, tüm modern modeller için optimize edilmiş modelleri kutudan çıkar çıkmaz sunacak şekilde çalışır. Bunlar <a href="https://huggingface.co/docs/text-generation-inference/supported_models">bu listede</a> bulunabilir.</p>
<p>Diğer mimariler ise en iyi çaba temelinde şu yöntemlerle desteklenir:</p>
<p><code>AutoModelForCausalLM.from_pretrained(&lt;model&gt;, device_map=&quot;auto&quot;)</code></p>
<p>veya</p>
<p><code>AutoModelForSeq2SeqLM.from_pretrained(&lt;model&gt;, device_map=&quot;auto&quot;)</code></p>
<h2>Yerel Çalıştırma</h2>
<h3>Çalıştırma</h3>
<pre><code class="language-shell">text-generation-launcher --model-id mistralai/Mistral-7B-Instruct-v0.2
</code></pre>
<h3>Kuantizasyon</h3>
<p>Önceden kuantize edilmiş ağırlıkları (AWQ, GPTQ, Marlin) çalıştırabilir veya VRAM ihtiyacını azaltmak için bitsandbytes, EETQ, fp8 ile ağırlıkları anlık olarak kuantize edebilirsiniz:</p>
<pre><code class="language-shell">text-generation-launcher --model-id mistralai/Mistral-7B-Instruct-v0.2 --quantize
</code></pre>
<p>4bit kuantizasyon, <a href="https://arxiv.org/pdf/2305.14314.pdf">bitsandbytes'tan NF4 ve FP4 veri tipleri</a> kullanılarak mümkündür. <code>text-generation-launcher</code>'a komut satırı argümanı olarak <code>--quantize bitsandbytes-nf4</code> veya <code>--quantize bitsandbytes-fp4</code> verilerek etkinleştirilebilir.</p>
<p>Kuantizasyon hakkında daha fazla bilgi için <a href="https://huggingface.co/docs/text-generation-inference/en/conceptual/quantization">Kuantizasyon dokümantasyonuna</a> bakınız.</p>
<h2>Geliştirme</h2>
<pre><code class="language-shell">make server-dev
make router-dev
</code></pre>
<h2>Test Etme</h2>
<pre><code class="language-shell"># python
make python-server-tests
make python-client-tests
# veya hem sunucu hem istemci testleri
make python-tests
# rust cargo testleri
make rust-tests
# entegrasyon testleri
make integration-tests
</code></pre>
<hr />
<p>Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx">Open Ai Tx</a> | Last indexed: 2025-06-11</p>
<hr />

        </div>
    </div>
    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Statcounter and other scripts can be added here -->
</body>
</html>