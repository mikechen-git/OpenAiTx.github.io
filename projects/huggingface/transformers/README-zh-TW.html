<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>transformers - huggingface/transformers zh-TW</title>
    <meta name="title" content="transformers - huggingface/transformers zh-TW | English | 简体中文 | 繁體中文 | 한국어 | Español | 日本語 | हिन्दी | Русский | Рortuguês | తెలుగు | Français | Deutsch | Tiếng Việt | العربية | اردو | 最先進的預訓練模型，支援推論與訓練 Trans...">
    <meta name="description" content="huggingface/transformers - GitHub repository zh-TW documentation and information | English | 简体中文 | 繁體中文 | 한국어 | Español | 日本語 | हिन्दी | Русский | Рortuguês | తెలుగు | Français | Deutsch | Tiếng Việt | العربية | اردو | 最先進的預訓練模型，支援推論與訓練 Trans...">
    <meta name="keywords" content="huggingface, transformers, GitHub, repository, zh-TW documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/huggingface/transformers/README-zh-TW.html">
    <meta property="og:title" content="transformers - huggingface/transformers zh-TW | English | 简体中文 | 繁體中文 | 한국어 | Español | 日本語 | हिन्दी | Русский | Рortuguês | తెలుగు | Français | Deutsch | Tiếng Việt | العربية | اردو | 最先進的預訓練模型，支援推論與訓練 Trans...">
    <meta property="og:description" content="huggingface/transformers - GitHub repository zh-TW documentation and information | English | 简体中文 | 繁體中文 | 한국어 | Español | 日本語 | हिन्दी | Русский | Рortuguês | తెలుగు | Français | Deutsch | Tiếng Việt | العربية | اردو | 最先進的預訓練模型，支援推論與訓練 Trans...">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">
    <script src="/js/marked.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        body { display: flex; flex-direction: column; min-height: 100vh; }
        .main-container { margin: 0 auto; width: 100%; max-width: 980px; padding: 0 20px; }
        @media (max-width: 768px) { .main-container { padding: 0 15px; } }
        .markdown-body img { max-width: 100%; height: auto; }
        .header { text-align: center; margin-bottom: 30px; padding: 20px; background-color: #f6f8fa; border-bottom: 1px solid #e1e4e8; position: relative; }
        .header .links { margin-top: 10px; font-size: 16px; }
        .header .links a { color: #0366d6; text-decoration: none; margin-left: 5px; }
        .header .links a:hover { text-decoration: underline; }
        .language-badges { margin-top: 15px; text-align: center; }
        .language-badges a { display: inline-block; margin: 2px; text-decoration: none; }
        .language-badges img { height: 20px; border-radius: 3px; }
        .language-badges a:hover img { opacity: 0.8; }
    </style>
</head>
<body>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/huggingface/transformers" id="githubRepoLink" target="_blank">huggingface/transformers</a>
<h1 style="display: none;">English | 简体中文 | 繁體中文 | 한국어 | Español | 日本語 | हिन्दी | Русский | Рortuguês | తెలుగు | Français | Deutsch | Tiếng Việt | العربية | اردو | 最先進的預訓練模型，支援推論與訓練 Trans...</h1>
        </div>
        <div class="language-badges" id="languageBadges">
            <!-- You can generate language badges here if needed -->
        </div>
    </div>
    <div class="main-container">
        <div class="markdown-body" id="content">
            <!---
Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
<p align="center">
  <picture>
    <source media="(prefers-color-scheme: dark)" srcset="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-dark.svg">
    <source media="(prefers-color-scheme: light)" srcset="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg">
    <img alt="Hugging Face Transformers Library" src="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg" width="352" height="59" style="max-width: 100%;">
  </picture>
  <br/>
  <br/>
</p>
<p align="center">
    <a href="https://huggingface.com/models"><img alt="Checkpoints on Hub" src="https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&color=brightgreen"></a>
    <a href="https://circleci.com/gh/huggingface/transformers"><img alt="Build" src="https://img.shields.io/circleci/build/github/huggingface/transformers/main"></a>
    <a href="https://github.com/huggingface/transformers/blob/main/LICENSE"><img alt="GitHub" src="https://img.shields.io/github/license/huggingface/transformers.svg?color=blue"></a>
    <a href="https://huggingface.co/docs/transformers/index"><img alt="Documentation" src="https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online"></a>
    <a href="https://github.com/huggingface/transformers/releases"><img alt="GitHub release" src="https://img.shields.io/github/release/huggingface/transformers.svg"></a>
    <a href="https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md"><img alt="Contributor Covenant" src="https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg"></a>
    <a href="https://zenodo.org/badge/latestdoi/155220641"><img src="https://zenodo.org/badge/155220641.svg" alt="DOI"></a>
</p>
<h4 align="center">
    <p>
        <b>English</b> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hans.md">简体中文</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hant.md">繁體中文</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ko.md">한국어</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_es.md">Español</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ja.md">日本語</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_hd.md">हिन्दी</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ru.md">Русский</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_pt-br.md">Рortuguês</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_te.md">తెలుగు</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_fr.md">Français</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_de.md">Deutsch</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_vi.md">Tiếng Việt</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ar.md">العربية</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ur.md">اردو</a> |
    </p>
</h4>
<h3 align="center">
    <p>最先進的預訓練模型，支援推論與訓練</p>
</h3>
<h3 align="center">
    <a href="https://hf.co/course"><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/course_banner.png"></a>
</h3>
<p>Transformers 是一個包含預訓練文本、電腦視覺、音訊、影像及多模態模型的函式庫，可用於推論與訓練。使用 Transformers 可針對您的資料微調模型、建構推論應用程式，並支援各種多模態生成式 AI 應用。</p>
<p>在 <a href="https://huggingface.com/models">Hugging Face Hub</a> 上有超過 50 萬個 Transformers <a href="https://huggingface.co/models?library=transformers&amp;sort=trending">模型檢查點</a> 可供使用。</p>
<p>立即探索 <a href="https://huggingface.com/">Hub</a>，尋找合適模型並透過 Transformers 快速啟動您的專案。</p>
<h2>安裝方式</h2>
<p>Transformers 支援 Python 3.9+、<a href="https://pytorch.org/get-started/locally/">PyTorch</a> 2.1+、<a href="https://www.tensorflow.org/install/pip">TensorFlow</a> 2.6+ 及 <a href="https://flax.readthedocs.io/en/latest/">Flax</a> 0.4.1+。</p>
<p>請使用 <a href="https://docs.python.org/3/library/venv.html">venv</a> 或 <a href="https://docs.astral.sh/uv/">uv</a>，建立與啟用虛擬環境。uv 是基於 Rust 的快速 Python 套件與專案管理工具。</p>
<pre><code class="language-py"># venv
python -m venv .my-env
source .my-env/bin/activate
# uv
uv venv .my-env
source .my-env/bin/activate
</code></pre>
<p>於您的虛擬環境中安裝 Transformers。</p>
<pre><code class="language-py"># pip
pip install &quot;transformers[torch]&quot;

# uv
uv pip install &quot;transformers[torch]&quot;
</code></pre>
<p>若您需要最新的函式庫變更或有貢獻需求，可從原始碼安裝 Transformers。但<em>最新</em>版本可能不穩定。若遇到錯誤請隨時提交 <a href="https://github.com/huggingface/transformers/issues">issue</a>。</p>
<pre><code class="language-shell">git clone https://github.com/huggingface/transformers.git
cd transformers

# pip
pip install .[torch]

# uv
uv pip install .[torch]
</code></pre>
<h2>快速入門</h2>
<p>使用 <a href="https://huggingface.co/docs/transformers/pipeline_tutorial">Pipeline</a> API 立即開始使用 Transformers。<code>Pipeline</code> 是一個高階推論類別，支援文本、音訊、視覺與多模態任務。它會自動預處理輸入並回傳對應的結果。</p>
<p>初始化一個 pipeline，並指定用於文本生成的模型。模型會自動下載並快取，之後可重複使用。最後，輸入提示文本給模型。</p>
<pre><code class="language-py">from transformers import pipeline

pipeline = pipeline(task=&quot;text-generation&quot;, model=&quot;Qwen/Qwen2.5-1.5B&quot;)
pipeline(&quot;the secret to baking a really good cake is &quot;)
[{'generated_text': 'the secret to baking a really good cake is 1) to use the right ingredients and 2) to follow the recipe exactly. the recipe for the cake is as follows: 1 cup of sugar, 1 cup of flour, 1 cup of milk, 1 cup of butter, 1 cup of eggs, 1 cup of chocolate chips. if you want to make 2 cakes, how much sugar do you need? To make 2 cakes, you will need 2 cups of sugar.'}]
</code></pre>
<p>要與模型進行對話時，使用方式相同。唯一的不同是，您需建立一個聊天歷史（<code>Pipeline</code> 的輸入）給模型。</p>
<blockquote>
<p>[!TIP]
您也可以直接從命令列與模型互動。</p>
<pre><code class="language-shell">transformers chat Qwen/Qwen2.5-0.5B-Instruct
</code></pre>
</blockquote>
<pre><code class="language-py">import torch
from transformers import pipeline

chat = [
    {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;你是一個1986年好萊塢電影中想像的機智幽默機器人。&quot;},
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;嗨，你可以告訴我在紐約有什麼有趣的活動嗎？&quot;}
]

pipeline = pipeline(task=&quot;text-generation&quot;, model=&quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;, torch_dtype=torch.bfloat16, device_map=&quot;auto&quot;)
response = pipeline(chat, max_new_tokens=512)
print(response[0][&quot;generated_text&quot;][-1][&quot;content&quot;])
</code></pre>
<p>展開下方範例，瞭解 <code>Pipeline</code> 如何應用於不同模態與任務。</p>
<details>
<summary>自動語音辨識</summary>
<pre><code class="language-py">from transformers import pipeline

pipeline = pipeline(task=&quot;automatic-speech-recognition&quot;, model=&quot;openai/whisper-large-v3&quot;)
pipeline(&quot;https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac&quot;)
{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}
</code></pre>
</details>
<details>
<summary>影像分類</summary>
<h3 align="center">
    <a><img src="https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png"></a>
</h3>
<pre><code class="language-py">from transformers import pipeline

pipeline = pipeline(task=&quot;image-classification&quot;, model=&quot;facebook/dinov2-small-imagenet1k-1-layer&quot;)
pipeline(&quot;https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png&quot;)
[{'label': 'macaw', 'score': 0.997848391532898},
 {'label': 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',
  'score': 0.0016551691805943847},
 {'label': 'lorikeet', 'score': 0.00018523589824326336},
 {'label': 'African grey, African gray, Psittacus erithacus',
  'score': 7.85409429227002e-05},
 {'label': 'quail', 'score': 5.502637941390276e-05}]
</code></pre>
</details>
<details>
<summary>視覺問答</summary>
<h3 align="center">
    <a><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg"></a>
</h3>
<pre><code class="language-py">from transformers import pipeline

pipeline = pipeline(task=&quot;visual-question-answering&quot;, model=&quot;Salesforce/blip-vqa-base&quot;)
pipeline(
    image=&quot;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg&quot;,
    question=&quot;What is in the image?&quot;,
)
[{'answer': 'statue of liberty'}]
</code></pre>
</details>
<h2>為什麼要使用 Transformers？</h2>
<ol>
<li><p>易於使用的最先進模型：</p>
<ul>
<li>在自然語言理解與生成、電腦視覺、音訊、影像與多模態任務上表現優異。</li>
<li>研究人員、工程師與開發者皆可輕鬆上手。</li>
<li>僅需學習三個主要類別，抽象簡單。</li>
<li>統一的 API，支援所有預訓練模型。</li>
</ul>
</li>
<li><p>降低運算成本，減少碳足跡：</p>
<ul>
<li>共享訓練好的模型，無需每次從頭訓練。</li>
<li>減少運算時間與生產成本。</li>
<li>涵蓋所有模態，超過百萬預訓練檢查點與多種模型架構。</li>
</ul>
</li>
<li><p>依照模型生命週期選擇合適框架：</p>
<ul>
<li>僅需三行程式碼即可訓練最先進模型。</li>
<li>單一模型可隨時於 PyTorch/JAX/TF2.0 間轉換。</li>
<li>針對訓練、評估與生產挑選最適合的框架。</li>
</ul>
</li>
<li><p>輕鬆自訂模型或範例以符合需求：</p>
<ul>
<li>提供各架構範例，可重現原始論文結果。</li>
<li>模型內部盡量一致地對外開放。</li>
<li>模型檔案可獨立於函式庫使用，方便快速實驗。</li>
</ul>
</li>
</ol>
<a target="_blank" href="https://huggingface.co/enterprise">
    <img alt="Hugging Face Enterprise Hub" src="https://github.com/user-attachments/assets/247fb16d-d251-4583-96c4-d3d76dda4925">
</a><br>
<h2>什麼情況下不建議使用 Transformers？</h2>
<ul>
<li>本函式庫不是神經網路組件的模組化工具箱。模型檔案中的程式碼並未特意做額外抽象，讓研究人員可直接針對每個模型快速迭代，而無需深入額外的抽象層或檔案。</li>
<li>訓練 API 已針對 Transformers 提供的 PyTorch 模型最佳化。若需一般機器學習訓練流程，請考慮使用其他函式庫，如 <a href="https://huggingface.co/docs/accelerate">Accelerate</a>。</li>
<li><a href="(https://github.com/huggingface/transformers/tree/main/examples)">範例腳本</a> 僅供<em>範例</em>。不一定可直接應用於您的特定場景，您需依需求調整程式碼。</li>
</ul>
<h2>100 個使用 Transformers 的專案</h2>
<p>Transformers 不僅僅是一個預訓練模型工具包，更是圍繞 Hugging Face Hub 所建立的開發者社群。我們希望 Transformers 能協助開發者、研究人員、學生、教授、工程師或任何人實現他們的理想專案。</p>
<p>為慶祝 Transformers 突破 10 萬星標，我們特別推出 <a href="./awesome-transformers.md">awesome-transformers</a> 頁面，收錄 100 個令人驚豔的 Transformers 專案。</p>
<p>如果您擁有或使用值得被納入的專案，歡迎提交 PR 加入名單！</p>
<h2>範例模型</h2>
<p>您可以直接在 <a href="https://huggingface.co/models">Hub 模型頁面</a> 測試我們大多數模型。</p>
<p>展開下方各模態，可查看不同應用場景的模型範例。</p>
<details>
<summary>音訊</summary>
<ul>
<li>使用 <a href="https://huggingface.co/openai/whisper-large-v3-turbo">Whisper</a> 進行音訊分類</li>
<li>使用 <a href="https://huggingface.co/UsefulSensors/moonshine">Moonshine</a> 進行自動語音辨識</li>
<li>使用 <a href="https://huggingface.co/superb/wav2vec2-base-superb-ks">Wav2Vec2</a> 進行關鍵字偵測</li>
<li>使用 <a href="https://huggingface.co/kyutai/moshiko-pytorch-bf16">Moshi</a> 進行語音轉語音生成</li>
<li>使用 <a href="https://huggingface.co/facebook/musicgen-large">MusicGen</a> 進行文字轉音訊</li>
<li>使用 <a href="https://huggingface.co/suno/bark">Bark</a> 進行文字轉語音</li>
</ul>
</details>
<details>
<summary>電腦視覺</summary>
<ul>
<li>使用 <a href="https://huggingface.co/facebook/sam-vit-base">SAM</a> 進行自動遮罩生成</li>
<li>使用 <a href="https://huggingface.co/apple/DepthPro-hf">DepthPro</a> 進行深度估測</li>
<li>使用 <a href="https://huggingface.co/facebook/dinov2-base">DINO v2</a> 進行影像分類</li>
<li>使用 <a href="https://huggingface.co/magic-leap-community/superglue_outdoor">SuperGlue</a> 進行關鍵點偵測</li>
<li>使用 <a href="https://huggingface.co/magic-leap-community/superglue">SuperGlue</a> 進行關鍵點匹配</li>
<li>使用 <a href="https://huggingface.co/PekingU/rtdetr_v2_r50vd">RT-DETRv2</a> 進行物件偵測</li>
<li>使用 <a href="https://huggingface.co/usyd-community/vitpose-base-simple">VitPose</a> 進行姿態估測</li>
<li>使用 <a href="https://huggingface.co/shi-labs/oneformer_ade20k_swin_large">OneFormer</a> 進行通用分割</li>
<li>使用 <a href="https://huggingface.co/MCG-NJU/videomae-large">VideoMAE</a> 進行影片分類</li>
</ul>
</details>
<details>
<summary>多模態</summary>
<ul>
<li>使用 <a href="https://huggingface.co/Qwen/Qwen2-Audio-7B">Qwen2-Audio</a> 進行音訊或文字轉文字</li>
<li>使用 <a href="https://huggingface.co/microsoft/layoutlmv3-base">LayoutLMv3</a> 進行文件問答</li>
<li>使用 <a href="https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct">Qwen-VL</a> 進行影像或文字轉文字</li>
<li>使用 <a href="https://huggingface.co/Salesforce/blip2-opt-2.7b">BLIP-2</a> 進行影像描述生成</li>
<li>使用 <a href="https://huggingface.co/stepfun-ai/GOT-OCR-2.0-hf">GOT-OCR2</a> 進行 OCR 文件理解</li>
<li>使用 <a href="https://huggingface.co/google/tapas-base">TAPAS</a> 進行表格問答</li>
<li>使用 <a href="https://huggingface.co/BAAI/Emu3-Gen">Emu3</a> 實現統一多模態理解與生成</li>
<li>使用 <a href="https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf">Llava-OneVision</a> 進行視覺轉文字</li>
<li>使用 <a href="https://huggingface.co/llava-hf/llava-1.5-7b-hf">Llava</a> 進行視覺問答</li>
<li>使用 <a href="https://huggingface.co/microsoft/kosmos-2-patch14-224">Kosmos-2</a> 進行視覺指稱表達分割</li>
</ul>
</details>
<details>
<summary>NLP</summary>
<ul>
<li>使用 <a href="https://huggingface.co/answerdotai/ModernBERT-base">ModernBERT</a> 進行遮罩詞補全</li>
<li>使用 <a href="https://huggingface.co/google/gemma-2-2b">Gemma</a> 進行命名實體辨識</li>
<li>使用 <a href="https://huggingface.co/mistralai/Mixtral-8x7B-v0.1">Mixtral</a> 進行問答</li>
<li>使用 <a href="https://huggingface.co/facebook/bart-large-cnn">BART</a> 進行摘要生成</li>
<li>使用 <a href="https://huggingface.co/google-t5/t5-base">T5</a> 進行翻譯</li>
<li>使用 <a href="https://huggingface.co/meta-llama/Llama-3.2-1B">Llama</a> 進行文本生成</li>
<li>使用 <a href="https://huggingface.co/Qwen/Qwen2.5-0.5B">Qwen</a> 進行文本分類</li>
</ul>
</details>
<h2>參考文獻</h2>
<p>我們已發表一篇有關 🤗 Transformers 函式庫的 <a href="https://www.aclweb.org/anthology/2020.emnlp-demos.6/">論文</a>，歡迎引用：</p>
<pre><code class="language-bibtex">@inproceedings{wolf-etal-2020-transformers,
    title = &quot;Transformers: State-of-the-Art Natural Language Processing&quot;,
    author = &quot;Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush&quot;,
    booktitle = &quot;Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations&quot;,
    month = oct,
    year = &quot;2020&quot;,
    address = &quot;Online&quot;,
    publisher = &quot;Association for Computational Linguistics&quot;,
    url = &quot;https://www.aclweb.org/anthology/2020.emnlp-demos.6&quot;,
    pages = &quot;38--45&quot;
}
</code></pre>
<hr />
<p><a href="https://github.com/OpenAiTx/OpenAiTx">Powered By OpenAiTx</a></p>
<hr />

        </div>
    </div>
    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Default Statcounter code for openaitx
https://openaitx.github.io/ -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>
    <!-- End of Statcounter Code -->
</body>
</html>