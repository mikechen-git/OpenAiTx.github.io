<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>transformers - huggingface/transformers</title>
    <meta name="title" content="transformers - huggingface/transformers">
    <meta name="description" content="huggingface/transformers - GitHub repository fr documentation and informationEnglish | 简体中文 | 繁體中文 | 한국어 | Español | 日本語 | हिन्दी | Русский | Рortuguês | తెలుగు | Français | Deutsch | Tiếng Việt | العربية | اردو | Modèles préentraînés à ...">
    <meta name="keywords" content="huggingface, transformers, GitHub, repository, fr documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/huggingface/transformers/README-fr.html">
    <meta property="og:title" content="transformers - huggingface/transformers">
    <meta property="og:description" content="huggingface/transformers - GitHub repository fr documentation and information">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">
    <script src="/js/marked.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        body { display: flex; flex-direction: column; min-height: 100vh; }
        .main-container { margin: 0 auto; width: 100%; max-width: 980px; padding: 0 20px; }
        @media (max-width: 768px) { .main-container { padding: 0 15px; } }
        .markdown-body img { max-width: 100%; height: auto; }
        .header { text-align: center; margin-bottom: 30px; padding: 20px; background-color: #f6f8fa; border-bottom: 1px solid #e1e4e8; position: relative; }
        .header .links { margin-top: 10px; font-size: 16px; }
        .header .links a { color: #0366d6; text-decoration: none; margin-left: 5px; }
        .header .links a:hover { text-decoration: underline; }
        .language-badges { margin-top: 15px; text-align: center; }
        .language-badges a { display: inline-block; margin: 2px; text-decoration: none; }
        .language-badges img { height: 20px; border-radius: 3px; }
        .language-badges a:hover img { opacity: 0.8; }
    </style>
</head>
<body>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/huggingface/transformers" id="githubRepoLink" target="_blank">huggingface/transformers</a>
<h1 style="display: none;">English | 简体中文 | 繁體中文 | 한국어 | Español | 日本語 | हिन्दी | Русский | Рortuguês | తెలుగు | Français | Deutsch | Tiếng Việt | العربية | اردو | Modèles préentraînés à ...</h1>
        </div>
        <div class="language-badges" id="languageBadges">
            <!-- You can generate language badges here if needed -->
        </div>
    </div>
    <div class="main-container">
        <div class="markdown-body" id="content">
            <!---
Copyright 2020 The HuggingFace Team. Tous droits réservés.

Sous licence Apache License, Version 2.0 (la "Licence");
vous ne pouvez pas utiliser ce fichier sauf en conformité avec la Licence.
Vous pouvez obtenir une copie de la Licence à l'adresse suivante :

    http://www.apache.org/licenses/LICENSE-2.0

Sauf si la loi l'exige ou sauf accord écrit, le logiciel distribué sous la Licence est distribué "EN L'ÉTAT",
SANS GARANTIE OU CONDITION D'AUCUNE SORTE, expresse ou implicite.
Voir la Licence pour connaître la langue spécifique régissant les permissions et limitations sous la Licence.
-->
<p align="center">
  <picture>
    <source media="(prefers-color-scheme: dark)" srcset="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-dark.svg">
    <source media="(prefers-color-scheme: light)" srcset="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg">
    <img alt="Bibliothèque Hugging Face Transformers" src="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg" width="352" height="59" style="max-width: 100%;">
  </picture>
  <br/>
  <br/>
</p>
<p align="center">
    <a href="https://huggingface.com/models"><img alt="Points de contrôle sur le Hub" src="https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&color=brightgreen"></a>
    <a href="https://circleci.com/gh/huggingface/transformers"><img alt="Build" src="https://img.shields.io/circleci/build/github/huggingface/transformers/main"></a>
    <a href="https://github.com/huggingface/transformers/blob/main/LICENSE"><img alt="GitHub" src="https://img.shields.io/github/license/huggingface/transformers.svg?color=blue"></a>
    <a href="https://huggingface.co/docs/transformers/index"><img alt="Documentation" src="https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online"></a>
    <a href="https://github.com/huggingface/transformers/releases"><img alt="GitHub release" src="https://img.shields.io/github/release/huggingface/transformers.svg"></a>
    <a href="https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md"><img alt="Contributor Covenant" src="https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg"></a>
    <a href="https://zenodo.org/badge/latestdoi/155220641"><img src="https://zenodo.org/badge/155220641.svg" alt="DOI"></a>
</p>
<h4 align="center">
    <p>
        <b>English</b> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hans.md">简体中文</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hant.md">繁體中文</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ko.md">한국어</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_es.md">Español</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ja.md">日本語</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_hd.md">हिन्दी</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ru.md">Русский</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_pt-br.md">Рortuguês</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_te.md">తెలుగు</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_fr.md">Français</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_de.md">Deutsch</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_vi.md">Tiếng Việt</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ar.md">العربية</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ur.md">اردو</a> |
    </p>
</h4>
<h3 align="center">
    <p>Modèles préentraînés à la pointe de la technologie pour l’inférence et l’entraînement</p>
</h3>
<h3 align="center">
    <a href="https://hf.co/course"><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/course_banner.png"></a>
</h3>
<p>Transformers est une bibliothèque de modèles préentraînés pour le texte, la vision par ordinateur, l’audio, la vidéo et le multimodal, dédiée à l’inférence et à l’entraînement. Utilisez Transformers pour ajuster les modèles sur vos données, construire des applications d’inférence et pour des cas d’usage d’IA générative sur de multiples modalités.</p>
<p>Il existe plus de 500 000 <a href="https://huggingface.co/models?library=transformers&amp;sort=trending">points de contrôle de modèles</a> Transformers sur le <a href="https://huggingface.com/models">Hub Hugging Face</a> que vous pouvez utiliser.</p>
<p>Explorez dès aujourd’hui le <a href="https://huggingface.com/">Hub</a> pour trouver un modèle et utiliser Transformers pour démarrer immédiatement.</p>
<h2>Installation</h2>
<p>Transformers fonctionne avec Python 3.9+, <a href="https://pytorch.org/get-started/locally/">PyTorch</a> 2.1+, <a href="https://www.tensorflow.org/install/pip">TensorFlow</a> 2.6+ et <a href="https://flax.readthedocs.io/en/latest/">Flax</a> 0.4.1+.</p>
<p>Créez et activez un environnement virtuel avec <a href="https://docs.python.org/3/library/venv.html">venv</a> ou <a href="https://docs.astral.sh/uv/">uv</a>, un gestionnaire de paquets et de projets Python rapide basé sur Rust.</p>
<pre><code class="language-py"># venv
python -m venv .my-env
source .my-env/bin/activate
# uv
uv venv .my-env
source .my-env/bin/activate
</code></pre>
<p>Installez Transformers dans votre environnement virtuel.</p>
<pre><code class="language-py"># pip
pip install &quot;transformers[torch]&quot;

# uv
uv pip install &quot;transformers[torch]&quot;
</code></pre>
<p>Installez Transformers à partir de la source si vous souhaitez bénéficier des dernières modifications de la bibliothèque ou contribuer au projet. Cependant, la version <em>la plus récente</em> peut ne pas être stable. N’hésitez pas à ouvrir une <a href="https://github.com/huggingface/transformers/issues">issue</a> si vous rencontrez une erreur.</p>
<pre><code class="language-shell">git clone https://github.com/huggingface/transformers.git
cd transformers

# pip
pip install .[torch]

# uv
uv pip install .[torch]
</code></pre>
<h2>Démarrage rapide</h2>
<p>Commencez avec Transformers dès maintenant avec l’API <a href="https://huggingface.co/docs/transformers/pipeline_tutorial">Pipeline</a>. Le <code>Pipeline</code> est une classe d’inférence de haut niveau prenant en charge les tâches de texte, d’audio, de vision et multimodales. Il gère le prétraitement de l’entrée et retourne la sortie appropriée.</p>
<p>Instanciez un pipeline et spécifiez le modèle à utiliser pour la génération de texte. Le modèle est téléchargé et mis en cache, ce qui vous permet de le réutiliser facilement. Enfin, fournissez un texte pour amorcer le modèle.</p>
<pre><code class="language-py">from transformers import pipeline

pipeline = pipeline(task=&quot;text-generation&quot;, model=&quot;Qwen/Qwen2.5-1.5B&quot;)
pipeline(&quot;the secret to baking a really good cake is &quot;)
[{'generated_text': 'the secret to baking a really good cake is 1) to use the right ingredients and 2) to follow the recipe exactly. the recipe for the cake is as follows: 1 cup of sugar, 1 cup of flour, 1 cup of milk, 1 cup of butter, 1 cup of eggs, 1 cup of chocolate chips. if you want to make 2 cakes, how much sugar do you need? To make 2 cakes, you will need 2 cups of sugar.'}]
</code></pre>
<p>Pour discuter avec un modèle, le schéma d’utilisation est le même. La seule différence est que vous devez construire un historique de conversation (l’entrée de <code>Pipeline</code>) entre vous et le système.</p>
<blockquote>
<p>[!TIP]
Vous pouvez également discuter avec un modèle directement depuis la ligne de commande.</p>
<pre><code class="language-shell">transformers chat Qwen/Qwen2.5-0.5B-Instruct
</code></pre>
</blockquote>
<pre><code class="language-py">import torch
from transformers import pipeline

chat = [
    {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;Vous êtes un robot impertinent et plein d'esprit, tel qu’imaginé par Hollywood vers 1986.&quot;},
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Salut, peux-tu me dire des choses amusantes à faire à New York ?&quot;}
]

pipeline = pipeline(task=&quot;text-generation&quot;, model=&quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;, torch_dtype=torch.bfloat16, device_map=&quot;auto&quot;)
response = pipeline(chat, max_new_tokens=512)
print(response[0][&quot;generated_text&quot;][-1][&quot;content&quot;])
</code></pre>
<p>Développez les exemples ci-dessous pour voir comment <code>Pipeline</code> fonctionne pour différentes modalités et tâches.</p>
<details>
<summary>Reconnaissance automatique de la parole</summary>
<pre><code class="language-py">from transformers import pipeline

pipeline = pipeline(task=&quot;automatic-speech-recognition&quot;, model=&quot;openai/whisper-large-v3&quot;)
pipeline(&quot;https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac&quot;)
{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}
</code></pre>
</details>
<details>
<summary>Classification d'images</summary>
<h3 align="center">
    <a><img src="https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png"></a>
</h3>
<pre><code class="language-py">from transformers import pipeline

pipeline = pipeline(task=&quot;image-classification&quot;, model=&quot;facebook/dinov2-small-imagenet1k-1-layer&quot;)
pipeline(&quot;https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png&quot;)
[{'label': 'macaw', 'score': 0.997848391532898},
 {'label': 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',
  'score': 0.0016551691805943847},
 {'label': 'lorikeet', 'score': 0.00018523589824326336},
 {'label': 'African grey, African gray, Psittacus erithacus',
  'score': 7.85409429227002e-05},
 {'label': 'quail', 'score': 5.502637941390276e-05}]
</code></pre>
</details>
<details>
<summary>Questions-réponses visuelles</summary>
<h3 align="center">
    <a><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg"></a>
</h3>
<pre><code class="language-py">from transformers import pipeline

pipeline = pipeline(task=&quot;visual-question-answering&quot;, model=&quot;Salesforce/blip-vqa-base&quot;)
pipeline(
    image=&quot;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg&quot;,
    question=&quot;What is in the image?&quot;,
)
[{'answer': 'statue of liberty'}]
</code></pre>
</details>
<h2>Pourquoi utiliser Transformers ?</h2>
<ol>
<li><p>Des modèles à la pointe de la technologie faciles à utiliser :</p>
<ul>
<li>Hautes performances en compréhension et génération du langage naturel, vision par ordinateur, audio, vidéo et tâches multimodales.</li>
<li>Faible barrière à l’entrée pour les chercheurs, ingénieurs et développeurs.</li>
<li>Peu d’abstractions côté utilisateur, seulement trois classes à apprendre.</li>
<li>Une API unifiée pour utiliser tous nos modèles préentraînés.</li>
</ul>
</li>
<li><p>Réduction des coûts de calcul et de l’empreinte carbone :</p>
<ul>
<li>Partagez des modèles entraînés au lieu d’entraîner depuis zéro.</li>
<li>Réduisez le temps de calcul et les coûts de production.</li>
<li>Des dizaines d’architectures de modèles avec plus de 1M de points de contrôle préentraînés toutes modalités confondues.</li>
</ul>
</li>
<li><p>Choisissez le bon framework à chaque étape du cycle de vie d’un modèle :</p>
<ul>
<li>Entraînez des modèles à la pointe de la technologie en 3 lignes de code.</li>
<li>Déplacez un modèle unique entre les frameworks PyTorch/JAX/TF2.0 à volonté.</li>
<li>Choisissez le bon framework pour l’entraînement, l’évaluation et la production.</li>
</ul>
</li>
<li><p>Personnalisez facilement un modèle ou un exemple selon vos besoins :</p>
<ul>
<li>Nous fournissons des exemples pour chaque architecture afin de reproduire les résultats publiés par leurs auteurs d’origine.</li>
<li>Les internals des modèles sont exposés aussi uniformément que possible.</li>
<li>Les fichiers de modèles peuvent être utilisés indépendamment de la bibliothèque pour des expérimentations rapides.</li>
</ul>
</li>
</ol>
<a target="_blank" href="https://huggingface.co/enterprise">
    <img alt="Hugging Face Enterprise Hub" src="https://github.com/user-attachments/assets/247fb16d-d251-4583-96c4-d3d76dda4925">
</a><br>
<h2>Pourquoi ne pas utiliser Transformers ?</h2>
<ul>
<li>Cette bibliothèque n’est pas une boîte à outils modulaire de briques de construction pour les réseaux neuronaux. Le code des fichiers de modèles n’est pas refactorisé avec des abstractions supplémentaires, afin que les chercheurs puissent itérer rapidement sur chaque modèle sans avoir à naviguer dans des abstractions/fichiers supplémentaires.</li>
<li>L’API d’entraînement est optimisée pour fonctionner avec les modèles PyTorch fournis par Transformers. Pour des boucles de machine learning génériques, vous devriez utiliser une autre bibliothèque comme <a href="https://huggingface.co/docs/accelerate">Accelerate</a>.</li>
<li>Les <a href="(https://github.com/huggingface/transformers/tree/main/examples)">scripts d’exemple</a> sont seulement <em>des exemples</em>. Ils peuvent ne pas fonctionner immédiatement pour votre cas d’usage spécifique et vous devrez adapter le code pour qu’il fonctionne.</li>
</ul>
<h2>100 projets utilisant Transformers</h2>
<p>Transformers est plus qu’un ensemble d’outils pour utiliser des modèles préentraînés, c’est une communauté de projets construits autour de lui et du Hub Hugging Face. Nous voulons que Transformers permette aux développeurs, chercheurs, étudiants, professeurs, ingénieurs et toute autre personne de réaliser leurs projets de rêve.</p>
<p>Pour célébrer les 100 000 étoiles de Transformers, nous avons voulu mettre en avant la communauté avec la page <a href="./awesome-transformers.md">awesome-transformers</a> qui recense 100 projets incroyables construits avec Transformers.</p>
<p>Si vous possédez ou utilisez un projet qui, selon vous, devrait faire partie de la liste, n’hésitez pas à ouvrir une PR pour l’ajouter !</p>
<h2>Exemples de modèles</h2>
<p>Vous pouvez tester la plupart de nos modèles directement sur leurs <a href="https://huggingface.co/models">pages modèles du Hub</a>.</p>
<p>Développez chaque modalité ci-dessous pour voir quelques exemples de modèles pour différents cas d’utilisation.</p>
<details>
<summary>Audio</summary>
<ul>
<li>Classification audio avec <a href="https://huggingface.co/openai/whisper-large-v3-turbo">Whisper</a></li>
<li>Reconnaissance automatique de la parole avec <a href="https://huggingface.co/UsefulSensors/moonshine">Moonshine</a></li>
<li>Détection de mots-clés avec <a href="https://huggingface.co/superb/wav2vec2-base-superb-ks">Wav2Vec2</a></li>
<li>Génération parole à parole avec <a href="https://huggingface.co/kyutai/moshiko-pytorch-bf16">Moshi</a></li>
<li>Texte vers audio avec <a href="https://huggingface.co/facebook/musicgen-large">MusicGen</a></li>
<li>Texte vers parole avec <a href="https://huggingface.co/suno/bark">Bark</a></li>
</ul>
</details>
<details>
<summary>Vision par ordinateur</summary>
<ul>
<li>Génération automatique de masque avec <a href="https://huggingface.co/facebook/sam-vit-base">SAM</a></li>
<li>Estimation de profondeur avec <a href="https://huggingface.co/apple/DepthPro-hf">DepthPro</a></li>
<li>Classification d’images avec <a href="https://huggingface.co/facebook/dinov2-base">DINO v2</a></li>
<li>Détection de points clés avec <a href="https://huggingface.co/magic-leap-community/superglue_outdoor">SuperGlue</a></li>
<li>Appariement de points clés avec <a href="https://huggingface.co/magic-leap-community/superglue">SuperGlue</a></li>
<li>Détection d’objets avec <a href="https://huggingface.co/PekingU/rtdetr_v2_r50vd">RT-DETRv2</a></li>
<li>Estimation de pose avec <a href="https://huggingface.co/usyd-community/vitpose-base-simple">VitPose</a></li>
<li>Segmentation universelle avec <a href="https://huggingface.co/shi-labs/oneformer_ade20k_swin_large">OneFormer</a></li>
<li>Classification vidéo avec <a href="https://huggingface.co/MCG-NJU/videomae-large">VideoMAE</a></li>
</ul>
</details>
<details>
<summary>Multimodal</summary>
<ul>
<li>Audio ou texte vers texte avec <a href="https://huggingface.co/Qwen/Qwen2-Audio-7B">Qwen2-Audio</a></li>
<li>Questions-réponses sur documents avec <a href="https://huggingface.co/microsoft/layoutlmv3-base">LayoutLMv3</a></li>
<li>Image ou texte vers texte avec <a href="https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct">Qwen-VL</a></li>
<li>Génération de légendes d’images avec <a href="https://huggingface.co/Salesforce/blip2-opt-2.7b">BLIP-2</a></li>
<li>Compréhension de documents basée OCR avec <a href="https://huggingface.co/stepfun-ai/GOT-OCR-2.0-hf">GOT-OCR2</a></li>
<li>Questions-réponses sur tableaux avec <a href="https://huggingface.co/google/tapas-base">TAPAS</a></li>
<li>Compréhension et génération multimodale unifiée avec <a href="https://huggingface.co/BAAI/Emu3-Gen">Emu3</a></li>
<li>Vision vers texte avec <a href="https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf">Llava-OneVision</a></li>
<li>Questions-réponses visuelles avec <a href="https://huggingface.co/llava-hf/llava-1.5-7b-hf">Llava</a></li>
<li>Segmentation d’expression référentielle visuelle avec <a href="https://huggingface.co/microsoft/kosmos-2-patch14-224">Kosmos-2</a></li>
</ul>
</details>
<details>
<summary>TALN</summary>
<ul>
<li>Complétion de mots masqués avec <a href="https://huggingface.co/answerdotai/ModernBERT-base">ModernBERT</a></li>
<li>Reconnaissance d’entités nommées avec <a href="https://huggingface.co/google/gemma-2-2b">Gemma</a></li>
<li>Questions-réponses avec <a href="https://huggingface.co/mistralai/Mixtral-8x7B-v0.1">Mixtral</a></li>
<li>Résumé avec <a href="https://huggingface.co/facebook/bart-large-cnn">BART</a></li>
<li>Traduction avec <a href="https://huggingface.co/google-t5/t5-base">T5</a></li>
<li>Génération de texte avec <a href="https://huggingface.co/meta-llama/Llama-3.2-1B">Llama</a></li>
<li>Classification de texte avec <a href="https://huggingface.co/Qwen/Qwen2.5-0.5B">Qwen</a></li>
</ul>
</details>
<h2>Citation</h2>
<p>Nous avons désormais un <a href="https://www.aclweb.org/anthology/2020.emnlp-demos.6/">article</a> que vous pouvez citer pour la bibliothèque 🤗 Transformers :</p>
<pre><code class="language-bibtex">@inproceedings{wolf-etal-2020-transformers,
    title = &quot;Transformers: State-of-the-Art Natural Language Processing&quot;,
    author = &quot;Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush&quot;,
    booktitle = &quot;Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations&quot;,
    month = oct,
    year = &quot;2020&quot;,
    address = &quot;Online&quot;,
    publisher = &quot;Association for Computational Linguistics&quot;,
    url = &quot;https://www.aclweb.org/anthology/2020.emnlp-demos.6&quot;,
    pages = &quot;38--45&quot;
}
</code></pre>
<hr />
<p><a href="https://github.com/OpenAiTx/OpenAiTx">Powered By OpenAiTx</a></p>
<hr />

        </div>
    </div>
    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Default Statcounter code for openaitx
https://openaitx.github.io/ -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>
    <!-- End of Statcounter Code -->
</body>
</html>