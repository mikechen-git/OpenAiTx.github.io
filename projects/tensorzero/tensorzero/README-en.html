<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>tensorzero - tensorzero/tensorzero</title>
    <meta name="title" content="tensorzero - tensorzero/tensorzero">
    <meta name="description" content="tensorzero/tensorzero - GitHub repository en documentation and informationTensorZero TensorZero creates a feedback loop for optimizing LLM applications — turning production data into smarter, faster, and cheaper models. Integrate our ...">
    <meta name="keywords" content="tensorzero, tensorzero, GitHub, repository, en documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/tensorzero/tensorzero/README-en.html">
    <meta property="og:title" content="tensorzero - tensorzero/tensorzero">
    <meta property="og:description" content="tensorzero/tensorzero - GitHub repository en documentation and information">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">
    <script src="/js/marked.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        body { display: flex; flex-direction: column; min-height: 100vh; }
        .main-container { margin: 0 auto; width: 100%; max-width: 980px; padding: 0 20px; }
        @media (max-width: 768px) { .main-container { padding: 0 15px; } }
        .markdown-body img { max-width: 100%; height: auto; }
        .header { text-align: center; margin-bottom: 30px; padding: 20px; background-color: #f6f8fa; border-bottom: 1px solid #e1e4e8; position: relative; }
        .header .links { margin-top: 10px; font-size: 16px; }
        .header .links a { color: #0366d6; text-decoration: none; margin-left: 5px; }
        .header .links a:hover { text-decoration: underline; }
        .language-badges { margin-top: 15px; text-align: center; }
        .language-badges a { display: inline-block; margin: 2px; text-decoration: none; }
        .language-badges img { height: 20px; border-radius: 3px; }
        .language-badges a:hover img { opacity: 0.8; }
    </style>
</head>
<body>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/tensorzero/tensorzero" id="githubRepoLink" target="_blank">tensorzero/tensorzero</a>
<h1 style="display: none;">TensorZero TensorZero creates a feedback loop for optimizing LLM applications — turning production data into smarter, faster, and cheaper models. Integrate our ...</h1>
        </div>
        <div class="language-badges" id="languageBadges">
            <!-- You can generate language badges here if needed -->
        </div>
    </div>
    <div class="main-container">
        <div class="markdown-body" id="content">
            <img src="https://github.com/user-attachments/assets/47d67430-386d-4675-82ad-d4734d3262d9" width=128 height=128>
<h1>TensorZero</h1>
<p><strong>TensorZero creates a feedback loop for optimizing LLM applications — turning production data into smarter, faster, and cheaper models.</strong></p>
<ol>
<li>Integrate our model gateway</li>
<li>Send metrics or feedback</li>
<li>Optimize prompts, models, and inference strategies</li>
<li>Watch your LLMs improve over time</li>
</ol>
<p>It provides a <strong>data &amp; learning flywheel for LLMs</strong> by unifying:</p>
<ul>
<li>[x] <strong>Inference:</strong> one API for all LLMs, with &lt;1ms P99 overhead</li>
<li>[x] <strong>Observability:</strong> inference &amp; feedback → your database</li>
<li>[x] <strong>Optimization:</strong> from prompts to fine-tuning and RL</li>
<li>[x] <strong>Evaluations:</strong> compare prompts, models, inference strategies</li>
<li>[x] <strong>Experimentation:</strong> built-in A/B testing, routing, fallbacks</li>
</ul>
<hr />
<p align="center">
  <b><a href="https://www.tensorzero.com/" target="_blank">Website</a></b>
  ·
  <b><a href="https://www.tensorzero.com/docs" target="_blank">Docs</a></b>
  ·
  <b><a href="https://www.x.com/tensorzero" target="_blank">Twitter</a></b>
  ·
  <b><a href="https://www.tensorzero.com/slack" target="_blank">Slack</a></b>
  ·
  <b><a href="https://www.tensorzero.com/discord" target="_blank">Discord</a></b>
  <br>
  <br>
  <b><a href="https://www.tensorzero.com/docs/quickstart" target="_blank">Quick Start (5min)</a></b>
  ·
  <b><a href="https://www.tensorzero.com/docs/gateway/tutorial" target="_blank">Comprehensive Tutorial</a></b>
  ·
  <b><a href="https://www.tensorzero.com/docs/gateway/deployment" target="_blank">Deployment Guide</a></b>
  ·
  <b><a href="https://www.tensorzero.com/docs/gateway/api-reference" target="_blank">API Reference</a></b>
  ·
  <b><a href="https://www.tensorzero.com/docs/gateway/deployment" target="_blank">Configuration Reference</a></b>
</p>
<hr />
<table>
  <tr>
    <td width="30%" valign="top"><b>What is TensorZero?</b></td>
    <td width="70%" valign="top">TensorZero is an open-source framework for building production-grade LLM applications. It unifies an LLM gateway, observability, optimization, evaluations, and experimentation.</td>
  </tr>
  <tr>
    <td width="30%" valign="top"><b>How is TensorZero different from other LLM frameworks?</b></td>
    <td width="70%" valign="top">
      1. TensorZero enables you to optimize complex LLM applications based on production metrics and human feedback.<br>
      2. TensorZero supports the needs of industrial-scale LLM applications: low latency, high throughput, type safety, self-hosted, GitOps, customizability, etc.<br>
      3. TensorZero unifies the entire LLMOps stack, creating compounding benefits. For example, LLM evaluations can be used for fine-tuning models alongside AI judges.
    </td>
  </tr>
  <tr>
    <td width="30%" valign="top"><b>Can I use TensorZero with ___?</b></td>
    <td width="70%" valign="top">Yes. Every major programming language is supported. You can use TensorZero with our Python client, any OpenAI SDK, or our HTTP API.</td>
  </tr>
  <tr>
    <td width="30%" valign="top"><b>Is TensorZero production-ready?</b></td>
    <td width="70%" valign="top">Yes. Here's a case study: <b><a href="https://www.tensorzero.com/blog/case-study-automating-code-changelogs-at-a-large-bank-with-llms">Automating Code Changelogs at a Large Bank with LLMs</a></b></td>
  </tr>
  <tr>
    <td width="30%" valign="top"><b>How much does TensorZero cost?</b></td>
    <td width="70%" valign="top">Nothing. TensorZero is 100% self-hosted and open-source. There are no paid features.</td>
  </tr>
  <tr>
    <td width="30%" valign="top"><b>Who is building TensorZero?</b></td>
    <td width="70%" valign="top">Our technical team includes a former Rust compiler maintainer, machine learning researchers (Stanford, CMU, Oxford, Columbia) with thousands of citations, and the chief product officer of a decacorn startup. We're backed by the same investors as leading open-source projects (e.g. ClickHouse, CockroachDB) and AI labs (e.g. OpenAI, Anthropic).</td>
  </tr>
  <tr>
    <td width="30%" valign="top"><b>How do I get started?</b></td>
    <td width="70%" valign="top">You can adopt TensorZero incrementally. Our <b><a href="https://www.tensorzero.com/docs/quickstart">Quick Start</a></b> goes from a vanilla OpenAI wrapper to a production-ready LLM application with observability and fine-tuning in just 5 minutes.</td>
  </tr>
</table>
<hr />
<h2>Features</h2>
<h3>🌐 LLM Gateway</h3>
<blockquote>
<p><strong>Integrate with TensorZero once and access every major LLM provider.</strong></p>
</blockquote>
<table>
  <tr></tr> <!-- flip highlight order -->
  <tr>
    <td width="50%" align="center" valign="middle"><b>Model Providers</b></td>
    <td width="50%" align="center" valign="middle"><b>Features</b></td>
  </tr>
  <tr>
    <td width="50%" align="left" valign="top">
      <p>
        The TensorZero Gateway natively supports:
      </p>
      <ul>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/anthropic">Anthropic</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/aws-bedrock">AWS Bedrock</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/aws-sagemaker">AWS SageMaker</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/azure">Azure OpenAI Service</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/deepseek">DeepSeek</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/fireworks">Fireworks</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/gcp-vertex-ai-anthropic">GCP Vertex AI Anthropic</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/gcp-vertex-ai-gemini">GCP Vertex AI Gemini</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/google-ai-studio-gemini">Google AI Studio (Gemini API)</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/hyperbolic">Hyperbolic</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/mistral">Mistral</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/openai">OpenAI</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/together">Together</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/vllm">vLLM</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/xai">xAI</a></b></li>
      </ul>
      <p>
        <em>
          Need something else?
          Your provider is most likely supported because TensorZero integrates with <b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/openai-compatible">any OpenAI-compatible API (e.g. Ollama)</a></b>.
          </em>
      </p>
    </td>
    <td width="50%" align="left" valign="top">
      <p>
        The TensorZero Gateway supports advanced features like:
      </p>
      <ul>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/retries-fallbacks">Retries & Fallbacks</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations">Inference-Time Optimizations</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/prompt-templates-schemas">Prompt Templates & Schemas</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/tutorial#experimentation">Experimentation (A/B Testing)</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/configuration-reference">Configuration-as-Code (GitOps)</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/batch-inference">Batch Inference</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/multimodal-inference">Multimodal Inference (VLMs)</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/inference-caching">Inference Caching</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/metrics-feedback">Metrics & Feedback</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/episodes">Multi-Step LLM Workflows (Episodes)</a></b></li>
        <li><em>& a lot more...</em></li>
      </ul>
      <p>
        The TensorZero Gateway is written in Rust 🦀 with <b>performance</b> in mind (&lt;1ms p99 latency overhead @ 10k QPS).
        See <b><a href="https://www.tensorzero.com/docs/gateway/benchmarks">Benchmarks</a></b>.<br>
      </p>
      <p>
        You can run inference using the <b>TensorZero client</b> (recommended), the <b>OpenAI client</b>, or the <b>HTTP API</b>.
      </p>
    </td>
  </tr>
</table>
<br>
<details open>
<summary><b>Usage: Python &mdash; TensorZero Client (Recommended)</b></summary>
<p>You can access any provider using the TensorZero Python client.</p>
<ol>
<li><code>pip install tensorzero</code></li>
<li>Optional: Set up the TensorZero configuration.</li>
<li>Run inference:</li>
</ol>
<pre><code class="language-python">from tensorzero import TensorZeroGateway  # or AsyncTensorZeroGateway


with TensorZeroGateway.build_embedded(clickhouse_url=&quot;...&quot;, config_file=&quot;...&quot;) as client:
    response = client.inference(
        model_name=&quot;openai::gpt-4o-mini&quot;,
        # Try other providers easily: &quot;anthropic::claude-3-7-sonnet-20250219&quot;
        input={
            &quot;messages&quot;: [
                {
                    &quot;role&quot;: &quot;user&quot;,
                    &quot;content&quot;: &quot;Write a haiku about artificial intelligence.&quot;,
                }
            ]
        },
    )
</code></pre>
<p>See <strong><a href="https://www.tensorzero.com/docs/quickstart">Quick Start</a></strong> for more information.</p>
</details>
<details>
<summary><b>Usage: Python &mdash; OpenAI Client</b></summary>
<p>You can access any provider using the OpenAI Python client with TensorZero.</p>
<ol>
<li><code>pip install tensorzero</code></li>
<li>Optional: Set up the TensorZero configuration.</li>
<li>Run inference:</li>
</ol>
<pre><code class="language-python">from openai import OpenAI  # or AsyncOpenAI
from tensorzero import patch_openai_client

client = OpenAI()
</code></pre>
<p>patch_openai_client(
client,
clickhouse_url=&quot;http://chuser:chpassword@localhost:8123/tensorzero&quot;,
config_file=&quot;config/tensorzero.toml&quot;,
async_setup=False,
)</p>
<p>response = client.chat.completions.create(
model=&quot;tensorzero::model_name::openai::gpt-4o-mini&quot;,
# Try other providers easily: &quot;tensorzero::model_name::anthropic::claude-3-7-sonnet-20250219&quot;
messages=[
{
&quot;role&quot;: &quot;user&quot;,
&quot;content&quot;: &quot;Write a haiku about artificial intelligence.&quot;,
}
],
)</p>
<pre><code>
See **[Quick Start](https://www.tensorzero.com/docs/quickstart)** for more information.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Usage: JavaScript / TypeScript (Node) &amp;mdash; OpenAI Client&lt;/b&gt;&lt;/summary&gt;

You can access any provider using the OpenAI Node client with TensorZero.

1. Deploy `tensorzero/gateway` using Docker.
   **[Detailed instructions →](https://www.tensorzero.com/docs/gateway/deployment)**
2. Set up the TensorZero configuration.
3. Run inference:

```ts
import OpenAI from &quot;openai&quot;;

const client = new OpenAI({
  baseURL: &quot;http://localhost:3000/openai/v1&quot;,
});

const response = await client.chat.completions.create({
  model: &quot;tensorzero::model_name::openai::gpt-4o-mini&quot;,
  // Try other providers easily: &quot;tensorzero::model_name::anthropic::claude-3-7-sonnet-20250219&quot;
  messages: [
    {
      role: &quot;user&quot;,
      content: &quot;Write a haiku about artificial intelligence.&quot;,
    },
  ],
});
</code></pre>
<p>See <strong><a href="https://www.tensorzero.com/docs/quickstart">Quick Start</a></strong> for more information.</p>
</details>
<details>
<summary><b>Usage: Other Languages & Platforms &mdash; HTTP API</b></summary>
<p>TensorZero supports virtually any programming language or platform via its HTTP API.</p>
<ol>
<li>Deploy <code>tensorzero/gateway</code> using Docker.
<strong><a href="https://www.tensorzero.com/docs/gateway/deployment">Detailed instructions →</a></strong></li>
<li>Optional: Set up the TensorZero configuration.</li>
<li>Run inference:</li>
</ol>
<pre><code class="language-bash">curl -X POST &quot;http://localhost:3000/inference&quot; \
  -H &quot;Content-Type: application/json&quot; \
  -d '{
    &quot;model_name&quot;: &quot;openai::gpt-4o-mini&quot;,
    &quot;input&quot;: {
      &quot;messages&quot;: [
        {
          &quot;role&quot;: &quot;user&quot;,
          &quot;content&quot;: &quot;Write a haiku about artificial intelligence.&quot;
        }
      ]
    }
  }'
</code></pre>
<p>See <strong><a href="https://www.tensorzero.com/docs/quickstart">Quick Start</a></strong> for more information.</p>
</details>
<br>
<h3>📈 LLM Optimization</h3>
<blockquote>
<p><strong>Send production metrics and human feedback to easily optimize your prompts, models, and inference strategies — using the UI or programmatically.</strong></p>
</blockquote>
<h4>Model Optimization</h4>
<p>Optimize closed-source and open-source models using supervised fine-tuning (SFT) and preference fine-tuning (DPO).</p>
<table>
  <tr></tr> <!-- flip highlight order -->
  <tr>
    <td width="50%" align="center" valign="middle"><b>Supervised Fine-tuning &mdash; UI</b></td>
    <td width="50%" align="center" valign="middle"><b>Preference Fine-tuning (DPO) &mdash; Jupyter Notebook</b></td>
  </tr>
  <tr>
    <td width="50%" align="center" valign="middle"><img src="https://github.com/user-attachments/assets/cf7acf66-732b-43b3-af2a-5eba1ce40f6f"></td>
    <td width="50%" align="center" valign="middle"><img src="https://github.com/user-attachments/assets/a67a0634-04a7-42b0-b934-9130cb7cdf51"></td>
  </tr>
</table>
<h4>Inference-Time Optimization</h4>
<p>Boost performance by dynamically updating your prompts with relevant examples, combining responses from multiple inferences, and more.</p>
<table>
  <tr></tr> <!-- flip highlight order -->
  <tr>
    <td width="50%" align="center" valign="middle"><b><a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#best-of-n-sampling">Best-of-N Sampling</a></b></td>
    <td width="50%" align="center" valign="middle"><b><a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#mixture-of-n-sampling">Mixture-of-N Sampling</a></b></td>
  </tr>
  <tr>
    <td width="50%" align="center" valign="middle"><img src="https://github.com/user-attachments/assets/c0edfa4c-713c-4996-9964-50c0d26e6970"></td>
    <td width="50%" align="center" valign="middle"><img src="https://github.com/user-attachments/assets/75b5bf05-4c1f-43c4-b158-d69d1b8d05be"></td>
  </tr>
  <tr>
    <td width="50%" align="center" valign="middle"><b><a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#dynamic-in-context-learning-dicl">Dynamic In-Context Learning (DICL)</a></b></td>
    <td width="50%" align="center" valign="middle"><b><a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#chain-of-thought-cot">Chain-of-Thought (CoT)</a></b></td>
  </tr>
  <tr>
    <td width="50%" align="center" valign="middle"><img src="https://github.com/user-attachments/assets/d8489e92-ce93-46ac-9aab-289ce19bb67d"></td>
    <td width="50%" align="center" valign="middle"><img src="https://github.com/user-attachments/assets/ea13d73c-76a4-4e0c-a35b-0c648f898311" height="320"></td>
  </tr>
</table>
<p><em>More coming soon...</em></p>
<br>
<h4>Prompt Optimization</h4>
<p>Optimize your prompts programmatically using research-driven optimization techniques.</p>
<table>
  <tr></tr> <!-- flip highlight order -->
  <tr>
    <td width="50%" align="center" valign="middle"><b><a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#best-of-n-sampling">MIPROv2</a></b></td>
    <td width="50%" align="center" valign="middle"><b><a href="https://github.com/tensorzero/tensorzero/tree/main/examples/gsm8k-custom-recipe-dspy">DSPy Integration</a></b></td>
  </tr>
  <tr>
    <td width="50%" align="center" valign="middle"><img src="https://github.com/user-attachments/assets/d81a7c37-382f-4c46-840f-e6c2593301db" alt="MIPROv2 diagram"></td>
    <td width="50%" align="center" valign="middle">
      TensorZero comes with several optimization recipes, but you can also easily create your own.
      This example shows how to optimize a TensorZero function using an arbitrary tool — here, DSPy, a popular library for automated prompt engineering.
    </td>
  </tr>
</table>
<p><em>More coming soon...</em></p>
<br>
<h3>🔍 LLM Observability</h3>
<blockquote>
<p><strong>Zoom in to debug individual API calls, or zoom out to monitor metrics across models and prompts over time — all using the open-source TensorZero UI.</strong></p>
</blockquote>
<table>
  <tr></tr> <!-- flip highlight order -->
  <tr>
    <td width="50%" align="center" valign="middle"><b>Observability » Inference</b></td>
    <td width="50%" align="center" valign="middle"><b>Observability » Function</b></td>
  </tr>
  <tr>
    <td width="50%" align="center" valign="middle"><img src="https://github.com/user-attachments/assets/2cc3cc9a-f33f-4e94-b8de-07522326f80a"></td>
    <td width="50%" align="center" valign="middle"><img src="https://github.com/user-attachments/assets/00ae6605-8fa0-4efd-8238-ae8ea589860f"></td>
  </tr>
</table>
<br>
<h3>📊 LLM Evaluations</h3>
<blockquote>
<p><strong>Compare prompts, models, and inference strategies using TensorZero Evaluations — with support for heuristics and LLM judges.</strong></p>
</blockquote>
<table>
  <tr></tr> <!-- flip highlight order -->
  <tr>
    <td width="50%" align="center" valign="middle"><b>Evaluation » UI</b></td>
    <td width="50%" align="center" valign="middle"><b>Evaluation » CLI</b></td>
  </tr>
  <tr>
    <td width="50%" align="center" valign="middle"><img src="https://github.com/user-attachments/assets/f4bf54e3-1b63-46c8-be12-2eaabf615699"></td>
    <td width="50%" align="left" valign="middle">
<pre><code class="language-bash">docker compose run --rm evaluations \
  --evaluation-name extract_data \
  --dataset-name hard_test_cases \
  --variant-name gpt_4o \
  --concurrency 5</code></pre>
<pre><code class="language-bash">Run ID: 01961de9-c8a4-7c60-ab8d-15491a9708e4
Number of datapoints: 100
██████████████████████████████████████ 100/100
exact_match: 0.83 ± 0.03
semantic_match: 0.98 ± 0.01  
item_count: 7.15 ± 0.39</code></pre>
    </td>
  </tr>
</table>
<h2>Demo</h2>
<blockquote>
<p><strong>Watch LLMs improve in data extraction in real-time with TensorZero!</strong></p>
<p><strong><a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#dynamic-in-context-learning-dicl">Dynamic in-context learning (DICL)</a></strong> is a powerful inference-time optimization that comes out of the box with TensorZero.
It enhances LLM performance by automatically incorporating relevant historical examples into the prompt, without requiring model fine-tuning.</p>
</blockquote>
<p>https://github.com/user-attachments/assets/4df1022e-886e-48c2-8f79-6af3cdad79cb</p>
<h2>LLM Engineering with TensorZero</h2>
<br>
<p align="center" >
  <a href="https://www.tensorzero.com/docs">
    <picture>
      <source media="(prefers-color-scheme: light)" srcset="https://github.com/user-attachments/assets/34a92c18-242e-4d76-a99c-861283de68a6">
      <source media="(prefers-color-scheme: dark)" srcset="https://github.com/user-attachments/assets/e8bc699b-6378-4c2a-9cc1-6d189025e270">
      <img alt="TensorZero Flywheel" src="https://github.com/user-attachments/assets/34a92c18-242e-4d76-a99c-861283de68a6" width=720>
    </picture>
  </a>
</p>
<br>
<ol>
<li>The <strong><a href="https://www.tensorzero.com/docs/gateway/">TensorZero Gateway</a></strong> is a high-performance model gateway written in Rust 🦀 that provides a unified API interface for all major LLM providers, allowing for seamless cross-platform integration and fallbacks.</li>
<li>It handles structured schema-based inference with &lt;1ms P99 latency overhead (see <strong><a href="https://www.tensorzero.com/docs/gateway/benchmarks">Benchmarks</a></strong>) and built-in observability, experimentation, and <strong><a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations">inference-time optimizations</a></strong>.</li>
<li>It also collects downstream metrics and feedback associated with these inferences, with first-class support for multi-step LLM systems.</li>
<li>Everything is stored in a ClickHouse data warehouse that you control for real-time, scalable, and developer-friendly analytics.</li>
<li>Over time, <strong><a href="https://www.tensorzero.com/docs/recipes">TensorZero Recipes</a></strong> leverage this structured dataset to optimize your prompts and models: run pre-built recipes for common workflows like fine-tuning, or create your own with complete flexibility using any language and platform.</li>
<li>Finally, the gateway's experimentation features and GitOps orchestration enable you to iterate and deploy with confidence, whether it's a single LLM or thousands of LLMs.</li>
</ol>
<p>Our goal is to help engineers build, manage, and optimize the next generation of LLM applications: systems that learn from real-world experience.<br />
Read more about our <strong><a href="https://www.tensorzero.com/docs/vision-roadmap/">Vision &amp; Roadmap</a></strong>.</p>
<h2>Get Started</h2>
<p><strong>Start building today.</strong><br />
The <strong><a href="https://www.tensorzero.com/docs/quickstart">Quick Start</a></strong> shows how easy it is to set up an LLM application with TensorZero.<br />
If you want to dive deeper, the <strong><a href="https://www.tensorzero.com/docs/gateway/tutorial">Tutorial</a></strong> teaches how to build a simple chatbot, an email copilot, a weather RAG system, and a structured data extraction pipeline.</p>
<p><strong>Questions?</strong><br />
Ask us on <strong><a href="https://www.tensorzero.com/slack">Slack</a></strong> or <strong><a href="https://www.tensorzero.com/discord">Discord</a></strong>.</p>
<p><strong>Using TensorZero at work?</strong><br />
Email us at <strong><a href="mailto:hello@tensorzero.com">hello@tensorzero.com</a></strong> to set up a Slack or Teams channel with your team (free).</p>
<p><strong>Work with us.</strong><br />
We're <strong><a href="https://www.tensorzero.com/jobs">hiring in NYC</a></strong>.<br />
We also welcome <strong><a href="https://github.com/tensorzero/tensorzero/blob/main/CONTRIBUTING.md">open-source contributions</a></strong>!</p>
<h2>Examples</h2>
<p>We are working on a series of <strong>complete runnable examples</strong> illustrating TensorZero's data &amp; learning flywheel.</p>
<blockquote>
<p><strong><a href="https://raw.githubusercontent.com/tensorzero/tensorzero/main/examples/data-extraction-ner">Optimizing Data Extraction (NER) with TensorZero</a></strong></p>
<p>This example shows how to use TensorZero to optimize a data extraction pipeline.
We demonstrate techniques like fine-tuning and dynamic in-context learning (DICL).
In the end, an optimized GPT-4o Mini model outperforms GPT-4o on this task — at a fraction of the cost and latency — using a small amount of training data.</p>
</blockquote>
<blockquote>
<p><strong><a href="https://raw.githubusercontent.com/tensorzero/tensorzero/main/examples/rag-retrieval-augmented-generation/simple-agentic-rag/">Agentic RAG — Multi-Hop Question Answering with LLMs</a></strong></p>
<p>This example shows how to build a multi-hop retrieval agent using TensorZero.
The agent iteratively searches Wikipedia to gather information, and decides when it has enough context to answer a complex question.</p>
</blockquote>
<blockquote>
<p><strong><a href="https://raw.githubusercontent.com/tensorzero/tensorzero/main/examples/haiku-hidden-preferences">Writing Haikus to Satisfy a Judge with Hidden Preferences</a></strong></p>
<p>This example fine-tunes GPT-4o Mini to generate haikus tailored to a specific taste.
You'll see TensorZero's &quot;data flywheel in a box&quot; in action: better variants lead to better data, and better data lead to better variants.
You'll see progress by fine-tuning the LLM multiple times.</p>
</blockquote>
<blockquote>
<p><strong><a href="https://raw.githubusercontent.com/tensorzero/tensorzero/main/examples/chess-puzzles/">Improving LLM Chess Ability with Best-of-N Sampling</a></strong></p>
<p>This example showcases how best-of-N sampling can significantly enhance an LLM's chess-playing abilities by selecting the most promising moves from multiple generated options.</p>
</blockquote>
<blockquote>
<p><strong><a href="https://raw.githubusercontent.com/tensorzero/tensorzero/main/examples/gsm8k-custom-recipe-dspy">Improving Math Reasoning with a Custom Recipe for Automated Prompt Engineering (DSPy)</a></strong></p>
<p>TensorZero provides a number of pre-built optimization recipes covering common LLM engineering workflows.
But you can also easily create your own recipes and workflows!
This example shows how to optimize a TensorZero function using an arbitrary tool — here, DSPy.</p>
</blockquote>
<p><em>&amp; many more on the way!</em></p>
<hr />
<p>Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx">Open Ai Tx</a> | Last indexed: 2025-06-09</p>
<hr />

        </div>
    </div>
    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Statcounter and other scripts can be added here -->
</body>
</html>