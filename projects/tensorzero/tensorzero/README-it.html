<!DOCTYPE html>
<html lang="it">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>tensorzero - tensorzero/tensorzero it</title>
    <meta name="title" content="tensorzero - tensorzero/tensorzero it | TensorZero TensorZero crea un ciclo di feedback per ottimizzare le applicazioni LLM — trasformando i dati di produzione in modelli più intelligenti, veloci ed e...">
    <meta name="description" content="tensorzero/tensorzero - GitHub repository it documentation and information | TensorZero TensorZero crea un ciclo di feedback per ottimizzare le applicazioni LLM — trasformando i dati di produzione in modelli più intelligenti, veloci ed e...">
    <meta name="keywords" content="tensorzero, tensorzero, GitHub, repository, it documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/tensorzero/tensorzero/README-it.html">
    <meta property="og:title" content="tensorzero - tensorzero/tensorzero it | TensorZero TensorZero crea un ciclo di feedback per ottimizzare le applicazioni LLM — trasformando i dati di produzione in modelli più intelligenti, veloci ed e...">
    <meta property="og:description" content="tensorzero/tensorzero - GitHub repository it documentation and information | TensorZero TensorZero crea un ciclo di feedback per ottimizzare le applicazioni LLM — trasformando i dati di produzione in modelli più intelligenti, veloci ed e...">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">
    <script src="/js/marked.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        body { display: flex; flex-direction: column; min-height: 100vh; }
        .main-container { margin: 0 auto; width: 100%; max-width: 980px; padding: 0 20px; }
        @media (max-width: 768px) { .main-container { padding: 0 15px; } }
        .markdown-body img { max-width: 100%; height: auto; }
        .header { text-align: center; margin-bottom: 30px; padding: 20px; background-color: #f6f8fa; border-bottom: 1px solid #e1e4e8; position: relative; }
        .header .links { margin-top: 10px; font-size: 16px; }
        .header .links a { color: #0366d6; text-decoration: none; margin-left: 5px; }
        .header .links a:hover { text-decoration: underline; }
        .language-badges { margin-top: 15px; text-align: center; }
        .language-badges a { display: inline-block; margin: 2px; text-decoration: none; }
        .language-badges img { height: 20px; border-radius: 3px; }
        .language-badges a:hover img { opacity: 0.8; }
    </style>
</head>
<body>
    <div style="position: fixed; top: 2px; left: 2px; z-index: 2000; background: rgba(255,255,255,0.95); border-radius: 6px; box-shadow: 0 2px 8px rgba(0,0,0,0.06); padding: 4px 14px; font-size: 15px; color: #222; font-family: 'Segoe UI', Arial, sans-serif; font-weight: 500; letter-spacing: 0.5px;">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" style="color: #0366d6; text-decoration: none; font-weight: 600;">Open AI Tx</a>
    </div>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/tensorzero/tensorzero" id="githubRepoLink" target="_blank">tensorzero/tensorzero</a>
<h1 style="display: none;">TensorZero TensorZero crea un ciclo di feedback per ottimizzare le applicazioni LLM — trasformando i dati di produzione in modelli più intelligenti, veloci ed e...</h1>
        </div>
        <div class="language-badges" id="languageBadges">
            <!-- You can generate language badges here if needed -->
        </div>
    </div>
    <div class="main-container">
        <div class="markdown-body" id="content">
            <img src="https://github.com/user-attachments/assets/47d67430-386d-4675-82ad-d4734d3262d9" width=128 height=128>
<h1>TensorZero</h1>
<p><strong>TensorZero crea un ciclo di feedback per ottimizzare le applicazioni LLM — trasformando i dati di produzione in modelli più intelligenti, veloci ed economici.</strong></p>
<ol>
<li>Integra il nostro gateway per modelli</li>
<li>Invia metriche o feedback</li>
<li>Ottimizza prompt, modelli e strategie di inferenza</li>
<li>Osserva i tuoi LLM migliorare nel tempo</li>
</ol>
<p>Fornisce un <strong>flywheel di dati &amp; apprendimento per LLM</strong> unificando:</p>
<ul>
<li>[x] <strong>Inferenza:</strong> una sola API per tutti i LLM, con overhead &lt;1ms P99</li>
<li>[x] <strong>Osservabilità:</strong> inferenza &amp; feedback → il tuo database</li>
<li>[x] <strong>Ottimizzazione:</strong> dai prompt al fine-tuning e RL</li>
<li>[x] <strong>Valutazioni:</strong> confronta prompt, modelli, strategie di inferenza</li>
<li>[x] <strong>Sperimentazione:</strong> A/B testing integrato, routing, fallback</li>
</ul>
<hr />
<p align="center">
  <b><a href="https://www.tensorzero.com/" target="_blank">Sito Web</a></b>
  ·
  <b><a href="https://www.tensorzero.com/docs" target="_blank">Documentazione</a></b>
  ·
  <b><a href="https://www.x.com/tensorzero" target="_blank">Twitter</a></b>
  ·
  <b><a href="https://www.tensorzero.com/slack" target="_blank">Slack</a></b>
  ·
  <b><a href="https://www.tensorzero.com/discord" target="_blank">Discord</a></b>
  <br>
  <br>
  <b><a href="https://www.tensorzero.com/docs/quickstart" target="_blank">Quick Start (5min)</a></b>
  ·
  <b><a href="https://www.tensorzero.com/docs/gateway/tutorial" target="_blank">Tutorial Completo</a></b>
  ·
  <b><a href="https://www.tensorzero.com/docs/gateway/deployment" target="_blank">Guida al Deployment</a></b>
  ·
  <b><a href="https://www.tensorzero.com/docs/gateway/api-reference" target="_blank">Riferimento API</a></b>
  ·
  <b><a href="https://www.tensorzero.com/docs/gateway/deployment" target="_blank">Riferimento Configurazione</a></b>
</p>
<hr />
<table>
  <tr>
    <td width="30%" valign="top"><b>Cos'è TensorZero?</b></td>
    <td width="70%" valign="top">TensorZero è un framework open-source per costruire applicazioni LLM di livello produzione. Unifica un gateway LLM, osservabilità, ottimizzazione, valutazioni e sperimentazione.</td>
  </tr>
  <tr>
    <td width="30%" valign="top"><b>In cosa è diverso TensorZero dagli altri framework LLM?</b></td>
    <td width="70%" valign="top">
      1. TensorZero ti permette di ottimizzare applicazioni LLM complesse basandoti su metriche di produzione e feedback umano.<br>
      2. TensorZero supporta le esigenze di applicazioni LLM su scala industriale: bassa latenza, alto throughput, type safety, self-hosted, GitOps, personalizzazione, ecc.<br>
      3. TensorZero unifica l'intero stack LLMOps, creando benefici cumulativi. Ad esempio, le valutazioni LLM possono essere usate per il fine-tuning dei modelli insieme a giudici AI.
    </td>
  </tr>
  <tr>
    <td width="30%" valign="top"><b>Posso usare TensorZero con ___?</b></td>
    <td width="70%" valign="top">Sì. Sono supportati tutti i principali linguaggi di programmazione. Puoi usare TensorZero con il nostro client Python, qualsiasi SDK OpenAI o la nostra API HTTP.</td>
  </tr>
  <tr>
    <td width="30%" valign="top"><b>TensorZero è pronto per la produzione?</b></td>
    <td width="70%" valign="top">Sì. Ecco un case study: <b><a href="https://www.tensorzero.com/blog/case-study-automating-code-changelogs-at-a-large-bank-with-llms">Automazione dei Changelog di Codice in una Grande Banca con LLM</a></b></td>
  </tr>
  <tr>
    <td width="30%" valign="top"><b>Quanto costa TensorZero?</b></td>
    <td width="70%" valign="top">Niente. TensorZero è 100% self-hosted e open-source. Non ci sono funzionalità a pagamento.</td>
  </tr>
  <tr>
    <td width="30%" valign="top"><b>Chi sviluppa TensorZero?</b></td>
    <td width="70%" valign="top">Il nostro team tecnico include un ex maintainer del compilatore Rust, ricercatori di machine learning (Stanford, CMU, Oxford, Columbia) con migliaia di citazioni, e il chief product officer di una startup decacorno. Siamo sostenuti dagli stessi investitori di progetti open-source leader (es. ClickHouse, CockroachDB) e AI lab (es. OpenAI, Anthropic).</td>
  </tr>
  <tr>
    <td width="30%" valign="top"><b>Come inizio?</b></td>
    <td width="70%" valign="top">Puoi adottare TensorZero in modo incrementale. La nostra <b><a href="https://www.tensorzero.com/docs/quickstart">Guida Rapida</a></b> passa da un semplice wrapper OpenAI a un'applicazione LLM pronta per la produzione con osservabilità e fine-tuning in soli 5 minuti.</td>
  </tr>
</table>
<hr />
<h2>Funzionalità</h2>
<h3>🌐 Gateway LLM</h3>
<blockquote>
<p><strong>Integra TensorZero una sola volta e accedi a tutti i principali provider LLM.</strong></p>
</blockquote>
<table>
  <tr></tr> <!-- flip highlight order -->
  <tr>
    <td width="50%" align="center" valign="middle"><b>Provider Modelli</b></td>
    <td width="50%" align="center" valign="middle"><b>Funzionalità</b></td>
  </tr>
  <tr>
    <td width="50%" align="left" valign="top">
      <p>
        Il Gateway TensorZero supporta nativamente:
      </p>
      <ul>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/anthropic">Anthropic</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/aws-bedrock">AWS Bedrock</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/aws-sagemaker">AWS SageMaker</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/azure">Azure OpenAI Service</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/deepseek">DeepSeek</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/fireworks">Fireworks</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/gcp-vertex-ai-anthropic">GCP Vertex AI Anthropic</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/gcp-vertex-ai-gemini">GCP Vertex AI Gemini</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/google-ai-studio-gemini">Google AI Studio (Gemini API)</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/hyperbolic">Hyperbolic</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/mistral">Mistral</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/openai">OpenAI</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/together">Together</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/vllm">vLLM</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/xai">xAI</a></b></li>
      </ul>
      <p>
        <em>
          Ti serve qualcos'altro?
          Il tuo provider è molto probabilmente supportato perché TensorZero si integra con <b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/openai-compatible">qualsiasi API compatibile OpenAI (es. Ollama)</a></b>.
          </em>
      </p>
    </td>
    <td width="50%" align="left" valign="top">
      <p>
        Il Gateway TensorZero supporta funzionalità avanzate come:
      </p>
      <ul>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/retries-fallbacks">Retry & Fallback</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations">Ottimizzazioni in fase di Inferenza</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/prompt-templates-schemas">Template & Schemi Prompt</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/tutorial#experimentation">Sperimentazione (A/B Testing)</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/configuration-reference">Configuration-as-Code (GitOps)</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/batch-inference">Inferenza Batch</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/multimodal-inference">Inferenza Multimodale (VLM)</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/inference-caching">Caching Inferenza</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/metrics-feedback">Metriche & Feedback</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/episodes">Workflow LLM Multi-Step (Episodi)</a></b></li>
        <li><em>& molto altro ancora...</em></li>
      </ul>
      <p>
        Il Gateway TensorZero è scritto in Rust 🦀 con <b>prestazioni</b> in mente (<1ms p99 di latenza @ 10k QPS).
        Vedi <b><a href="https://www.tensorzero.com/docs/gateway/benchmarks">Benchmark</a></b>.<br>
      </p>
      <p>
        Puoi eseguire inferenza usando il <b>client TensorZero</b> (consigliato), il <b>client OpenAI</b>, o la <b>API HTTP</b>.
      </p>
    </td>
  </tr>
</table>
<br>
<details open>
<summary><b>Utilizzo: Python &mdash; Client TensorZero (Consigliato)</b></summary>
<p>Puoi accedere a qualsiasi provider usando il client Python TensorZero.</p>
<ol>
<li><code>pip install tensorzero</code></li>
<li>Facoltativo: configura TensorZero.</li>
<li>Esegui inferenza:</li>
</ol>
<pre><code class="language-python">from tensorzero import TensorZeroGateway  # o AsyncTensorZeroGateway


with TensorZeroGateway.build_embedded(clickhouse_url=&quot;...&quot;, config_file=&quot;...&quot;) as client:
    response = client.inference(
        model_name=&quot;openai::gpt-4o-mini&quot;,
        # Prova altri provider facilmente: &quot;anthropic::claude-3-7-sonnet-20250219&quot;
        input={
            &quot;messages&quot;: [
                {
                    &quot;role&quot;: &quot;user&quot;,
                    &quot;content&quot;: &quot;Scrivi un haiku sull'intelligenza artificiale.&quot;,
                }
            ]
        },
    )
</code></pre>
<p>Consulta <strong><a href="https://www.tensorzero.com/docs/quickstart">Guida Rapida</a></strong> per maggiori informazioni.</p>
</details>
<details>
<summary><b>Utilizzo: Python &mdash; Client OpenAI</b></summary>
<p>Puoi accedere a qualsiasi provider usando il client OpenAI Python con TensorZero.</p>
<ol>
<li><code>pip install tensorzero</code></li>
<li>Facoltativo: configura TensorZero.</li>
<li>Esegui inferenza:</li>
</ol>
<pre><code class="language-python">from openai import OpenAI  # o AsyncOpenAI
from tensorzero import patch_openai_client

client = OpenAI()
patch_openai_client(
    client,
    clickhouse_url=&quot;http://chuser:chpassword@localhost:8123/tensorzero&quot;,
    config_file=&quot;config/tensorzero.toml&quot;,
    async_setup=False,
)

response = client.chat.completions.create(
    model=&quot;tensorzero::model_name::openai::gpt-4o-mini&quot;,
    # Prova facilmente altri provider: &quot;tensorzero::model_name::anthropic::claude-3-7-sonnet-20250219&quot;
    messages=[
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: &quot;Scrivi un haiku sull'intelligenza artificiale.&quot;,
        }
    ],
)
</code></pre>
<p>Consulta <strong><a href="https://www.tensorzero.com/docs/quickstart">Inizio rapido</a></strong> per ulteriori informazioni.</p>
</details>
<details>
<summary><b>Utilizzo: JavaScript / TypeScript (Node) &mdash; OpenAI Client</b></summary>
<p>Puoi accedere a qualsiasi provider utilizzando il client OpenAI Node con TensorZero.</p>
<ol>
<li>Distribuisci <code>tensorzero/gateway</code> tramite Docker.
<strong><a href="https://www.tensorzero.com/docs/gateway/deployment">Istruzioni dettagliate →</a></strong></li>
<li>Configura TensorZero.</li>
<li>Esegui l'inferenza:</li>
</ol>
<pre><code class="language-ts">import OpenAI from &quot;openai&quot;;

const client = new OpenAI({
  baseURL: &quot;http://localhost:3000/openai/v1&quot;,
});

const response = await client.chat.completions.create({
  model: &quot;tensorzero::model_name::openai::gpt-4o-mini&quot;,
  // Prova facilmente altri provider: &quot;tensorzero::model_name::anthropic::claude-3-7-sonnet-20250219&quot;
  messages: [
    {
      role: &quot;user&quot;,
      content: &quot;Scrivi un haiku sull'intelligenza artificiale.&quot;,
    },
  ],
});
</code></pre>
<p>Consulta <strong><a href="https://www.tensorzero.com/docs/quickstart">Inizio rapido</a></strong> per ulteriori informazioni.</p>
</details>
<details>
<summary><b>Utilizzo: Altri Linguaggi & Piattaforme &mdash; HTTP API</b></summary>
<p>TensorZero supporta praticamente qualsiasi linguaggio di programmazione o piattaforma tramite la sua HTTP API.</p>
<ol>
<li>Distribuisci <code>tensorzero/gateway</code> tramite Docker.
<strong><a href="https://www.tensorzero.com/docs/gateway/deployment">Istruzioni dettagliate →</a></strong></li>
<li>Facoltativo: configura TensorZero.</li>
<li>Esegui l'inferenza:</li>
</ol>
<pre><code class="language-bash">curl -X POST &quot;http://localhost:3000/inference&quot; \
  -H &quot;Content-Type: application/json&quot; \
  -d '{
    &quot;model_name&quot;: &quot;openai::gpt-4o-mini&quot;,
    &quot;input&quot;: {
      &quot;messages&quot;: [
        {
          &quot;role&quot;: &quot;user&quot;,
          &quot;content&quot;: &quot;Scrivi un haiku sull'intelligenza artificiale.&quot;
        }
      ]
    }
  }'
</code></pre>
<p>Consulta <strong><a href="https://www.tensorzero.com/docs/quickstart">Inizio rapido</a></strong> per ulteriori informazioni.</p>
</details>
<br>
<h3>📈 Ottimizzazione LLM</h3>
<blockquote>
<p><strong>Invia metriche di produzione e feedback umano per ottimizzare facilmente i tuoi prompt, modelli e strategie di inferenza — tramite UI o programmaticamente.</strong></p>
</blockquote>
<h4>Ottimizzazione del Modello</h4>
<p>Ottimizza modelli closed-source e open-source utilizzando supervised fine-tuning (SFT) e preference fine-tuning (DPO).</p>
<table>
  <tr></tr> <!-- flip highlight order -->
  <tr>
    <td width="50%" align="center" valign="middle"><b>Supervised Fine-tuning &mdash; UI</b></td>
    <td width="50%" align="center" valign="middle"><b>Preference Fine-tuning (DPO) &mdash; Jupyter Notebook</b></td>
  </tr>
  <tr>
    <td width="50%" align="center" valign="middle"><img src="https://github.com/user-attachments/assets/cf7acf66-732b-43b3-af2a-5eba1ce40f6f"></td>
    <td width="50%" align="center" valign="middle"><img src="https://github.com/user-attachments/assets/a67a0634-04a7-42b0-b934-9130cb7cdf51"></td>
  </tr>
</table>
<h4>Ottimizzazione all'Inferenza</h4>
<p>Aumenta le prestazioni aggiornando dinamicamente i tuoi prompt con esempi rilevanti, combinando le risposte da più inferenze, e altro ancora.</p>
<table>
  <tr></tr> <!-- flip highlight order -->
  <tr>
    <td width="50%" align="center" valign="middle"><b><a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#best-of-n-sampling">Best-of-N Sampling</a></b></td>
    <td width="50%" align="center" valign="middle"><b><a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#mixture-of-n-sampling">Mixture-of-N Sampling</a></b></td>
  </tr>
  <tr>
    <td width="50%" align="center" valign="middle"><img src="https://github.com/user-attachments/assets/c0edfa4c-713c-4996-9964-50c0d26e6970"></td>
    <td width="50%" align="center" valign="middle"><img src="https://github.com/user-attachments/assets/75b5bf05-4c1f-43c4-b158-d69d1b8d05be"></td>
  </tr>
  <tr>
    <td width="50%" align="center" valign="middle"><b><a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#dynamic-in-context-learning-dicl">Dynamic In-Context Learning (DICL)</a></b></td>
    <td width="50%" align="center" valign="middle"><b><a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#chain-of-thought-cot">Chain-of-Thought (CoT)</a></b></td>
  </tr>
  <tr>
    <td width="50%" align="center" valign="middle"><img src="https://github.com/user-attachments/assets/d8489e92-ce93-46ac-9aab-289ce19bb67d"></td>
    <td width="50%" align="center" valign="middle"><img src="https://github.com/user-attachments/assets/ea13d73c-76a4-4e0c-a35b-0c648f898311" height="320"></td>
  </tr>
</table>
<p><em>Altre novità in arrivo...</em></p>
<br>
<h4>Ottimizzazione del Prompt</h4>
<p>Ottimizza i tuoi prompt in modo programmatico utilizzando tecniche di ottimizzazione basate sulla ricerca.</p>
<table>
  <tr></tr> <!-- flip highlight order -->
  <tr>
    <td width="50%" align="center" valign="middle"><b><a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#best-of-n-sampling">MIPROv2</a></b></td>
    <td width="50%" align="center" valign="middle"><b><a href="https://github.com/tensorzero/tensorzero/tree/main/examples/gsm8k-custom-recipe-dspy">Integrazione DSPy</a></b></td>
  </tr>
  <tr>
    <td width="50%" align="center" valign="middle"><img src="https://github.com/user-attachments/assets/d81a7c37-382f-4c46-840f-e6c2593301db" alt="MIPROv2 diagram"></td>
    <td width="50%" align="center" valign="middle">
      TensorZero include diverse ricette di ottimizzazione, ma puoi anche creare facilmente le tue.
      Questo esempio mostra come ottimizzare una funzione TensorZero utilizzando uno strumento arbitrario — in questo caso, DSPy, una libreria popolare per l'automazione dell'ingegneria dei prompt.
    </td>
  </tr>
</table>
<p><em>Altre novità in arrivo...</em></p>
<br>
<h3>🔍 Osservabilità LLM</h3>
<blockquote>
<p><strong>Zooma per effettuare il debug delle singole chiamate API, oppure allarga la visuale per monitorare le metriche su modelli e prompt nel tempo — tutto tramite la UI open-source di TensorZero.</strong></p>
</blockquote>
<table>
  <tr></tr> <!-- flip highlight order -->
  <tr>
    <td width="50%" align="center" valign="middle"><b>Osservabilità » Inferenza</b></td>
    <td width="50%" align="center" valign="middle"><b>Osservabilità » Funzione</b></td>
  </tr>
  <tr>
    <td width="50%" align="center" valign="middle"><img src="https://github.com/user-attachments/assets/2cc3cc9a-f33f-4e94-b8de-07522326f80a"></td>
    <td width="50%" align="center" valign="middle"><img src="https://github.com/user-attachments/assets/00ae6605-8fa0-4efd-8238-ae8ea589860f"></td>
  </tr>
</table>
<br>
<h3>📊 Valutazioni LLM</h3>
<blockquote>
<p><strong>Confronta prompt, modelli e strategie di inferenza utilizzando TensorZero Evaluations — con supporto per euristiche e giudici LLM.</strong></p>
</blockquote>
<table>
  <tr></tr> <!-- flip highlight order -->
  <tr>
    <td width="50%" align="center" valign="middle"><b>Valutazione » UI</b></td>
    <td width="50%" align="center" valign="middle"><b>Valutazione » CLI</b></td>
  </tr>
  <tr>
    <td width="50%" align="center" valign="middle"><img src="https://github.com/user-attachments/assets/f4bf54e3-1b63-46c8-be12-2eaabf615699"></td>
    <td width="50%" align="left" valign="middle">
<pre><code class="language-bash">docker compose run --rm evaluations \
  --evaluation-name extract_data \
  --dataset-name hard_test_cases \
  --variant-name gpt_4o \
  --concurrency 5</code></pre>
<pre><code class="language-bash">Run ID: 01961de9-c8a4-7c60-ab8d-15491a9708e4
Number of datapoints: 100
██████████████████████████████████████ 100/100
exact_match: 0.83 ± 0.03
semantic_match: 0,98 ± 0,01  
item_count: 7,15 ± 0,39</code></pre>
    </td>
  </tr>
</table>
<h2>Demo</h2>
<blockquote>
<p><strong>Guarda gli LLM migliorare nell’estrazione dati in tempo reale con TensorZero!</strong></p>
<p><strong><a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#dynamic-in-context-learning-dicl">Dynamic in-context learning (DICL)</a></strong> è una potente ottimizzazione in tempo di inferenza disponibile di default con TensorZero.
Migliora le prestazioni degli LLM incorporando automaticamente esempi storici rilevanti nel prompt, senza la necessità di effettuare un fine-tuning del modello.</p>
</blockquote>
<p>https://github.com/user-attachments/assets/4df1022e-886e-48c2-8f79-6af3cdad79cb</p>
<h2>LLM Engineering con TensorZero</h2>
<br>
<p align="center" >
  <a href="https://www.tensorzero.com/docs">
    <picture>
      <source media="(prefers-color-scheme: light)" srcset="https://github.com/user-attachments/assets/34a92c18-242e-4d76-a99c-861283de68a6">
      <source media="(prefers-color-scheme: dark)" srcset="https://github.com/user-attachments/assets/e8bc699b-6378-4c2a-9cc1-6d189025e270">
      <img alt="TensorZero Flywheel" src="https://github.com/user-attachments/assets/34a92c18-242e-4d76-a99c-861283de68a6" width=720>
    </picture>
  </a>
</p>
<br>
<ol>
<li>Il <strong><a href="https://www.tensorzero.com/docs/gateway/">TensorZero Gateway</a></strong> è un gateway di modelli ad alte prestazioni scritto in Rust 🦀 che fornisce un’interfaccia API unificata per tutti i principali provider LLM, consentendo un’integrazione cross-platform senza soluzione di continuità e fallback automatici.</li>
<li>Gestisce inferenze strutturate basate su schema con una latenza P99 &lt;1ms (vedi <strong><a href="https://www.tensorzero.com/docs/gateway/benchmarks">Benchmarks</a></strong>) e offre osservabilità integrata, sperimentazione e <strong><a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations">ottimizzazioni in tempo di inferenza</a></strong>.</li>
<li>Raccoglie anche metriche a valle e feedback associati a queste inferenze, con supporto di prima classe per sistemi LLM multi-step.</li>
<li>Tutto viene archiviato in un data warehouse ClickHouse che controlli tu, per analytics in tempo reale, scalabili e adatti agli sviluppatori.</li>
<li>Nel tempo, le <strong><a href="https://www.tensorzero.com/docs/recipes">TensorZero Recipes</a></strong> sfruttano questo dataset strutturato per ottimizzare i tuoi prompt e modelli: esegui ricette predefinite per workflow comuni come il fine-tuning, oppure crea le tue con totale flessibilità usando qualsiasi linguaggio e piattaforma.</li>
<li>Infine, le funzionalità di sperimentazione e l’orchestrazione GitOps del gateway ti permettono di iterare e distribuire con sicurezza, sia che si tratti di un singolo LLM che di migliaia di LLM.</li>
</ol>
<p>Il nostro obiettivo è aiutare gli ingegneri a costruire, gestire e ottimizzare la prossima generazione di applicazioni LLM: sistemi che apprendono dall’esperienza reale.
Leggi di più sulla nostra <strong><a href="https://www.tensorzero.com/docs/vision-roadmap/">Visione &amp; Roadmap</a></strong>.</p>
<h2>Inizia Subito</h2>
<p><strong>Inizia a costruire oggi stesso.</strong><br />
La <strong><a href="https://www.tensorzero.com/docs/quickstart">Guida Rapida</a></strong> mostra quanto sia facile configurare un’applicazione LLM con TensorZero.<br />
Se vuoi approfondire, il <strong><a href="https://www.tensorzero.com/docs/gateway/tutorial">Tutorial</a></strong> insegna come costruire una semplice chatbot, un copilot per email, un sistema RAG meteo e una pipeline di estrazione dati strutturati.</p>
<p><strong>Domande?</strong><br />
Chiedici su <strong><a href="https://www.tensorzero.com/slack">Slack</a></strong> o <strong><a href="https://www.tensorzero.com/discord">Discord</a></strong>.</p>
<p><strong>Usi TensorZero al lavoro?</strong><br />
Scrivici a <strong><a href="mailto:hello@tensorzero.com">hello@tensorzero.com</a></strong> per creare un canale Slack o Teams con il tuo team (gratuito).</p>
<p><strong>Lavora con noi.</strong><br />
Stiamo <strong><a href="https://www.tensorzero.com/jobs">assumendo a NYC</a></strong>.<br />
Accogliamo volentieri anche <strong><a href="https://github.com/tensorzero/tensorzero/blob/main/CONTRIBUTING.md">contributi open-source</a></strong>!</p>
<h2>Esempi</h2>
<p>Stiamo lavorando a una serie di <strong>esempi completi ed eseguibili</strong> che illustrano il ciclo di dati &amp; apprendimento di TensorZero.</p>
<blockquote>
<p><strong><a href="https://github.com/tensorzero/tensorzero/tree/main/examples/data-extraction-ner">Ottimizzazione dell’Estrazione Dati (NER) con TensorZero</a></strong></p>
<p>Questo esempio mostra come usare TensorZero per ottimizzare una pipeline di estrazione dati.
Dimostriamo tecniche come il fine-tuning e il dynamic in-context learning (DICL).
Alla fine, un modello GPT-4o Mini ottimizzato supera GPT-4o in questo compito — a una frazione del costo e della latenza — usando una piccola quantità di dati di training.</p>
</blockquote>
<blockquote>
<p><strong><a href="https://github.com/tensorzero/tensorzero/tree/main/examples/rag-retrieval-augmented-generation/simple-agentic-rag/">Agentic RAG — Multi-Hop Question Answering con LLM</a></strong></p>
<p>Questo esempio mostra come costruire un agente di retrieval multi-hop usando TensorZero.
L’agente cerca iterativamente su Wikipedia per raccogliere informazioni e decide quando ha abbastanza contesto per rispondere a una domanda complessa.</p>
</blockquote>
<blockquote>
<p><strong><a href="https://github.com/tensorzero/tensorzero/tree/main/examples/haiku-hidden-preferences">Scrivere Haiku per Soddisfare un Giudice con Preferenze Nascoste</a></strong></p>
<p>Questo esempio fa fine-tuning di GPT-4o Mini per generare haiku su misura per un gusto specifico.
Vedrai il “data flywheel in a box” di TensorZero in azione: varianti migliori portano a dati migliori, e dati migliori portano a varianti migliori.
Vedrai i progressi effettuando il fine-tuning dell’LLM più volte.</p>
</blockquote>
<blockquote>
<p><strong><a href="https://github.com/tensorzero/tensorzero/tree/main/examples/chess-puzzles/">Migliorare le Abilità LLM negli Scacchi con Best-of-N Sampling</a></strong></p>
<p>Questo esempio mostra come il best-of-N sampling possa migliorare significativamente le abilità scacchistiche di un LLM selezionando le mosse più promettenti tra varie opzioni generate.</p>
</blockquote>
<blockquote>
<p><strong><a href="https://github.com/tensorzero/tensorzero/tree/main/examples/gsm8k-custom-recipe-dspy">Migliorare il Ragionamento Matematico con una Ricetta Personalizzata per Prompt Engineering Automatizzato (DSPy)</a></strong></p>
<p>TensorZero offre numerose ricette di ottimizzazione predefinite che coprono i workflow comuni dell’LLM engineering.
Ma puoi anche creare facilmente le tue ricette e workflow!
Questo esempio mostra come ottimizzare una funzione TensorZero usando uno strumento arbitrario — in questo caso, DSPy.</p>
</blockquote>
<p><em>&amp; molti altri in arrivo!</em></p>
<hr />
<p>Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx">Open Ai Tx</a> | Last indexed: 2025-06-09</p>
<hr />

        </div>
    </div>
    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Default Statcounter code for openaitx
https://openaitx.github.io/ -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>
    <!-- End of Statcounter Code -->
</body>
</html>