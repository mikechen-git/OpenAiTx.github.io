<!DOCTYPE html>
<html lang="it">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>KAI-Scheduler - NVIDIA/KAI-Scheduler</title>
    <meta name="title" content="KAI-Scheduler - NVIDIA/KAI-Scheduler">
    <meta name="description" content="NVIDIA/KAI-Scheduler - GitHub repository it documentation and informationKAI Scheduler KAI Scheduler è un scheduler Kubernetes robusto, efficiente e scalabile che ottimizza l'allocazione delle risorse GPU per carichi di lavoro di AI e machine learning. Progettato per gestire cluster GPU su larga scala, inclusi migliaia di nodi e un elevato throughput di carichi di lavoro, KAI Scheduler è ideale per ambienti estesi ed esigenti. KAI Scheduler consente agli amministratori dei cluster Kubernetes di allocare dinamicamente le risorse GPU ai carichi di lavoro. KAI Scheduler supporta l'intero ciclo di vita dell'AI, dai piccoli job interattivi che richiedono risorse minime fino a grandi attività di training e inferenza, tutto all'interno dello stesso cluster. Garantisce un'allocazione ottimale delle risorse mantenendo l'equità tra i diversi consumatori. Può essere eseguito insieme ad altri scheduler installati sul cluster. Caratteristiche principali Batch Scheduling: Garantisce che tutti i pod in un gruppo vengano schedulati simultaneamente o nessuno venga schedulato. Bin Packing &amp; Spread Scheduling: Ottimizza l'utilizzo...">
    <meta name="keywords" content="NVIDIA, KAI-Scheduler, GitHub, repository, it documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/NVIDIA/KAI-Scheduler/README-it.html">
    <meta property="og:title" content="KAI-Scheduler - NVIDIA/KAI-Scheduler">
    <meta property="og:description" content="NVIDIA/KAI-Scheduler - GitHub repository it documentation and information">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">
    <script src="/js/marked.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        body { display: flex; flex-direction: column; min-height: 100vh; }
        .main-container { margin: 0 auto; width: 100%; max-width: 980px; padding: 0 20px; }
        @media (max-width: 768px) { .main-container { padding: 0 15px; } }
        .markdown-body img { max-width: 100%; height: auto; }
        .header { text-align: center; margin-bottom: 30px; padding: 20px; background-color: #f6f8fa; border-bottom: 1px solid #e1e4e8; position: relative; }
        .header .links { margin-top: 10px; font-size: 16px; }
        .header .links a { color: #0366d6; text-decoration: none; margin-left: 5px; }
        .header .links a:hover { text-decoration: underline; }
        .language-badges { margin-top: 15px; text-align: center; }
        .language-badges a { display: inline-block; margin: 2px; text-decoration: none; }
        .language-badges img { height: 20px; border-radius: 3px; }
        .language-badges a:hover img { opacity: 0.8; }
    </style>
</head>
<body>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/NVIDIA/KAI-Scheduler" id="githubRepoLink" target="_blank">NVIDIA/KAI-Scheduler</a>
<br>
<h1 style="display: none;">KAI Scheduler KAI Scheduler è un scheduler Kubernetes robusto, efficiente e scalabile che ottimizza l'allocazione delle risorse GPU per carichi di lavoro di AI e machine learning. Progettato per gestire cluster GPU su larga scala, inclusi migliaia di nodi e un elevato throughput di carichi di lavoro, KAI Scheduler è ideale per ambienti estesi ed esigenti. KAI Scheduler consente agli amministratori dei cluster Kubernetes di allocare dinamicamente le risorse GPU ai carichi di lavoro. KAI Scheduler supporta l'intero ciclo di vita dell'AI, dai piccoli job interattivi che richiedono risorse minime fino a grandi attività di training e inferenza, tutto all'interno dello stesso cluster. Garantisce un'allocazione ottimale delle risorse mantenendo l'equità tra i diversi consumatori. Può essere eseguito insieme ad altri scheduler installati sul cluster. Caratteristiche principali Batch Scheduling: Garantisce che tutti i pod in un gruppo vengano schedulati simultaneamente o nessuno venga schedulato. Bin Packing &amp; Spread Scheduling: Ottimizza l'utilizzo...</h1>
        </div>
        <div class="language-badges" id="languageBadges">
            <!-- You can generate language badges here if needed -->
        </div>
    </div>
    <div class="main-container">
        <div class="markdown-body" id="content">
            <p><a href="LICENSE"><img src="https://img.shields.io/badge/License-Apache_2.0-blue.svg" alt="Licenza" /></a> <a href="https://github.com/NVIDIA/KAI-Scheduler/blob/main/.github/workflows/update-coverage-badge.yaml"><img src="https://github.com/NVIDIA/KAI-Scheduler/raw/coverage-badge/badges/coverage.svg" alt="Copertura" /></a></p>
<h1>KAI Scheduler</h1>
<p>KAI Scheduler è un <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/">scheduler Kubernetes</a> robusto, efficiente e scalabile che ottimizza l'allocazione delle risorse GPU per carichi di lavoro di AI e machine learning.</p>
<p>Progettato per gestire cluster GPU su larga scala, inclusi migliaia di nodi e un elevato throughput di carichi di lavoro, KAI Scheduler è ideale per ambienti estesi ed esigenti.
KAI Scheduler consente agli amministratori dei cluster Kubernetes di allocare dinamicamente le risorse GPU ai carichi di lavoro.</p>
<p>KAI Scheduler supporta l'intero ciclo di vita dell'AI, dai piccoli job interattivi che richiedono risorse minime fino a grandi attività di training e inferenza, tutto all'interno dello stesso cluster.
Garantisce un'allocazione ottimale delle risorse mantenendo l'equità tra i diversi consumatori.
Può essere eseguito insieme ad altri scheduler installati sul cluster.</p>
<h2>Caratteristiche principali</h2>
<ul>
<li><a href="docs/batch/README.md">Batch Scheduling</a>: Garantisce che tutti i pod in un gruppo vengano schedulati simultaneamente o nessuno venga schedulato.</li>
<li>Bin Packing &amp; Spread Scheduling: Ottimizza l'utilizzo dei nodi minimizzando la frammentazione (bin-packing) o aumentando la resilienza e il bilanciamento del carico (spread scheduling).</li>
<li><a href="docs/priority/README.md">Workload Priority</a>: Prioritizza efficacemente i carichi di lavoro all'interno delle code.</li>
<li><a href="docs/queues/README.md">Hierarchical Queues</a>: Gestisce carichi di lavoro con gerarchie di code a due livelli per un controllo organizzativo flessibile.</li>
<li><a href="docs/fairness/README.md#resource-division-algorithm">Resource distribution</a>: Personalizza quote, pesi over-quota, limiti e priorità per coda.</li>
<li><a href="docs/fairness/README.md#reclaim-strategies">Fairness Policies</a>: Garantisce una distribuzione equa delle risorse usando Dominant Resource Fairness (DRF) e il recupero delle risorse tra code.</li>
<li>Consolidamento dei carichi di lavoro: rialloca in modo intelligente i carichi di lavoro in esecuzione per ridurre la frammentazione e aumentare l'utilizzo del cluster.</li>
<li><a href="docs/elastic/README.md">Elastic Workloads</a>: Scala dinamicamente i carichi di lavoro tra un numero minimo e massimo di pod definiti.</li>
<li>Dynamic Resource Allocation (DRA): Supporta risorse hardware specifiche del fornitore tramite Kubernetes ResourceClaims (ad es. GPU NVIDIA o AMD).</li>
<li><a href="docs/gpu-sharing/README.md">GPU Sharing</a>: Consente a più carichi di lavoro di condividere efficacemente una o più GPU, massimizzando l'utilizzo delle risorse.</li>
<li>Supporto Cloud &amp; On-premise: pienamente compatibile con infrastrutture cloud dinamiche (inclusi auto-scalers come Karpenter) e con deployment statici on-premise.</li>
</ul>
<h2>Prerequisiti</h2>
<p>Prima di installare KAI Scheduler, assicurarsi di avere:</p>
<ul>
<li>Un cluster Kubernetes attivo</li>
<li>CLI <a href="https://helm.sh/docs/intro/install">Helm</a> installata</li>
<li><a href="https://github.com/NVIDIA/gpu-operator">NVIDIA GPU-Operator</a> installato per schedulare carichi di lavoro che richiedono risorse GPU</li>
</ul>
<h2>Installazione</h2>
<p>KAI Scheduler sarà installato nel namespace <code>kai-scheduler</code>. Quando si inviano carichi di lavoro, assicurarsi di utilizzare un namespace dedicato.</p>
<h3>Metodi di installazione</h3>
<p>KAI Scheduler può essere installato:</p>
<ul>
<li><strong>Da Production (Consigliato)</strong></li>
<li><strong>Dal Sorgente (Build it Yourself)</strong></li>
</ul>
<h4>Installazione da Production</h4>
<p>Individuare l'ultima versione release nella pagina <a href="https://github.com/NVIDIA/KAI-Scheduler/releases">releases</a>.
Eseguire il seguente comando, sostituendo <code>&lt;VERSION&gt;</code> con la versione desiderata:</p>
<pre><code class="language-sh">helm upgrade -i kai-scheduler oci://ghcr.io/nvidia/kai-scheduler/kai-scheduler -n kai-scheduler --create-namespace --version &lt;VERSION&gt;
</code></pre>
<h4>Build dal Sorgente</h4>
<p>Seguire le istruzioni <a href="docs/developer/building-from-source.md">qui</a></p>
<h2>Quick Start</h2>
<p>Per iniziare a schedulare carichi di lavoro con KAI Scheduler, continuare con l'<a href="docs/quickstart/README.md">esempio Quick Start</a></p>
<h2>Roadmap</h2>
<h3>Panoramica generale delle priorità principali per il 2025</h3>
<ul>
<li>Refactoring del codice per migliorare la neutralità rispetto al fornitore</li>
<li>Supporto a Scheduling Gates https://github.com/NVIDIA/KAI-Scheduler/issues/63</li>
<li>Ricerca su possibili integrazioni con Kueue https://github.com/NVIDIA/KAI-Scheduler/issues/68</li>
<li>Aggiunta del supporto Topology Aware Scheduling per pod-group https://github.com/NVIDIA/KAI-Scheduler/issues/66</li>
<li>Supporto Min Run Time per carichi di lavoro</li>
<li>Supporto Max Run Time per carico di lavoro (con requeue ritardato)</li>
<li>Aggiunta di più PriorityClasses come parte dell'installazione predefinita di KAI</li>
<li>Supporto JobSet</li>
<li>Supporto LWS (LeaderWorkerSet)</li>
<li>Aggiunta di metriche per preemption di pod e pod-group</li>
<li>Separazione di Priority e Preemption</li>
</ul>
<h3>Obiettivi a lungo termine</h3>
<ul>
<li>Supporto decay temporale per coda</li>
<li>Miglioramenti hyper scale</li>
<li>Supporto consolidamento carichi di lavoro di inferenza per deframmentazione del cluster</li>
<li>Supporto a n-livelli di code gerarchiche</li>
<li>Rollout graduale dei carichi di lavoro di inferenza (nuovo aggiornamento di revisione usando over-quota temporaneo della coda)</li>
</ul>
<h2>Community, Discussione e Supporto</h2>
<p>Ci farebbe piacere sentirti! Ecco i modi migliori per entrare in contatto:</p>
<h3>Slack</h3>
<p>Unisciti prima al <a href="https://communityinviter.com/apps/cloud-native/cncf">CNCF Slack</a> e visita il canale <a href="https://cloud-native.slack.com/archives/kai-scheduler">#kai-scheduler</a>.</p>
<h3>Community Call bisettimanale</h3>
<p><strong>Quando:</strong> Ogni due lunedì alle 17:00 CEST<br />
<a href="https://dateful.com/time-zone-converter?t=17&amp;tz2=Germany">Converti nel tuo fuso orario</a> | <a href="https://calendar.google.com/calendar/event?action=TEMPLATE&amp;tmeid=N2Q2bjhoNXAzMGc0cWpnZTQ4OGtpdXFhanFfMjAyNTA2MDlUMTUwMDAwWiAxZjQ2OTZiOWVlM2JiMWE1ZWIzMTAwODBkNDZiZmMwMDZjNTUxYWFiZmU1YTM3ZGM2YTc0NTFhYmNhMmE1ODk0QGc&amp;tmsrc=1f4696b9ee3bb1a5eb310080d46bfc006c551aabfe5a37dc6a7451abca2a5894%40group.calendar.google.com&amp;scp=ALL">Aggiungi al tuo calendario</a>  | <a href="https://docs.google.com/document/d/13K7NGdPebOstlrsif1YLjGz1x-aJafMXeIgqbO7WghI/edit?usp=sharing">Note e agenda della riunione</a></p>
<h3>Mailing List</h3>
<p>Iscriviti alla <a href="https://groups.google.com/g/kai-scheduler">mailing list kai-scheduler</a> per ricevere aggiornamenti sulle riunioni bisettimanali.</p>
<h3>Problemi tecnici &amp; Richieste di funzionalità</h3>
<p>Apri una <a href="https://github.com/NVIDIA/KAI-Scheduler/issues/new/choose">issue GitHub</a> per bug, suggerimenti di funzionalità o aiuto tecnico. Questo ci aiuta a tenere traccia delle richieste e rispondere in modo efficace.</p>
<hr />
<p>Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx">Open Ai Tx</a> | Last indexed: 2025-06-08</p>
<hr />

        </div>
    </div>
    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Statcounter and other scripts can be added here -->
</body>
</html>