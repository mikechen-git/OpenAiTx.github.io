<!DOCTYPE html>
<html lang="hi">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Megatron-LM - NVIDIA/Megatron-LM</title>
    <meta name="title" content="Megatron-LM - NVIDIA/Megatron-LM">
    <meta name="description" content="NVIDIA/Megatron-LM - GitHub repository hi documentation and informationMegatron-LM &amp; Megatron-Core ट्रांसफॉर्मर मॉडल्स को बड़े पैमाने पर प्रशिक्षण देने के लिए GPU अनुकूलित तकनीकें नवीनतम समाचार [2024/7] Megatron-Core v0.7 ने स्...">
    <meta name="keywords" content="NVIDIA, Megatron-LM, GitHub, repository, hi documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/NVIDIA/Megatron-LM/README-hi.html">
    <meta property="og:title" content="Megatron-LM - NVIDIA/Megatron-LM">
    <meta property="og:description" content="NVIDIA/Megatron-LM - GitHub repository hi documentation and information">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">
    <script src="/js/marked.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        body { display: flex; flex-direction: column; min-height: 100vh; }
        .main-container { margin: 0 auto; width: 100%; max-width: 980px; padding: 0 20px; }
        @media (max-width: 768px) { .main-container { padding: 0 15px; } }
        .markdown-body img { max-width: 100%; height: auto; }
        .header { text-align: center; margin-bottom: 30px; padding: 20px; background-color: #f6f8fa; border-bottom: 1px solid #e1e4e8; position: relative; }
        .header .links { margin-top: 10px; font-size: 16px; }
        .header .links a { color: #0366d6; text-decoration: none; margin-left: 5px; }
        .header .links a:hover { text-decoration: underline; }
        .language-badges { margin-top: 15px; text-align: center; }
        .language-badges a { display: inline-block; margin: 2px; text-decoration: none; }
        .language-badges img { height: 20px; border-radius: 3px; }
        .language-badges a:hover img { opacity: 0.8; }
    </style>
</head>
<body>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/NVIDIA/Megatron-LM" id="githubRepoLink" target="_blank">NVIDIA/Megatron-LM</a>
<h1 style="display: none;">Megatron-LM &amp; Megatron-Core ट्रांसफॉर्मर मॉडल्स को बड़े पैमाने पर प्रशिक्षण देने के लिए GPU अनुकूलित तकनीकें नवीनतम समाचार [2024/7] Megatron-Core v0.7 ने स्...</h1>
        </div>
        <div class="language-badges" id="languageBadges">
            <!-- You can generate language badges here if needed -->
        </div>
    </div>
    <div class="main-container">
        <div class="markdown-body" id="content">
            <div align="center">
<h1>Megatron-LM &amp; Megatron-Core</h1>
<h4>ट्रांसफॉर्मर मॉडल्स को बड़े पैमाने पर प्रशिक्षण देने के लिए GPU अनुकूलित तकनीकें</h4>
<p><a href="https://docs.nvidia.com/megatron-core/developer-guide/latest/index.html"><img src="https://img.shields.io/badge/docs-latest-brightgreen.svg?style=flat" alt="Documentation" /></a>
<a href="./setup.py"><img src="https://img.shields.io/badge/release-0.5.0-green" alt="version" /></a>
<a href="./LICENSE"><img src="https://img.shields.io/badge/license-OpenBSD-blue" alt="license" /></a></p>
<div align="left">
<h1>नवीनतम समाचार</h1>
<ul>
<li><strong>[2024/7]</strong> Megatron-Core v0.7 ने स्केलेबिलिटी और प्रशिक्षण की लचीलापन में सुधार किया है और मल्टीमोडल प्रशिक्षण के लिए समर्थन जोड़ा है (<a href="https://developer.nvidia.com/blog/train-generative-ai-models-more-efficiently-with-new-nvidia-megatron-core-functionalities/">ब्लॉग</a>)।</li>
<li><strong>[2024/6]</strong> Megatron-Core ने Mamba-आधारित मॉडलों के लिए समर्थन जोड़ा है। हमारा पेपर देखें <a href="https://arxiv.org/pdf/2406.07887">An Empirical Study of Mamba-based Language Models</a> और <a href="https://github.com/NVIDIA/Megatron-LM/tree/ssm/examples/mamba">कोड उदाहरण</a>।</li>
<li><strong>[2024/1 घोषणा]</strong> NVIDIA ने <strong>Megatron-LM</strong> की मुख्य क्षमताओं को इस रिपॉजिटरी में <a href="https://github.com/NVIDIA/Megatron-LM/tree/main/megatron/core"><strong>Megatron-Core</strong></a> में जारी किया है। Megatron-Core, Megatron-LM की GPU-अनुकूलित तकनीकों को सिस्टम-स्तरीय अनुकूलनों और कंपोजेबल व मॉड्यूलर API के साथ आगे बढ़ाता है। अधिक जानकारी के लिए <a href="#megatron-core">Megatron-Core परिचय</a> देखें।</li>
</ul>
<h1>सामग्री सूची</h1>
<ul>
<li><a href="#megatron-lm--megatron-core">Megatron-LM &amp; Megatron-Core</a></li>
<li><a href="#latest-news">नवीनतम समाचार</a></li>
<li><a href="#table-of-contents">सामग्री सूची</a></li>
<li><a href="#megatron-overview">Megatron अवलोकन</a>
<ul>
<li><a href="#megatron-lm">Megatron-LM</a></li>
<li><a href="#megatron-core">Megatron-Core</a></li>
</ul>
</li>
<li><a href="#training-speed-and-scalability">प्रशिक्षण गति और स्केलेबिलिटी</a></li>
<li><a href="#setup">सेटअप</a>
<ul>
<li><a href="#docker-recommended">डॉकर (अनुशंसित)</a></li>
<li><a href="#installation-options">इंस्टॉलेशन विकल्प</a>
<ul>
<li><a href="#install-from-pypi">PyPI से इंस्टॉल करें</a></li>
<li><a href="#install-from-source">स्रोत से इंस्टॉल करें</a></li>
</ul>
</li>
<li><a href="#prerequisites">पूर्व-आवश्यकताएँ</a></li>
<li><a href="#downloading-checkpoints">चेकपॉइंट डाउनलोड करना</a></li>
</ul>
</li>
<li><a href="#usage">उपयोग</a></li>
<li><a href="#training">प्रशिक्षण</a>
<ul>
<li><a href="#data-preprocessing">डेटा पूर्वप्रसंस्करण</a></li>
<li><a href="#bert-pretraining">BERT पूर्वप्रशिक्षण</a></li>
<li><a href="#gpt-pretraining">GPT पूर्वप्रशिक्षण</a></li>
<li><a href="#t5-pretraining">T5 पूर्वप्रशिक्षण</a></li>
<li><a href="#distributed-pretraining">वितरित पूर्वप्रशिक्षण</a></li>
<li><a href="#activation-checkpointing-and-recomputation">सक्रियता चेकपॉइंटिंग और पुनःगणना</a></li>
<li><a href="#distributed-optimizer">वितरित ऑप्टिमाइज़र</a></li>
<li><a href="#flashattention">FlashAttention</a></li>
<li><a href="#gpt-3-example">GPT-3 उदाहरण</a></li>
<li><a href="#retro-and-instructretro">Retro और InstructRetro</a></li>
<li><a href="#mamba-based-language-models">Mamba-आधारित भाषा मॉडल</a></li>
<li><a href="#mixture-of-experts">विशेषज्ञों का मिश्रण</a></li>
</ul>
</li>
<li><a href="#evaluation-and-tasks">मूल्यांकन और कार्य</a>
<ul>
<li><a href="#gpt-text-generation">GPT पाठ निर्माण</a>
<ul>
<li><a href="#detoxify-gpt-via-self-generation">स्व-जनरेशन द्वारा GPT को डिटॉक्सिफाई करें</a></li>
</ul>
</li>
<li><a href="#gpt-evaluation">GPT मूल्यांकन</a>
<ul>
<li><a href="#wikitext-perplexity-evaluation">WikiText Perplexity मूल्यांकन</a></li>
<li><a href="#lambada-cloze-accuracy">LAMBADA Cloze सटीकता</a></li>
</ul>
</li>
<li><a href="#bert-task-evaluation">BERT कार्य मूल्यांकन</a>
<ul>
<li><a href="#race-evaluation">RACE मूल्यांकन</a></li>
<li><a href="#mnli-evaluation">MNLI मूल्यांकन</a></li>
</ul>
</li>
<li><a href="#llama-2-inference-and-finetuning">Llama-2 इंफरेंस और फाइनट्यूनिंग</a></li>
</ul>
</li>
<li><a href="#model-optimization-and-deployment">मॉडल अनुकूलन और परिनियोजन</a>
<ul>
<li><a href="#quantization-and-tensorrt-llm-deployment">क्वांटाइजेशन और TensorRT-LLM परिनियोजन</a></li>
</ul>
</li>
<li><a href="#datasets">डेटासेट्स</a>
<ul>
<li><a href="#collecting-wikipedia-training-data">विकिपीडिया प्रशिक्षण डेटा एकत्र करना</a></li>
<li><a href="#collecting-gpt-webtext-data">GPT वेबटेक्स्ट डेटा एकत्र करना</a></li>
</ul>
</li>
<li><a href="#reproducibility">पुनरुत्पादनशीलता</a></li>
<li><a href="#checkpoint-conversion">चेकपॉइंट रूपांतरण</a>
<ul>
<li><a href="#model-class-conversion">मॉडल क्लास रूपांतरण</a></li>
<li><a href="#checkpoint-format-conversion">चेकपॉइंट फॉर्मेट रूपांतरण</a></li>
</ul>
</li>
<li><a href="#projects-using-megatron">Megatron का उपयोग करने वाले प्रोजेक्ट्स</a></li>
</ul>
<h1>Megatron अवलोकन</h1>
<p>यह रिपॉजिटरी दो मुख्य घटकों से बनी है: <strong>Megatron-LM</strong> और <strong>Megatron-Core</strong>। Megatron-LM एक अनुसंधान-उन्मुख फ्रेमवर्क है जो बड़े भाषा मॉडल (LLM) प्रशिक्षण के लिए Megatron-Core का उपयोग करता है। दूसरी ओर, Megatron-Core एक GPU-अनुकूलित प्रशिक्षण तकनीकों की लाइब्रेरी है, जिसमें संस्करण-वार API और नियमित रिलीज़ सहित औपचारिक उत्पाद समर्थन होता है। आप Megatron-Core को Megatron-LM या <a href="https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/nemo_megatron/mcore_customization.html">Nvidia NeMo Framework</a> के साथ एंड-टू-एंड और क्लाउड-नेटिव समाधान के लिए उपयोग कर सकते हैं। वैकल्पिक रूप से, आप Megatron-Core के बिल्डिंग ब्लॉक्स को अपनी पसंद के प्रशिक्षण फ्रेमवर्क में एकीकृत कर सकते हैं।</p>
<h2>Megatron-LM</h2>
<p>2019 में पहली बार पेश किए गए, Megatron (<a href="https://arxiv.org/pdf/1909.08053.pdf">1</a>, <a href="https://arxiv.org/pdf/2104.04473.pdf">2</a>, और <a href="https://arxiv.org/pdf/2205.05198">3</a>) ने AI समुदाय में नवाचार की एक लहर शुरू की, जिससे शोधकर्ताओं और डेवलपर्स को इस लाइब्रेरी का उपयोग करके LLM में और प्रगति करने में सहायता मिली। आज, कई लोकप्रिय LLM डेवलपर फ्रेमवर्क्स, ओपन-सोर्स Megatron-LM लाइब्रेरी से प्रेरित होकर या सीधे उस पर आधारित बने हैं, जिससे आधारभूत मॉडल्स और AI स्टार्टअप्स की एक लहर उत्पन्न हुई है। Megatron-LM पर आधारित कुछ लोकप्रिय LLM फ्रेमवर्क्स में <a href="https://github.com/hpcaitech/ColossalAI">Colossal-AI</a>, <a href="https://github.com/huggingface/accelerate">HuggingFace Accelerate</a>, और <a href="https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/">NVIDIA NeMo Framework</a> शामिल हैं। Megatron का सीधे उपयोग करने वाले प्रोजेक्ट्स की सूची <a href="#projects-using-megatron">यहाँ</a> देखी जा सकती है।</p>
<h2>Megatron-Core</h2>
<p>Megatron-Core एक ओपन-सोर्स PyTorch-आधारित लाइब्रेरी है जिसमें GPU-अनुकूलित तकनीकें और अत्याधुनिक सिस्टम-स्तरीय अनुकूलन शामिल हैं। यह इन्हें कंपोजेबल और मॉड्यूलर API में संक्षिप्त करता है, जिससे डेवलपर्स और मॉडल शोधकर्ताओं को NVIDIA त्वरित कंप्यूटिंग इंफ्रास्ट्रक्चर पर बड़े पैमाने पर कस्टम ट्रांसफॉर्मर प्रशिक्षित करने की पूरी लचीलापन मिलती है। यह लाइब्रेरी सभी NVIDIA Tensor Core GPU के साथ संगत है, जिसमें <a href="https://www.nvidia.com/en-us/data-center/technologies/hopper-architecture/">NVIDIA Hopper आर्किटेक्चर</a> के लिए FP8 त्वरक समर्थन शामिल है।</p>
<p>Megatron-Core कोर बिल्डिंग ब्लॉक्स जैसे अटेंशन मैकेनिज्म, ट्रांसफॉर्मर ब्लॉक और लेयर, सामान्यीकरण लेयर और एम्बेडिंग तकनीकें प्रदान करता है। अतिरिक्त कार्यक्षमता जैसे सक्रियता पुनःगणना, वितरित चेकपॉइंटिंग भी लाइब्रेरी में अंतर्निहित है। बिल्डिंग ब्लॉक्स और कार्यक्षमता सभी GPU-अनुकूलित हैं, और NVIDIA त्वरित कंप्यूटिंग इंफ्रास्ट्रक्चर पर इष्टतम प्रशिक्षण गति और स्थिरता के लिए उन्नत पैरेललाइजेशन रणनीतियों के साथ बनाई जा सकती हैं। Megatron-Core लाइब्रेरी का एक अन्य प्रमुख घटक उन्नत मॉडल पैरेललिज्म तकनीकों (टेंसर, अनुक्रम, पाइपलाइन, संदर्भ, और MoE विशेषज्ञ पैरेललिज्म) को शामिल करता है।</p>
<p>Megatron-Core को <a href="https://www.nvidia.com/en-us/ai-data-science/products/nemo/">NVIDIA NeMo</a> के साथ, जो एक एंटरप्राइज-ग्रेड AI प्लेटफॉर्म है, उपयोग किया जा सकता है। वैकल्पिक रूप से, आप Megatron-Core को नेटिव PyTorch प्रशिक्षण लूप के साथ <a href="https://github.com/NVIDIA/Megatron-LM/tree/main/examples">यहाँ</a> एक्सप्लोर कर सकते हैं। अधिक जानने के लिए <a href="https://docs.nvidia.com/megatron-core/developer-guide/latest/index.html">Megatron-Core प्रलेखन</a> देखें।</p>
<h1>प्रशिक्षण गति और स्केलेबिलिटी</h1>
<p>हमारा कोडबेस बड़े भाषा मॉडलों (जैसे, सैकड़ों अरब पैरामीटर वाले मॉडल्स) को मॉडल और डेटा पैरेललिज्म के साथ कुशलतापूर्वक प्रशिक्षित करने में सक्षम है। यह दिखाने के लिए कि हमारा सॉफ़्टवेयर कई GPU और मॉडल साइज़ के साथ कैसे स्केल होता है, हम 2 अरब पैरामीटर से लेकर 462 अरब पैरामीटर तक के GPT मॉडल्स को मानते हैं। सभी मॉडलों में शब्दावली आकार 131,072 और अनुक्रम लंबाई 4096 है। हम छिपे हुए आकार, अटेंशन हेड्स की संख्या, और लेयर्स की संख्या को बदलते हैं ताकि एक विशिष्ट मॉडल आकार तक पहुँचें। जैसे-जैसे मॉडल आकार बढ़ता है, हम बैच साइज़ को भी थोड़ा बढ़ाते हैं। हमारे प्रयोगों में 6144 तक <a href="https://www.nvidia.com/en-us/data-center/h100/">H100</a> GPU का उपयोग किया गया है। हम डेटा-पैरेलल (<code>--overlap-grad-reduce --overlap-param-gather</code>), टेंसर-पैरेलल (<code>--tp-comm-overlap</code>), और पाइपलाइन-पैरेलल संचार (डिफ़ॉल्ट रूप से सक्षम) को गणना के साथ बारीकी से ओवरलैप करते हैं ताकि स्केलेबिलिटी में सुधार हो सके। रिपोर्ट की गई थ्रूपुट्स समग्र प्रशिक्षण के लिए मापी जाती हैं और इसमें डेटा लोडिंग, ऑप्टिमाइज़र स्टेप्स, कम्युनिकेशन, और यहां तक कि लॉगिंग भी शामिल है। ध्यान दें कि हमने इन मॉडलों को कन्वर्जेंस तक प्रशिक्षित नहीं किया है।</p>
<p><img src="images/model_table.png" alt="Model table" /></p>
<p>हमारे वीक स्केल्ड परिणाम सुपरलीनियर स्केलिंग दिखाते हैं (MFU सबसे छोटे मॉडल के लिए 41% से बढ़कर सबसे बड़े मॉडलों के लिए 47-48% तक बढ़ता है); ऐसा इसलिए है क्योंकि बड़े GEMMs में अधिक अंकगणितीय तीव्रता होती है और इसलिए उन्हें निष्पादित करना अधिक कुशल होता है।</p>
<p><img src="images/weak_scaling.png" alt="Weak scaling" /></p>
<p>हमने मानक GPT-3 मॉडल (हमारे संस्करण में थोड़ी बड़ी शब्दावली के कारण 175 अरब पैरामीटर से थोड़ा अधिक हैं) को 96 H100 GPU से 4608 GPU तक भी स्ट्रॉन्ग स्केल किया है, जिसमें पूरे समय 1152 अनुक्रमों का समान बैच साइज़ रखा गया है। बड़े पैमाने पर कम्युनिकेशन अधिक उजागर हो जाता है, जिससे MFU 47% से घटकर 42% हो जाता है।</p>
<p><img src="images/strong_scaling.png" alt="Strong scaling" /></p>
<h1>सेटअप</h1>
<h2>डॉकर (अनुशंसित)</h2>
<p>हम दृढ़ता से अनुशंसा करते हैं कि <a href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch">PyTorch NGC कंटेनर</a> के पिछले रिलीज़ का उपयोग करें, नवीनतम का नहीं। हमारे रिलीज़ हमेशा पिछले माह के NGC कंटेनर पर आधारित होते हैं, जिससे अनुकूलता और स्थिरता सुनिश्चित होती है। यह कंटेनर सभी निर्भरताओं के साथ, संगत संस्करणों और NVIDIA GPU के लिए अनुकूलित कॉन्फ़िगरेशन के साथ आता है।</p>
<pre><code class="language-bash"># पिछले NGC कंटेनर को खींचें (25.04 को पिछले माह के संस्करण से बदलें)
docker pull nvcr.io/nvidia/pytorch:25.04-py3
</code></pre>
<pre><code class="language-bash"># माउंटेड डाइरेक्टरीज़ के साथ कंटेनर चलाएं
docker run --gpus all -it --rm \
  -v /path/to/megatron:/workspace/megatron \
  -v /path/to/dataset:/workspace/dataset \
  -v /path/to/checkpoints:/workspace/checkpoints \
  nvcr.io/nvidia/pytorch:25.04-py3
</code></pre>
<h2>इंस्टॉलेशन विकल्प</h2>
<h3>पूर्व-आवश्यकताएँ</h3>
<ul>
<li>PyTorch (नवीनतम स्थिर संस्करण)</li>
<li>CUDA, cuDNN, NCCL (नवीनतम स्थिर संस्करण)</li>
<li>NVIDIA Hopper, Ada, और Blackwell GPU पर FP8 के लिए समर्थन</li>
<li>सर्वोत्तम प्रदर्शन के लिए NVIDIA Turing GPU आर्किटेक्चर जनरेशन या बाद के GPU का उपयोग करें</li>
</ul>
<h3>PyPI से इंस्टॉल करें</h3>
<p>नवीनतम स्थिर संस्करण को pip से इंस्टॉल करने के लिए:</p>
<pre><code class="language-bash"># नवीनतम रिलीज़ इंस्टॉल करें
pip install megatron-core
</code></pre>
<h3>स्रोत से इंस्टॉल करें</h3>
<pre><code class="language-bash"># रिपॉजिटरी क्लोन करें
git clone https://github.com/NVIDIA/Megatron-LM.git
cd Megatron-LM

# वैकल्पिक रूप से किसी विशिष्ट रिलीज़ पर चेकआउट करें
git checkout v0.11.0

# विकास मोड में इंस्टॉल करें
pip install -e .
</code></pre>
<h2>चेकपॉइंट डाउनलोड करना</h2>
<p>हमने पूर्वप्रशिक्षित <a href="https://ngc.nvidia.com/catalog/models/nvidia:megatron_bert_345m">BERT-345M</a> और <a href="https://ngc.nvidia.com/catalog/models/nvidia:megatron_lm_345m">GPT-345M</a> चेकपॉइंट्स प्रदान किए हैं, जिन्हें आप मूल्यांकन या डाउनस्ट्रीम कार्यों के लिए फाइनट्यूनिंग के लिए उपयोग कर सकते हैं। इन चेकपॉइंट्स तक पहुँचने के लिए पहले <a href="https://ngc.nvidia.com/signup">साइन अप</a> करें और <a href="https://ngc.nvidia.com/setup/installers/cli">सेटअप</a> करें NVIDIA GPU Cloud (NGC) रजिस्ट्री CLI। मॉडल डाउनलोड करने के लिए आगे प्रलेखन <a href="https://docs.nvidia.com/dgx/ngc-registry-cli-user-guide/index.html#topic_6_4_1">NGC प्रलेखन</a> में पाया जा सकता है।</p>
<p>वैकल्पिक रूप से, आप चेकपॉइंट्स सीधे भी डाउनलोड कर सकते हैं:</p>
<pre>
BERT-345M-uncased: wget --content-disposition https://api.ngc.nvidia.com/v2/models/nvidia/megatron_bert_345m/versions/v0.1_uncased/zip -O megatron_bert_345m_v0.1_uncased.zip
BERT-345M-cased: wget --content-disposition https://api.ngc.nvidia.com/v2/models/nvidia/megatron_bert_345m/versions/v0.1_cased/zip -O megatron_bert_345m_v0.1_cased.zip
GPT-345M: wget --content-disposition https://api.ngc.nvidia.com/v2/models/nvidia/megatron_lm_345m/versions/v0.0/zip -O megatron_lm_345m_v0.0.zip
</pre>
<p>मॉडलों को चलाने के लिए शब्दावली (vocab) फ़ाइलों की आवश्यकता होती है। BERT WordPiece शब्दावली फ़ाइल को Google के पूर्वप्रशिक्षित BERT मॉडलों से निकाला जा सकता है: <a href="https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt">uncased</a>, <a href="https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-vocab.txt">cased</a>। GPT <a href="https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json">vocab फ़ाइल</a> और <a href="https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt">merge टेबल</a> को सीधे डाउनलोड किया जा सकता है।</p>
<h1>उपयोग</h1>
<p>इंस्टॉलेशन के बाद, कई संभावित वर्कफ़्लो हैं। सबसे व्यापक वर्कफ़्लो है:</p>
<ol>
<li>डेटा पूर्वप्रसंस्करण</li>
<li>पूर्वप्रशिक्षण</li>
<li>फाइनट्यूनिंग (शून्य-शॉट कार्यों के लिए वैकल्पिक)</li>
<li>डाउनस्ट्रीम कार्य मूल्यांकन या पाठ निर्माण</li>
</ol>
<p>हालांकि, चरण 1 और 2 को ऊपर उल्लिखित किसी एक पूर्वप्रशिक्षित मॉडल का उपयोग करके भी बदला जा सकता है।</p>
<p>हमने <a href="./examples"><code>examples</code></a> डाइरेक्टरी में BERT और GPT दोनों के पूर्वप्रशिक्षण के लिए कई स्क्रिप्ट्स प्रदान की हैं, साथ ही MNLI, RACE, WikiText103, और LAMBADA मूल्यांकन सहित शून्य-शॉट और फाइनट्यून किए गए डाउनस्ट्रीम कार्यों के लिए स्क्रिप्ट्स भी दी हैं। GPT इंटरएक्टिव टेक्स्ट जनरेशन के लिए भी एक स्क्रिप्ट है।</p>
<h1>प्रशिक्षण</h1>
<h2>डेटा पूर्वप्रसंस्करण</h2>
<p>प्रशिक्षण डेटा के लिए पूर्वप्रसंस्करण की आवश्यकता होती है। पहले, अपने प्रशिक्षण डेटा को लूज़ json फॉर्मेट में रखें, जिसमें प्रत्येक लाइन में एक json हो जिसमें एक पाठ नमूना हो। उदाहरण के लिए:</p>
<pre>
{"src": "www.nvidia.com", "text": "The quick brown fox", "type": "Eng", "id": "0", "title": "First Part"}
{"src": "The Internet", "text": "jumps over the lazy dog", "type": "Eng", "id": "42", "title": "Second Part"}
</pre>
<p>json के <code>text</code> फ़ील्ड का नाम <a href="./tools/preprocess_data.py"><code>preprocess_data.py</code></a> में <code>--json-key</code> फ्लैग का उपयोग करके बदला जा सकता है। अन्य मेटाडेटा वैकल्पिक हैं और प्रशिक्षण में उपयोग नहीं होते।</p>
<p>लूज़ json को फिर प्रशिक्षण के लिए बाइनरी फॉर्मेट में प्रोसेस किया जाता है। json को mmap फॉर्मेट में बदलने के लिए <code>preprocess_data.py</code> का उपयोग करें। BERT प्रशिक्षण के लिए डेटा तैयार करने का एक उदाहरण स्क्रिप्ट:</p>
<pre>
python tools/preprocess_data.py \
       --input my-corpus.json \
       --output-prefix my-bert \
       --vocab-file bert-vocab.txt \
       --tokenizer-type BertWordPieceLowerCase \
       --split-sentences
</pre>
<p>आउटपुट में दो फाइलें होंगी, इस मामले में, <code>my-bert_text_sentence.bin</code> और <code>my-bert_text_sentence.idx</code>। बाद में BERT प्रशिक्षण में निर्दिष्ट <code>--data-path</code> पूर्ण पथ और नई फ़ाइल का नाम है, लेकिन फ़ाइल एक्सटेंशन के बिना।</p>
<p>T5 के लिए भी BERT की तरह ही पूर्वप्रसंस्करण करें, शायद इसे बदलकर:</p>
<pre>
       --output-prefix my-t5 \
</pre>
<p>GPT डेटा पूर्वप्रसंस्करण के लिए कुछ मामूली संशोधनों की आवश्यकता होती है, अर्थात्, एक मर्ज टेबल का जोड़ना, एक दस्तावेज़ के अंत का टोकन, वाक्य विभाजन को हटाना, और टोकनाइज़र प्रकार में परिवर्तन:</p>
<pre>
python tools/preprocess_data.py \
       --input my-corpus.json \
       --output-prefix my-gpt2 \
       --vocab-file gpt2-vocab.json \
       --tokenizer-type GPT2BPETokenizer \
       --merge-file gpt2-merges.txt \
       --append-eod
</pre>
<p>यहाँ आउटपुट फाइलों के नाम <code>my-gpt2_text_document.bin</code> और <code>my-gpt2_text_document.idx</code> हैं। पहले की तरह, GPT प्रशिक्षण में, एक्सटेंशन के बिना लंबे नाम को <code>--data-path</code> के रूप में उपयोग करें।</p>
<p>अन्य कमांड लाइन आर्ग्युमेंट्स स्रोत फाइल <a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/tools/preprocess_data.py"><code>preprocess_data.py</code></a> में वर्णित हैं।</p>
<h2>BERT प्रीट्रेनिंग</h2>
<p><a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/examples/bert/train_bert_340m_distributed.sh"><code>examples/bert/train_bert_340m_distributed.sh</code></a> स्क्रिप्ट एकल GPU 345M पैरामीटर BERT प्रीट्रेनिंग चलाती है। डिबगिंग मुख्य उद्देश्य है एकल GPU प्रशिक्षण का, क्योंकि कोडबेस और कमांड लाइन आर्ग्युमेंट्स अत्यधिक वितरित प्रशिक्षण के लिए अनुकूलित हैं। अधिकांश आर्ग्युमेंट्स काफी स्पष्ट हैं। डिफ़ॉल्ट रूप से, लर्निंग रेट प्रशिक्षण पुनरावृत्तियों पर रैखिक रूप से घटती है, जो <code>--lr</code> से शुरू होती है और <code>--min-lr</code> तक जाती है, <code>--lr-decay-iters</code> पुनरावृत्तियों में। प्रशिक्षण पुनरावृत्तियों का अंश जो वॉर्मअप के लिए उपयोग किया जाता है, वह <code>--lr-warmup-fraction</code> द्वारा सेट किया जाता है। जबकि यह एकल GPU प्रशिक्षण है, <code>--micro-batch-size</code> द्वारा निर्दिष्ट बैच आकार एक सिंगल फॉरवर्ड-बैकवर्ड पथ बैच-साइज़ है और कोड ग्रेडिएंट एक्युमुलेशन स्टेप्स करेगा जब तक कि वह <code>global-batch-size</code> तक नहीं पहुंच जाता, जो प्रति पुनरावृत्ति बैच आकार है। डेटा को प्रशिक्षण/मान्यकरण/परीक्षण सेट के लिए 949:50:1 अनुपात में विभाजित किया जाता है (डिफ़ॉल्ट 969:30:1 है)। यह विभाजन रन के दौरान होता है, लेकिन एक ही रैंडम सीड (डिफ़ॉल्ट 1234, या मैन्युअल रूप से <code>--seed</code> के साथ निर्दिष्ट) के साथ रन के बीच सुसंगत रहता है। हम <code>train-iters</code> का उपयोग अनुरोधित प्रशिक्षण पुनरावृत्तियों के रूप में करते हैं। वैकल्पिक रूप से, कोई <code>--train-samples</code> भी प्रदान कर सकता है, जो कुल नमूनों की संख्या है जिन पर प्रशिक्षण देना है। यदि यह विकल्प मौजूद है, तो <code>--lr-decay-iters</code> प्रदान करने के बजाय, <code>--lr-decay-samples</code> प्रदान करना होगा।</p>
<p>लॉगिंग, चेकपॉइंट-बचत, और मूल्यांकन अंतराल विकल्प निर्दिष्ट किए गए हैं। ध्यान दें कि <code>--data-path</code> में अब वह अतिरिक्त <code>_text_sentence</code> उपसर्ग शामिल है जो प्रीप्रोसेसिंग में जोड़ा गया था, लेकिन इसमें फाइल एक्सटेंशन शामिल नहीं है।</p>
<p>अन्य कमांड लाइन आर्ग्युमेंट्स स्रोत फाइल <a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/megatron/training/arguments.py"><code>arguments.py</code></a> में वर्णित हैं।</p>
<p><code>train_bert_340m_distributed.sh</code> चलाने के लिए, वांछित संशोधन करें, जिसमें <code>CHECKPOINT_PATH</code>, <code>VOCAB_FILE</code>, और <code>DATA_PATH</code> के लिए पर्यावरण वेरिएबल सेट करना शामिल है। सुनिश्चित करें कि ये वेरिएबल कंटेनर में अपने पथों पर सेट हों। फिर कंटेनर को Megatron और आवश्यक पथों के साथ माउंट करके लॉन्च करें (जैसा कि <a href="#setup">Setup</a> में बताया गया है) और उदाहरण स्क्रिप्ट चलाएँ।</p>
<h2>GPT प्रीट्रेनिंग</h2>
<p><code>examples/gpt3/train_gpt3_175b_distributed.sh</code> स्क्रिप्ट एकल GPU 345M पैरामीटर GPT प्रीट्रेनिंग चलाती है। जैसा कि ऊपर उल्लेखित है, एकल GPU प्रशिक्षण मुख्य रूप से डिबगिंग उद्देश्यों के लिए है, क्योंकि कोड वितरित प्रशिक्षण के लिए अनुकूलित है।</p>
<p>यह पिछले BERT स्क्रिप्ट के समान ही प्रारूप का अनुसरण करता है, कुछ प्रमुख अंतर के साथ: टोकनाइज़ेशन स्कीम BPE है (जो मर्ज टेबल और <code>json</code> शब्दावली फाइल की आवश्यकता होती है) WordPiece के बजाय, मॉडल आर्किटेक्चर लंबी अनुक्रमों की अनुमति देता है (ध्यान दें कि अधिकतम पोजीशन एम्बेडिंग अधिकतम अनुक्रम लंबाई के बराबर या उससे अधिक होनी चाहिए), और <code>--lr-decay-style</code> को कोसाइन डिके पर सेट किया गया है। ध्यान दें कि <code>--data-path</code> में अब वह अतिरिक्त <code>_text_document</code> उपसर्ग शामिल है जो प्रीप्रोसेसिंग में जोड़ा गया था, लेकिन इसमें फाइल एक्सटेंशन शामिल नहीं है।</p>
<p>अन्य कमांड लाइन आर्ग्युमेंट्स स्रोत फाइल <a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/megatron/training/arguments.py"><code>arguments.py</code></a> में वर्णित हैं।</p>
<p><code>train_gpt3_175b_distributed.sh</code> को BERT के लिए वर्णित तरीके से ही लॉन्च किया जा सकता है। एनवायरनमेंट वेरिएबल सेट करें और अन्य आवश्यक संशोधन करें, उपयुक्त माउंट्स के साथ कंटेनर लॉन्च करें, और स्क्रिप्ट चलाएँ।
अधिक जानकारी <a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/examples/gpt3/README.md"><code>examples/gpt3/README.md</code></a> में।</p>
<h2>T5 प्रीट्रेनिंग</h2>
<p>BERT और GPT के समान, <code>examples/t5/train_t5_220m_distributed.sh</code> स्क्रिप्ट एकल GPU &quot;बेस&quot; (~220M पैरामीटर) T5 प्रीट्रेनिंग चलाती है। BERT और GPT से मुख्य अंतर T5 आर्किटेक्चर को समायोजित करने के लिए निम्नलिखित आर्ग्युमेंट्स का जोड़ना है:</p>
<ul>
<li><p><code>--kv-channels</code> मॉडल में सभी अटेंशन मैकेनिज्म के &quot;key&quot; और &quot;value&quot; मैट्रिक्स के आंतरिक डायमेंशन को सेट करता है। BERT और GPT में यह डिफ़ॉल्ट रूप से छिपे हुए आकार को अटेंशन हेड्स की संख्या से विभाजित करता है, लेकिन T5 के लिए इसे कॉन्फ़िगर किया जा सकता है।</p>
</li>
<li><p><code>--ffn-hidden-size</code> ट्रांसफॉर्मर लेयर के भीतर फीड-फॉरवर्ड नेटवर्क्स में छुपे हुए आकार को सेट करता है। BERT और GPT में यह डिफ़ॉल्ट रूप से ट्रांसफॉर्मर छिपे हुए आकार का 4 गुना होता है, लेकिन T5 के लिए इसे कॉन्फ़िगर किया जा सकता है।</p>
</li>
<li><p><code>--encoder-seq-length</code> और <code>--decoder-seq-length</code> एन्कोडर और डिकोडर के लिए अनुक्रम लंबाई अलग-अलग सेट करते हैं।</p>
</li>
</ul>
<p>अन्य सभी आर्ग्युमेंट्स BERT और GPT प्रीट्रेनिंग के लिए जैसे थे वैसे ही रहते हैं। इस उदाहरण को ऊपर वर्णित उन्हीं चरणों के साथ चलाएँ।</p>
<p>अधिक जानकारी <a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/examples/t5/README.md"><code>examples/t5/README.md</code></a> में।</p>
<h2>वितरित प्रीट्रेनिंग</h2>
<p><code>pretrain_{bert,gpt,t5}_distributed.sh</code> स्क्रिप्ट्स वितरित प्रशिक्षण के लिए PyTorch वितरित लॉन्चर का उपयोग करती हैं। इस प्रकार, मल्टी-नोड प्रशिक्षण पर्यावरण वेरिएबल्स को ठीक से सेट करके प्राप्त किया जा सकता है। आगे विवरण के लिए आधिकारिक PyTorch <a href="https://pytorch.org/docs/stable/elastic/run.html#launcher-api">दस्तावेज़</a> देखें और इन <a href="https://pytorch.org/docs/stable/distributed.html#environment-variable-initialization">पर्यावरण वेरिएबल्स</a> की व्याख्या पढ़ें। डिफ़ॉल्ट रूप से, मल्टी-नोड प्रशिक्षण <a href="https://developer.nvidia.com/nccl">nccl</a> वितरित बैकएंड का उपयोग करता है। वितरित प्रशिक्षण अपनाने के लिए कुछ अतिरिक्त आर्ग्युमेंट्स और PyTorch वितरित मॉड्यूल का उपयोग <code>torchrun</code> इलास्टिक लॉन्चर (जो <code>python -m torch.distributed.run</code> के बराबर है) के साथ आवश्यक हैं। अधिक विवरण के लिए किसी भी <code>pretrain_{bert,gpt,t5}_distributed.sh</code> देखें।</p>
<p>हम दो प्रकार की पैरेललिज़्म का उपयोग करते हैं: डेटा और मॉडल पैरेललिज़्म। हमारा डेटा पैरेललिज़्म इम्प्लीमेंटेशन <code>megatron/core/distributed</code> में है, और जब <code>--overlap-grad-reduce</code> कमांड लाइन विकल्प का उपयोग किया जाता है, तो यह बैकवर्ड पास के साथ ग्रेडिएंट रिडक्शन के ओवरलैप को सपोर्ट करता है।</p>
<p>दूसरे, हमने एक सरल और कुशल दो-आयामी मॉडल-पैरेलल दृष्टिकोण विकसित किया है। पहले डायमेंशन, टेन्सर मॉडल पैरेललिज़्म (एकल ट्रांसफॉर्मर मॉड्यूल के निष्पादन को कई GPUs में विभाजित करना, <a href="https://arxiv.org/pdf/1909.08053.pdf">हमारे पेपर</a> के अनुभाग 3 देखें) का उपयोग करने के लिए, <code>--tensor-model-parallel-size</code> फ्लैग जोड़ें ताकि मॉडल को विभाजित करने के लिए GPUs की संख्या निर्दिष्ट हो, साथ ही ऊपर उल्लिखित वितरित लॉन्चर को दिए गए आर्ग्युमेंट्स के साथ। दूसरे डायमेंशन, अनुक्रम पैरेललिज़्म का उपयोग करने के लिए, <code>--sequence-parallel</code> निर्दिष्ट करें, जिसे सक्षम करने के लिए टेन्सर मॉडल पैरेललिज़्म भी आवश्यक है क्योंकि यह उन्हीं GPUs में विभाजित करता है (अधिक विवरण <a href="https://arxiv.org/pdf/2205.05198.pdf">हमारे पेपर</a> के अनुभाग 4.2.2 में)।</p>
<p>पाइपलाइन मॉडल पैरेललिज़्म (ट्रांसफॉर्मर मॉड्यूल को स्टेजेस में विभाजित करना, प्रत्येक स्टेज पर समान संख्या में ट्रांसफॉर्मर मॉड्यूल के साथ, और फिर माइक्रोबैचेज़ में बैच को विभाजित कर निष्पादन को पाइपलाइन करना, <a href="https://arxiv.org/pdf/2104.04473.pdf">हमारे पेपर</a> के अनुभाग 2.2 देखें) का उपयोग करने के लिए, <code>--pipeline-model-parallel-size</code> फ्लैग का उपयोग करें ताकि मॉडल को विभाजित करने के लिए स्टेजेस की संख्या निर्दिष्ट हो (उदाहरण के लिए, 24 ट्रांसफॉर्मर लेयर्स वाले मॉडल को 4 स्टेजेस में विभाजित करने का मतलब है प्रत्येक स्टेज को 6 ट्रांसफॉर्मर लेयर मिलती है)।</p>
<p>हमारे पास इन दो अलग-अलग मॉडल पैरेललिज़्म रूपों का उपयोग करने के उदाहरण स्क्रिप्ट्स हैं, जो <code>distributed_with_mp.sh</code> पर समाप्त होती हैं।</p>
<p>इन छोटे परिवर्तनों के अलावा, वितरित प्रशिक्षण एकल GPU पर प्रशिक्षण के समान है।</p>
<p>इंटरलीव्ड पाइपलाइनिंग शेड्यूल (अधिक विवरण <a href="https://arxiv.org/pdf/2104.04473.pdf">हमारे पेपर</a> के अनुभाग 2.2.2 में) को <code>--num-layers-per-virtual-pipeline-stage</code> आर्ग्युमेंट का उपयोग करके सक्षम किया जा सकता है, जो वर्चुअल स्टेज में ट्रांसफॉर्मर लेयर्स की संख्या को नियंत्रित करता है (डिफ़ॉल्ट रूप से गैर-इंटरलीव्ड शेड्यूल के साथ, प्रत्येक GPU एकल वर्चुअल स्टेज निष्पादित करेगा जिसमें <code>NUM_LAYERS / PIPELINE_MP_SIZE</code> ट्रांसफॉर्मर लेयर्स होंगी)। ट्रांसफॉर्मर मॉडल में कुल लेयरों की संख्या इस आर्ग्युमेंट मान से विभाज्य होनी चाहिए। इसके अलावा, पाइपलाइन में माइक्रोबैचेज़ की संख्या (<code>GLOBAL_BATCH_SIZE / (DATA_PARALLEL_SIZE * MICRO_BATCH_SIZE)</code> के रूप में गणना की जाती है) इस शेड्यूल का उपयोग करते समय <code>PIPELINE_MP_SIZE</code> से विभाज्य होनी चाहिए (यह स्थिति कोड में एक असर्शन में जांची जाती है)। इंटरलीव्ड शेड्यूल 2 स्टेजेस (<code>PIPELINE_MP_SIZE=2</code>) वाली पाइपलाइनों के लिए समर्थित नहीं है।</p>
<h2>एक्टिवेशन चेकपॉइंटिंग और पुनःगणना</h2>
<p>जब बड़े मॉडल का प्रशिक्षण करते समय GPU मेमोरी उपयोग कम करने के लिए, हम एक्टिवेशन चेकपॉइंटिंग और पुनःगणना के विभिन्न रूपों का समर्थन करते हैं। पारंपरिक डीप लर्निंग मॉडल्स में जैसा होता था, सभी एक्टिवेशन मेमोरी में संग्रहीत किए जाते थे ताकि वे बैकप्रॉप के दौरान उपयोग किए जा सकें, लेकिन अब केवल &quot;चेकपॉइंट्स&quot; पर कुछ एक्टिवेशन मेमोरी में बनाए रखते हैं (या संग्रहीत करते हैं), और अन्य एक्टिवेशन की आवश्यकता पड़ने पर ऑन-द-फ्लाई पुनःगणना की जाती है। ध्यान दें कि इस प्रकार की चेकपॉइंटिंग, <em>एक्टिवेशन</em> चेकपॉइंटिंग, मॉडल पैरामीटर और ऑप्टिमाइज़र स्टेट की चेकपॉइंटिंग से बहुत अलग है, जिसका उल्लेख अन्यत्र किया गया है।</p>
<p>हम पुनःगणना की दो स्तरों की ग्रैन्युलैरिटी का समर्थन करते हैं: <code>selective</code> और <code>full</code>। डिफ़ॉल्ट और लगभग सभी मामलों में अनुशंसित है चयनात्मक पुनःगणना। यह मोड वे एक्टिवेशन मेमोरी में रखता है जो कम स्थान लेते हैं और पुनःगणना में महंगे हैं, और उन एक्टिवेशन की पुनःगणना करता है जो अधिक मेमोरी लेते हैं लेकिन पुनःगणना में अपेक्षाकृत सस्ते हैं। विवरण के लिए <a href="https://arxiv.org/pdf/2205.05198">हमारा पेपर</a> देखें। आप पाएंगे कि यह मोड प्रदर्शन को अधिकतम करते हुए एक्टिवेशन संग्रहीत करने के लिए आवश्यक मेमोरी को न्यूनतम करता है। चयनात्मक एक्टिवेशन पुनःगणना सक्षम करने के लिए बस <code>--recompute-activations</code> का उपयोग करें।</p>
<p>जहाँ मेमोरी बहुत सीमित है, वहाँ <code>full</code> पुनःगणना केवल ट्रांसफॉर्मर लेयर, या लेयरों के समूह, या ब्लॉक के इनपुट को संग्रहीत करती है, और बाकी सब पुनःगणना करती है। पूर्ण एक्टिवेशन पुनःगणना सक्षम करने के लिए <code>--recompute-granularity full</code> का उपयोग करें। <code>full</code> एक्टिवेशन पुनःगणना का उपयोग करते समय, दो विधियाँ हैं: <code>uniform</code> और <code>block</code>, जिन्हें <code>--recompute-method</code> आर्ग्युमेंट के माध्यम से चुना जाता है।</p>
<ul>
<li><p><code>uniform</code> विधि ट्रांसफॉर्मर लेयरों को समान आकार के समूहों में विभाजित करती है (प्रत्येक समूह का आकार <code>--recompute-num-layers</code>), और प्रत्येक समूह के इनपुट एक्टिवेशन को मेमोरी में संग्रहीत करती है। बेसलाइन समूह का आकार 1 है और इस स्थिति में प्रत्येक ट्रांसफॉर्मर लेयर का इनपुट एक्टिवेशन संग्रहीत होता है। जब GPU मेमोरी अपर्याप्त हो, तो प्रति समूह लेयरों की संख्या बढ़ाने से मेमोरी उपयोग कम होता है, जिससे बड़े मॉडल का प्रशिक्षण संभव होता है। उदाहरण के लिए, जब <code>--recompute-num-layers</code> को 4 पर सेट किया जाता है, केवल 4 ट्रांसफॉर्मर लेयरों के प्रत्येक समूह का इनपुट एक्टिवेशन संग्रहीत किया जाता है।</p>
</li>
<li><p><code>block</code> विधि प्रत्येक पाइपलाइन स्टेज में निर्दिष्ट संख्या (<code>--recompute-num-layers</code>) की व्यक्तिगत ट्रांसफॉर्मर लेयरों के इनपुट एक्टिवेशन की पुनःगणना करती है और शेष लेयरों के इनपुट एक्टिवेशन को पाइपलाइन स्टेज में संग्रहीत करती है। <code>--recompute-num-layers</code> घटाने से अधिक ट्रांसफॉर्मर लेयरों के इनपुट एक्टिवेशन संग्रहीत होते हैं, जिससे बैकप्रॉप में पुनःगणना कम होती है, इस प्रकार प्रशिक्षण प्रदर्शन में सुधार होता है लेकिन मेमोरी उपयोग बढ़ जाता है। उदाहरण के लिए, जब हम प्रति पाइपलाइन स्टेज 8 लेयरों में से 5 लेयर पुनःगणना के लिए निर्दिष्ट करते हैं, तो केवल पहली 5 ट्रांसफॉर्मर लेयरों के इनपुट एक्टिवेशन बैकप्रॉप स्टेप में पुनःगणना की जाती है, जबकि अंतिम 3 लेयरों के इनपुट एक्टिवेशन संग्रहीत होते हैं। <code>--recompute-num-layers</code> को धीरे-धीरे बढ़ाया जा सकता है जब तक कि आवश्यक मेमोरी बस उपलब्ध मेमोरी में फिट न हो जाए, इस प्रकार मेमोरी का अधिकतम उपयोग और प्रदर्शन अधिकतम किया जा सके।</p>
</li>
</ul>
<h2>वितरित ऑप्टिमाइज़र</h2>
<p>उपयोग: <code>--use-distributed-optimizer</code>। सभी मॉडल और डेटा प्रकारों के साथ संगत।</p>
<p>वितरित ऑप्टिमाइज़र एक मेमोरी बचत तकनीक है, जिसमें ऑप्टिमाइज़र स्टेट डेटा पैरेलल रैंक्स में समान रूप से वितरित किया जाता है (परंपरागत तरीके के बजाय जिसमें ऑप्टिमाइज़र स्टेट डेटा पैरेलल रैंक्स में डुप्लिकेट किया जाता है)। जैसा कि <a href="https://arxiv.org/abs/1910.02054">ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</a> में वर्णित है, हमारी इम्प्लीमेंटेशन सभी ऑप्टिमाइज़र स्टेट को वितरित करती है जो मॉडल स्टेट से ओवरलैप नहीं करता। उदाहरण के लिए, जब fp16 मॉडल पैरामीटर का उपयोग किया जाता है, तो वितरित ऑप्टिमाइज़र अपनी स्वयं की अलग fp32 मुख्य पैरामीटर एवं ग्रेडिएंट्स की कॉपी बनाए रखता है, जो DP रैंक्स में वितरित होते हैं। जब bf16 मॉडल पैरामीटर का उपयोग होता है, तो वितरित ऑप्टिमाइज़र के fp32 मुख्य ग्रेडिएंट्स मॉडल के fp32 ग्रेडिएंट्स के समान होते हैं, और इसलिए इस मामले में ग्रेडिएंट्स वितरित नहीं होते (हालांकि fp32 मुख्य पैरामीटर अभी भी वितरित होते हैं, क्योंकि वे bf16 मॉडल पैरामीटर से अलग होते हैं)।</p>
<p>सैद्धांतिक मेमोरी बचत मॉडल के पैरामीटर dtype और grad dtype के संयोजन पर निर्भर करती है। हमारी इम्प्लीमेंटेशन में, सैद्धांतिक प्रति पैरामीटर बाइट्स की संख्या (जहाँ 'd' डेटा पैरेलल साइज़ है):</p>
<p>| | गैर-वितरित ऑप्टिम | वितरित ऑप्टिम |
|-|-|-|
| fp16 पैराम, fp16 ग्रेड | 20 | 4 + 16/d |
| bf16 पैराम, fp32 ग्रेड | 18 | 6 + 12/d |
| fp32 पैराम, fp32 ग्रेड | 16 | 8 + 8/d |</p>
<p>सामान्य डेटा पैरेललिज़्म की तरह, ग्रेडिएंट रिडक्शन (इस मामले में, रिड्यूस-स्कैटर) को बैकवर्ड पास के साथ ओवरलैप करने के लिए <code>--overlap-grad-reduce</code> फ्लैग का उपयोग किया जा सकता है। साथ ही, पैरामीटर ऑल-गैदर को फॉरवर्ड पास के साथ ओवरलैप करने के लिए <code>--overlap-param-gather</code> का उपयोग किया जा सकता है।</p>
<h2>FlashAttention</h2>
<p>उपयोग: <code>--use-flash-attn</code>। अधिकतम 128 अटेंशन हेड डायमेंशन का समर्थन।</p>
<p><a href="https://github.com/HazyResearch/flash-attention">FlashAttention</a> एक तेज़ और
मेमोरी-कुशल एल्गोरिदम है जो सटीक अटेंशन की गणना करता है। यह मॉडल
प्रशिक्षण को तेज़ करता है और मेमोरी आवश्यकता को कम करता है।</p>
<p>FlashAttention स्थापित करने के लिए:</p>
<pre><code class="language-sh">pip install flash-attn
</code></pre>
<h2>GPT-3 उदाहरण</h2>
<p><code>examples/gpt3/train_gpt3_175b_distributed.sh</code> में हमने उदाहरण दिया है कि कैसे Megatron को 175 अरब पैरामीटर वाले <a href="https://arxiv.org/abs/2005.14165">GPT-3</a> को 1024 GPUs पर प्रशिक्षित करने के लिए कॉन्फ़िगर किया जाए। स्क्रिप्ट <a href="https://slurm.schedmd.com/documentation.html">slurm</a> के लिए डिज़ाइन की गई है, जिसमें <a href="https://github.com/NVIDIA/pyxis">pyxis</a> प्लगइन है, लेकिन इसे किसी भी अन्य शेड्यूलर के लिए आसानी से अनुकूलित किया जा सकता है। यह 8-वे टेन्सर पैरेललिज़्म और 16-वे पाइपलाइन पैरेललिज़्म का उपयोग करता है। विकल्पों <code>global-batch-size 1536</code> और <code>rampup-batch-size 16 16 5859375</code> के साथ, प्रशिक्षण ग्लोबल बैच साइज़ 16 के साथ शुरू होगा और 5,859,375 सैंपल्स पर 16 के इंक्रीमेंटल स्टेप्स के साथ ग्लोबल बैच साइज़ को 1536 तक बढ़ाएगा। प्रशिक्षण डेटासेट एकल सेट या वेट्स के साथ कई डेटासेट्स का संयोजन हो सकता है।</p>
<p>1024 A100 GPUs पर पूर्ण ग्लोबल बैच साइज़ 1536 के साथ, प्रत्येक पुनरावृत्ति में लगभग 32 सेकंड लगते हैं, जिससे प्रति GPU 138 टेराFLOPs प्राप्त होते हैं, जो कि सैद्धांतिक चरम FLOPs का 44% है।</p>
<h2>Retro और InstructRetro</h2>
<p>Retro <a href="https://arxiv.org/abs/2112.04426">(Borgeaud et al., 2022)</a> एक ऑटोरेग्रेसिव डिकोडर-ओनली लैंग्वेज मॉडल (LM) है जिसे रिट्रीवल-अगमेंटेशन के साथ प्रीट्रेन किया गया है।
Retro में स्केलेबिलिटी है जिससे ट्रिलियनों टोकन से बड़े पैमाने पर प्रीट्रेनिंग संभव होती है।
रिट्रीवल के साथ प्रीट्रेनिंग फैक्चुअल नॉलेज के लिए एक अधिक कुशल स्टोरेज मैकेनिज्म प्रदान करती है, तुलना में जब फैक्चुअल नॉलेज नेटवर्क के पैरामीटरों में अंतर्निहित रूप से स्टोर होती है, जिससे मॉडल पैरामीटरों में काफी कमी आती है और GPT की तुलना में कम पर्प्लेक्सिटी प्राप्त होती है।
Retro यह भी लचीलापन देता है कि LMs में स्टोर किए गए नॉलेज को अपडेट किया जा सकता है <a href="https://arxiv.org/abs/2304.06762">(Wang et al., 2023a)</a>
रिट्रीवल डेटाबेस को अपडेट करके बिना LMs को फिर से प्रशिक्षित किए।</p>
<p>InstructRetro <a href="https://arxiv.org/abs/2310.07713">(Wang et al., 2023b)</a> ने Retro के आकार को 48B तक बढ़ा दिया है, जो अब तक का सबसे बड़ा रिट्रीवल प्रीट्रेन LLM है (दिसंबर 2023 तक)।
प्राप्त फाउंडेशन मॉडल, Retro 48B, पर्प्लेक्सिटी के मामले में GPT समकक्ष की तुलना में काफी बेहतर है।
Retro पर इंस्ट्रक्शन ट्यूनिंग के साथ, InstructRetro डाउनस्ट्रीम टास्क्स में जीरो-शॉट सेटिंग में इंस्ट्रक्शन ट्यून GPT की तुलना में महत्वपूर्ण सुधार दिखाता है। विशेष रूप से, InstructRetro का औसत सुधार 8 छोटे QA टास्क्स में GPT की तुलना में 7% और 4 चुनौतीपूर्ण लंबे QA टास्क्स में GPT की तुलना में 10% है। हमने यह भी पाया कि कोई InstructRetro आर्किटेक्चर से एन्कोडर को हटा सकता है और सीधे InstructRetro डिकोडर बैकबोन को GPT के रूप में उपयोग कर सकता है, फिर भी तुलनीय परिणाम प्राप्त कर सकता है।</p>
<p>इस रिपॉजिटरी में, हम Retro और InstructRetro को लागू करने के लिए एंड-टू-एंड रीप्रोडक्शन गाइड प्रदान करते हैं, जिसमें शामिल है:</p>
<ul>
<li><strong>रिट्रीवल डेटाबेस निर्माण</strong>, जो अरबों या ट्रिलियनों टोकन को बड़े पैमाने पर रिट्रीवल डेटाबेस के रूप में समर्थन करता है।</li>
<li><strong>रिट्रीवल के साथ प्रीट्रेनिंग</strong>, जो स्क्रैच से प्रीट्रेनिंग और प्रीट्रेन किए गए GPT मॉडल से प्रीट्रेनिंग (Retro-fitting) को समर्थन करता है।</li>
<li><strong>इंस्ट्रक्शन ट्यूनिंग</strong>, जहाँ हम ओपन-सोर्स इंस्ट्रक्शन ट्यूनिंग डेटासेट और Retro पर इंस्ट्रक्शन ट्यूनिंग के लिए प्रशिक्षण विधि प्रदान करते हैं।</li>
<li><strong>डाउनस्ट्रीम टास्क मूल्यांकन</strong>, जहाँ हम जीरो-शॉट प्रश्नोत्तर कार्यों के लिए टेक्स्ट जनरेशन और मूल्यांकन स्क्रिप्ट्स प्रदान करते हैं।</li>
</ul>
<p>विस्तृत अवलोकन के लिए देखें <a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/tools/retro/README.md">tools/retro/README.md</a>।</p>
<h2>Mamba-आधारित भाषा मॉडल</h2>
<p>विवरण के लिए देखें <a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/examples/mamba">examples/mamba</a>।</p>
<!--
## REALM पाइपलाइन
हम [REALM](https://arxiv.org/pdf/2002.08909.pdf) सिस्टम को लागू करने पर काम कर रहे हैं। निम्नलिखित अनुभाग (होंगे) इसके प्रशिक्षण के तीन चरणों को दर्शाते हैं। फिलहाल यहाँ केवल ICT कोड है।
संक्षेप में, सबसे पहले रिट्रीवर मॉड्यूल्स का प्रीट्रेनिंग, फिर भाषा मॉडल और रिट्रीवर का संयुक्त प्रशिक्षण, और फिर फिक्स्ड रिट्रीवर के साथ भाषा मॉडल पर प्रश्नोत्तर हेड का फाइनट्यूनिंग।

### इनवर्स क्लोज़ टास्क (ICT) प्रीट्रेनिंग
1. एक कॉर्पस रखें जो लूज JSON प्रारूप में हो, जिसमें उद्देश्य निश्चित आकार के ब्लॉक बनाना हो जो डेटा की मूल इकाइयाँ हों। विकिपीडिया जैसे कॉर्पस के लिए, इसका अर्थ होगा प्रति ब्लॉक कई वाक्य, लेकिन प्रति दस्तावेज़ कई ब्लॉक भी।
`tools/preprocess_data.py` चलाएँ ताकि `--split-sentences` आर्ग्युमेंट के साथ एक या एक से अधिक इंडेक्स्ड डेटासेट बनाए जा सकें, ताकि वाक्य मौलिक इकाई बनें। मूल REALM सिस्टम के लिए, हम दो डेटासेट बनाते हैं, एक प्रत्येक दस्तावेज़ के शीर्षक के साथ, और दूसरा बॉडी के साथ।
निम्नलिखित स्क्रिप्ट देखें
<pre>
python preprocess_data.py \
    --input /path/to/corpus.json \
    --json-keys text title \
    --split-sentences \
    --tokenizer-type BertWordPieceLowerCase \
    --vocab-file /path/to/vocab.txt \
    --output-prefix corpus_indexed \
    --workers 5  # 10 CPU कोर के लिए उपयुक्त। उसके अनुसार बढ़ाएँ।
</pre>

2. यदि आवश्यक हो तो `megatron/legacy/data/realm_dataset_utils.get_block_samples_mapping` के स्थान पर एक कस्टम सैंपल्स मैपिंग फंक्शन का उपयोग करें। ऐसा करने के लिए, आपको `megatron/core/datasets/helpers.cpp` के अंदर C++ में एक नया फंक्शन लागू करना होगा। सैंपल्स मैपिंग डेटा संरचना का उपयोग प्रत्येक प्रशिक्षण सैंपल के लिए आवश्यक डेटा का चयन करने के लिए किया जाता है, प्रशिक्षण लूप से पहले।
  सैंपल्स मैपिंग सभी आवश्यक मेटाडेटा को रखने के लिए जिम्मेदार है जो एक या अधिक इंडेक्स्ड डेटासेट से सैंपल बनाने के लिए आवश्यक है। REALM में, सैंपल्स मैपिंग में स्टार्ट और एंड वाक्य सूचकांक, साथ ही दस्तावेज़ सूचकांक (सही शीर्षक पाने के लिए) और प्रत्येक ब्लॉक के लिए एक अद्वितीय ID शामिल है।
3. `pretrain_bert.py` का उपयोग करके एक BERT भाषा मॉडल का प्रीट्रेन करें, जिसमें अनुक्रम लंबाई टोकन IDs में ब्लॉक के आकार के बराबर हो। यह मॉडल उसी इंडेक्स्ड डेटासेट पर प्रशिक्षित होना चाहिए, जिसका उपयोग सूचना पुनःप्राप्ति कार्य के लिए ब्लॉकों की आपूर्ति के लिए किया जाता है।
REALM में, यह एक अनकेस्ड bert बेस मॉडल है जिसे मानक हाइपरपैरामीटर्स के साथ प्रशिक्षित किया गया है।
4. `pretrain_ict.py` का उपयोग करें ताकि एक `ICTBertModel` को प्रशिक्षित किया जा सके, जिसमें क्वेरी और ब्लॉकों को एन्कोड करने के लिए दो BERT-आधारित एन्कोडर होते हैं, ताकि पुनःप्राप्ति की जा सके।
नीचे दी गई स्क्रिप्ट REALM के ICT मॉडल को प्रशिक्षित करती है। इसमें `--bert-load` आर्ग्युमेंट में प्रीट्रेन किए गए BERT मॉडल (चरण 3) का संदर्भ होता है। पेपर में प्रयुक्त बैच साइज़ 4096 है, इसलिए इसे डेटा पैरेलल वर्ल्ड साइज़ 32 के साथ चलाना होगा।
<pre>
python pretrain_ict.py \
    --num-layers 12 \
    --num-attention-heads 12 \
    --hidden-size 768 \
    --batch-size 128 \
    --seq-length 256 \
    --max-position-embeddings 256 \
    --ict-head-size 128 \
    --train-iters 100000 \
    --bert-load /path/to/pretrained_bert \
    --load checkpoints \
    --save checkpoints \
    --data-path /path/to/indexed_dataset \
    --titles-data-path /path/to/titles_indexed_dataset \
    --vocab-file /path/to/vocab.txt \
    --lr 0.0001 \
    --num-workers 2 \
    --lr-decay-style linear \
    --weight-decay 1e-2 \
    --clip-grad 1.0 \
    --lr-warmup-fraction .01 \
    --save-interval 3000 \
    --query-in-block-prob 0.1 \
    --fp16

</pre>

### ब्लॉक एम्बेडिंग्स का इंडेक्स बनाना
एक ICT मॉडल प्रशिक्षित करने के बाद, अब आप एक संपूर्ण ब्लॉकों के डेटासेट को एम्बेड कर सकते हैं, इसके लिए `BlockData` संरचना बनाएं। उसके बाद, आप इसे सेव कर सकते हैं,
और उसे `FaissMIPSIndex` के साथ लोड करके तेज़ समानता खोज (similarity search) कर सकते हैं, जो सीखा हुआ सूचना पुनःप्राप्ति पाइपलाइन में मुख्य है। प्रारंभिक इंडेक्स निम्नलिखित स्क्रिप्ट के साथ बनाया जा सकता है, जिसे इंटरएक्टिव सत्र में चलाने के लिए डिज़ाइन किया गया है। यह बड़े डेटासेट्स को तेज़ी से इंडेक्स करने के लिए कई GPUs और नोड्स का लाभ उठा सकता है।

<pre>
python tools/create_doc_index.py \
    --num-layers 12 \
    --hidden-size 768 \
    --ict-head-size 128 \
    --num-attention-heads 12 \
    --batch-size 128 \
    --seq-length 256 \
    --max-position-embeddings 256 \
    --ict-load /path/to/pretrained_ict \
    --data-path /path/to/indexed_dataset \
    --titles-data-path /path/to/titles_indexed_dataset \
    --block-data-path embedded_blocks.pkl \
    --indexer-log-interval 1000 \
    --indexer-batch-size 128 \
    --vocab-file /path/to/vocab.txt \
    --num-workers 2 \
    --fp16
</pre>

-->
<h2>विशेषज्ञों का मिश्रण (Mixture of Experts)</h2>
<p>MoE (विशेषज्ञों का मिश्रण) एक शक्तिशाली LLM आर्किटेक्चर है जिसे Megatron-Core फ्रेमवर्क में लागू किया गया है, जो बड़े भाषा मॉडल की दक्षता और स्केलेबिलिटी को बढ़ाने के लिए डिज़ाइन किया गया है। यह <strong>विशेषज्ञ समानांतरता (Expert Parallelism)</strong> का लाभ उठाता है, जिससे विभिन्न वर्करों में कई विशेषज्ञ वितरित किए जा सकते हैं, जहाँ प्रत्येक वर्कर प्रशिक्षण नमूनों के अलग-अलग बैचों को प्रोसेस करता है। यह विधि कम्प्यूटेशनल थ्रूपुट को काफी हद तक बढ़ा देती है, जिससे मॉडल उच्च प्रदर्शन मेट्रिक्स प्राप्त कर सकते हैं, जैसे कि H100 पर 8x7B के लिए BF16 प्रशिक्षण के दौरान 47% MFU।</p>
<p>MoE की प्रमुख विशेषताएँ:</p>
<ul>
<li><strong>समानांतरता तकनीकें</strong>: MoE विभिन्न समानांतरता रणनीतियों को जोड़ता है, जिनमें विशेषज्ञ समानांतरता, डेटा समानांतरता, टेन्सर समानांतरता, अनुक्रम समानांतरता, पाइपलाइन समानांतरता और संदर्भ समानांतरता शामिल हैं। यह संयोजन बड़े मॉडल वेरिएंट्स को प्रभावी ढंग से संभालने की अनुमति देता है।</li>
<li><strong>राउटर और लोड संतुलन</strong>: सिस्टम उन्नत रूटिंग तंत्र जैसे Top-K राउटर का उपयोग करता है और विशेषज्ञों के बीच टोकन वितरण को अनुकूलित करने के लिए लोड बैलेंसिंग एल्गोरिदम का उपयोग करता है।</li>
<li><strong>प्रदर्शन अनुकूलन</strong>: GroupedGEMM और FP8 प्रशिक्षण जैसी तकनीकें MoE मॉडलों की दक्षता बढ़ाती हैं, खासकर जब कई विशेषज्ञ शामिल होते हैं।</li>
<li><strong>टोकन डिस्पैच तंत्र</strong>: MoE विशेषज्ञों के बीच टोकन वितरण को प्रभावी ढंग से प्रबंधित करने के लिए dropless और token drop दोनों रणनीतियों का समर्थन करता है।</li>
</ul>
<p>MoE प्रशिक्षण कॉन्फ़िगरेशन और अनुकूलन का संपूर्ण अवलोकन प्राप्त करने के लिए, कृपया विस्तृत README देखें: <a href="./megatron/core/transformer/moe/README.md">megatron/core/transformer/moe/README.md</a>।</p>
<h1>मूल्यांकन और कार्य</h1>
<p>हम विभिन्न शून्य-शॉट और फाइन-ट्यून किए गए डाउनस्ट्रीम कार्यों को संभालने के लिए कई कमांड लाइन तर्क प्रदान करते हैं, जिनका विवरण नीचे दिए गए स्क्रिप्ट्स में है। हालाँकि, आप अपनी आवश्यकता अनुसार अन्य कॉर्पस पर प्रीट्रेंड चेकपॉइंट से अपने मॉडल को फाइनट्यून भी कर सकते हैं। ऐसा करने के लिए, बस <code>--finetune</code> फ्लैग जोड़ें और इनपुट फाइलें तथा प्रशिक्षण पैरामीटर मूल प्रशिक्षण स्क्रिप्ट में समायोजित करें। इटरेशन काउंट शून्य पर रीसेट हो जाएगा, और ऑप्टिमाइज़र तथा आंतरिक स्थिति को पुनः आरंभ किया जाएगा। यदि किसी कारणवश फाइन-ट्यूनिंग बाधित हो जाती है, तो जारी रखने से पहले <code>--finetune</code> फ्लैग अवश्य हटा दें, अन्यथा प्रशिक्षण फिर से प्रारंभ से शुरू हो जाएगा।</p>
<p>चूँकि मूल्यांकन के लिए प्रशिक्षण की तुलना में काफी कम मेमोरी की आवश्यकता होती है, इसलिए डाउनस्ट्रीम कार्यों में कम GPU पर उपयोग के लिए समानांतर में प्रशिक्षित मॉडल को मर्ज करना फायदेमंद हो सकता है। निम्नलिखित स्क्रिप्ट यह कार्य करती है। यह उदाहरण 4-वे टेन्सर और 4-वे पाइपलाइन मॉडल समानांतरता के साथ GPT मॉडल को पढ़ता है और 2-वे टेन्सर और 2-वे पाइपलाइन मॉडल समानांतरता के साथ मॉडल को लिखता है।</p>
<pre>
python tools/checkpoint/convert.py \
        --model-type GPT \
        --load-dir checkpoints/gpt3_tp4_pp4 \
        --save-dir checkpoints/gpt3_tp2_pp2 \
        --target-tensor-parallel-size 2 \
        --target-pipeline-parallel-size 2

</pre>
<p>नीचे GPT और BERT दोनों मॉडलों के लिए कई डाउनस्ट्रीम कार्यों का वर्णन किया गया है। इन्हें प्रशिक्षण स्क्रिप्ट्स में उपयोग किए गए समान परिवर्तनों के साथ वितरित और मॉडल समानांतर मोड में चलाया जा सकता है।</p>
<h2>GPT पाठ निर्माण (Text Generation)</h2>
<p>हमने <code>tools/run_text_generation_server.py</code> में टेक्स्ट जनरेशन के लिए एक सरल REST सर्वर शामिल किया है। आप इसे उसी तरह चला सकते हैं जैसे आप प्रीट्रेनिंग जॉब शुरू करते हैं, जिसमें उपयुक्त प्रीट्रेंड चेकपॉइंट निर्दिष्ट करते हैं। इसमें कुछ वैकल्पिक पैरामीटर भी हैं: <code>temperature</code>, <code>top-k</code> और <code>top-p</code>। अधिक जानकारी के लिए <code>--help</code> या सोर्स फाइल देखें। सर्वर कैसे चलाना है इसका उदाहरण देखें: <a href="examples/inference/run_text_generation_server_345M.sh">examples/inference/run_text_generation_server_345M.sh</a>।</p>
<p>एक बार सर्वर चलने के बाद आप <code>tools/text_generation_cli.py</code> का उपयोग करके इसे क्वेरी कर सकते हैं, इसमें एक तर्क देना होता है जो सर्वर का होस्ट है।</p>
<pre>
tools/text_generation_cli.py localhost:5000
</pre>
<p>आप CURL या अन्य किसी भी टूल का उपयोग करके सर्वर को सीधे क्वेरी कर सकते हैं:</p>
<pre>
curl 'http://localhost:5000/api' -X 'PUT' -H 'Content-Type: application/json; charset=UTF-8'  -d '{"prompts":["Hello world"], "tokens_to_generate":1}'
</pre>
<p>अधिक API विकल्पों के लिए देखें: <a href="megatron/inference/text_generation_server.py">megatron/inference/text_generation_server.py</a></p>
<h3>GPT को स्वयं-निर्माण द्वारा डिटॉक्सिफाई करना</h3>
<p>हमने <code>examples/academic_paper_scripts/detxoify_lm/</code> में एक उदाहरण शामिल किया है, जो भाषा मॉडलों की जनरेटिव क्षमता का लाभ उठाकर उन्हें डिटॉक्सिफाई करता है।</p>
<p>डोमेन-अनुकूलन प्रशिक्षण और स्व-जनित कॉर्पस का उपयोग कर LM को डिटॉक्सिफाई करने के लिए चरण-दर-चरण ट्यूटोरियल देखें: <a href="examples/academic_paper_scripts/detxoify_lm/README.md">examples/academic_paper_scripts/detxoify_lm/README.md</a>।</p>
<h2>GPT मूल्यांकन</h2>
<p>हमने GPT मूल्यांकन के लिए WikiText पर्प्लेक्सिटी मूल्यांकन और LAMBADA क्लोज़ एक्युरेसी के उदाहरण स्क्रिप्ट्स शामिल किए हैं।</p>
<h3>WikiText पर्प्लेक्सिटी मूल्यांकन</h3>
<p>पिछले कार्यों के साथ उचित तुलना के लिए, हम शब्द-स्तरीय <a href="https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip">WikiText-103 टेस्ट डेटासेट</a> पर पर्प्लेक्सिटी का मूल्यांकन करते हैं, और हमारे सबवर्ड टोकनाइज़र का उपयोग करने पर टोकन में परिवर्तन को ध्यान में रखते हुए उपयुक्त पर्प्लेक्सिटी की गणना करते हैं।</p>
<p>हम 345M पैरामीटर मॉडल पर WikiText-103 मूल्यांकन चलाने के लिए निम्नलिखित कमांड का उपयोग करते हैं।</p>
<pre>
TASK="WIKITEXT103"

VALID_DATA=&#60;wikitext path&#62;.txt
VOCAB_FILE=gpt2-vocab.json
MERGE_FILE=gpt2-merges.txt
CHECKPOINT_PATH=checkpoints/gpt2_345m

COMMON_TASK_ARGS="--num-layers 24 \
                  --hidden-size 1024 \
                  --num-attention-heads 16 \
                  --seq-length 1024 \
                  --max-position-embeddings 1024 \
                  --fp16 \
                  --vocab-file $VOCAB_FILE"

python tasks/main.py \
       --task $TASK \
       $COMMON_TASK_ARGS \
       --valid-data $VALID_DATA \
       --tokenizer-type GPT2BPETokenizer \
       --merge-file $MERGE_FILE \
       --load $CHECKPOINT_PATH \
       --micro-batch-size 8 \
       --log-interval 10 \
       --no-load-optim \
       --no-load-rng
</pre>
<h3>LAMBADA क्लोज़ एक्युरेसी</h3>
<p>LAMBADA क्लोज़ एक्युरेसी (पूर्ववर्ती टोकनों के आधार पर अंतिम टोकन की सटीकता) की गणना के लिए हम <a href="https://github.com/cybertronai/bflm/blob/master/lambada_test.jsonl">LAMBADA डेटासेट</a> के डिटोकनाइज़्ड, प्रोसेस्ड संस्करण का उपयोग करते हैं।</p>
<p>हम 345M पैरामीटर मॉडल पर LAMBADA मूल्यांकन चलाने के लिए निम्नलिखित कमांड का उपयोग करते हैं। ध्यान दें कि पूरे शब्द मिलान की आवश्यकता के लिए <code>--strict-lambada</code> फ्लैग का उपयोग किया जाना चाहिए। सुनिश्चित करें कि <code>lambada</code> फाइल पथ का हिस्सा है।</p>
<pre>
TASK="LAMBADA"

VALID_DATA=&#60;lambada path&#62;.json
VOCAB_FILE=gpt2-vocab.json
MERGE_FILE=gpt2-merges.txt
CHECKPOINT_PATH=checkpoints/gpt2_345m
COMMON_TASK_ARGS=&#60;same as those in <a href="#wikitext-perplexity-evaluation">WikiText Perplexity Evaluation</a> above&#62;

python tasks/main.py \
       --task $TASK \
       $COMMON_TASK_ARGS \
       --valid-data $VALID_DATA \
       --tokenizer-type GPT2BPETokenizer \
       --strict-lambada \
       --merge-file $MERGE_FILE \
       --load $CHECKPOINT_PATH \
       --micro-batch-size 8 \
       --log-interval 10 \
       --no-load-optim \
       --no-load-rng
</pre>
<p>अधिक कमांड लाइन तर्कों का विवरण स्रोत फाइल <a href="./tasks/main.py"><code>main.py</code></a> में दिया गया है।</p>
<h2>BERT कार्य मूल्यांकन</h2>
<h3>RACE मूल्यांकन</h3>
<p>निम्नलिखित स्क्रिप्ट BERT मॉडल को <a href="http://www.cs.cmu.edu/%7Eglai1/data/race/">RACE डेटासेट</a> पर मूल्यांकन के लिए फाइनट्यून करती है। <code>TRAIN_DATA</code> और <code>VALID_DATA</code> डायरेक्टरी में RACE डेटासेट अलग-अलग <code>.txt</code> फाइलों के रूप में होता है। ध्यान दें कि RACE के लिए, बैच साइज़ मूल्यांकन के लिए RACE क्वेरी की संख्या है। चूंकि प्रत्येक RACE क्वेरी में चार सैंपल होते हैं, मॉडल के माध्यम से जाने वाला प्रभावी बैच साइज़ कमांड लाइन पर निर्दिष्ट बैच साइज़ का चार गुना होगा।</p>
<pre>
TRAIN_DATA="data/RACE/train/middle"
VALID_DATA="data/RACE/dev/middle \
            data/RACE/dev/high"
VOCAB_FILE=bert-vocab.txt
PRETRAINED_CHECKPOINT=checkpoints/bert_345m
CHECKPOINT_PATH=checkpoints/bert_345m_race
COMMON_TASK_ARGS="--num-layers 24 \
                  --hidden-size 1024 \
                  --num-attention-heads 16 \
                  --seq-length 512 \
                  --max-position-embeddings 512 \
                  --fp16 \
                  --vocab-file $VOCAB_FILE"

COMMON_TASK_ARGS_EXT="--train-data $TRAIN_DATA \
                      --valid-data $VALID_DATA \
                      --pretrained-checkpoint $PRETRAINED_CHECKPOINT \
                      --save-interval 10000 \
                      --save $CHECKPOINT_PATH \
                      --log-interval 100 \
                      --eval-interval 1000 \
                      --eval-iters 10 \
                      --weight-decay 1.0e-1"

python tasks/main.py \
       --task RACE \
       $COMMON_TASK_ARGS \
       $COMMON_TASK_ARGS_EXT \
       --tokenizer-type BertWordPieceLowerCase \
       --epochs 3 \
       --micro-batch-size 4 \
       --lr 1.0e-5 \
       --lr-warmup-fraction 0.06
</pre>
<h3>MNLI मूल्यांकन</h3>
<p>निम्नलिखित स्क्रिप्ट BERT मॉडल को <a href="https://www.nyu.edu/projects/bowman/multinli/">MultiNLI वाक्य युग्म कॉर्पस</a> के साथ मूल्यांकन के लिए फाइनट्यून करती है। चूँकि मिलान कार्य काफी समान हैं, स्क्रिप्ट को आसानी से <a href="https://www.kaggle.com/quora/question-pairs-dataset">Quora Question Pairs</a> (QQP) डेटासेट के साथ काम करने के लिए भी बदला जा सकता है।</p>
<pre>

TRAIN_DATA="data/glue_data/MNLI/train.tsv"
VALID_DATA="data/glue_data/MNLI/dev_matched.tsv \
            data/glue_data/MNLI/dev_mismatched.tsv"
PRETRAINED_CHECKPOINT=checkpoints/bert_345m
VOCAB_FILE=bert-vocab.txt
CHECKPOINT_PATH=checkpoints/bert_345m_mnli
COMMON_TASK_ARGS=&#60;same as those in <a href="#race-evaluation">RACE Evaluation</a> above&#62;
COMMON_TASK_ARGS_EXT=&#60;same as those in <a href="#race-evaluation">RACE Evaluation</a> above&#62;

python tasks/main.py \
       --task MNLI \
       $COMMON_TASK_ARGS \
       $COMMON_TASK_ARGS_EXT \
```
   --tokenizer-type BertWordPieceLowerCase \
   --epochs 5 \
   --micro-batch-size 8 \
   --lr 5.0e-5 \
   --lr-warmup-fraction 0.065
</pre>
<h2>Llama-2 इंफरेंस और फाइनट्यूनिंग</h2>
<p>Llama-2 <a href="https://ai.meta.com/llama/">मॉडल परिवार</a> एक ओपन-सोर्स प्रीट्रेंड और फाइनट्यूनड (चैट के लिए) मॉडल का सेट है, जिसने कई बेन्चमार्क्स में बेहतरीन परिणाम हासिल किए हैं। रिलीज के समय, Llama-2 मॉडल्स ने ओपन-सोर्स मॉडल्स में सबसे अच्छे परिणाम प्राप्त किए और क्लोज्ड-सोर्स GPT-3.5 मॉडल के साथ प्रतिस्पर्धी रहे (देखें https://arxiv.org/pdf/2307.09288.pdf)।</p>
<p>Llama-2 चेकपॉइंट्स को इंफरेंस और फाइनट्यूनिंग के लिए Megatron में लोड किया जा सकता है। डाक्यूमेंटेशन देखें <a href="docs/llama_mistral.md">यहाँ</a>।</p>
<h1>मॉडल ऑप्टिमाइज़ेशन और डिप्लॉयमेंट</h1>
<p>Megatron-Core (MCore) <code>GPTModel</code> परिवार उन्नत क्वांटाइज़ेशन एल्गोरिद्म और TensorRT-LLM के माध्यम से उच्च-प्रदर्शन इंफरेंस को सपोर्ट करता है।</p>
<h2>क्वांटाइज़ेशन और TensorRT-LLM डिप्लॉयमेंट</h2>
<p><code>llama2</code> और <code>nemotron3</code> उदाहरणों के लिए देखें <a href="examples/inference/quantization/README.md">Megatron Model Optimization and Deployment</a>।</p>
<h1>डेटासेट्स</h1>
<p>हम GPT या BERT ट्रेनिंग के लिए कोई डेटासेट होस्ट नहीं करते हैं, लेकिन हम उनके संग्रह की प्रक्रिया बताते हैं ताकि हमारे परिणामों को पुन: उत्पन्न किया जा सके।</p>
<h2>विकिपीडिया ट्रेनिंग डेटा एकत्र करना</h2>
<p>हम Google रिसर्च द्वारा निर्दिष्ट विकिपीडिया डेटा एक्सट्रैक्शन प्रक्रिया का पालन करने की सलाह देते हैं: &quot;अनुशंसित प्री-प्रोसेसिंग है <a href="https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2">लेटेस्ट डंप</a> डाउनलोड करें, <a href="https://github.com/attardi/wikiextractor">WikiExtractor.py</a> से टेक्स्ट एक्सट्रैक्ट करें, और फिर आवश्यक क्लीनअप लागू करें ताकि इसे प्लेन टेक्स्ट में बदला जा सके।&quot;</p>
<p>हम WikiExtractor का उपयोग करते समय <code>--json</code> आर्ग्युमेंट का उपयोग करने की सलाह देते हैं, जिससे विकिपीडिया डेटा को लूज़ json फॉर्मेट (प्रत्येक लाइन में एक json ऑब्जेक्ट) में डंप किया जाएगा, जिससे यह फ़ाइल सिस्टम पर अधिक प्रबंधनीय हो जाता है और हमारी कोडबेस द्वारा आसानी से उपभोग योग्य हो जाता है। हम इस json डेटासेट को आगे nltk पंक्चुएशन स्टैंडर्डाइज़ेशन के साथ प्री-प्रोसेस करने की सलाह देते हैं। BERT ट्रेनिंग के लिए, वाक्य ब्रेक को बनाए रखने हेतु <a href="#data-preprocessing">ऊपर</a> बताए अनुसार <code>preprocess_data.py</code> को <code>--split-sentences</code> फ्लैग के साथ चलाएं। यदि आप GPT ट्रेनिंग के लिए विकिपीडिया डेटा का उपयोग करना चाहते हैं, तो फिर भी इसे nltk/spacy/ftfy से साफ करें, लेकिन <code>--split-sentences</code> फ्लैग का उपयोग न करें।</p>
<h2>GPT Webtext डेटा एकत्र करना</h2>
<p>हम सार्वजनिक रूप से उपलब्ध <a href="https://github.com/eukaryote31/openwebtext">OpenWebText</a> लाइब्रेरी का उपयोग करते हैं, जिसे <a href="https://github.com/jcpeterson/openwebtext">jcpeterson</a> और <a href="https://github.com/eukaryote31/openwebtext">eukaryote31</a> द्वारा विकसित किया गया है, URLs डाउनलोड करने के लिए। इसके बाद हम सभी डाउनलोड की गई सामग्री को <a href="./tools/openwebtext">openwebtext</a> निर्देशिका में वर्णित प्रक्रिया के अनुसार फ़िल्टर, क्लीन और डेडुप्लिकेट करते हैं। अक्टूबर 2018 तक की Reddit URLs के लिए, हमारे पास लगभग 37GB सामग्री थी।</p>
<h1>पुनरुत्पादकता (Reproducibility)</h1>
<p>Megatron ट्रेनिंग बिटवाइज़ पुनरुत्पादक हो सकती है; इस मोड को सक्षम करने के लिए <code>--deterministic-mode</code> का उपयोग करें। इसका अर्थ है कि एक ही ट्रेनिंग कॉन्फ़िग दो बार एक ही HW और SW वातावरण में चलाने पर एक समान मॉडल चेकपॉइंट्स, लॉस और एक्यूरेसी मेट्रिक वैल्यू (इटरेशन समय मेट्रिक्स भिन्न हो सकते हैं) उत्पन्न करेगा।</p>
<p>वर्तमान में तीन ज्ञात Megatron ऑप्टिमाइज़ेशन हैं जो पुनरुत्पादकता को तोड़ते हैं, जबकि लगभग समान ट्रेनिंग रन उत्पन्न करते हैं:</p>
<ol>
<li>ऑल-रिड्यूस के दौरान प्रयुक्त विशिष्ट NCCL एल्गोरिद्म (जैसा कि एनवायरनमेंट वेरिएबल <code>NCCL_ALGO</code> द्वारा निर्दिष्ट है) महत्वपूर्ण है। हमने निम्नलिखित का परीक्षण किया है: <code>^NVLS</code>, <code>Tree</code>, <code>Ring</code>, <code>CollnetDirect</code>, <code>CollnetChain</code>। कोड <code>^NVLS</code> के उपयोग की अनुमति देता है, जो NCCL को non-NVLS एल्गोरिद्म चुनने की अनुमति देता है; इसका चयन स्थिर प्रतीत होता है।</li>
<li>फ्लैश अटेंशन अन-डिटरमिनिस्टिक है; <code>--use-flash-attn</code> का उपयोग न करें।</li>
<li>यदि Transformer Engine का उपयोग कर रहे हैं, तो आपको एनवायरनमेंट वेरिएबल <code>NVTE_ALLOW_NONDETERMINISTIC_ALGO=0</code> भी सेट करना होगा।</li>
</ol>
<p>इसके अतिरिक्त, डिटरमिनिज्म केवल NGC PyTorch कंटेनर संस्करण 23.12 तक और उसके बाद सत्यापित किया गया है। यदि आप अन्य परिस्थितियों में Megatron ट्रेनिंग में nondeterminism देखते हैं, तो कृपया एक issue खोलें।</p>
<h1>चेकपॉइंट कन्वर्ज़न</h1>
<p>हम दो प्रकार के मॉडल कन्वर्ज़न को सपोर्ट करते हैं:</p>
<ol>
<li>मॉडल क्लास कन्वर्ज़न (जैसे, <code>model.legacy</code> में <code>GPTModel</code> बनाम <code>model.core</code>)</li>
<li>चेकपॉइंट फॉर्मेट कन्वर्ज़न (जैसे, distributed बनाम non-distributed चेकपॉइंट)</li>
</ol>
<h2>मॉडल क्लास कन्वर्ज़न</h2>
<p>Megatron विभिन्न मॉडल क्लासेस के बीच कन्वर्ज़न को सपोर्ट करता है, जिसमें आंतरिक मॉडल क्लासेस (वर्तमान में हमारे पास पुराने <code>legacy</code> मॉडल और नए <code>core</code> मॉडल हैं) और बाहरी मॉडल क्लासेस (जैसे Meta, Huggingface, Mistral, और Mixtral मॉडल्स) शामिल हैं। इसके अलावा, इस कन्वर्ज़न के दौरान, आप मॉडल की समानांतर स्थिति (tensor और pipeline model parallelism) को अपडेट कर सकते हैं।</p>
<p>हम टूल <code>tools/checkpoint/convert.py</code> प्रदान करते हैं, जो मॉडल क्लासेस के बीच कन्वर्ट करने के लिए है। कुछ महत्वपूर्ण आर्ग्युमेंट्स हैं:</p>
<ul>
<li><code>--model-type</code>: <code>GPT</code> या <code>BERT</code></li>
<li><code>--loader</code>: मौजूदा चेकपॉइंट का फॉर्मेट। समर्थित फॉर्मेट्स में शामिल हैं:
<ul>
<li><code>legacy</code>: हमारे पुराने मॉडल क्लासेस ( <code>megatron.legacy.model</code> के तहत)</li>
<li><code>core</code>: हमारे नए मॉडल क्लासेस (<code>megatron.core.models</code> के तहत)</li>
<li><code>llama_mistral</code>: Llama और Mistral मॉडल्स लोड करने के लिए (Meta और Huggingface फॉर्मेट्स सपोर्टेड)</li>
<li><code>mixtral_hf</code>: Mixtral मॉडल्स लोड करने के लिए (केवल Huggingface)</li>
</ul>
</li>
<li><code>--load-dir</code>: मौजूदा चेकपॉइंट लोड करने की निर्देशिका</li>
<li><code>--saver</code>: <code>legacy</code> या <code>core</code> (ऊपर <code>--loader</code> के तहत विवरण देखें)</li>
<li><code>--save-dir</code>: नया चेकपॉइंट सेव करने की निर्देशिका</li>
<li><code>--target-tensor-parallel-size</code>: नया tensor मॉडल पैरेलल साइज</li>
<li><code>--target-pipeline-parallel-size</code>: नया pipeline मॉडल पैरेलल साइज</li>
</ul>
<p>अधिक आर्ग्युमेंट विवरण के लिए कृपया मुख्य स्क्रिप्ट (<code>convert.py</code>), लोडर स्क्रिप्ट्स (<code>loader_core.py</code>, <code>loader_legacy.py</code>, <code>loader_llama_mistral.py</code>, <code>loader_mixtral_hf.py</code>), या सेवर स्क्रिप्ट्स (<code>saver_core.py</code>, <code>saver_legacy.py</code>) देखें।</p>
<p>पुराने फॉर्मेट (<code>legacy</code>) से नए फॉर्मेट (<code>core</code>) में GPT मॉडल को कन्वर्ट करने के लिए एक उदाहरण कमांड इस प्रकार है:</p>
<pre><code>python tools/checkpoint/convert.py \
&gt;   --model-type GPT \
&gt;   --loader legacy \
&gt;   --load-dir ${LEGACY_FORMAT_DIR} \
&gt;   --saver core \
&gt;   --save-dir ${CORE_FORMAT_DIR} \
&gt;   --target-tensor-parallel-size ${TP} \
&gt;   --target-pipeline-parallel-size ${PP} \
</code></pre>
<p>Llama/Mistral मॉडल्स को Megatron में कन्वर्ट करने के उदाहरण के लिए, कृपया देखें <a href="docs/llama_mistral.md">यहाँ</a>।</p>
<h2>चेकपॉइंट फॉर्मेट कन्वर्ज़न</h2>
<p>Megatron कई चेकपॉइंट फॉर्मेट्स प्रदान करता है, जिनमें शामिल हैं:</p>
<ul>
<li><code>torch</code>: बेसिक चेकपॉइंट फॉर्मेट जिसमें सीक्वेंशियल रीड और राइट्स होते हैं, और यह विशिष्ट tensor/pipeline मॉडल पैरेलल स्थिति (TP/PP स्टेट्स) से जुड़ा होता है। (जबकि एक विशिष्ट चेकपॉइंट विशिष्ट TP/PP स्टेट से जुड़ा होता है, चेकपॉइंट को ऊपर वर्णित मॉडल क्लास कन्वर्टर के माध्यम से मैन्युअली कन्वर्ट किया जा सकता है।)</li>
<li><code>torch_dist</code>: डिस्ट्रीब्यूटेड चेकपॉइंट फॉर्मेट, तेज समानांतर रीड और राइट्स के लिए, और यह पैरेलल स्थिति से स्वतंत्र है (यानी, एक ही चेकपॉइंट को विभिन्न TP/PP सेटअप्स में लोड किया जा सकता है)।</li>
</ul>
<p>सामान्यतः, <code>torch_dist</code> अधिक आधुनिक और अनुशंसित चेकपॉइंट फॉर्मेट है, इसकी गति के कारण। हालांकि, उपयोग के अनुसार, इन दोनों फॉर्मेट्स के बीच कन्वर्ट करना वांछनीय हो सकता है। ऐसा करने के लिए, अपने <em>training</em> स्क्रिप्ट (जैसे, <code>pretrain_gpt.py</code>) को सामान्य रूप से लॉन्च करें, लेकिन दो अतिरिक्त आर्ग्युमेंट्स के साथ:</p>
<ul>
<li><code>--ckpt-convert-format ${FORMAT}</code>: <code>${FORMAT}</code> ऊपर वर्णित अनुसार <code>torch</code> या <code>torch_dist</code> में से एक हो सकता है।</li>
<li><code>--ckpt-convert-save ${PATH_TO_SAVE_NEW_FORMAT}</code>: यह पथ आपके मौजूदा <code>--load</code>/<code>--save</code> पथों से अलग होना चाहिए, ताकि मौजूदा चेकपॉइंट ओवरराइट न हो। कन्वर्ट करने के बाद, अपने <code>--load</code>/<code>--save</code> पथों के लिए इस नए पथ का उपयोग करें।</li>
</ul>
<p>इस चेकपॉइंट फॉर्मेट कन्वर्टर का सामान्य विचार यह है कि यह मॉडल को वैसे ही लॉन्च करता है जैसे सामान्यत: ट्रेनिंग के लिए किया जाता है, लेकिन किसी भी ट्रेनिंग इटरेशन को चलाने से पहले, यह नए चेकपॉइंट फॉर्मेट में सेव करता है, और फिर बाहर निकल जाता है। यह महत्वपूर्ण है कि सभी अन्य लॉन्च आर्ग्युमेंट्स समान रहें, ताकि सिस्टम पिछले चेकपॉइंट फॉर्मेट को समझ सके।</p>
<h1>Megatron का उपयोग करने वाली परियोजनाएँ</h1>
<p>नीचे वे कुछ परियोजनाएँ दी गई हैं जहाँ हमने सीधे Megatron का उपयोग किया है:</p>
<ul>
<li><a href="https://arxiv.org/pdf/1909.08053.pdf">BERT and GPT Studies Using Megatron</a></li>
<li><a href="https://www.aclweb.org/anthology/2020.emnlp-main.379.pdf">BioMegatron: Larger Biomedical Domain Language Model</a></li>
<li><a href="https://arxiv.org/abs/2101.00408">End-to-End Training of Neural Retrievers for Open-Domain Question Answering</a></li>
<li><a href="https://www.aclweb.org/anthology/2020.acl-main.8.pdf">Large Scale Multi-Actor Generative Dialog Modeling</a></li>
<li><a href="https://arxiv.org/abs/2010.10150">Local Knowledge Powered Conversational Agents</a></li>
<li><a href="https://www.aclweb.org/anthology/2020.emnlp-main.226.pdf">MEGATRON-CNTRL: Controllable Story Generation with External Knowledge Using Large-Scale Language Models</a></li>
<li><a href="http://www.qizhexie.com/data/RACE_leaderboard.html">RACE Reading Comprehension Dataset Leaderboard</a></li>
<li><a href="https://www.aclweb.org/anthology/2020.emnlp-main.468.pdf">Training Question Answering Models From Synthetic Data</a></li>
<li><a href="https://arxiv.org/abs/2112.07868">Few-shot Instruction Prompts for Pretrained Language Models to Detect Social Biases</a></li>
<li><a href="https://arxiv.org/abs/2202.04173">Exploring the Limits of Domain-Adaptive Training for Detoxifying Large-Scale Language Models</a></li>
<li><a href="https://arxiv.org/abs/2201.11990">Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model</a></li>
<li><a href="https://arxiv.org/abs/2203.08745">Multi-Stage Prompting for Knowledgeable Dialogue Generation</a></li>
<li><a href="https://aclanthology.org/2022.emnlp-main.319.pdf">Evaluating Parameter Efficient Learning for Generation</a></li>
<li><a href="https://arxiv.org/abs/2202.04173">Exploring the Limits of Domain-Adaptive Training for Detoxifying Large-Scale Language Models</a></li>
<li><a href="https://arxiv.org/abs/2304.06762">Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study</a></li>
<li><a href="https://arxiv.org/abs/2310.07713">InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining</a></li>
<li><a href="https://arxiv.org/abs/2406.07887">An Empirical Study of Mamba-based Language Models</a></li>
</ul>
<pre><code>

---


Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-06-11


---
</code></pre>

        </div>
    </div>
    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Statcounter and other scripts can be added here -->
</body>
</html>