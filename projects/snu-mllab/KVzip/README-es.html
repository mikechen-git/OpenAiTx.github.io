<!DOCTYPE html><html lang="en"><head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>KVzip: Compresión Agnóstica de Caché KV con Reconstrucción de Contexto - snu-mllab/KVzip</title>

    <!-- Primary Meta Tags -->
    <meta name="title" content="KVzip: Compresión Agnóstica de Caché KV con Reconstrucción de Contexto - snu-mllab/KVzip">
    <meta name="description" content="snu-mllab/KVzip - GitHub repository documentation and information">
    <meta name="keywords" content="snu-mllab, KVzip, GitHub, repository, documentation, ">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">

    <!-- Open Graph -->
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=es">
    <meta property="og:title" content="KVzip: Compresión Agnóstica de Caché KV con Reconstrucción de Contexto - snu-mllab/KVzip">
    <meta property="og:description" content="snu-mllab/KVzip - GitHub repository documentation and information">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">

    <!-- Favicon -->
    <link rel="icon" type="image/jpeg" href="icon.jpg">
    <link rel="apple-touch-icon" href="icon.jpg">

    <!-- Marked.js for Markdown rendering -->
    <script type="text/javascript" async="" src="https://www.statcounter.com/counter/recorder.js"></script><script src="js/marked.min.js?v=20250613"></script>
    <!-- Highlight.js for code syntax highlighting -->
    <link rel="stylesheet" href="css/github.min.css?v=20250613">
    <script src="js/highlight.min.js?v=20250613"></script>
    <!-- Custom CSS -->
    <link rel="stylesheet" href="view.css?v=20250613">
    <style>
        /* Layout */
        body {
            display: flex;
            flex-direction: column;
            min-height: 100vh;
        }

        .main-container {
            margin: 0 auto;
            width: 100%;
            max-width: 980px;
            padding: 0 20px;
        }

        @media (max-width: 768px) {
            .main-container {
                padding: 0 15px;
            }
        }

        /* Image size restrictions */
        .markdown-body img {
            max-width: 100%;
            height: auto;
        }

        /* Existing styles */
        .header {
            text-align: center;
            margin-bottom: 30px;
            padding: 20px;
            background-color: #f6f8fa;
            border-bottom: 1px solid #e1e4e8;
            position: relative;
        }

        .back-button {
            position: absolute;
            left: 20px;
            top: 50%;
            transform: translateY(-50%);
            color: #0366d6;
            text-decoration: none;
            display: flex;
            align-items: center;
            font-size: 14px;
            padding: 5px 10px;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            background-color: #fff;
        }

        .back-button:hover {
            background-color: #f6f8fa;
            border-color: #0366d6;
        }

        .back-button::before {
            content: "←";
            margin-right: 5px;
            font-size: 16px;
        }

        .header .links {
            margin-top: 10px;
            font-size: 16px;
        }

        .header .links a {
            color: #0366d6;
            text-decoration: none;
            margin-left: 5px;
        }

        .header .links a:hover {
            text-decoration: underline;
        }
        
        /* Language badges styles */
        .language-badges {
            margin-top: 15px;
            text-align: center;
        }
        .language-badges a {
            display: inline-block;
            margin: 2px;
            text-decoration: none;
        }
        .language-badges img {
            height: 20px;
            border-radius: 3px;
        }
        .language-badges a:hover img {
            opacity: 0.8;
        }
    </style>
</head>

<body>
    <div class="header">
        <a href="javascript:history.back()" class="back-button">Back</a>
        <div class="links">
            GitHub Repository: <a href="https://github.com/snu-mllab/KVzip" id="githubRepoLink" target="_blank">snu-mllab/KVzip</a>
        </div>
        <div class="language-badges" id="languageBadges"><a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=en"><img src="https://img.shields.io/badge/EN-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=zh-CN"><img src="https://img.shields.io/badge/简中-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=zh-TW"><img src="https://img.shields.io/badge/繁中-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=ja"><img src="https://img.shields.io/badge/日本語-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=ko"><img src="https://img.shields.io/badge/한국어-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=th"><img src="https://img.shields.io/badge/ไทย-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=fr"><img src="https://img.shields.io/badge/Français-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=de"><img src="https://img.shields.io/badge/Deutsch-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=es"><img src="https://img.shields.io/badge/Español-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=it"><img src="https://img.shields.io/badge/Italiano-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=ru"><img src="https://img.shields.io/badge/Русский-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=pt"><img src="https://img.shields.io/badge/Português-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=nl"><img src="https://img.shields.io/badge/Nederlands-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=pl"><img src="https://img.shields.io/badge/Polski-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=ar"><img src="https://img.shields.io/badge/العربية-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=tr"><img src="https://img.shields.io/badge/Türkçe-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=vi"><img src="https://img.shields.io/badge/Tiếng Việt-white" alt="version"></a></div>
    </div>

    <div class="main-container">
        <div class="markdown-body" id="content"><h1>KVzip: Compresión Agnóstica de Caché KV con Reconstrucción de Contexto</h1>
<p>[<a href="https://arxiv.org/abs/2505.23416">Artículo</a>] [<a href="https://janghyun1230.github.io/kvzip/">Blog</a>] </p>
<img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/method.png" width="800">

<h2>¿Qué hay de nuevo?</h2>
<ul>
<li>KVzip comprime la caché KV para soportar <strong>diversas consultas futuras</strong>.</li>
<li>[Dependiente del contexto] Logra una <strong>reducción de 3 a 4 veces en el tamaño de la caché KV</strong> y una <strong>disminución de 2 veces en la latencia de decodificación</strong>, con una degradación mínima del rendimiento.</li>
<li>[Independiente del contexto] Mejora la compresión KV a nivel de cabeza estilo <a href="https://github.com/mit-han-lab/duo-attention">DuoAttention</a>, utilizando sólo <strong>unas pocas pasadas hacia adelante en menos de un minuto</strong> para la optimización de la puntuación de importancia a nivel de cabeza (100 veces más rápido).</li>
<li>Ejecutar demo.py:</li>
</ul>
<img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/demo.png" width="800">


<h3>Benchmark en configuración agnóstica a la consulta</h3>
<ul>
<li>Tareas: <a href="https://huggingface.co/datasets/rajpurkar/squad">SQuAD</a>, <a href="https://github.com/gkamradt/LLMTest_NeedleInAHaystack">NIAH</a>, <a href="https://github.com/microsoft/MInference/tree/main/scbench">SCBench</a>, <a href="https://huggingface.co/datasets/openai/gsm8k/viewer/main/train?row=7294">GSM8K</a>. </li>
<li>Modelo: <a href="https://huggingface.co/Qwen/Qwen2.5-7B-Instruct">Qwen2.5-7B-Instruct-1M</a></li>
</ul>
<img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/benchmark.png" width="800">


<h2>Instalación</h2>
<p>Usamos CUDA 12.1 y Python 3.10</p>
<pre><code class="language-bash hljs"><span class="hljs-built_in">cd</span> KVzip
pip install -r requirements.txt
pip install flash-attn==2.7.4.post1 --no-build-isolation
make i
</code></pre>
<ul>
<li>Para usar la cuantización de <a href="https://github.com/mit-han-lab/omniserve">QServe</a>, por favor siga <a href="https://github.com/snu-mllab/KVzip/tree/main/model/quant_model"><code>./model/quant_model</code></a>.</li>
</ul>
<h3>Dataset</h3>
<ul>
<li>Por favor descargue el dataset SCBench preprocesado desde <a href="https://drive.google.com/file/d/1cqoR6pxxFcjFqvPZkuAmF-fBSAlAbjbN/view?usp=share_link">Google Drive</a>.</li>
<li>Si descargó los archivos descomprimidos, simplemente mueva la carpeta scbench.</li>
</ul>
<pre><code class="language-bash hljs"><span class="hljs-built_in">mv</span> scbench.zip kvzip/data/
<span class="hljs-built_in">cd</span> kvzip/data
unzip scbench.zip  
</code></pre>
<h2>Inicio rápido</h2>
<pre><code class="language-python hljs"><span class="hljs-keyword">from</span> model <span class="hljs-keyword">import</span> ModelKVzip

model = ModelKVzip(<span class="hljs-string">"Qwen/Qwen2.5-7B-Instruct-1M"</span>)
context = <span class="hljs-string">"Este es mi perfil básico. Mi nombre es Kim y vivo en Seúl. Mi especialidad es ciencias de la computación."</span>
queries = [<span class="hljs-string">"¿Cuál es mi nombre?"</span>, <span class="hljs-string">"¿Vivo en Seúl?"</span>]

kv = model.prefill(context, load_score=<span class="hljs-literal">False</span>)  <span class="hljs-comment"># precarga la caché KV + puntuación de importancia</span>
kv.prune(ratio=<span class="hljs-number">0.3</span>)  <span class="hljs-comment"># ratio de compresión, elimina el 70% del KV</span>

<span class="hljs-keyword">for</span> q <span class="hljs-keyword">in</span> queries:
    query_ids = model.apply_template(q)
    output = model.generate(query_ids, kv=kv, update_cache=<span class="hljs-literal">False</span>)  <span class="hljs-comment"># inferencia eficiente</span>
    <span class="hljs-built_in">print</span>(q, output)
</code></pre>
<ul>
<li>Los modelos soportados están listados en <a href="https://github.com/snu-mllab/KVzip/blob/main/model/load.py"><code>model/load.py</code></a>, incluyendo <strong>LLaMA3, Qwen2.5/3, Gemma3</strong>.</li>
<li>Configure <code>load_score=True</code> para eliminar la sobrecarga de compresión. Esto habilita la eliminación de KV independiente del contexto, con un compromiso en la relación de compresión de <code>ratio=0.6</code>.</li>
<li>Después de la generación, los pares KV correspondientes a las consultas y los tokens generados se eliminan selectivamente de la caché para un procesamiento posterior. Configure <code>update_cache=True</code> para habilitar inferencia multi-turno, reteniendo todo el historial de interacción durante la inferencia.</li>
</ul>
<h2>Perfilando memoria y tiempo de computación</h2>
<h3>Eliminación dependiente del contexto</h3>
<pre><code class="language-bash hljs">python -B test.py -m [nombre_modelo] -d [nombre_datos] --kv_type evict --ratio 0.3
</code></pre>
<ul>
<li>El código anterior también compara salidas generadas con cachés KV completas versus podadas.</li>
<li>Para una prueba rápida, use <code>-d squad</code>. Para pruebas de contexto largo, use <code>-d scbench_kv</code>.<ul>
<li>Nombres de datos disponibles: <a href="https://github.com/snu-mllab/KVzip/blob/main/data/load.py"><code>data/load.py</code></a>.</li>
<li>Nombres de modelos disponibles: <a href="https://github.com/snu-mllab/KVzip/blob/main/model/load.py"><code>model/load.py</code></a>, por ejemplo, llama3.1-8b, qwen2.5-7b (o Qwen/Qwen2.5-7B-Instruct-1M).</li>
</ul>
</li>
<li>Adaptamos el kernel CUDA de <a href="https://github.com/FFY0/AdaKV/tree/main">AdaKV</a>, soportando asignación de presupuesto no uniforme por cabeza.<ul>
<li>Actualmente, nuestro código carece de un kernel optimizado para Gemma3 que usa caché KV estática, por lo que el código no produce ganancias reales de eficiencia. Sin embargo, el rendimiento del modelo aún puede evaluarse usando atención reducida con submuestreo KV (<code>--kv_type retain</code>).</li>
</ul>
</li>
</ul>
<h3>Eliminación independiente del contexto (sin sobrecarga de compresión en tiempo de ejecución)</h3>
<ul>
<li>Use el flag <code>--level head</code> con <code>--ratio 0.6</code> (recomendado).<ul>
<li>Eliminamos todos los pares KV de contexto asociados a una cabeza específica mientras retenemos los pares KV del prompt del sistema y la consulta.</li>
<li>Las puntuaciones de cabeza precomputadas están disponibles para LLaMA3.1-8B y Qwen2.5-7/14B en <code>./utils/head_score</code>.</li>
</ul>
</li>
<li>Para calcular las puntuaciones de cabeza para otros modelos:<pre><code class="language-bash hljs">python -B test.py -m [nombre_modelo] -d scbench_qa_eng --save_head_score
</code></pre>
<ul>
<li>Los resultados se guardarán en <code>./utils/head_score</code>.</li>
<li>Si el objetivo es una tarea de codificación, recomendamos ejecutar adicionalmente el comando con <code>-d scbench_repoqa</code>. Esto permite que el modelo use las puntuaciones máximas de cabeza tanto de lenguajes naturales como de codificación, lo que mejora el rendimiento.</li>
</ul>
</li>
<li>Estas puntuaciones pueden integrarse perfectamente con el motor de inferencia optimizado de <a href="https://github.com/mit-han-lab/duo-attention">DuoAttention</a> reemplazando sus datos de puntuación de cabeza con los nuestros.</li>
</ul>
<h2>Evaluación</h2>
<ul>
<li>Para generar respuestas del modelo con ratios de compresión KV desde 0.1 hasta 1.0:<pre><code class="language-bash hljs">python -B eval.py -m [nombre_modelo] -d [nombre_datos] --kv_type retain --num 100
</code></pre>
<ul>
<li>Los resultados se guardarán en <code>./results/[nombre_datos]</code>.</li>
<li>Los datasets soportados están listados en <code>data/load.py</code>.</li>
</ul>
</li>
<li>Para calcular métricas de evaluación a partir de los resultados generados:<pre><code class="language-bash hljs">python -B -m results.parse -m [nombre_modelo] -d [nombre_datos]
</code></pre>
</li>
</ul>
<h2>Aplicación a nuevos modelos</h2>
<p>Para integrar KVzip en un nuevo modelo, necesitará actualizar los siguientes archivos:</p>
<ul>
<li><code>attention/attn.py</code><br>Modifique la lógica del pase hacia adelante de atención según sea necesario. En ciertos casos, también podrían requerirse actualizaciones en kvcache.py y score.py.</li>
<li><code>model/monkeypatch.py</code><br>Implemente el parcheo específico del modelo para la integración.</li>
<li><code>model/template.py</code><br>Defina el prompt del sistema y las plantillas de formato de chat del modelo.</li>
</ul>
<h2>Citas</h2>
<pre><code class="language-bibtex">@article{kim2025kvzip,
        title={KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction},
        author={Kim, Jang-Hyun and Kim, Jinuk and Kwon, Sangwoo and Lee, Jae W and Yun, Sangdoo and Song, Hyun Oh},
        journal={arXiv preprint arXiv:2505.23416},
        year={2025}
}
</code></pre>
<h2>Licencia</h2>
<p>Licencia MIT</p>
<hr>
<p>Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx">Open Ai Tx</a> | Last indexed: 2025-06-11</p>
<hr>
</div>
    </div>

    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Default Statcounter code for openaitx
https://openaitx.github.io/ -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async=""></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>
    <!-- End of Statcounter Code -->
    <script src="view.js?v=20250613"></script>


</body></html>