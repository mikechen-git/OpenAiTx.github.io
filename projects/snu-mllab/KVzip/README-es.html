<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>KVzip - snu-mllab/KVzip es</title>
    <meta name="title" content="KVzip - snu-mllab/KVzip es | KVzip: Compresión Agnóstica de Caché KV con Reconstrucción de Contexto [Artículo] [Blog] ¿Qué hay de nuevo? KVzip comprime la caché KV para soportar diversas co...">
    <meta name="description" content="snu-mllab/KVzip - GitHub repository es documentation and information | KVzip: Compresión Agnóstica de Caché KV con Reconstrucción de Contexto [Artículo] [Blog] ¿Qué hay de nuevo? KVzip comprime la caché KV para soportar diversas co...">
    <meta name="keywords" content="snu-mllab, KVzip, GitHub, repository, es documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/snu-mllab/KVzip/README-es.html">
    <meta property="og:title" content="KVzip - snu-mllab/KVzip es | KVzip: Compresión Agnóstica de Caché KV con Reconstrucción de Contexto [Artículo] [Blog] ¿Qué hay de nuevo? KVzip comprime la caché KV para soportar diversas co...">
    <meta property="og:description" content="snu-mllab/KVzip - GitHub repository es documentation and information | KVzip: Compresión Agnóstica de Caché KV con Reconstrucción de Contexto [Artículo] [Blog] ¿Qué hay de nuevo? KVzip comprime la caché KV para soportar diversas co...">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">
    <script src="/js/marked.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        body { display: flex; flex-direction: column; min-height: 100vh; }
        .main-container { margin: 0 auto; width: 100%; max-width: 980px; padding: 0 20px; }
        @media (max-width: 768px) { .main-container { padding: 0 15px; } }
        .markdown-body img { max-width: 100%; height: auto; }
        .header { text-align: center; margin-bottom: 30px; padding: 20px; background-color: #f6f8fa; border-bottom: 1px solid #e1e4e8; position: relative; }
        .header .links { margin-top: 10px; font-size: 16px; }
        .header .links a { color: #0366d6; text-decoration: none; margin-left: 5px; }
        .header .links a:hover { text-decoration: underline; }
        .language-badges { margin-top: 15px; text-align: center; }
        .language-badges a { display: inline-block; margin: 2px; text-decoration: none; }
        .language-badges img { height: 20px; border-radius: 3px; }
        .language-badges a:hover img { opacity: 0.8; }
    </style>
</head>
<body>
    <div style="position: fixed; top: 2px; left: 2px; z-index: 2000; background: rgba(255,255,255,0.95); border-radius: 6px; box-shadow: 0 2px 8px rgba(0,0,0,0.06); padding: 4px 14px; font-size: 15px; color: #222; font-family: 'Segoe UI', Arial, sans-serif; font-weight: 500; letter-spacing: 0.5px;">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" style="color: #0366d6; text-decoration: none; font-weight: 600;">Open AI Tx</a>
    </div>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/snu-mllab/KVzip" id="githubRepoLink" target="_blank">snu-mllab/KVzip</a>
<h1 style="display: none;">KVzip: Compresión Agnóstica de Caché KV con Reconstrucción de Contexto [Artículo] [Blog] ¿Qué hay de nuevo? KVzip comprime la caché KV para soportar diversas co...</h1>
        </div>
        <div class="language-badges" id="languageBadges">
            <!-- You can generate language badges here if needed -->
        </div>
    </div>
    <div class="main-container">
        <div class="markdown-body" id="content">
            <h1>KVzip: Compresión Agnóstica de Caché KV con Reconstrucción de Contexto</h1>
<p>[<a href="https://arxiv.org/abs/2505.23416">Artículo</a>] [<a href="https://janghyun1230.github.io/kvzip/">Blog</a>]</p>
<img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/method.png" width="800">
<h2>¿Qué hay de nuevo?</h2>
<ul>
<li>KVzip comprime la caché KV para soportar <strong>diversas consultas futuras</strong>.</li>
<li>[Dependiente del contexto] Logra una <strong>reducción de 3 a 4 veces en el tamaño de la caché KV</strong> y una <strong>disminución de 2 veces en la latencia de decodificación</strong>, con una degradación mínima del rendimiento.</li>
<li>[Independiente del contexto] Mejora la compresión KV a nivel de cabeza estilo <a href="https://github.com/mit-han-lab/duo-attention">DuoAttention</a>, utilizando sólo <strong>unas pocas pasadas hacia adelante en menos de un minuto</strong> para la optimización de la puntuación de importancia a nivel de cabeza (100 veces más rápido).</li>
<li>Ejecutar demo.py:
<img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/demo.png" width="800"></li>
</ul>
<h3>Benchmark en configuración agnóstica a la consulta</h3>
<ul>
<li>Tareas: <a href="https://huggingface.co/datasets/rajpurkar/squad">SQuAD</a>, <a href="https://github.com/gkamradt/LLMTest_NeedleInAHaystack">NIAH</a>, <a href="https://github.com/microsoft/MInference/tree/main/scbench">SCBench</a>, <a href="https://huggingface.co/datasets/openai/gsm8k/viewer/main/train?row=7294">GSM8K</a>.</li>
<li>Modelo: <a href="https://huggingface.co/Qwen/Qwen2.5-7B-Instruct">Qwen2.5-7B-Instruct-1M</a></li>
</ul>
<img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/benchmark.png" width="800">
<h2>Instalación</h2>
<p>Usamos CUDA 12.1 y Python 3.10</p>
<pre><code class="language-bash">cd KVzip
pip install -r requirements.txt
pip install flash-attn==2.7.4.post1 --no-build-isolation
make i
</code></pre>
<ul>
<li>Para usar la cuantización de <a href="https://github.com/mit-han-lab/omniserve">QServe</a>, por favor siga <a href="https://github.com/snu-mllab/KVzip/tree/main/model/quant_model"><code>./model/quant_model</code></a>.</li>
</ul>
<h3>Dataset</h3>
<ul>
<li>Por favor descargue el dataset SCBench preprocesado desde <a href="https://drive.google.com/file/d/1cqoR6pxxFcjFqvPZkuAmF-fBSAlAbjbN/view?usp=share_link">Google Drive</a>.</li>
<li>Si descargó los archivos descomprimidos, simplemente mueva la carpeta scbench.</li>
</ul>
<pre><code class="language-bash">mv scbench.zip kvzip/data/
cd kvzip/data
unzip scbench.zip  
</code></pre>
<h2>Inicio rápido</h2>
<pre><code class="language-python">from model import ModelKVzip

model = ModelKVzip(&quot;Qwen/Qwen2.5-7B-Instruct-1M&quot;)
context = &quot;Este es mi perfil básico. Mi nombre es Kim y vivo en Seúl. Mi especialidad es ciencias de la computación.&quot;
queries = [&quot;¿Cuál es mi nombre?&quot;, &quot;¿Vivo en Seúl?&quot;]

kv = model.prefill(context, load_score=False)  # precarga la caché KV + puntuación de importancia
kv.prune(ratio=0.3)  # ratio de compresión, elimina el 70% del KV

for q in queries:
    query_ids = model.apply_template(q)
    output = model.generate(query_ids, kv=kv, update_cache=False)  # inferencia eficiente
    print(q, output)
</code></pre>
<ul>
<li>Los modelos soportados están listados en <a href="https://github.com/snu-mllab/KVzip/blob/main/model/load.py"><code>model/load.py</code></a>, incluyendo <strong>LLaMA3, Qwen2.5/3, Gemma3</strong>.</li>
<li>Configure <code>load_score=True</code> para eliminar la sobrecarga de compresión. Esto habilita la eliminación de KV independiente del contexto, con un compromiso en la relación de compresión de <code>ratio=0.6</code>.</li>
<li>Después de la generación, los pares KV correspondientes a las consultas y los tokens generados se eliminan selectivamente de la caché para un procesamiento posterior. Configure <code>update_cache=True</code> para habilitar inferencia multi-turno, reteniendo todo el historial de interacción durante la inferencia.</li>
</ul>
<h2>Perfilando memoria y tiempo de computación</h2>
<h3>Eliminación dependiente del contexto</h3>
<pre><code class="language-bash">python -B test.py -m [nombre_modelo] -d [nombre_datos] --kv_type evict --ratio 0.3
</code></pre>
<ul>
<li>El código anterior también compara salidas generadas con cachés KV completas versus podadas.</li>
<li>Para una prueba rápida, use <code>-d squad</code>. Para pruebas de contexto largo, use <code>-d scbench_kv</code>.
<ul>
<li>Nombres de datos disponibles: <a href="https://github.com/snu-mllab/KVzip/blob/main/data/load.py"><code>data/load.py</code></a>.</li>
<li>Nombres de modelos disponibles: <a href="https://github.com/snu-mllab/KVzip/blob/main/model/load.py"><code>model/load.py</code></a>, por ejemplo, llama3.1-8b, qwen2.5-7b (o Qwen/Qwen2.5-7B-Instruct-1M).</li>
</ul>
</li>
<li>Adaptamos el kernel CUDA de <a href="https://github.com/FFY0/AdaKV/tree/main">AdaKV</a>, soportando asignación de presupuesto no uniforme por cabeza.
<ul>
<li>Actualmente, nuestro código carece de un kernel optimizado para Gemma3 que usa caché KV estática, por lo que el código no produce ganancias reales de eficiencia. Sin embargo, el rendimiento del modelo aún puede evaluarse usando atención reducida con submuestreo KV (<code>--kv_type retain</code>).</li>
</ul>
</li>
</ul>
<h3>Eliminación independiente del contexto (sin sobrecarga de compresión en tiempo de ejecución)</h3>
<ul>
<li>Use el flag <code>--level head</code> con <code>--ratio 0.6</code> (recomendado).
<ul>
<li>Eliminamos todos los pares KV de contexto asociados a una cabeza específica mientras retenemos los pares KV del prompt del sistema y la consulta.</li>
<li>Las puntuaciones de cabeza precomputadas están disponibles para LLaMA3.1-8B y Qwen2.5-7/14B en <code>./utils/head_score</code>.</li>
</ul>
</li>
<li>Para calcular las puntuaciones de cabeza para otros modelos:
<pre><code class="language-bash">python -B test.py -m [nombre_modelo] -d scbench_qa_eng --save_head_score
</code></pre>
<ul>
<li>Los resultados se guardarán en <code>./utils/head_score</code>.</li>
<li>Si el objetivo es una tarea de codificación, recomendamos ejecutar adicionalmente el comando con <code>-d scbench_repoqa</code>. Esto permite que el modelo use las puntuaciones máximas de cabeza tanto de lenguajes naturales como de codificación, lo que mejora el rendimiento.</li>
</ul>
</li>
<li>Estas puntuaciones pueden integrarse perfectamente con el motor de inferencia optimizado de <a href="https://github.com/mit-han-lab/duo-attention">DuoAttention</a> reemplazando sus datos de puntuación de cabeza con los nuestros.</li>
</ul>
<h2>Evaluación</h2>
<ul>
<li>Para generar respuestas del modelo con ratios de compresión KV desde 0.1 hasta 1.0:
<pre><code class="language-bash">python -B eval.py -m [nombre_modelo] -d [nombre_datos] --kv_type retain --num 100
</code></pre>
<ul>
<li>Los resultados se guardarán en <code>./results/[nombre_datos]</code>.</li>
<li>Los datasets soportados están listados en <code>data/load.py</code>.</li>
</ul>
</li>
<li>Para calcular métricas de evaluación a partir de los resultados generados:
<pre><code class="language-bash">python -B -m results.parse -m [nombre_modelo] -d [nombre_datos]
</code></pre>
</li>
</ul>
<h2>Aplicación a nuevos modelos</h2>
<p>Para integrar KVzip en un nuevo modelo, necesitará actualizar los siguientes archivos:</p>
<ul>
<li><code>attention/attn.py</code><br />
Modifique la lógica del pase hacia adelante de atención según sea necesario. En ciertos casos, también podrían requerirse actualizaciones en kvcache.py y score.py.</li>
<li><code>model/monkeypatch.py</code><br />
Implemente el parcheo específico del modelo para la integración.</li>
<li><code>model/template.py</code><br />
Defina el prompt del sistema y las plantillas de formato de chat del modelo.</li>
</ul>
<h2>Citas</h2>
<pre><code class="language-bibtex">@article{kim2025kvzip,
        title={KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction},
        author={Kim, Jang-Hyun and Kim, Jinuk and Kwon, Sangwoo and Lee, Jae W and Yun, Sangdoo and Song, Hyun Oh},
        journal={arXiv preprint arXiv:2505.23416},
        year={2025}
}
</code></pre>
<h2>Licencia</h2>
<p>Licencia MIT</p>
<hr />
<p>Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx">Open Ai Tx</a> | Last indexed: 2025-06-11</p>
<hr />

        </div>
    </div>
    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Default Statcounter code for openaitx
https://openaitx.github.io/ -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>
    <!-- End of Statcounter Code -->
</body>
</html>