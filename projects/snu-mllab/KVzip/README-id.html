<!DOCTYPE html>
<html lang="id">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>KVzip - snu-mllab/KVzip</title>
    <meta name="title" content="KVzip - snu-mllab/KVzip">
    <meta name="description" content="snu-mllab/KVzip - GitHub repository id documentation and informationKVzip: Kompresi Cache KV Query-Agnostik dengan Rekonstruksi Konteks [Paper] [Blog] Apa yang Baru? KVzip mengompresi cache KV untuk mendukung berbagai query masa...">
    <meta name="keywords" content="snu-mllab, KVzip, GitHub, repository, id documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/snu-mllab/KVzip/README-id.html">
    <meta property="og:title" content="KVzip - snu-mllab/KVzip">
    <meta property="og:description" content="snu-mllab/KVzip - GitHub repository id documentation and information">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">
    <script src="/js/marked.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        body { display: flex; flex-direction: column; min-height: 100vh; }
        .main-container { margin: 0 auto; width: 100%; max-width: 980px; padding: 0 20px; }
        @media (max-width: 768px) { .main-container { padding: 0 15px; } }
        .markdown-body img { max-width: 100%; height: auto; }
        .header { text-align: center; margin-bottom: 30px; padding: 20px; background-color: #f6f8fa; border-bottom: 1px solid #e1e4e8; position: relative; }
        .header .links { margin-top: 10px; font-size: 16px; }
        .header .links a { color: #0366d6; text-decoration: none; margin-left: 5px; }
        .header .links a:hover { text-decoration: underline; }
        .language-badges { margin-top: 15px; text-align: center; }
        .language-badges a { display: inline-block; margin: 2px; text-decoration: none; }
        .language-badges img { height: 20px; border-radius: 3px; }
        .language-badges a:hover img { opacity: 0.8; }
    </style>
</head>
<body>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/snu-mllab/KVzip" id="githubRepoLink" target="_blank">snu-mllab/KVzip</a>
<h1 style="display: none;">KVzip: Kompresi Cache KV Query-Agnostik dengan Rekonstruksi Konteks [Paper] [Blog] Apa yang Baru? KVzip mengompresi cache KV untuk mendukung berbagai query masa...</h1>
        </div>
        <div class="language-badges" id="languageBadges">
            <!-- You can generate language badges here if needed -->
        </div>
    </div>
    <div class="main-container">
        <div class="markdown-body" id="content">
            <h1>KVzip: Kompresi Cache KV Query-Agnostik dengan Rekonstruksi Konteks</h1>
<p>[<a href="https://arxiv.org/abs/2505.23416">Paper</a>] [<a href="https://janghyun1230.github.io/kvzip/">Blog</a>]</p>
<img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/method.png" width="800">
<h2>Apa yang Baru?</h2>
<ul>
<li>KVzip mengompresi cache KV untuk mendukung <strong>berbagai query masa depan</strong>.</li>
<li>[Bergantung konteks] Mencapai <strong>pengurangan ukuran cache KV sebesar 3–4×</strong> dan <strong>penurunan latensi decoding sebesar 2×</strong>, dengan degradasi performa minimal.</li>
<li>[Tidak bergantung konteks] Meningkatkan kompresi KV tingkat head ala <a href="https://github.com/mit-han-lab/duo-attention">DuoAttention</a>, hanya dengan <strong>beberapa forward pass dalam satu menit</strong> untuk optimasi skor penting tingkat head (100x lebih cepat).</li>
<li>Jalankan demo.py:
<img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/demo.png" width="800"></li>
</ul>
<h3>Benchmarking pada pengaturan query-agnostik</h3>
<ul>
<li>Tugas: <a href="https://huggingface.co/datasets/rajpurkar/squad">SQuAD</a>, <a href="https://github.com/gkamradt/LLMTest_NeedleInAHaystack">NIAH</a>, <a href="https://github.com/microsoft/MInference/tree/main/scbench">SCBench</a>, <a href="https://huggingface.co/datasets/openai/gsm8k/viewer/main/train?row=7294">GSM8K</a>.</li>
<li>Model: <a href="https://huggingface.co/Qwen/Qwen2.5-7B-Instruct">Qwen2.5-7B-Instruct-1M</a></li>
</ul>
<img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/benchmark.png" width="800">
<h2>Instalasi</h2>
<p>Kami menggunakan CUDA 12.1 dan Python 3.10</p>
<pre><code class="language-bash">cd KVzip
pip install -r requirements.txt
pip install flash-attn==2.7.4.post1 --no-build-isolation
make i
</code></pre>
<ul>
<li>Untuk menggunakan kuantisasi <a href="https://github.com/mit-han-lab/omniserve">QServe</a>, silakan ikuti <a href="https://github.com/snu-mllab/KVzip/tree/main/model/quant_model"><code>./model/quant_model</code></a>.</li>
</ul>
<h3>Dataset</h3>
<ul>
<li>Silakan unduh dataset SCBench yang sudah diproses dari <a href="https://drive.google.com/file/d/1cqoR6pxxFcjFqvPZkuAmF-fBSAlAbjbN/view?usp=share_link">Google Drive</a>.</li>
<li>Jika Anda mengunduh file yang sudah diekstrak, cukup pindahkan folder scbench.</li>
</ul>
<pre><code class="language-bash">mv scbench.zip kvzip/data/
cd kvzip/data
unzip scbench.zip  
</code></pre>
<h2>Mulai Cepat</h2>
<pre><code class="language-python">from model import ModelKVzip

model = ModelKVzip(&quot;Qwen/Qwen2.5-7B-Instruct-1M&quot;)
context = &quot;This is my basic profile. My name is Kim living in Seoul. My major is computer science.&quot;
queries = [&quot;What is my name?&quot;, &quot;Do I live in Seoul?&quot;]

kv = model.prefill(context, load_score=False)  # prefill cache KV + penilaian penting
kv.prune(ratio=0.3)  # rasio kompresi, keluarkan 70% KV

for q in queries:
    query_ids = model.apply_template(q)
    output = model.generate(query_ids, kv=kv, update_cache=False)  # inferensi efisien
    print(q, output)
</code></pre>
<ul>
<li>Model yang didukung tercantum di <a href="https://github.com/snu-mllab/KVzip/blob/main/model/load.py"><code>model/load.py</code></a>, termasuk <strong>LLaMA3, Qwen2.5/3, Gemma3</strong>.</li>
<li>Atur <code>load_score=True</code> untuk menghilangkan overhead kompresi. Ini memungkinkan pengeluaran KV yang tidak bergantung konteks, dengan trade-off rasio kompresi <code>ratio=0.6</code>.</li>
<li>Setelah generasi, pasangan KV yang terkait dengan query dan token yang dihasilkan secara selektif dikeluarkan dari cache untuk pemrosesan selanjutnya. Atur <code>update_cache=True</code> untuk mengaktifkan inferensi multi-giliran, mempertahankan seluruh riwayat interaksi selama inferensi.</li>
</ul>
<h2>Profiling Memori dan Waktu Komputasi</h2>
<h3>Pengeluaran bergantung konteks</h3>
<pre><code class="language-bash">python -B test.py -m [model_name] -d [data_name] --kv_type evict --ratio 0.3
</code></pre>
<ul>
<li>Kode di atas juga membandingkan output yang dihasilkan dengan cache KV penuh versus yang sudah dipruning.</li>
<li>Untuk pengujian cepat, gunakan <code>-d squad</code>. Untuk pengujian konteks panjang, gunakan <code>-d scbench_kv</code>.
<ul>
<li>Nama data yang tersedia: <a href="https://github.com/snu-mllab/KVzip/blob/main/data/load.py"><code>data/load.py</code></a>.</li>
<li>Nama model yang tersedia: <a href="https://github.com/snu-mllab/KVzip/blob/main/model/load.py"><code>model/load.py</code></a>, misal llama3.1-8b, qwen2.5-7b (atau Qwen/Qwen2.5-7B-Instruct-1M).</li>
</ul>
</li>
<li>Kami mengadaptasi kernel CUDA dari <a href="https://github.com/FFY0/AdaKV/tree/main">AdaKV</a>, mendukung alokasi anggaran head yang tidak uniform.
<ul>
<li>Saat ini, kode kami belum memiliki kernel yang teroptimasi untuk Gemma3 yang menggunakan cache KV statis, sehingga kode tidak menghasilkan peningkatan efisiensi nyata. Namun, performa model masih bisa dievaluasi menggunakan perhatian yang dikurangi dengan subsampling KV (<code>--kv_type retain</code>).</li>
</ul>
</li>
</ul>
<h3>Pengeluaran tidak bergantung konteks (tanpa overhead kompresi runtime)</h3>
<ul>
<li>Gunakan flag <code>--level head</code> dengan <code>--ratio 0.6</code> (direkomendasikan).
<ul>
<li>Kami menghapus semua pasangan KV konteks yang terkait dengan head tertentu sambil mempertahankan pasangan KV prompt sistem dan query.</li>
<li>Skor head yang sudah dihitung tersedia untuk LLaMA3.1-8B dan Qwen2.5-7/14B di <code>./utils/head_score</code>.</li>
</ul>
</li>
<li>Untuk menghitung skor head untuk model lain:
<pre><code class="language-bash">python -B test.py -m [model_name] -d scbench_qa_eng --save_head_score
</code></pre>
<ul>
<li>Hasil akan disimpan di <code>./utils/head_score</code>.</li>
<li>Jika menargetkan tugas pemrograman, kami sarankan juga menjalankan perintah dengan <code>-d scbench_repoqa</code>. Ini memungkinkan model menggunakan skor head maksimal dari bahasa alami dan bahasa pemrograman, yang meningkatkan performa.</li>
</ul>
</li>
<li>Skor ini dapat diintegrasikan secara mulus dengan mesin inferensi teroptimasi <a href="https://github.com/mit-han-lab/duo-attention">DuoAttention</a> dengan mengganti data skor head mereka dengan skor kami.</li>
</ul>
<h2>Evaluasi</h2>
<ul>
<li>Untuk menghasilkan respons model dengan rasio kompresi KV dari 0.1 hingga 1.0:
<pre><code class="language-bash">python -B eval.py -m [model_name] -d [data_name] --kv_type retain --num 100
</code></pre>
<ul>
<li>Hasil akan disimpan di <code>./results/[data_name]</code>.</li>
<li>Dataset yang didukung tercantum di <code>data/load.py</code>.</li>
</ul>
</li>
<li>Untuk menghitung metrik evaluasi dari hasil yang dihasilkan:
<pre><code class="language-bash">python -B -m results.parse -m [model_name] -d [data_name]
</code></pre>
</li>
</ul>
<h2>Menerapkan ke Model Baru</h2>
<p>Untuk mengintegrasikan KVzip ke model baru, Anda perlu memperbarui file-file berikut:</p>
<ul>
<li><code>attention/attn.py</code><br />
Modifikasi logika forward pass attention sesuai kebutuhan. Dalam beberapa kasus, pembaruan pada kvcache.py dan score.py juga mungkin diperlukan.</li>
<li><code>model/monkeypatch.py</code><br />
Implementasi monkey patching spesifik model untuk integrasi.</li>
<li><code>model/template.py</code><br />
Definisikan prompt sistem dan template format chat model.</li>
</ul>
<h2>Sitasi</h2>
<pre><code class="language-bibtex">@article{kim2025kvzip,
        title={KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction},
        author={Kim, Jang-Hyun and Kim, Jinuk and Kwon, Sangwoo and Lee, Jae W and Yun, Sangdoo and Song, Hyun Oh},
        journal={arXiv preprint arXiv:2505.23416},
        year={2025}
}
</code></pre>
<h2>Lisensi</h2>
<p>Lisensi MIT</p>
<hr />
<p>Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx">Open Ai Tx</a> | Last indexed: 2025-06-11</p>
<hr />

        </div>
    </div>
    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Default Statcounter code for openaitx
https://openaitx.github.io/ -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>
    <!-- End of Statcounter Code -->
</body>
</html>