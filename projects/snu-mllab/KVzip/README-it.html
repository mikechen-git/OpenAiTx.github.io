<!DOCTYPE html>
<html lang="it">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>KVzip - snu-mllab/KVzip</title>
    <meta name="title" content="KVzip - snu-mllab/KVzip">
    <meta name="description" content="snu-mllab/KVzip - GitHub repository it documentation and informationKVzip: Compressione della Cache KV Indipendente dalla Query con Ricostruzione del Contesto [Articolo] [Blog] Novità KVzip comprime la cache KV per supportare di...">
    <meta name="keywords" content="snu-mllab, KVzip, GitHub, repository, it documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/snu-mllab/KVzip/README-it.html">
    <meta property="og:title" content="KVzip - snu-mllab/KVzip">
    <meta property="og:description" content="snu-mllab/KVzip - GitHub repository it documentation and information">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">
    <script src="/js/marked.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        body { display: flex; flex-direction: column; min-height: 100vh; }
        .main-container { margin: 0 auto; width: 100%; max-width: 980px; padding: 0 20px; }
        @media (max-width: 768px) { .main-container { padding: 0 15px; } }
        .markdown-body img { max-width: 100%; height: auto; }
        .header { text-align: center; margin-bottom: 30px; padding: 20px; background-color: #f6f8fa; border-bottom: 1px solid #e1e4e8; position: relative; }
        .header .links { margin-top: 10px; font-size: 16px; }
        .header .links a { color: #0366d6; text-decoration: none; margin-left: 5px; }
        .header .links a:hover { text-decoration: underline; }
        .language-badges { margin-top: 15px; text-align: center; }
        .language-badges a { display: inline-block; margin: 2px; text-decoration: none; }
        .language-badges img { height: 20px; border-radius: 3px; }
        .language-badges a:hover img { opacity: 0.8; }
    </style>
</head>
<body>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/snu-mllab/KVzip" id="githubRepoLink" target="_blank">snu-mllab/KVzip</a>
<h1 style="display: none;">KVzip: Compressione della Cache KV Indipendente dalla Query con Ricostruzione del Contesto [Articolo] [Blog] Novità KVzip comprime la cache KV per supportare di...</h1>
        </div>
        <div class="language-badges" id="languageBadges">
            <!-- You can generate language badges here if needed -->
        </div>
    </div>
    <div class="main-container">
        <div class="markdown-body" id="content">
            <h1>KVzip: Compressione della Cache KV Indipendente dalla Query con Ricostruzione del Contesto</h1>
<p>[<a href="https://arxiv.org/abs/2505.23416">Articolo</a>] [<a href="https://janghyun1230.github.io/kvzip/">Blog</a>]</p>
<img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/method.png" width="800">
<h2>Novità</h2>
<ul>
<li>KVzip comprime la cache KV per supportare <strong>diverse query future</strong>.</li>
<li>[Dipendente dal contesto] Raggiunge una <strong>riduzione di 3–4× della dimensione della cache KV</strong> e una <strong>diminuzione di 2× della latenza di decodifica</strong>, con una degradazione minima delle prestazioni.</li>
<li>[Indipendente dal contesto] Migliora la compressione KV a livello di testa in stile <a href="https://github.com/mit-han-lab/duo-attention">DuoAttention</a>, utilizzando solo <strong>alcuni forward pass in meno di un minuto</strong> per l’ottimizzazione del punteggio di importanza a livello di testa (100x più veloce).</li>
<li>Esegui demo.py:
<img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/demo.png" width="800"></li>
</ul>
<h3>Benchmark in configurazione query-agnostica</h3>
<ul>
<li>Task: <a href="https://huggingface.co/datasets/rajpurkar/squad">SQuAD</a>, <a href="https://github.com/gkamradt/LLMTest_NeedleInAHaystack">NIAH</a>, <a href="https://github.com/microsoft/MInference/tree/main/scbench">SCBench</a>, <a href="https://huggingface.co/datasets/openai/gsm8k/viewer/main/train?row=7294">GSM8K</a>.</li>
<li>Modello: <a href="https://huggingface.co/Qwen/Qwen2.5-7B-Instruct">Qwen2.5-7B-Instruct-1M</a></li>
</ul>
<img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/benchmark.png" width="800">
<h2>Installazione</h2>
<p>Abbiamo utilizzato CUDA 12.1 e Python 3.10</p>
<pre><code class="language-bash">cd KVzip
pip install -r requirements.txt
pip install flash-attn==2.7.4.post1 --no-build-isolation
make i
</code></pre>
<ul>
<li>Per utilizzare la quantizzazione di <a href="https://github.com/mit-han-lab/omniserve">QServe</a>, segui <a href="https://github.com/snu-mllab/KVzip/tree/main/model/quant_model"><code>./model/quant_model</code></a>.</li>
</ul>
<h3>Dataset</h3>
<ul>
<li>Scarica il dataset SCBench preprocessato da <a href="https://drive.google.com/file/d/1cqoR6pxxFcjFqvPZkuAmF-fBSAlAbjbN/view?usp=share_link">Google Drive</a>.</li>
<li>Se hai scaricato i file decompressi, sposta semplicemente la cartella scbench.</li>
</ul>
<pre><code class="language-bash">mv scbench.zip kvzip/data/
cd kvzip/data
unzip scbench.zip  
</code></pre>
<h2>Avvio Rapido</h2>
<pre><code class="language-python">from model import ModelKVzip

model = ModelKVzip(&quot;Qwen/Qwen2.5-7B-Instruct-1M&quot;)
context = &quot;This is my basic profile. My name is Kim living in Seoul. My major is computer science.&quot;
queries = [&quot;What is my name?&quot;, &quot;Do I live in Seoul?&quot;]

kv = model.prefill(context, load_score=False)  # prefill cache KV + valutazione importanza
kv.prune(ratio=0.3)  # rapporto di compressione, elimina il 70% della cache KV

for q in queries:
    query_ids = model.apply_template(q)
    output = model.generate(query_ids, kv=kv, update_cache=False)  # inferenza efficiente
    print(q, output)
</code></pre>
<ul>
<li>I modelli supportati sono elencati in <a href="https://github.com/snu-mllab/KVzip/blob/main/model/load.py"><code>model/load.py</code></a>, inclusi <strong>LLaMA3, Qwen2.5/3, Gemma3</strong>.</li>
<li>Imposta <code>load_score=True</code> per eliminare l’overhead di compressione. Questo abilita l’eliminazione KV indipendente dal contesto, con un compromesso nel rapporto di compressione a <code>ratio=0.6</code>.</li>
<li>Dopo la generazione, le coppie KV corrispondenti alle query e ai token generati vengono selettivamente eliminate dalla cache per ulteriori elaborazioni. Imposta <code>update_cache=True</code> per abilitare l’inferenza multi-turno, mantenendo l’intera cronologia delle interazioni durante l’inferenza.</li>
</ul>
<h2>Profilazione di Memoria e Tempo di Calcolo</h2>
<h3>Eliminazione dipendente dal contesto</h3>
<pre><code class="language-bash">python -B test.py -m [model_name] -d [data_name] --kv_type evict --ratio 0.3
</code></pre>
<ul>
<li>Il codice sopra confronta anche gli output generati con cache KV completa rispetto a quella potata.</li>
<li>Per un test rapido, usa <code>-d squad</code>. Per test a lungo contesto, usa <code>-d scbench_kv</code>.
<ul>
<li>Nomi dei dati disponibili: <a href="https://github.com/snu-mllab/KVzip/blob/main/data/load.py"><code>data/load.py</code></a>.</li>
<li>Nomi dei modelli disponibili: <a href="https://github.com/snu-mllab/KVzip/blob/main/model/load.py"><code>model/load.py</code></a>, ad es. llama3.1-8b, qwen2.5-7b (o Qwen/Qwen2.5-7B-Instruct-1M).</li>
</ul>
</li>
<li>Abbiamo adattato il kernel CUDA da <a href="https://github.com/FFY0/AdaKV/tree/main">AdaKV</a>, che supporta l’allocazione non uniforme del budget per testa.
<ul>
<li>Attualmente, il nostro codice non ha un kernel ottimizzato per Gemma3 che usa cache KV statica, quindi il codice non produce guadagni effettivi in efficienza. Tuttavia, le prestazioni del modello possono ancora essere valutate usando l’attenzione ridotta con sottocampionamento KV (<code>--kv_type retain</code>).</li>
</ul>
</li>
</ul>
<h3>Eliminazione indipendente dal contesto (senza overhead di compressione a runtime)</h3>
<ul>
<li>Usa il flag <code>--level head</code> con <code>--ratio 0.6</code> (consigliato).
<ul>
<li>Rimuoviamo tutte le coppie KV di contesto associate a una testa specifica mantenendo le coppie KV del prompt di sistema e della query.</li>
<li>I punteggi precomputati per le teste sono disponibili per LLaMA3.1-8B e Qwen2.5-7/14B in <code>./utils/head_score</code>.</li>
</ul>
</li>
<li>Per calcolare i punteggi delle teste per altri modelli:
<pre><code class="language-bash">python -B test.py -m [model_name] -d scbench_qa_eng --save_head_score
</code></pre>
<ul>
<li>I risultati saranno salvati in <code>./utils/head_score</code>.</li>
<li>Se si punta a un task di coding, si consiglia di eseguire anche il comando con <code>-d scbench_repoqa</code>. Questo permette al modello di usare i punteggi massimi delle teste sia dal linguaggio naturale che da quello di programmazione, migliorando le prestazioni.</li>
</ul>
</li>
<li>Questi punteggi possono essere integrati senza problemi con il motore di inferenza ottimizzato di <a href="https://github.com/mit-han-lab/duo-attention">DuoAttention</a> sostituendo i loro dati di punteggio delle teste con i nostri.</li>
</ul>
<h2>Valutazione</h2>
<ul>
<li>Per generare risposte del modello con rapporti di compressione KV da 0.1 a 1.0:
<pre><code class="language-bash">python -B eval.py -m [model_name] -d [data_name] --kv_type retain --num 100
</code></pre>
<ul>
<li>I risultati saranno salvati in <code>./results/[data_name]</code>.</li>
<li>I dataset supportati sono elencati in <code>data/load.py</code>.</li>
</ul>
</li>
<li>Per calcolare le metriche di valutazione dai risultati generati:
<pre><code class="language-bash">python -B -m results.parse -m [model_name] -d [data_name]
</code></pre>
</li>
</ul>
<h2>Applicazione a Nuovi Modelli</h2>
<p>Per integrare KVzip in un nuovo modello, è necessario aggiornare i seguenti file:</p>
<ul>
<li><code>attention/attn.py</code><br />
Modifica la logica del forward pass dell’attenzione come necessario. In alcuni casi, sono richiesti aggiornamenti anche a kvcache.py e score.py.</li>
<li><code>model/monkeypatch.py</code><br />
Implementa il monkey patching specifico per il modello per l’integrazione.</li>
<li><code>model/template.py</code><br />
Definisci il prompt di sistema del modello e i template di formattazione della chat.</li>
</ul>
<h2>Citazione</h2>
<pre><code class="language-bibtex">@article{kim2025kvzip,
        title={KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction},
        author={Kim, Jang-Hyun and Kim, Jinuk and Kwon, Sangwoo and Lee, Jae W and Yun, Sangdoo and Song, Hyun Oh},
        journal={arXiv preprint arXiv:2505.23416},
        year={2025}
}
</code></pre>
<h2>Licenza</h2>
<p>Licenza MIT</p>
<hr />
<p>Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx">Open Ai Tx</a> | Last indexed: 2025-06-11</p>
<hr />

        </div>
    </div>
    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Statcounter and other scripts can be added here -->
</body>
</html>