<!DOCTYPE html><html lang="en"><head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>KVzip: Compressione della Cache KV Indipendente dalla Query con Ricostruzione del Contesto - snu-mllab/KVzip</title>

    <!-- Primary Meta Tags -->
    <meta name="title" content="KVzip: Compressione della Cache KV Indipendente dalla Query con Ricostruzione del Contesto - snu-mllab/KVzip">
    <meta name="description" content="snu-mllab/KVzip - GitHub repository documentation and information">
    <meta name="keywords" content="snu-mllab, KVzip, GitHub, repository, documentation, ">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">

    <!-- Open Graph -->
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=it">
    <meta property="og:title" content="KVzip: Compressione della Cache KV Indipendente dalla Query con Ricostruzione del Contesto - snu-mllab/KVzip">
    <meta property="og:description" content="snu-mllab/KVzip - GitHub repository documentation and information">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">

    <!-- Favicon -->
    <link rel="icon" type="image/jpeg" href="icon.jpg">
    <link rel="apple-touch-icon" href="icon.jpg">

    <!-- Marked.js for Markdown rendering -->
    <script type="text/javascript" async="" src="https://www.statcounter.com/counter/recorder.js"></script><script src="js/marked.min.js?v=20250613"></script>
    <!-- Highlight.js for code syntax highlighting -->
    <link rel="stylesheet" href="css/github.min.css?v=20250613">
    <script src="js/highlight.min.js?v=20250613"></script>
    <!-- Custom CSS -->
    <link rel="stylesheet" href="view.css?v=20250613">
    <style>
        /* Layout */
        body {
            display: flex;
            flex-direction: column;
            min-height: 100vh;
        }

        .main-container {
            margin: 0 auto;
            width: 100%;
            max-width: 980px;
            padding: 0 20px;
        }

        @media (max-width: 768px) {
            .main-container {
                padding: 0 15px;
            }
        }

        /* Image size restrictions */
        .markdown-body img {
            max-width: 100%;
            height: auto;
        }

        /* Existing styles */
        .header {
            text-align: center;
            margin-bottom: 30px;
            padding: 20px;
            background-color: #f6f8fa;
            border-bottom: 1px solid #e1e4e8;
            position: relative;
        }

        .back-button {
            position: absolute;
            left: 20px;
            top: 50%;
            transform: translateY(-50%);
            color: #0366d6;
            text-decoration: none;
            display: flex;
            align-items: center;
            font-size: 14px;
            padding: 5px 10px;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            background-color: #fff;
        }

        .back-button:hover {
            background-color: #f6f8fa;
            border-color: #0366d6;
        }

        .back-button::before {
            content: "←";
            margin-right: 5px;
            font-size: 16px;
        }

        .header .links {
            margin-top: 10px;
            font-size: 16px;
        }

        .header .links a {
            color: #0366d6;
            text-decoration: none;
            margin-left: 5px;
        }

        .header .links a:hover {
            text-decoration: underline;
        }
        
        /* Language badges styles */
        .language-badges {
            margin-top: 15px;
            text-align: center;
        }
        .language-badges a {
            display: inline-block;
            margin: 2px;
            text-decoration: none;
        }
        .language-badges img {
            height: 20px;
            border-radius: 3px;
        }
        .language-badges a:hover img {
            opacity: 0.8;
        }
    </style>
</head>

<body>
    <div class="header">
        <a href="javascript:history.back()" class="back-button">Back</a>
        <div class="links">
            GitHub Repository: <a href="https://github.com/snu-mllab/KVzip" id="githubRepoLink" target="_blank">snu-mllab/KVzip</a>
        </div>
        <div class="language-badges" id="languageBadges"><a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=en"><img src="https://img.shields.io/badge/EN-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=zh-CN"><img src="https://img.shields.io/badge/简中-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=zh-TW"><img src="https://img.shields.io/badge/繁中-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=ja"><img src="https://img.shields.io/badge/日本語-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=ko"><img src="https://img.shields.io/badge/한국어-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=th"><img src="https://img.shields.io/badge/ไทย-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=fr"><img src="https://img.shields.io/badge/Français-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=de"><img src="https://img.shields.io/badge/Deutsch-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=es"><img src="https://img.shields.io/badge/Español-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=it"><img src="https://img.shields.io/badge/Italiano-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=ru"><img src="https://img.shields.io/badge/Русский-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=pt"><img src="https://img.shields.io/badge/Português-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=nl"><img src="https://img.shields.io/badge/Nederlands-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=pl"><img src="https://img.shields.io/badge/Polski-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=ar"><img src="https://img.shields.io/badge/العربية-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=tr"><img src="https://img.shields.io/badge/Türkçe-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=vi"><img src="https://img.shields.io/badge/Tiếng Việt-white" alt="version"></a></div>
    </div>

    <div class="main-container">
        <div class="markdown-body" id="content"><h1>KVzip: Compressione della Cache KV Indipendente dalla Query con Ricostruzione del Contesto</h1>
<p>[<a href="https://arxiv.org/abs/2505.23416">Articolo</a>] [<a href="https://janghyun1230.github.io/kvzip/">Blog</a>] </p>
<img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/method.png" width="800">

<h2>Novità</h2>
<ul>
<li>KVzip comprime la cache KV per supportare <strong>diverse query future</strong>.</li>
<li>[Dipendente dal contesto] Raggiunge una <strong>riduzione di 3–4× della dimensione della cache KV</strong> e una <strong>diminuzione di 2× della latenza di decodifica</strong>, con una degradazione minima delle prestazioni.</li>
<li>[Indipendente dal contesto] Migliora la compressione KV a livello di testa in stile <a href="https://github.com/mit-han-lab/duo-attention">DuoAttention</a>, utilizzando solo <strong>alcuni forward pass in meno di un minuto</strong> per l’ottimizzazione del punteggio di importanza a livello di testa (100x più veloce).</li>
<li>Esegui demo.py:</li>
</ul>
<img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/demo.png" width="800">


<h3>Benchmark in configurazione query-agnostica</h3>
<ul>
<li>Task: <a href="https://huggingface.co/datasets/rajpurkar/squad">SQuAD</a>, <a href="https://github.com/gkamradt/LLMTest_NeedleInAHaystack">NIAH</a>, <a href="https://github.com/microsoft/MInference/tree/main/scbench">SCBench</a>, <a href="https://huggingface.co/datasets/openai/gsm8k/viewer/main/train?row=7294">GSM8K</a>. </li>
<li>Modello: <a href="https://huggingface.co/Qwen/Qwen2.5-7B-Instruct">Qwen2.5-7B-Instruct-1M</a></li>
</ul>
<img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/benchmark.png" width="800">


<h2>Installazione</h2>
<p>Abbiamo utilizzato CUDA 12.1 e Python 3.10</p>
<pre><code class="language-bash hljs"><span class="hljs-built_in">cd</span> KVzip
pip install -r requirements.txt
pip install flash-attn==2.7.4.post1 --no-build-isolation
make i
</code></pre>
<ul>
<li>Per utilizzare la quantizzazione di <a href="https://github.com/mit-han-lab/omniserve">QServe</a>, segui <a href="https://github.com/snu-mllab/KVzip/tree/main/model/quant_model"><code>./model/quant_model</code></a>.</li>
</ul>
<h3>Dataset</h3>
<ul>
<li>Scarica il dataset SCBench preprocessato da <a href="https://drive.google.com/file/d/1cqoR6pxxFcjFqvPZkuAmF-fBSAlAbjbN/view?usp=share_link">Google Drive</a>.</li>
<li>Se hai scaricato i file decompressi, sposta semplicemente la cartella scbench.</li>
</ul>
<pre><code class="language-bash hljs"><span class="hljs-built_in">mv</span> scbench.zip kvzip/data/
<span class="hljs-built_in">cd</span> kvzip/data
unzip scbench.zip  
</code></pre>
<h2>Avvio Rapido</h2>
<pre><code class="language-python hljs"><span class="hljs-keyword">from</span> model <span class="hljs-keyword">import</span> ModelKVzip

model = ModelKVzip(<span class="hljs-string">"Qwen/Qwen2.5-7B-Instruct-1M"</span>)
context = <span class="hljs-string">"This is my basic profile. My name is Kim living in Seoul. My major is computer science."</span>
queries = [<span class="hljs-string">"What is my name?"</span>, <span class="hljs-string">"Do I live in Seoul?"</span>]

kv = model.prefill(context, load_score=<span class="hljs-literal">False</span>)  <span class="hljs-comment"># prefill cache KV + valutazione importanza</span>
kv.prune(ratio=<span class="hljs-number">0.3</span>)  <span class="hljs-comment"># rapporto di compressione, elimina il 70% della cache KV</span>

<span class="hljs-keyword">for</span> q <span class="hljs-keyword">in</span> queries:
    query_ids = model.apply_template(q)
    output = model.generate(query_ids, kv=kv, update_cache=<span class="hljs-literal">False</span>)  <span class="hljs-comment"># inferenza efficiente</span>
    <span class="hljs-built_in">print</span>(q, output)
</code></pre>
<ul>
<li>I modelli supportati sono elencati in <a href="https://github.com/snu-mllab/KVzip/blob/main/model/load.py"><code>model/load.py</code></a>, inclusi <strong>LLaMA3, Qwen2.5/3, Gemma3</strong>.</li>
<li>Imposta <code>load_score=True</code> per eliminare l’overhead di compressione. Questo abilita l’eliminazione KV indipendente dal contesto, con un compromesso nel rapporto di compressione a <code>ratio=0.6</code>.</li>
<li>Dopo la generazione, le coppie KV corrispondenti alle query e ai token generati vengono selettivamente eliminate dalla cache per ulteriori elaborazioni. Imposta <code>update_cache=True</code> per abilitare l’inferenza multi-turno, mantenendo l’intera cronologia delle interazioni durante l’inferenza.</li>
</ul>
<h2>Profilazione di Memoria e Tempo di Calcolo</h2>
<h3>Eliminazione dipendente dal contesto</h3>
<pre><code class="language-bash hljs">python -B test.py -m [model_name] -d [data_name] --kv_type evict --ratio 0.3
</code></pre>
<ul>
<li>Il codice sopra confronta anche gli output generati con cache KV completa rispetto a quella potata.</li>
<li>Per un test rapido, usa <code>-d squad</code>. Per test a lungo contesto, usa <code>-d scbench_kv</code>.<ul>
<li>Nomi dei dati disponibili: <a href="https://github.com/snu-mllab/KVzip/blob/main/data/load.py"><code>data/load.py</code></a>.</li>
<li>Nomi dei modelli disponibili: <a href="https://github.com/snu-mllab/KVzip/blob/main/model/load.py"><code>model/load.py</code></a>, ad es. llama3.1-8b, qwen2.5-7b (o Qwen/Qwen2.5-7B-Instruct-1M).</li>
</ul>
</li>
<li>Abbiamo adattato il kernel CUDA da <a href="https://github.com/FFY0/AdaKV/tree/main">AdaKV</a>, che supporta l’allocazione non uniforme del budget per testa.<ul>
<li>Attualmente, il nostro codice non ha un kernel ottimizzato per Gemma3 che usa cache KV statica, quindi il codice non produce guadagni effettivi in efficienza. Tuttavia, le prestazioni del modello possono ancora essere valutate usando l’attenzione ridotta con sottocampionamento KV (<code>--kv_type retain</code>).</li>
</ul>
</li>
</ul>
<h3>Eliminazione indipendente dal contesto (senza overhead di compressione a runtime)</h3>
<ul>
<li>Usa il flag <code>--level head</code> con <code>--ratio 0.6</code> (consigliato).<ul>
<li>Rimuoviamo tutte le coppie KV di contesto associate a una testa specifica mantenendo le coppie KV del prompt di sistema e della query.</li>
<li>I punteggi precomputati per le teste sono disponibili per LLaMA3.1-8B e Qwen2.5-7/14B in <code>./utils/head_score</code>.</li>
</ul>
</li>
<li>Per calcolare i punteggi delle teste per altri modelli:<pre><code class="language-bash hljs">python -B test.py -m [model_name] -d scbench_qa_eng --save_head_score
</code></pre>
<ul>
<li>I risultati saranno salvati in <code>./utils/head_score</code>.</li>
<li>Se si punta a un task di coding, si consiglia di eseguire anche il comando con <code>-d scbench_repoqa</code>. Questo permette al modello di usare i punteggi massimi delle teste sia dal linguaggio naturale che da quello di programmazione, migliorando le prestazioni.</li>
</ul>
</li>
<li>Questi punteggi possono essere integrati senza problemi con il motore di inferenza ottimizzato di <a href="https://github.com/mit-han-lab/duo-attention">DuoAttention</a> sostituendo i loro dati di punteggio delle teste con i nostri.</li>
</ul>
<h2>Valutazione</h2>
<ul>
<li>Per generare risposte del modello con rapporti di compressione KV da 0.1 a 1.0:<pre><code class="language-bash hljs">python -B eval.py -m [model_name] -d [data_name] --kv_type retain --num 100
</code></pre>
<ul>
<li>I risultati saranno salvati in <code>./results/[data_name]</code>.</li>
<li>I dataset supportati sono elencati in <code>data/load.py</code>.</li>
</ul>
</li>
<li>Per calcolare le metriche di valutazione dai risultati generati:<pre><code class="language-bash hljs">python -B -m results.parse -m [model_name] -d [data_name]
</code></pre>
</li>
</ul>
<h2>Applicazione a Nuovi Modelli</h2>
<p>Per integrare KVzip in un nuovo modello, è necessario aggiornare i seguenti file:</p>
<ul>
<li><code>attention/attn.py</code><br>Modifica la logica del forward pass dell’attenzione come necessario. In alcuni casi, sono richiesti aggiornamenti anche a kvcache.py e score.py.</li>
<li><code>model/monkeypatch.py</code><br>Implementa il monkey patching specifico per il modello per l’integrazione.</li>
<li><code>model/template.py</code><br>Definisci il prompt di sistema del modello e i template di formattazione della chat.</li>
</ul>
<h2>Citazione</h2>
<pre><code class="language-bibtex">@article{kim2025kvzip,
        title={KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction},
        author={Kim, Jang-Hyun and Kim, Jinuk and Kwon, Sangwoo and Lee, Jae W and Yun, Sangdoo and Song, Hyun Oh},
        journal={arXiv preprint arXiv:2505.23416},
        year={2025}
}
</code></pre>
<h2>Licenza</h2>
<p>Licenza MIT</p>
<hr>
<p>Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx">Open Ai Tx</a> | Last indexed: 2025-06-11</p>
<hr>
</div>
    </div>

    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Default Statcounter code for openaitx
https://openaitx.github.io/ -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async=""></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>
    <!-- End of Statcounter Code -->
    <script src="view.js?v=20250613"></script>


</body></html>