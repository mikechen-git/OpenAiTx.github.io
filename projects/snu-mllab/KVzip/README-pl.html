<!DOCTYPE html><html lang="en"><head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>KVzip: Kompresja pamięci podręcznej KV niezależna od zapytań z rekonstrukcją kontekstu - snu-mllab/KVzip</title>

    <!-- Primary Meta Tags -->
    <meta name="title" content="KVzip: Kompresja pamięci podręcznej KV niezależna od zapytań z rekonstrukcją kontekstu - snu-mllab/KVzip">
    <meta name="description" content="Query-agnostic KV cache eviction: 3–4× reduction in memory and 2× decrease in latency (Qwen3/2.5, Gemma3, LLaMA3)">
    <meta name="keywords" content="snu-mllab, KVzip, GitHub, repository, documentation, Python">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">

    <!-- Open Graph -->
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=pl">
    <meta property="og:title" content="KVzip: Kompresja pamięci podręcznej KV niezależna od zapytań z rekonstrukcją kontekstu - snu-mllab/KVzip">
    <meta property="og:description" content="Query-agnostic KV cache eviction: 3–4× reduction in memory and 2× decrease in latency (Qwen3/2.5, Gemma3, LLaMA3)">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">

    <!-- Favicon -->
    <link rel="icon" type="image/jpeg" href="icon.jpg">
    <link rel="apple-touch-icon" href="icon.jpg">

    <!-- Marked.js for Markdown rendering -->
    <script type="text/javascript" async="" src="https://www.statcounter.com/counter/recorder.js"></script><script src="js/marked.min.js?v=20250613"></script>
    <!-- Highlight.js for code syntax highlighting -->
    <link rel="stylesheet" href="css/github.min.css?v=20250613">
    <script src="js/highlight.min.js?v=20250613"></script>
    <!-- Custom CSS -->
    <link rel="stylesheet" href="view.css?v=20250613">
    <style>
        /* Layout */
        body {
            display: flex;
            flex-direction: column;
            min-height: 100vh;
        }

        .main-container {
            margin: 0 auto;
            width: 100%;
            max-width: 980px;
            padding: 0 20px;
        }

        @media (max-width: 768px) {
            .main-container {
                padding: 0 15px;
            }
        }

        /* Image size restrictions */
        .markdown-body img {
            max-width: 100%;
            height: auto;
        }

        /* Existing styles */
        .header {
            text-align: center;
            margin-bottom: 30px;
            padding: 20px;
            background-color: #f6f8fa;
            border-bottom: 1px solid #e1e4e8;
            position: relative;
        }

        .back-button {
            position: absolute;
            left: 20px;
            top: 50%;
            transform: translateY(-50%);
            color: #0366d6;
            text-decoration: none;
            display: flex;
            align-items: center;
            font-size: 14px;
            padding: 5px 10px;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            background-color: #fff;
        }

        .back-button:hover {
            background-color: #f6f8fa;
            border-color: #0366d6;
        }

        .back-button::before {
            content: "←";
            margin-right: 5px;
            font-size: 16px;
        }

        .header .links {
            margin-top: 10px;
            font-size: 16px;
        }

        .header .links a {
            color: #0366d6;
            text-decoration: none;
            margin-left: 5px;
        }

        .header .links a:hover {
            text-decoration: underline;
        }
        
        /* Language badges styles */
        .language-badges {
            margin-top: 15px;
            text-align: center;
        }
        .language-badges a {
            display: inline-block;
            margin: 2px;
            text-decoration: none;
        }
        .language-badges img {
            height: 20px;
            border-radius: 3px;
        }
        .language-badges a:hover img {
            opacity: 0.8;
        }
    </style>
</head>

<body>
    <div class="header">
        <a href="javascript:history.back()" class="back-button">Back</a>
        <div class="links">
            GitHub Repository: <a href="https://github.com/snu-mllab/KVzip" id="githubRepoLink" target="_blank">snu-mllab/KVzip</a>
        </div>
        <div class="language-badges" id="languageBadges"><a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=en"><img src="https://img.shields.io/badge/EN-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=zh-CN"><img src="https://img.shields.io/badge/简中-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=zh-TW"><img src="https://img.shields.io/badge/繁中-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=ja"><img src="https://img.shields.io/badge/日本語-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=ko"><img src="https://img.shields.io/badge/한국어-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=th"><img src="https://img.shields.io/badge/ไทย-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=fr"><img src="https://img.shields.io/badge/Français-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=de"><img src="https://img.shields.io/badge/Deutsch-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=es"><img src="https://img.shields.io/badge/Español-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=it"><img src="https://img.shields.io/badge/Italiano-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=ru"><img src="https://img.shields.io/badge/Русский-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=pt"><img src="https://img.shields.io/badge/Português-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=nl"><img src="https://img.shields.io/badge/Nederlands-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=pl"><img src="https://img.shields.io/badge/Polski-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=ar"><img src="https://img.shields.io/badge/العربية-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=tr"><img src="https://img.shields.io/badge/Türkçe-white" alt="version"></a> <a href="https://openaitx.github.io/view.html?user=snu-mllab&amp;project=KVzip&amp;lang=vi"><img src="https://img.shields.io/badge/Tiếng Việt-white" alt="version"></a></div>
    </div>

    <div class="main-container">
        <div class="markdown-body" id="content"><h1>KVzip: Kompresja pamięci podręcznej KV niezależna od zapytań z rekonstrukcją kontekstu</h1>
<p>[<a href="https://arxiv.org/abs/2505.23416">Artykuł</a>] [<a href="https://janghyun1230.github.io/kvzip/">Blog</a>] </p>
<img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/method.png" width="800">

<h2>Co nowego?</h2>
<ul>
<li>KVzip kompresuje pamięć podręczną KV, aby wspierać <strong>różnorodne przyszłe zapytania</strong>.</li>
<li>[Zależne od kontekstu] Osiągnięcie <strong>3–4× redukcji rozmiaru pamięci podręcznej KV</strong> oraz <strong>2× skrócenia czasu dekodowania</strong>, przy minimalnej degradacji wydajności.</li>
<li>[Niezależne od kontekstu] Ulepszenie kompresji na poziomie głów DuoAttention-style, wykorzystując tylko <strong>kilka przejść w przód w ciągu minuty</strong> do optymalizacji ważności głów (100x szybciej).</li>
<li>Uruchom demo.py:</li>
</ul>
<img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/demo.png" width="800">


<h3>Benchmark w ustawieniu niezależnym od zapytań</h3>
<ul>
<li>Zadania: <a href="https://huggingface.co/datasets/rajpurkar/squad">SQuAD</a>, <a href="https://github.com/gkamradt/LLMTest_NeedleInAHaystack">NIAH</a>, <a href="https://github.com/microsoft/MInference/tree/main/scbench">SCBench</a>, <a href="https://huggingface.co/datasets/openai/gsm8k/viewer/main/train?row=7294">GSM8K</a>. </li>
<li>Model: <a href="https://huggingface.co/Qwen/Qwen2.5-7B-Instruct">Qwen2.5-7B-Instruct-1M</a></li>
</ul>
<img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/benchmark.png" width="800">


<h2>Instalacja</h2>
<p>Używaliśmy CUDA 12.1 i Python 3.10</p>
<pre><code class="language-bash hljs"><span class="hljs-built_in">cd</span> KVzip
pip install -r requirements.txt
pip install flash-attn==2.7.4.post1 --no-build-isolation
make i
</code></pre>
<ul>
<li>Aby używać kwantyzacji <a href="https://github.com/mit-han-lab/omniserve">QServe</a>, proszę postępować zgodnie z <a href="https://github.com/snu-mllab/KVzip/tree/main/model/quant_model"><code>./model/quant_model</code></a>.</li>
</ul>
<h3>Zbiór danych</h3>
<ul>
<li>Proszę pobrać przetworzony zbiór danych SCBench z <a href="https://drive.google.com/file/d/1cqoR6pxxFcjFqvPZkuAmF-fBSAlAbjbN/view?usp=share_link">Google Drive</a>.</li>
<li>Jeśli pobrałeś rozpakowane pliki, po prostu przenieś folder scbench.</li>
</ul>
<pre><code class="language-bash hljs"><span class="hljs-built_in">mv</span> scbench.zip kvzip/data/
<span class="hljs-built_in">cd</span> kvzip/data
unzip scbench.zip  
</code></pre>
<h2>Szybki start</h2>
<pre><code class="language-python hljs"><span class="hljs-keyword">from</span> model <span class="hljs-keyword">import</span> ModelKVzip

model = ModelKVzip(<span class="hljs-string">"Qwen/Qwen2.5-7B-Instruct-1M"</span>)
context = <span class="hljs-string">"To jest mój podstawowy profil. Nazywam się Kim i mieszkam w Seulu. Mój kierunek to informatyka."</span>
queries = [<span class="hljs-string">"Jak mam na imię?"</span>, <span class="hljs-string">"Czy mieszkam w Seulu?"</span>]

kv = model.prefill(context, load_score=<span class="hljs-literal">False</span>)  <span class="hljs-comment"># wstępne wypełnienie pamięci KV + ocena ważności</span>
kv.prune(ratio=<span class="hljs-number">0.3</span>)  <span class="hljs-comment"># współczynnik kompresji, usuwamy 70% KV</span>

<span class="hljs-keyword">for</span> q <span class="hljs-keyword">in</span> queries:
    query_ids = model.apply_template(q)
    output = model.generate(query_ids, kv=kv, update_cache=<span class="hljs-literal">False</span>)  <span class="hljs-comment"># wydajne wnioskowanie</span>
    <span class="hljs-built_in">print</span>(q, output)
</code></pre>
<ul>
<li>Obsługiwane modele są wymienione w <a href="https://github.com/snu-mllab/KVzip/blob/main/model/load.py"><code>model/load.py</code></a>, w tym <strong>LLaMA3, Qwen2.5/3, Gemma3</strong>.</li>
<li>Ustaw <code>load_score=True</code>, aby wyeliminować narzut kompresji. Pozwala to na niezależne od kontekstu usuwanie KV, kosztem gorszego współczynnika kompresji <code>ratio=0.6</code>.</li>
<li>Po generacji, pary KV odpowiadające zapytaniom i wygenerowanym tokenom są selektywnie usuwane z pamięci podręcznej do dalszego przetwarzania. Ustaw <code>update_cache=True</code>, aby włączyć wnioskowanie wieloetapowe, zachowując pełną historię interakcji przez cały czas inferencji.</li>
</ul>
<h2>Profilowanie pamięci i czasu obliczeń</h2>
<h3>Usuwanie zależne od kontekstu</h3>
<pre><code class="language-bash hljs">python -B test.py -m [model_name] -d [data_name] --kv_type evict --ratio 0.3
</code></pre>
<ul>
<li>Powyższy kod porównuje także wyniki generowane przy pełnej i przyciętej pamięci KV.</li>
<li>Do szybkich testów użyj <code>-d squad</code>. Do testów długiego kontekstu użyj <code>-d scbench_kv</code>.<ul>
<li>Dostępne nazwy danych: <a href="https://github.com/snu-mllab/KVzip/blob/main/data/load.py"><code>data/load.py</code></a>.</li>
<li>Dostępne nazwy modeli: <a href="https://github.com/snu-mllab/KVzip/blob/main/model/load.py"><code>model/load.py</code></a>, np. llama3.1-8b, qwen2.5-7b (lub Qwen/Qwen2.5-7B-Instruct-1M).</li>
</ul>
</li>
<li>Adaptujemy jądro CUDA z <a href="https://github.com/FFY0/AdaKV/tree/main">AdaKV</a>, wspierające nierównomierną alokację budżetu głów.<ul>
<li>Obecnie nasz kod nie zawiera zoptymalizowanego jądra dla Gemma3, które używa statycznej pamięci KV, więc kod nie przynosi faktycznych zysków wydajnościowych. Jednak wydajność modelu można nadal ocenić, używając zredukowanej uwagi z podpróbkowaniem KV (<code>--kv_type retain</code>).</li>
</ul>
</li>
</ul>
<h3>Usuwanie niezależne od kontekstu (bez narzutu kompresji w czasie działania)</h3>
<ul>
<li>Użyj flagi <code>--level head</code> z <code>--ratio 0.6</code> (zalecane).<ul>
<li>Usuwamy wszystkie pary KV kontekstu powiązane z daną głową, pozostawiając pary KV systemowego promptu i zapytań.</li>
<li>Wstępnie obliczone oceny głów są dostępne dla LLaMA3.1-8B i Qwen2.5-7/14B w <code>./utils/head_score</code>.</li>
</ul>
</li>
<li>Aby obliczyć oceny głów dla innych modeli:<pre><code class="language-bash hljs">python -B test.py -m [model_name] -d scbench_qa_eng --save_head_score
</code></pre>
<ul>
<li>Wyniki zostaną zapisane w <code>./utils/head_score</code>.</li>
<li>Jeśli celem jest zadanie programistyczne, zalecamy dodatkowo uruchomić polecenie z <code>-d scbench_repoqa</code>. Pozwala to modelowi korzystać z maksymalnych ocen głów z języka naturalnego i programowania, co poprawia wydajność.</li>
</ul>
</li>
<li>Te oceny można płynnie zintegrować z zoptymalizowanym silnikiem inferencji <a href="https://github.com/mit-han-lab/duo-attention">DuoAttention</a>, zastępując ich dane ocen głowami naszymi.</li>
</ul>
<h2>Ewaluacja</h2>
<ul>
<li>Aby wygenerować odpowiedzi modelu z współczynnikami kompresji KV od 0.1 do 1.0:<pre><code class="language-bash hljs">python -B eval.py -m [model_name] -d [data_name] --kv_type retain --num 100
</code></pre>
<ul>
<li>Wyniki zostaną zapisane w <code>./results/[data_name]</code>.</li>
<li>Obsługiwane zbiory danych są wymienione w <code>data/load.py</code>.</li>
</ul>
</li>
<li>Aby obliczyć metryki ewaluacyjne na podstawie wygenerowanych wyników:<pre><code class="language-bash hljs">python -B -m results.parse -m [model_name] -d [data_name]
</code></pre>
</li>
</ul>
<h2>Zastosowanie do nowych modeli</h2>
<p>Aby zintegrować KVzip z nowym modelem, należy zaktualizować następujące pliki:</p>
<ul>
<li><code>attention/attn.py</code><br>Modyfikacja logiki przekazywania uwagi (forward pass) według potrzeb. W niektórych przypadkach wymagane są również zmiany w kvcache.py i score.py.</li>
<li><code>model/monkeypatch.py</code><br>Implementacja specyficznego dla modelu monkey patchingu do integracji.</li>
<li><code>model/template.py</code><br>Definicja systemowego promptu modelu oraz szablonów formatowania czatu.</li>
</ul>
<h2>Cytowanie</h2>
<pre><code class="language-bibtex">@article{kim2025kvzip,
        title={KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction},
        author={Kim, Jang-Hyun and Kim, Jinuk and Kwon, Sangwoo and Lee, Jae W and Yun, Sangdoo and Song, Hyun Oh},
        journal={arXiv preprint arXiv:2505.23416},
        year={2025}
}
</code></pre>
<h2>Licencja</h2>
<p>Licencja MIT</p>
<hr>
<p>Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx">Open Ai Tx</a> | Last indexed: 2025-06-11</p>
<hr>
</div>
    </div>

    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Default Statcounter code for openaitx
https://openaitx.github.io/ -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async=""></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>
    <!-- End of Statcounter Code -->
    <script src="view.js?v=20250613"></script>


</body></html>