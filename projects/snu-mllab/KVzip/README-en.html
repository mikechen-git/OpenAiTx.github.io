<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>KVzip - snu-mllab/KVzip</title>
    <meta name="title" content="KVzip - snu-mllab/KVzip">
    <meta name="description" content="snu-mllab/KVzip - GitHub repository en documentation and informationKVzip: Query-Agnostic KV Cache Compression with Context Reconstruction [Paper] [Blog] What's New? KVzip compresses the KV cache to support diverse future queries. [Context-dependent] Achieve a 3–4× reduction in KV cache size and a 2× decrease in decoding latency, with minimal performance degradation. [Context-independent] Enhance DuoAttention-style head-level KV compression, using only a few forward passes within one minute for head-level importance-score optimization (100x faster). Run demo.py: Benchmarking on query-agnostic setting Tasks: SQuAD, NIAH, SCBench, GSM8K. Model: Qwen2.5-7B-Instruct-1M Installation We used CUDA 12.1 and Python 3.10 cd KVzip pip install -r requirements.txt pip install flash-attn==2.7.4.post1 --no-build-isolation make i To use QServe quantization, please follow ./model/quant_model. Dataset Please download the preprocessed SCBench dataset from Google Drive. If you download the unzipped the files, simply move the scbench folder. mv scbench.zip kvzip/data/ cd kvzip/data unzip scbench.zip Quick Start from model import ModelKVzip model = ModelKVzip(&quot;Qwen/Qwen2.5-7B-Instruct-1M&quot;) context = &quot;This is my basic profile. My name...">
    <meta name="keywords" content="snu-mllab, KVzip, GitHub, repository, en documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/snu-mllab/KVzip/README-en.html">
    <meta property="og:title" content="KVzip - snu-mllab/KVzip">
    <meta property="og:description" content="snu-mllab/KVzip - GitHub repository en documentation and information">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">
    <script src="/js/marked.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        body { display: flex; flex-direction: column; min-height: 100vh; }
        .main-container { margin: 0 auto; width: 100%; max-width: 980px; padding: 0 20px; }
        @media (max-width: 768px) { .main-container { padding: 0 15px; } }
        .markdown-body img { max-width: 100%; height: auto; }
        .header { text-align: center; margin-bottom: 30px; padding: 20px; background-color: #f6f8fa; border-bottom: 1px solid #e1e4e8; position: relative; }
        .header .links { margin-top: 10px; font-size: 16px; }
        .header .links a { color: #0366d6; text-decoration: none; margin-left: 5px; }
        .header .links a:hover { text-decoration: underline; }
        .language-badges { margin-top: 15px; text-align: center; }
        .language-badges a { display: inline-block; margin: 2px; text-decoration: none; }
        .language-badges img { height: 20px; border-radius: 3px; }
        .language-badges a:hover img { opacity: 0.8; }
    </style>
</head>
<body>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/snu-mllab/KVzip" id="githubRepoLink" target="_blank">snu-mllab/KVzip</a>
<br>
<h1 style="display: none;">KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction [Paper] [Blog] What's New? KVzip compresses the KV cache to support diverse future queries. [Context-dependent] Achieve a 3–4× reduction in KV cache size and a 2× decrease in decoding latency, with minimal performance degradation. [Context-independent] Enhance DuoAttention-style head-level KV compression, using only a few forward passes within one minute for head-level importance-score optimization (100x faster). Run demo.py: Benchmarking on query-agnostic setting Tasks: SQuAD, NIAH, SCBench, GSM8K. Model: Qwen2.5-7B-Instruct-1M Installation We used CUDA 12.1 and Python 3.10 cd KVzip pip install -r requirements.txt pip install flash-attn==2.7.4.post1 --no-build-isolation make i To use QServe quantization, please follow ./model/quant_model. Dataset Please download the preprocessed SCBench dataset from Google Drive. If you download the unzipped the files, simply move the scbench folder. mv scbench.zip kvzip/data/ cd kvzip/data unzip scbench.zip Quick Start from model import ModelKVzip model = ModelKVzip(&quot;Qwen/Qwen2.5-7B-Instruct-1M&quot;) context = &quot;This is my basic profile. My name...</h1>
        </div>
        <div class="language-badges" id="languageBadges">
            <!-- You can generate language badges here if needed -->
        </div>
    </div>
    <div class="main-container">
        <div class="markdown-body" id="content">
            <h1>KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction</h1>
<p>[<a href="https://arxiv.org/abs/2505.23416">Paper</a>] [<a href="https://janghyun1230.github.io/kvzip/">Blog</a>]</p>
<img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/method.png" width="800">
<h2>What's New?</h2>
<ul>
<li>KVzip compresses the KV cache to support <strong>diverse future queries</strong>.</li>
<li>[Context-dependent] Achieve a <strong>3–4× reduction in KV cache size</strong> and a <strong>2× decrease in decoding latency</strong>, with minimal performance degradation.</li>
<li>[Context-independent] Enhance <a href="https://github.com/mit-han-lab/duo-attention">DuoAttention</a>-style head-level KV compression, using only <strong>a few forward passes within one minute</strong> for head-level importance-score optimization (100x faster).</li>
<li>Run demo.py:
<img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/demo.png" width="800"></li>
</ul>
<h3>Benchmarking on query-agnostic setting</h3>
<ul>
<li>Tasks: <a href="https://huggingface.co/datasets/rajpurkar/squad">SQuAD</a>, <a href="https://github.com/gkamradt/LLMTest_NeedleInAHaystack">NIAH</a>, <a href="https://github.com/microsoft/MInference/tree/main/scbench">SCBench</a>, <a href="https://huggingface.co/datasets/openai/gsm8k/viewer/main/train?row=7294">GSM8K</a>.</li>
<li>Model: <a href="https://huggingface.co/Qwen/Qwen2.5-7B-Instruct">Qwen2.5-7B-Instruct-1M</a></li>
</ul>
<img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/benchmark.png" width="800">
<h2>Installation</h2>
<p>We used CUDA 12.1 and Python 3.10</p>
<pre><code class="language-bash">cd KVzip
pip install -r requirements.txt
pip install flash-attn==2.7.4.post1 --no-build-isolation
make i
</code></pre>
<ul>
<li>To use <a href="https://github.com/mit-han-lab/omniserve">QServe</a> quantization, please follow <a href="https://github.com/snu-mllab/KVzip/tree/main/model/quant_model"><code>./model/quant_model</code></a>.</li>
</ul>
<h3>Dataset</h3>
<ul>
<li>Please download the preprocessed SCBench dataset from <a href="https://drive.google.com/file/d/1cqoR6pxxFcjFqvPZkuAmF-fBSAlAbjbN/view?usp=share_link">Google Drive</a>.</li>
<li>If you download the unzipped the files, simply move the scbench folder.</li>
</ul>
<pre><code class="language-bash">mv scbench.zip kvzip/data/
cd kvzip/data
unzip scbench.zip  
</code></pre>
<h2>Quick Start</h2>
<pre><code class="language-python">from model import ModelKVzip

model = ModelKVzip(&quot;Qwen/Qwen2.5-7B-Instruct-1M&quot;)
context = &quot;This is my basic profile. My name is Kim living in Seoul. My major is computer science.&quot;
queries = [&quot;What is my name?&quot;, &quot;Do I live in Seoul?&quot;]

kv = model.prefill(context, load_score=False)  # prefill KV cache + importance scoring
kv.prune(ratio=0.3)  # compression ratio, evict 70% KV

for q in queries:
    query_ids = model.apply_template(q)
    output = model.generate(query_ids, kv=kv, update_cache=False)  # efficient inference
    print(q, output)
</code></pre>
<ul>
<li>Supported models are listed in <a href="https://github.com/snu-mllab/KVzip/blob/main/model/load.py"><code>model/load.py</code></a>, including <strong>LLaMA3, Qwen2.5/3, Gemma3</strong>.</li>
<li>Set <code>load_score=True</code> to eliminate compression overhead. This enables context-independent KV eviction, with a trade-off in compression ratio of <code>ratio=0.6</code>.</li>
<li>After generation, KV pairs corresponding to the queries and generated tokens are selectively evicted from the cache for further processing. Set <code>update_cache=True</code> to enable multi-turn inference, retaining full interaction histories throughout the inference.</li>
</ul>
<h2>Profiling Memory and Computation Time</h2>
<h3>Context-dependent eviction</h3>
<pre><code class="language-bash">python -B test.py -m [model_name] -d [data_name] --kv_type evict --ratio 0.3
</code></pre>
<ul>
<li>The code above also compares outputs generated with full versus pruned KV caches.</li>
<li>To quick test, use <code>-d squad</code>. For long-context testing, use <code>-d scbench_kv</code>.
<ul>
<li>Available data names: <a href="https://github.com/snu-mllab/KVzip/blob/main/data/load.py"><code>data/load.py</code></a>.</li>
<li>Available model names: <a href="https://github.com/snu-mllab/KVzip/blob/main/model/load.py"><code>model/load.py</code></a>, e.g., llama3.1-8b, qwen2.5-7b (or Qwen/Qwen2.5-7B-Instruct-1M).</li>
</ul>
</li>
<li>We adapt CUDA kernel from <a href="https://github.com/FFY0/AdaKV/tree/main">AdaKV</a>, supporting non-uniform head budget allocation.
<ul>
<li>Currently, our code lacks an optimized kernel for Gemma3 which uses static KV cache, so the code does not yield actual efficiency gains. However, model performance can still be evaluated using reduced attention with KV subsampling (<code>--kv_type retain</code>).</li>
</ul>
</li>
</ul>
<h3>Context-independent eviction (no runtime compression overhead)</h3>
<ul>
<li>Use the <code>--level head</code> flag with <code>--ratio 0.6</code> (recommended).
<ul>
<li>We remove all context KV pairs associated with a specific head while retaining system prompt and query KV pairs.</li>
<li>Precomputed head scores are available for LLaMA3.1-8B and Qwen2.5-7/14B in <code>./utils/head_score</code>.</li>
</ul>
</li>
<li>To compute head scores for other models:
<pre><code class="language-bash">python -B test.py -m [model_name] -d scbench_qa_eng --save_head_score
</code></pre>
<ul>
<li>Results will be saved in <code>./utils/head_score</code>.</li>
<li>If targeting a coding task, we recommend additionally running the command with <code>-d scbench_repoqa</code>. This allows the model to use the max head scores from both natural and coding languages, which improves performance.</li>
</ul>
</li>
<li>These scores can be seamlessly integrated with <a href="https://github.com/mit-han-lab/duo-attention">DuoAttention</a>'s optimized inference engine by replacing their head score data with ours.</li>
</ul>
<h2>Evaluation</h2>
<ul>
<li>To generate model responses with KV compression ratios ranging from 0.1 to 1.0:
<pre><code class="language-bash">python -B eval.py -m [model_name] -d [data_name] --kv_type retain --num 100
</code></pre>
<ul>
<li>Results will be saved in <code>./results/[data_name]</code>.</li>
<li>Supported datasets are listed in <code>data/load.py</code>.</li>
</ul>
</li>
<li>To compute evaluation metrics from generated results:
<pre><code class="language-bash">python -B -m results.parse -m [model_name] -d [data_name]
</code></pre>
</li>
</ul>
<h2>Applying to New Models</h2>
<p>To integrate KVzip for a new model, you will need to update the following files:</p>
<ul>
<li><code>attention/attn.py</code><br />
Modify the attention forward pass logic as needed. In certain cases, updates to kvcache.py and score.py may also be required.</li>
<li><code>model/monkeypatch.py</code><br />
Implement model-specific monkey patching for integration.</li>
<li><code>model/template.py</code><br />
Define the model's system prompt and chat formatting templates.</li>
</ul>
<h2>Citation</h2>
<pre><code class="language-bibtex">@article{kim2025kvzip,
        title={KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction},
        author={Kim, Jang-Hyun and Kim, Jinuk and Kwon, Sangwoo and Lee, Jae W and Yun, Sangdoo and Song, Hyun Oh},
        journal={arXiv preprint arXiv:2505.23416},
        year={2025}
}
</code></pre>
<h2>License</h2>
<p>MIT License</p>
<hr />
<p>Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx">Open Ai Tx</a> | Last indexed: 2025-06-11</p>
<hr />

        </div>
    </div>
    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Statcounter and other scripts can be added here -->
</body>
</html>