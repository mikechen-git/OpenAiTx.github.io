<!DOCTYPE html>
<html lang="hi">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>KVzip - snu-mllab/KVzip</title>
    <meta name="title" content="KVzip - snu-mllab/KVzip">
    <meta name="description" content="snu-mllab/KVzip - GitHub repository hi documentation and informationKVzip: संदर्भ पुनर्निर्माण के साथ क्वेरी-एग्नोस्टिक KV कैश संपीड़न [पेपर] [ब्लॉग] क्या नया है? KVzip KV कैश को संपीड़ित करता है ताकि विविध भविष्य की क्वेरियों क...">
    <meta name="keywords" content="snu-mllab, KVzip, GitHub, repository, hi documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/snu-mllab/KVzip/README-hi.html">
    <meta property="og:title" content="KVzip - snu-mllab/KVzip">
    <meta property="og:description" content="snu-mllab/KVzip - GitHub repository hi documentation and information">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">
    <script src="/js/marked.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        body { display: flex; flex-direction: column; min-height: 100vh; }
        .main-container { margin: 0 auto; width: 100%; max-width: 980px; padding: 0 20px; }
        @media (max-width: 768px) { .main-container { padding: 0 15px; } }
        .markdown-body img { max-width: 100%; height: auto; }
        .header { text-align: center; margin-bottom: 30px; padding: 20px; background-color: #f6f8fa; border-bottom: 1px solid #e1e4e8; position: relative; }
        .header .links { margin-top: 10px; font-size: 16px; }
        .header .links a { color: #0366d6; text-decoration: none; margin-left: 5px; }
        .header .links a:hover { text-decoration: underline; }
        .language-badges { margin-top: 15px; text-align: center; }
        .language-badges a { display: inline-block; margin: 2px; text-decoration: none; }
        .language-badges img { height: 20px; border-radius: 3px; }
        .language-badges a:hover img { opacity: 0.8; }
    </style>
</head>
<body>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/snu-mllab/KVzip" id="githubRepoLink" target="_blank">snu-mllab/KVzip</a>
<h1 style="display: none;">KVzip: संदर्भ पुनर्निर्माण के साथ क्वेरी-एग्नोस्टिक KV कैश संपीड़न [पेपर] [ब्लॉग] क्या नया है? KVzip KV कैश को संपीड़ित करता है ताकि विविध भविष्य की क्वेरियों क...</h1>
        </div>
        <div class="language-badges" id="languageBadges">
            <!-- You can generate language badges here if needed -->
        </div>
    </div>
    <div class="main-container">
        <div class="markdown-body" id="content">
            <h1>KVzip: संदर्भ पुनर्निर्माण के साथ क्वेरी-एग्नोस्टिक KV कैश संपीड़न</h1>
<p>[<a href="https://arxiv.org/abs/2505.23416">पेपर</a>] [<a href="https://janghyun1230.github.io/kvzip/">ब्लॉग</a>]</p>
<img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/method.png" width="800">
<h2>क्या नया है?</h2>
<ul>
<li>KVzip KV कैश को संपीड़ित करता है ताकि <strong>विविध भविष्य की क्वेरियों</strong> का समर्थन किया जा सके।</li>
<li>[संदर्भ-निर्भर] <strong>3–4× KV कैश आकार में कमी</strong> और <strong>2× डिकोडिंग विलंबता में कमी</strong> प्राप्त करें, न्यूनतम प्रदर्शन ह्रास के साथ।</li>
<li>[संदर्भ-स्वतंत्र] <a href="https://github.com/mit-han-lab/duo-attention">DuoAttention</a>-शैली के हेड-स्तर KV संपीड़न को बढ़ाएं, केवल <strong>एक मिनट के भीतर कुछ फॉरवर्ड पास</strong> का उपयोग करके हेड-स्तर महत्व-स्कोर अनुकूलन के लिए (100 गुना तेज)।</li>
<li>demo.py चलाएँ:
<img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/demo.png" width="800"></li>
</ul>
<h3>क्वेरी-एग्नोस्टिक सेटिंग पर बेंचमार्किंग</h3>
<ul>
<li>कार्य: <a href="https://huggingface.co/datasets/rajpurkar/squad">SQuAD</a>, <a href="https://github.com/gkamradt/LLMTest_NeedleInAHaystack">NIAH</a>, <a href="https://github.com/microsoft/MInference/tree/main/scbench">SCBench</a>, <a href="https://huggingface.co/datasets/openai/gsm8k/viewer/main/train?row=7294">GSM8K</a>।</li>
<li>मॉडल: <a href="https://huggingface.co/Qwen/Qwen2.5-7B-Instruct">Qwen2.5-7B-Instruct-1M</a></li>
</ul>
<img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/benchmark.png" width="800">
<h2>इंस्टॉलेशन</h2>
<p>हमने CUDA 12.1 और Python 3.10 का उपयोग किया</p>
<pre><code class="language-bash">cd KVzip
pip install -r requirements.txt
pip install flash-attn==2.7.4.post1 --no-build-isolation
make i
</code></pre>
<ul>
<li><a href="https://github.com/mit-han-lab/omniserve">QServe</a> क्वांटाइजेशन का उपयोग करने के लिए कृपया <a href="https://github.com/snu-mllab/KVzip/tree/main/model/quant_model"><code>./model/quant_model</code></a> का पालन करें।</li>
</ul>
<h3>डेटासेट</h3>
<ul>
<li>कृपया पूर्व-संसाधित SCBench डेटासेट <a href="https://drive.google.com/file/d/1cqoR6pxxFcjFqvPZkuAmF-fBSAlAbjbN/view?usp=share_link">Google Drive</a> से डाउनलोड करें।</li>
<li>यदि आपने फाइलों को अनजिप कर लिया है, तो बस scbench फ़ोल्डर को स्थानांतरित करें।</li>
</ul>
<pre><code class="language-bash">mv scbench.zip kvzip/data/
cd kvzip/data
unzip scbench.zip  
</code></pre>
<h2>त्वरित प्रारंभ</h2>
<pre><code class="language-python">from model import ModelKVzip

model = ModelKVzip(&quot;Qwen/Qwen2.5-7B-Instruct-1M&quot;)
context = &quot;यह मेरी मूल प्रोफ़ाइल है। मेरा नाम किम है जो सियोल में रहता है। मेरा प्रमुख विषय कंप्यूटर विज्ञान है।&quot;
queries = [&quot;मेरा नाम क्या है?&quot;, &quot;क्या मैं सियोल में रहता हूँ?&quot;]

kv = model.prefill(context, load_score=False)  # KV कैश भरें + महत्व स्कोरिंग
kv.prune(ratio=0.3)  # संपीड़न अनुपात, 70% KV निकालें

for q in queries:
    query_ids = model.apply_template(q)
    output = model.generate(query_ids, kv=kv, update_cache=False)  # कुशल अनुमान
    print(q, output)
</code></pre>
<ul>
<li>समर्थित मॉडल <a href="https://github.com/snu-mllab/KVzip/blob/main/model/load.py"><code>model/load.py</code></a> में सूचीबद्ध हैं, जिनमें <strong>LLaMA3, Qwen2.5/3, Gemma3</strong> शामिल हैं।</li>
<li>संपीड़न ओवरहेड को समाप्त करने के लिए <code>load_score=True</code> सेट करें। यह संदर्भ-स्वतंत्र KV निकासी सक्षम करता है, जिसमें <code>ratio=0.6</code> के संपीड़न अनुपात का व्यापार होता है।</li>
<li>जनरेशन के बाद, क्वेरियों और उत्पन्न टोकनों से संबंधित KV जोड़े चयनात्मक रूप से कैश से निकाले जाते हैं ताकि आगे प्रसंस्करण किया जा सके। मल्टी-टर्न अनुमान सक्षम करने के लिए <code>update_cache=True</code> सेट करें, जिससे अनुमान के दौरान पूर्ण इंटरैक्शन इतिहास संरक्षित रहता है।</li>
</ul>
<h2>मेमोरी और कम्प्यूटेशन समय प्रोफाइलिंग</h2>
<h3>संदर्भ-निर्भर निकासी</h3>
<pre><code class="language-bash">python -B test.py -m [model_name] -d [data_name] --kv_type evict --ratio 0.3
</code></pre>
<ul>
<li>ऊपर दिया गया कोड पूर्ण और संकुचित KV कैश के साथ उत्पन्न आउटपुट की तुलना भी करता है।</li>
<li>त्वरित परीक्षण के लिए <code>-d squad</code> का उपयोग करें। लंबी संदर्भ परीक्षण के लिए <code>-d scbench_kv</code> का उपयोग करें।
<ul>
<li>उपलब्ध डेटा नाम: <a href="https://github.com/snu-mllab/KVzip/blob/main/data/load.py"><code>data/load.py</code></a>।</li>
<li>उपलब्ध मॉडल नाम: <a href="https://github.com/snu-mllab/KVzip/blob/main/model/load.py"><code>model/load.py</code></a>, उदाहरण के लिए llama3.1-8b, qwen2.5-7b (या Qwen/Qwen2.5-7B-Instruct-1M)।</li>
</ul>
</li>
<li>हम <a href="https://github.com/FFY0/AdaKV/tree/main">AdaKV</a> से CUDA कर्नेल को अनुकूलित करते हैं, जो गैर-समान हेड बजट आवंटन का समर्थन करता है।
<ul>
<li>वर्तमान में, हमारे कोड में Gemma3 के लिए एक अनुकूलित कर्नेल नहीं है जो स्थैतिक KV कैश का उपयोग करता है, इसलिए यह कोड वास्तविक दक्षता लाभ प्रदान नहीं करता। हालांकि, KV सबसैंपलिंग (<code>--kv_type retain</code>) के साथ कम ध्यान का उपयोग करके मॉडल प्रदर्शन का मूल्यांकन अभी भी किया जा सकता है।</li>
</ul>
</li>
</ul>
<h3>संदर्भ-स्वतंत्र निकासी (कोई रनटाइम संपीड़न ओवरहेड नहीं)</h3>
<ul>
<li><code>--level head</code> ध्वज के साथ <code>--ratio 0.6</code> (अनुशंसित) का उपयोग करें।
<ul>
<li>हम एक विशिष्ट हेड से जुड़े सभी संदर्भ KV जोड़े निकाल देते हैं जबकि सिस्टम प्रॉम्प्ट और क्वेरी KV जोड़े बनाए रखते हैं।</li>
<li>पूर्व-गणना किए गए हेड स्कोर LLaMA3.1-8B और Qwen2.5-7/14B के लिए <code>./utils/head_score</code> में उपलब्ध हैं।</li>
</ul>
</li>
<li>अन्य मॉडलों के लिए हेड स्कोर गणना करने के लिए:
<pre><code class="language-bash">python -B test.py -m [model_name] -d scbench_qa_eng --save_head_score
</code></pre>
<ul>
<li>परिणाम <code>./utils/head_score</code> में सहेजे जाएंगे।</li>
<li>यदि कोडिंग कार्य लक्षित है, तो हम अतिरिक्त रूप से <code>-d scbench_repoqa</code> के साथ कमांड चलाने की सलाह देते हैं। यह मॉडल को प्राकृतिक और कोडिंग भाषाओं दोनों से अधिकतम हेड स्कोर का उपयोग करने की अनुमति देता है, जो प्रदर्शन में सुधार करता है।</li>
</ul>
</li>
<li>ये स्कोर <a href="https://github.com/mit-han-lab/duo-attention">DuoAttention</a> के अनुकूलित अनुमान इंजन के साथ निर्बाध रूप से एकीकृत किए जा सकते हैं, बस उनके हेड स्कोर डेटा को हमारे स्कोर से बदलकर।</li>
</ul>
<h2>मूल्यांकन</h2>
<ul>
<li>0.1 से 1.0 तक के KV संपीड़न अनुपात के साथ मॉडल प्रतिक्रियाएँ उत्पन्न करने के लिए:
<pre><code class="language-bash">python -B eval.py -m [model_name] -d [data_name] --kv_type retain --num 100
</code></pre>
<ul>
<li>परिणाम <code>./results/[data_name]</code> में सहेजे जाएंगे।</li>
<li>समर्थित डेटासेट <code>data/load.py</code> में सूचीबद्ध हैं।</li>
</ul>
</li>
<li>उत्पन्न परिणामों से मूल्यांकन मेट्रिक्स गणना करने के लिए:
<pre><code class="language-bash">python -B -m results.parse -m [model_name] -d [data_name]
</code></pre>
</li>
</ul>
<h2>नए मॉडलों पर लागू करना</h2>
<p>KVzip को नए मॉडल के लिए एकीकृत करने के लिए, आपको निम्न फाइलों को अपडेट करना होगा:</p>
<ul>
<li><code>attention/attn.py</code><br />
आवश्यकतानुसार ध्यान फॉरवर्ड पास लॉजिक संशोधित करें। कुछ मामलों में, kvcache.py और score.py में भी अपडेट आवश्यक हो सकते हैं।</li>
<li><code>model/monkeypatch.py</code><br />
एकीकरण के लिए मॉडल-विशिष्ट मंकी पैचिंग लागू करें।</li>
<li><code>model/template.py</code><br />
मॉडल के सिस्टम प्रॉम्प्ट और चैट फॉर्मेटिंग टेम्पलेट परिभाषित करें।</li>
</ul>
<h2>संदर्भ</h2>
<pre><code class="language-bibtex">@article{kim2025kvzip,
        title={KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction},
        author={Kim, Jang-Hyun and Kim, Jinuk and Kwon, Sangwoo and Lee, Jae W and Yun, Sangdoo and Song, Hyun Oh},
        journal={arXiv preprint arXiv:2505.23416},
        year={2025}
}
</code></pre>
<h2>लाइसेंस</h2>
<p>MIT लाइसेंस</p>
<hr />
<p>Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx">Open Ai Tx</a> | Last indexed: 2025-06-11</p>
<hr />

        </div>
    </div>
    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Statcounter and other scripts can be added here -->
</body>
</html>