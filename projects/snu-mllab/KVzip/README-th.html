<!DOCTYPE html>
<html lang="th">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>KVzip - snu-mllab/KVzip th</title>
    <meta name="title" content="KVzip - snu-mllab/KVzip th | KVzip: การบีบอัดแคช KV แบบไม่ขึ้นกับคำถามด้วยการสร้างบริบทใหม่ [Paper] [Blog] มีอะไรใหม่? KVzip บีบอัดแคช KV เพื่อรองรับ คำถามในอนาคตที่หลากหลาย [ขึ้นกับบริบท] ...">
    <meta name="description" content="snu-mllab/KVzip - GitHub repository th documentation and information | KVzip: การบีบอัดแคช KV แบบไม่ขึ้นกับคำถามด้วยการสร้างบริบทใหม่ [Paper] [Blog] มีอะไรใหม่? KVzip บีบอัดแคช KV เพื่อรองรับ คำถามในอนาคตที่หลากหลาย [ขึ้นกับบริบท] ...">
    <meta name="keywords" content="snu-mllab, KVzip, GitHub, repository, th documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/snu-mllab/KVzip/README-th.html">
    <meta property="og:title" content="KVzip - snu-mllab/KVzip th | KVzip: การบีบอัดแคช KV แบบไม่ขึ้นกับคำถามด้วยการสร้างบริบทใหม่ [Paper] [Blog] มีอะไรใหม่? KVzip บีบอัดแคช KV เพื่อรองรับ คำถามในอนาคตที่หลากหลาย [ขึ้นกับบริบท] ...">
    <meta property="og:description" content="snu-mllab/KVzip - GitHub repository th documentation and information | KVzip: การบีบอัดแคช KV แบบไม่ขึ้นกับคำถามด้วยการสร้างบริบทใหม่ [Paper] [Blog] มีอะไรใหม่? KVzip บีบอัดแคช KV เพื่อรองรับ คำถามในอนาคตที่หลากหลาย [ขึ้นกับบริบท] ...">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">
    <script src="/js/marked.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        body { display: flex; flex-direction: column; min-height: 100vh; }
        .main-container { margin: 0 auto; width: 100%; max-width: 980px; padding: 0 20px; }
        @media (max-width: 768px) { .main-container { padding: 0 15px; } }
        .markdown-body img { max-width: 100%; height: auto; }
        .header { text-align: center; margin-bottom: 30px; padding: 20px; background-color: #f6f8fa; border-bottom: 1px solid #e1e4e8; position: relative; }
        .header .links { margin-top: 10px; font-size: 16px; }
        .header .links a { color: #0366d6; text-decoration: none; margin-left: 5px; }
        .header .links a:hover { text-decoration: underline; }
        .language-badges { margin-top: 15px; text-align: center; }
        .language-badges a { display: inline-block; margin: 2px; text-decoration: none; }
        .language-badges img { height: 20px; border-radius: 3px; }
        .language-badges a:hover img { opacity: 0.8; }
    </style>
</head>
<body>
    <div style="position: fixed; top: 2px; left: 2px; z-index: 2000; background: rgba(255,255,255,0.95); border-radius: 6px; box-shadow: 0 2px 8px rgba(0,0,0,0.06); padding: 4px 14px; font-size: 15px; color: #222; font-family: 'Segoe UI', Arial, sans-serif; font-weight: 500; letter-spacing: 0.5px;">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" style="color: #0366d6; text-decoration: none; font-weight: 600;">Open AI Tx</a>
    </div>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/snu-mllab/KVzip" id="githubRepoLink" target="_blank">snu-mllab/KVzip</a>
<h1 style="display: none;">KVzip: การบีบอัดแคช KV แบบไม่ขึ้นกับคำถามด้วยการสร้างบริบทใหม่ [Paper] [Blog] มีอะไรใหม่? KVzip บีบอัดแคช KV เพื่อรองรับ คำถามในอนาคตที่หลากหลาย [ขึ้นกับบริบท] ...</h1>
        </div>
        <div class="language-badges" id="languageBadges">
            <!-- You can generate language badges here if needed -->
        </div>
    </div>
    <div class="main-container">
        <div class="markdown-body" id="content">
            <h1>KVzip: การบีบอัดแคช KV แบบไม่ขึ้นกับคำถามด้วยการสร้างบริบทใหม่</h1>
<p>[<a href="https://arxiv.org/abs/2505.23416">Paper</a>] [<a href="https://janghyun1230.github.io/kvzip/">Blog</a>]</p>
<img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/method.png" width="800">
<h2>มีอะไรใหม่?</h2>
<ul>
<li>KVzip บีบอัดแคช KV เพื่อรองรับ <strong>คำถามในอนาคตที่หลากหลาย</strong></li>
<li>[ขึ้นกับบริบท] บรรลุการ <strong>ลดขนาดแคช KV 3–4 เท่า</strong> และ <strong>ลดเวลาในการถอดรหัสลง 2 เท่า</strong> โดยมีการลดประสิทธิภาพเพียงเล็กน้อย</li>
<li>[ไม่ขึ้นกับบริบท] พัฒนาการบีบอัด KV ในระดับหัว <a href="https://github.com/mit-han-lab/duo-attention">DuoAttention</a> โดยใช้เพียง <strong>การผ่านหน้าเดียวไม่กี่ครั้งภายในหนึ่งนาที</strong> สำหรับการปรับแต่งคะแนนความสำคัญในระดับหัว (เร็วขึ้น 100 เท่า)</li>
<li>รัน demo.py:
<img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/demo.png" width="800"></li>
</ul>
<h3>การทดสอบประสิทธิภาพในสภาพแวดล้อมไม่ขึ้นกับคำถาม</h3>
<ul>
<li>งานทดสอบ: <a href="https://huggingface.co/datasets/rajpurkar/squad">SQuAD</a>, <a href="https://github.com/gkamradt/LLMTest_NeedleInAHaystack">NIAH</a>, <a href="https://github.com/microsoft/MInference/tree/main/scbench">SCBench</a>, <a href="https://huggingface.co/datasets/openai/gsm8k/viewer/main/train?row=7294">GSM8K</a></li>
<li>โมเดล: <a href="https://huggingface.co/Qwen/Qwen2.5-7B-Instruct">Qwen2.5-7B-Instruct-1M</a></li>
</ul>
<img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/benchmark.png" width="800">
<h2>การติดตั้ง</h2>
<p>เราใช้ CUDA 12.1 และ Python 3.10</p>
<pre><code class="language-bash">cd KVzip
pip install -r requirements.txt
pip install flash-attn==2.7.4.post1 --no-build-isolation
make i
</code></pre>
<ul>
<li>หากต้องการใช้การควอนไทซ์จาก <a href="https://github.com/mit-han-lab/omniserve">QServe</a> กรุณาปฏิบัติตาม <a href="https://github.com/snu-mllab/KVzip/tree/main/model/quant_model"><code>./model/quant_model</code></a></li>
</ul>
<h3>ชุดข้อมูล</h3>
<ul>
<li>กรุณาดาวน์โหลดชุดข้อมูล SCBench ที่ผ่านการประมวลผลแล้วจาก <a href="https://drive.google.com/file/d/1cqoR6pxxFcjFqvPZkuAmF-fBSAlAbjbN/view?usp=share_link">Google Drive</a></li>
<li>หากคุณดาวน์โหลดไฟล์ที่แตกไฟล์แล้ว ให้ย้ายโฟลเดอร์ scbench ไปยังที่ต้องการ</li>
</ul>
<pre><code class="language-bash">mv scbench.zip kvzip/data/
cd kvzip/data
unzip scbench.zip  
</code></pre>
<h2>เริ่มต้นอย่างรวดเร็ว</h2>
<pre><code class="language-python">from model import ModelKVzip

model = ModelKVzip(&quot;Qwen/Qwen2.5-7B-Instruct-1M&quot;)
context = &quot;This is my basic profile. My name is Kim living in Seoul. My major is computer science.&quot;
queries = [&quot;What is my name?&quot;, &quot;Do I live in Seoul?&quot;]

kv = model.prefill(context, load_score=False)  # เตรียมแคช KV + การให้คะแนนความสำคัญ
kv.prune(ratio=0.3)  # อัตราการบีบอัด, กำจัด KV 70%

for q in queries:
    query_ids = model.apply_template(q)
    output = model.generate(query_ids, kv=kv, update_cache=False)  # การสืบค้นที่มีประสิทธิภาพ
    print(q, output)
</code></pre>
<ul>
<li>โมเดลที่รองรับถูกระบุไว้ใน <a href="https://github.com/snu-mllab/KVzip/blob/main/model/load.py"><code>model/load.py</code></a> รวมถึง <strong>LLaMA3, Qwen2.5/3, Gemma3</strong></li>
<li>ตั้งค่า <code>load_score=True</code> เพื่อขจัดภาระในการบีบอัด ซึ่งจะเปิดใช้การลบ KV แบบไม่ขึ้นกับบริบท โดยแลกกับอัตราบีบอัดที่ <code>ratio=0.6</code></li>
<li>หลังการสร้างผลลัพธ์ คู่ KV ที่สอดคล้องกับคำถามและโทเค็นที่สร้างจะถูกคัดเลือกเพื่อลบออกจากแคชสำหรับการประมวลผลต่อไป ตั้งค่า <code>update_cache=True</code> เพื่อเปิดใช้งานการสืบค้นแบบหลายรอบ โดยเก็บประวัติการโต้ตอบทั้งหมดไว้ตลอดการสืบค้น</li>
</ul>
<h2>การวัดประสิทธิภาพหน่วยความจำและเวลาในการคำนวณ</h2>
<h3>การลบ KV ขึ้นกับบริบท</h3>
<pre><code class="language-bash">python -B test.py -m [model_name] -d [data_name] --kv_type evict --ratio 0.3
</code></pre>
<ul>
<li>โค้ดข้างต้นยังเปรียบเทียบผลลัพธ์ที่สร้างด้วยแคช KV เต็มและแคช KV ที่ถูกตัดแต่ง</li>
<li>สำหรับการทดสอบอย่างรวดเร็ว ใช้ <code>-d squad</code> สำหรับการทดสอบบริบทยาว ใช้ <code>-d scbench_kv</code>
<ul>
<li>ชื่อชุดข้อมูลที่ใช้ได้: <a href="https://github.com/snu-mllab/KVzip/blob/main/data/load.py"><code>data/load.py</code></a></li>
<li>ชื่อโมเดลที่ใช้ได้: <a href="https://github.com/snu-mllab/KVzip/blob/main/model/load.py"><code>model/load.py</code></a> เช่น llama3.1-8b, qwen2.5-7b (หรือ Qwen/Qwen2.5-7B-Instruct-1M)</li>
</ul>
</li>
<li>เราปรับแต่งเคอร์เนล CUDA จาก <a href="https://github.com/FFY0/AdaKV/tree/main">AdaKV</a> โดยรองรับการจัดสรรงบประมาณหัวแบบไม่สม่ำเสมอ
<ul>
<li>ปัจจุบัน โค้ดของเราไม่มีเคอร์เนลที่ปรับแต่งสำหรับ Gemma3 ซึ่งใช้แคช KV แบบสถิต จึงยังไม่มีประสิทธิภาพที่เพิ่มขึ้นจริง แต่สามารถประเมินประสิทธิภาพของโมเดลได้โดยใช้การลดความสนใจด้วยการเลือกตัวอย่าง KV (<code>--kv_type retain</code>)</li>
</ul>
</li>
</ul>
<h3>การลบ KV ไม่ขึ้นกับบริบท (ไม่มีภาระการบีบอัดขณะรัน)</h3>
<ul>
<li>ใช้แฟล็ก <code>--level head</code> พร้อม <code>--ratio 0.6</code> (แนะนำ)
<ul>
<li>เราจะลบคู่ KV ทั้งหมดที่เกี่ยวข้องกับหัวเฉพาะในขณะที่ยังคงเก็บระบบพรอมต์และคู่ KV ของคำถามไว้</li>
<li>คะแนนหัวที่คำนวณล่วงหน้ามีให้สำหรับ LLaMA3.1-8B และ Qwen2.5-7/14B ใน <code>./utils/head_score</code></li>
</ul>
</li>
<li>หากต้องการคำนวณคะแนนหัวสำหรับโมเดลอื่น:
<pre><code class="language-bash">python -B test.py -m [model_name] -d scbench_qa_eng --save_head_score
</code></pre>
<ul>
<li>ผลลัพธ์จะถูกบันทึกใน <code>./utils/head_score</code></li>
<li>หากทำงานกับงานเขียนโค้ด แนะนำให้รันคำสั่งเพิ่มเติมกับ <code>-d scbench_repoqa</code> เพื่อให้โมเดลใช้คะแนนหัวสูงสุดจากทั้งภาษาธรรมชาติและภาษาโค้ด ซึ่งจะช่วยปรับปรุงประสิทธิภาพ</li>
</ul>
</li>
<li>คะแนนเหล่านี้สามารถนำไปใช้ร่วมกับเครื่องมือสืบค้นที่ปรับแต่งของ <a href="https://github.com/mit-han-lab/duo-attention">DuoAttention</a> โดยการแทนที่ข้อมูลคะแนนหัวด้วยของเราได้อย่างราบรื่น</li>
</ul>
<h2>การประเมินผล</h2>
<ul>
<li>เพื่อสร้างคำตอบของโมเดลด้วยอัตราการบีบอัด KV ตั้งแต่ 0.1 ถึง 1.0:
<pre><code class="language-bash">python -B eval.py -m [model_name] -d [data_name] --kv_type retain --num 100
</code></pre>
<ul>
<li>ผลลัพธ์จะถูกบันทึกใน <code>./results/[data_name]</code></li>
<li>ชุดข้อมูลที่รองรับระบุไว้ใน <code>data/load.py</code></li>
</ul>
</li>
<li>เพื่อคำนวณเมตริกการประเมินจากผลลัพธ์ที่สร้าง:
<pre><code class="language-bash">python -B -m results.parse -m [model_name] -d [data_name]
</code></pre>
</li>
</ul>
<h2>การนำไปใช้กับโมเดลใหม่</h2>
<p>เพื่อรวม KVzip สำหรับโมเดลใหม่ คุณจะต้องอัปเดตไฟล์ต่อไปนี้:</p>
<ul>
<li><code>attention/attn.py</code><br />
แก้ไขตรรกะการส่งผ่านความสนใจตามความจำเป็น ในบางกรณีอาจต้องอัปเดต kvcache.py และ score.py ด้วย</li>
<li><code>model/monkeypatch.py</code><br />
ทำการแพตช์แบบมังกี้เฉพาะโมเดลสำหรับการรวมระบบ</li>
<li><code>model/template.py</code><br />
กำหนดพรอมต์ระบบและแม่แบบการจัดรูปแบบแชทของโมเดล</li>
</ul>
<h2>การอ้างอิง</h2>
<pre><code class="language-bibtex">@article{kim2025kvzip,
        title={KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction},
        author={Kim, Jang-Hyun and Kim, Jinuk and Kwon, Sangwoo and Lee, Jae W and Yun, Sangdoo and Song, Hyun Oh},
        journal={arXiv preprint arXiv:2505.23416},
        year={2025}
}
</code></pre>
<h2>ใบอนุญาต</h2>
<p>MIT License</p>
<hr />
<p>Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx">Open Ai Tx</a> | Last indexed: 2025-06-11</p>
<hr />

        </div>
    </div>
    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Default Statcounter code for openaitx
https://openaitx.github.io/ -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>
    <!-- End of Statcounter Code -->
</body>
</html>