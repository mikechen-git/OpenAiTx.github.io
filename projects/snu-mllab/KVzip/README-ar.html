<!DOCTYPE html>
<html lang="ar">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>KVzip - snu-mllab/KVzip ar</title>
    <meta name="title" content="KVzip - snu-mllab/KVzip ar | KVzip: ضغط ذاكرة KV للكاش مستقل عن الاستعلام مع إعادة بناء السياق [ورقة بحثية] [مدونة] ما الجديد؟ يقوم KVzip بضغط ذاكرة KV للكاش لدعم استعلامات مستقبلية متنوعة....">
    <meta name="description" content="snu-mllab/KVzip - GitHub repository ar documentation and information | KVzip: ضغط ذاكرة KV للكاش مستقل عن الاستعلام مع إعادة بناء السياق [ورقة بحثية] [مدونة] ما الجديد؟ يقوم KVzip بضغط ذاكرة KV للكاش لدعم استعلامات مستقبلية متنوعة....">
    <meta name="keywords" content="snu-mllab, KVzip, GitHub, repository, ar documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/snu-mllab/KVzip/README-ar.html">
    <meta property="og:title" content="KVzip - snu-mllab/KVzip ar | KVzip: ضغط ذاكرة KV للكاش مستقل عن الاستعلام مع إعادة بناء السياق [ورقة بحثية] [مدونة] ما الجديد؟ يقوم KVzip بضغط ذاكرة KV للكاش لدعم استعلامات مستقبلية متنوعة....">
    <meta property="og:description" content="snu-mllab/KVzip - GitHub repository ar documentation and information | KVzip: ضغط ذاكرة KV للكاش مستقل عن الاستعلام مع إعادة بناء السياق [ورقة بحثية] [مدونة] ما الجديد؟ يقوم KVzip بضغط ذاكرة KV للكاش لدعم استعلامات مستقبلية متنوعة....">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">
    <script src="/js/marked.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        body { display: flex; flex-direction: column; min-height: 100vh; }
        .main-container { margin: 0 auto; width: 100%; max-width: 980px; padding: 0 20px; }
        @media (max-width: 768px) { .main-container { padding: 0 15px; } }
        .markdown-body img { max-width: 100%; height: auto; }
        .header { text-align: center; margin-bottom: 30px; padding: 20px; background-color: #f6f8fa; border-bottom: 1px solid #e1e4e8; position: relative; }
        .header .links { margin-top: 10px; font-size: 16px; }
        .header .links a { color: #0366d6; text-decoration: none; margin-left: 5px; }
        .header .links a:hover { text-decoration: underline; }
        .language-badges { margin-top: 15px; text-align: center; }
        .language-badges a { display: inline-block; margin: 2px; text-decoration: none; }
        .language-badges img { height: 20px; border-radius: 3px; }
        .language-badges a:hover img { opacity: 0.8; }
    </style>
</head>
<body>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/snu-mllab/KVzip" id="githubRepoLink" target="_blank">snu-mllab/KVzip</a>
<h1 style="display: none;">KVzip: ضغط ذاكرة KV للكاش مستقل عن الاستعلام مع إعادة بناء السياق [ورقة بحثية] [مدونة] ما الجديد؟ يقوم KVzip بضغط ذاكرة KV للكاش لدعم استعلامات مستقبلية متنوعة....</h1>
        </div>
        <div class="language-badges" id="languageBadges">
            <!-- You can generate language badges here if needed -->
        </div>
    </div>
    <div class="main-container">
        <div class="markdown-body" id="content">
            <h1>KVzip: ضغط ذاكرة KV للكاش مستقل عن الاستعلام مع إعادة بناء السياق</h1>
<p>[<a href="https://arxiv.org/abs/2505.23416">ورقة بحثية</a>] [<a href="https://janghyun1230.github.io/kvzip/">مدونة</a>]</p>
<img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/method.png" width="800">
<h2>ما الجديد؟</h2>
<ul>
<li>يقوم KVzip بضغط ذاكرة KV للكاش لدعم <strong>استعلامات مستقبلية متنوعة</strong>.</li>
<li>[معتمد على السياق] تحقيق <strong>تقليل بحجم ذاكرة KV بمقدار 3–4×</strong> و <strong>تقليل زمن فك التشفير بمقدار 2×</strong>، مع أدنى تدهور في الأداء.</li>
<li>[غير معتمد على السياق] تحسين ضغط KV على مستوى الرؤوس بأسلوب <a href="https://github.com/mit-han-lab/duo-attention">DuoAttention</a>، باستخدام فقط <strong>بضع تمريرات أمامية خلال دقيقة واحدة</strong> لتحسين درجات أهمية الرؤوس (أسرع 100 مرة).</li>
<li>تشغيل demo.py:
<img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/demo.png" width="800"></li>
</ul>
<h3>تقييم الأداء في إعداد مستقل عن الاستعلام</h3>
<ul>
<li>المهام: <a href="https://huggingface.co/datasets/rajpurkar/squad">SQuAD</a>، <a href="https://github.com/gkamradt/LLMTest_NeedleInAHaystack">NIAH</a>، <a href="https://github.com/microsoft/MInference/tree/main/scbench">SCBench</a>، <a href="https://huggingface.co/datasets/openai/gsm8k/viewer/main/train?row=7294">GSM8K</a>.</li>
<li>النموذج: <a href="https://huggingface.co/Qwen/Qwen2.5-7B-Instruct">Qwen2.5-7B-Instruct-1M</a></li>
</ul>
<img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/benchmark.png" width="800">
<h2>التثبيت</h2>
<p>استخدمنا CUDA 12.1 و Python 3.10</p>
<pre><code class="language-bash">cd KVzip
pip install -r requirements.txt
pip install flash-attn==2.7.4.post1 --no-build-isolation
make i
</code></pre>
<ul>
<li>لاستخدام التكميم <a href="https://github.com/mit-han-lab/omniserve">QServe</a>، يرجى اتباع <a href="https://github.com/snu-mllab/KVzip/tree/main/model/quant_model"><code>./model/quant_model</code></a>.</li>
</ul>
<h3>مجموعة البيانات</h3>
<ul>
<li>يرجى تحميل مجموعة بيانات SCBench المعالجة مسبقًا من <a href="https://drive.google.com/file/d/1cqoR6pxxFcjFqvPZkuAmF-fBSAlAbjbN/view?usp=share_link">Google Drive</a>.</li>
<li>إذا قمت بتحميل الملفات غير مضغوطة، فقط انقل مجلد scbench.</li>
</ul>
<pre><code class="language-bash">mv scbench.zip kvzip/data/
cd kvzip/data
unzip scbench.zip  
</code></pre>
<h2>بداية سريعة</h2>
<pre><code class="language-python">from model import ModelKVzip

model = ModelKVzip(&quot;Qwen/Qwen2.5-7B-Instruct-1M&quot;)
context = &quot;This is my basic profile. My name is Kim living in Seoul. My major is computer science.&quot;
queries = [&quot;What is my name?&quot;, &quot;Do I live in Seoul?&quot;]

kv = model.prefill(context, load_score=False)  # تعبئة ذاكرة KV + تقييم الأهمية
kv.prune(ratio=0.3)  # نسبة الضغط، التخلص من 70% من KV

for q in queries:
    query_ids = model.apply_template(q)
    output = model.generate(query_ids, kv=kv, update_cache=False)  # استدلال فعال
    print(q, output)
</code></pre>
<ul>
<li>النماذج المدعومة مدرجة في <a href="https://github.com/snu-mllab/KVzip/blob/main/model/load.py"><code>model/load.py</code></a>، بما في ذلك <strong>LLaMA3، Qwen2.5/3، Gemma3</strong>.</li>
<li>ضبط <code>load_score=True</code> لإلغاء تحميل ضغط الذاكرة. هذا يتيح التخلص من KV مستقل عن السياق، مع تنازل في نسبة الضغط عند <code>ratio=0.6</code>.</li>
<li>بعد التوليد، يتم التخلص انتقائيًا من أزواج KV المرتبطة بالاستعلامات والرموز المولدة من الكاش للمعالجة اللاحقة. اضبط <code>update_cache=True</code> لتمكين الاستدلال متعدد الجولات، مع الاحتفاظ بسجلات التفاعل الكاملة طوال الاستدلال.</li>
</ul>
<h2>قياس الذاكرة ووقت الحوسبة</h2>
<h3>التخلص المعتمد على السياق</h3>
<pre><code class="language-bash">python -B test.py -m [model_name] -d [data_name] --kv_type evict --ratio 0.3
</code></pre>
<ul>
<li>الكود أعلاه يقارن أيضًا بين المخرجات الناتجة باستخدام ذاكرة KV كاملة مقابل مضغوطة.</li>
<li>للاختبار السريع، استخدم <code>-d squad</code>. للاختبار في سياق طويل، استخدم <code>-d scbench_kv</code>.
<ul>
<li>أسماء البيانات المتاحة: <a href="https://github.com/snu-mllab/KVzip/blob/main/data/load.py"><code>data/load.py</code></a>.</li>
<li>أسماء النماذج المتاحة: <a href="https://github.com/snu-mllab/KVzip/blob/main/model/load.py"><code>model/load.py</code></a>، مثل llama3.1-8b، qwen2.5-7b (أو Qwen/Qwen2.5-7B-Instruct-1M).</li>
</ul>
</li>
<li>قمنا بتكييف نواة CUDA من <a href="https://github.com/FFY0/AdaKV/tree/main">AdaKV</a>، تدعم تخصيص ميزانية غير متجانسة للرؤوس.
<ul>
<li>حاليًا، لا يحتوي كودنا على نواة محسنة لـ Gemma3 التي تستخدم ذاكرة KV ثابتة، لذلك لا يحقق الكود كفاءة فعلية. ومع ذلك، يمكن تقييم أداء النموذج باستخدام الانتباه المخفض مع أخذ عينات KV (<code>--kv_type retain</code>).</li>
</ul>
</li>
</ul>
<h3>التخلص غير المعتمد على السياق (بدون تحميل ضغط وقت التشغيل)</h3>
<ul>
<li>استخدم العلامة <code>--level head</code> مع <code>--ratio 0.6</code> (موصى به).
<ul>
<li>نقوم بإزالة جميع أزواج KV المرتبطة برأس معين مع الاحتفاظ بأزواج KV الخاصة بنظام المطالبة والاستعلام.</li>
<li>درجات الرؤوس المحسوبة مسبقًا متوفرة لـ LLaMA3.1-8B و Qwen2.5-7/14B في <code>./utils/head_score</code>.</li>
</ul>
</li>
<li>لحساب درجات الرؤوس لنماذج أخرى:
<pre><code class="language-bash">python -B test.py -m [model_name] -d scbench_qa_eng --save_head_score
</code></pre>
<ul>
<li>سيتم حفظ النتائج في <code>./utils/head_score</code>.</li>
<li>إذا كنت تستهدف مهمة ترميز، نوصي أيضًا بتشغيل الأمر مع <code>-d scbench_repoqa</code>. هذا يسمح للنموذج باستخدام درجات الرأس القصوى من اللغتين الطبيعية والبرمجية، مما يحسن الأداء.</li>
</ul>
</li>
<li>يمكن دمج هذه الدرجات بسهولة مع محرك الاستدلال المحسن لـ <a href="https://github.com/mit-han-lab/duo-attention">DuoAttention</a> عن طريق استبدال بيانات درجات الرأس الخاصة بهم ببياناتنا.</li>
</ul>
<h2>التقييم</h2>
<ul>
<li>لتوليد ردود النموذج مع نسب ضغط KV تتراوح من 0.1 إلى 1.0:
<pre><code class="language-bash">python -B eval.py -m [model_name] -d [data_name] --kv_type retain --num 100
</code></pre>
<ul>
<li>سيتم حفظ النتائج في <code>./results/[data_name]</code>.</li>
<li>مجموعات البيانات المدعومة مدرجة في <code>data/load.py</code>.</li>
</ul>
</li>
<li>لحساب مقاييس التقييم من النتائج المولدة:
<pre><code class="language-bash">python -B -m results.parse -m [model_name] -d [data_name]
</code></pre>
</li>
</ul>
<h2>التطبيق على نماذج جديدة</h2>
<p>لدمج KVzip مع نموذج جديد، ستحتاج إلى تحديث الملفات التالية:</p>
<ul>
<li><code>attention/attn.py</code><br />
تعديل منطق تمرير الانتباه الأمامي حسب الحاجة. في بعض الحالات، قد يكون مطلوبًا تحديث <code>kvcache.py</code> و <code>score.py</code>.</li>
<li><code>model/monkeypatch.py</code><br />
تنفيذ تصحيح القرد الخاص بالنموذج للدمج.</li>
<li><code>model/template.py</code><br />
تعريف مطالبة نظام النموذج وقوالب تنسيق المحادثة.</li>
</ul>
<h2>الاقتباس</h2>
<pre><code class="language-bibtex">@article{kim2025kvzip,
        title={KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction},
        author={Kim, Jang-Hyun and Kim, Jinuk and Kwon, Sangwoo and Lee, Jae W and Yun, Sangdoo and Song, Hyun Oh},
        journal={arXiv preprint arXiv:2505.23416},
        year={2025}
}
</code></pre>
<h2>الترخيص</h2>
<p>رخصة MIT</p>
<hr />
<p>Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx">Open Ai Tx</a> | Last indexed: 2025-06-11</p>
<hr />

        </div>
    </div>
    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Default Statcounter code for openaitx
https://openaitx.github.io/ -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>
    <!-- End of Statcounter Code -->
</body>
</html>