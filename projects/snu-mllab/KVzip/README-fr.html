<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>KVzip - snu-mllab/KVzip</title>
    <meta name="title" content="KVzip - snu-mllab/KVzip">
    <meta name="description" content="snu-mllab/KVzip - GitHub repository fr documentation and informationKVzip : Compression du cache KV indépendante des requêtes avec reconstruction de contexte [Article] [Blog] Quoi de neuf ? KVzip compresse le cache KV pour suppo...">
    <meta name="keywords" content="snu-mllab, KVzip, GitHub, repository, fr documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/snu-mllab/KVzip/README-fr.html">
    <meta property="og:title" content="KVzip - snu-mllab/KVzip">
    <meta property="og:description" content="snu-mllab/KVzip - GitHub repository fr documentation and information">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">
    <script src="/js/marked.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        body { display: flex; flex-direction: column; min-height: 100vh; }
        .main-container { margin: 0 auto; width: 100%; max-width: 980px; padding: 0 20px; }
        @media (max-width: 768px) { .main-container { padding: 0 15px; } }
        .markdown-body img { max-width: 100%; height: auto; }
        .header { text-align: center; margin-bottom: 30px; padding: 20px; background-color: #f6f8fa; border-bottom: 1px solid #e1e4e8; position: relative; }
        .header .links { margin-top: 10px; font-size: 16px; }
        .header .links a { color: #0366d6; text-decoration: none; margin-left: 5px; }
        .header .links a:hover { text-decoration: underline; }
        .language-badges { margin-top: 15px; text-align: center; }
        .language-badges a { display: inline-block; margin: 2px; text-decoration: none; }
        .language-badges img { height: 20px; border-radius: 3px; }
        .language-badges a:hover img { opacity: 0.8; }
    </style>
</head>
<body>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/snu-mllab/KVzip" id="githubRepoLink" target="_blank">snu-mllab/KVzip</a>
<h1 style="display: none;">KVzip : Compression du cache KV indépendante des requêtes avec reconstruction de contexte [Article] [Blog] Quoi de neuf ? KVzip compresse le cache KV pour suppo...</h1>
        </div>
        <div class="language-badges" id="languageBadges">
            <!-- You can generate language badges here if needed -->
        </div>
    </div>
    <div class="main-container">
        <div class="markdown-body" id="content">
            <h1>KVzip : Compression du cache KV indépendante des requêtes avec reconstruction de contexte</h1>
<p>[<a href="https://arxiv.org/abs/2505.23416">Article</a>] [<a href="https://janghyun1230.github.io/kvzip/">Blog</a>]</p>
<img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/method.png" width="800">
<h2>Quoi de neuf ?</h2>
<ul>
<li>KVzip compresse le cache KV pour supporter <strong>diverses requêtes futures</strong>.</li>
<li>[Dépendant du contexte] Obtention d'une <strong>réduction de 3 à 4× de la taille du cache KV</strong> et d'une <strong>diminution de 2× de la latence de décodage</strong>, avec une dégradation de performance minimale.</li>
<li>[Indépendant du contexte] Amélioration de la compression KV au niveau des têtes de type <a href="https://github.com/mit-han-lab/duo-attention">DuoAttention</a>, en utilisant seulement <strong>quelques passages avant en moins d'une minute</strong> pour l'optimisation des scores d'importance au niveau des têtes (100x plus rapide).</li>
<li>Exécutez demo.py :
<img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/demo.png" width="800"></li>
</ul>
<h3>Benchmark dans un cadre indépendant des requêtes</h3>
<ul>
<li>Tâches : <a href="https://huggingface.co/datasets/rajpurkar/squad">SQuAD</a>, <a href="https://github.com/gkamradt/LLMTest_NeedleInAHaystack">NIAH</a>, <a href="https://github.com/microsoft/MInference/tree/main/scbench">SCBench</a>, <a href="https://huggingface.co/datasets/openai/gsm8k/viewer/main/train?row=7294">GSM8K</a>.</li>
<li>Modèle : <a href="https://huggingface.co/Qwen/Qwen2.5-7B-Instruct">Qwen2.5-7B-Instruct-1M</a></li>
</ul>
<img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/benchmark.png" width="800">
<h2>Installation</h2>
<p>Nous avons utilisé CUDA 12.1 et Python 3.10</p>
<pre><code class="language-bash">cd KVzip
pip install -r requirements.txt
pip install flash-attn==2.7.4.post1 --no-build-isolation
make i
</code></pre>
<ul>
<li>Pour utiliser la quantification <a href="https://github.com/mit-han-lab/omniserve">QServe</a>, veuillez suivre <a href="https://github.com/snu-mllab/KVzip/tree/main/model/quant_model"><code>./model/quant_model</code></a>.</li>
</ul>
<h3>Jeu de données</h3>
<ul>
<li>Veuillez télécharger le jeu de données SCBench pré-traité depuis <a href="https://drive.google.com/file/d/1cqoR6pxxFcjFqvPZkuAmF-fBSAlAbjbN/view?usp=share_link">Google Drive</a>.</li>
<li>Si vous avez téléchargé les fichiers décompressés, déplacez simplement le dossier scbench.</li>
</ul>
<pre><code class="language-bash">mv scbench.zip kvzip/data/
cd kvzip/data
unzip scbench.zip  
</code></pre>
<h2>Démarrage rapide</h2>
<pre><code class="language-python">from model import ModelKVzip

model = ModelKVzip(&quot;Qwen/Qwen2.5-7B-Instruct-1M&quot;)
context = &quot;This is my basic profile. My name is Kim living in Seoul. My major is computer science.&quot;
queries = [&quot;What is my name?&quot;, &quot;Do I live in Seoul?&quot;]

kv = model.prefill(context, load_score=False)  # préremplissage du cache KV + score d'importance
kv.prune(ratio=0.3)  # taux de compression, éviction de 70% du KV

for q in queries:
    query_ids = model.apply_template(q)
    output = model.generate(query_ids, kv=kv, update_cache=False)  # inférence efficace
    print(q, output)
</code></pre>
<ul>
<li>Les modèles supportés sont listés dans <a href="https://github.com/snu-mllab/KVzip/blob/main/model/load.py"><code>model/load.py</code></a>, incluant <strong>LLaMA3, Qwen2.5/3, Gemma3</strong>.</li>
<li>Définissez <code>load_score=True</code> pour éliminer le surcoût de compression. Cela permet une éviction KV indépendante du contexte, avec un compromis sur le taux de compression à <code>ratio=0.6</code>.</li>
<li>Après la génération, les paires KV correspondant aux requêtes et aux tokens générés sont évincées sélectivement du cache pour un traitement ultérieur. Définissez <code>update_cache=True</code> pour permettre une inférence multi-tours, en conservant l'historique complet des interactions pendant toute l'inférence.</li>
</ul>
<h2>Profilage de la mémoire et du temps de calcul</h2>
<h3>Éviction dépendante du contexte</h3>
<pre><code class="language-bash">python -B test.py -m [model_name] -d [data_name] --kv_type evict --ratio 0.3
</code></pre>
<ul>
<li>Le code ci-dessus compare également les sorties générées avec le cache KV complet versus le cache KV élagué.</li>
<li>Pour un test rapide, utilisez <code>-d squad</code>. Pour un test en contexte long, utilisez <code>-d scbench_kv</code>.
<ul>
<li>Noms des données disponibles : <a href="https://github.com/snu-mllab/KVzip/blob/main/data/load.py"><code>data/load.py</code></a>.</li>
<li>Noms des modèles disponibles : <a href="https://github.com/snu-mllab/KVzip/blob/main/model/load.py"><code>model/load.py</code></a>, ex. llama3.1-8b, qwen2.5-7b (ou Qwen/Qwen2.5-7B-Instruct-1M).</li>
</ul>
</li>
<li>Nous adaptons le kernel CUDA de <a href="https://github.com/FFY0/AdaKV/tree/main">AdaKV</a>, supportant une allocation non uniforme du budget par tête.
<ul>
<li>Actuellement, notre code ne comprend pas de kernel optimisé pour Gemma3 qui utilise un cache KV statique, donc le code n'apporte pas de gains d'efficacité réels. Cependant, la performance du modèle peut toujours être évaluée en utilisant une attention réduite avec sous-échantillonnage KV (<code>--kv_type retain</code>).</li>
</ul>
</li>
</ul>
<h3>Éviction indépendante du contexte (sans surcoût de compression à l'exécution)</h3>
<ul>
<li>Utilisez le flag <code>--level head</code> avec <code>--ratio 0.6</code> (recommandé).
<ul>
<li>Nous supprimons toutes les paires KV de contexte associées à une tête spécifique tout en conservant les paires KV du prompt système et des requêtes.</li>
<li>Les scores de tête pré-calculés sont disponibles pour LLaMA3.1-8B et Qwen2.5-7/14B dans <code>./utils/head_score</code>.</li>
</ul>
</li>
<li>Pour calculer les scores de tête pour d'autres modèles :
<pre><code class="language-bash">python -B test.py -m [model_name] -d scbench_qa_eng --save_head_score
</code></pre>
<ul>
<li>Les résultats seront sauvegardés dans <code>./utils/head_score</code>.</li>
<li>Si vous ciblez une tâche de codage, nous recommandons également d'exécuter la commande avec <code>-d scbench_repoqa</code>. Cela permet au modèle d'utiliser les scores de tête maximum issus des langues naturelles et du codage, ce qui améliore les performances.</li>
</ul>
</li>
<li>Ces scores peuvent être intégrés sans difficulté avec le moteur d'inférence optimisé de <a href="https://github.com/mit-han-lab/duo-attention">DuoAttention</a> en remplaçant leurs données de score de tête par les nôtres.</li>
</ul>
<h2>Évaluation</h2>
<ul>
<li>Pour générer des réponses de modèle avec des taux de compression KV allant de 0.1 à 1.0 :
<pre><code class="language-bash">python -B eval.py -m [model_name] -d [data_name] --kv_type retain --num 100
</code></pre>
<ul>
<li>Les résultats seront sauvegardés dans <code>./results/[data_name]</code>.</li>
<li>Les jeux de données supportés sont listés dans <code>data/load.py</code>.</li>
</ul>
</li>
<li>Pour calculer les métriques d'évaluation à partir des résultats générés :
<pre><code class="language-bash">python -B -m results.parse -m [model_name] -d [data_name]
</code></pre>
</li>
</ul>
<h2>Application à de nouveaux modèles</h2>
<p>Pour intégrer KVzip à un nouveau modèle, vous devrez mettre à jour les fichiers suivants :</p>
<ul>
<li><code>attention/attn.py</code><br />
Modifiez la logique du passage avant de l'attention selon les besoins. Dans certains cas, des mises à jour de kvcache.py et score.py peuvent aussi être nécessaires.</li>
<li><code>model/monkeypatch.py</code><br />
Implémentez le monkey patch spécifique au modèle pour l'intégration.</li>
<li><code>model/template.py</code><br />
Définissez le prompt système et les templates de formatage de chat du modèle.</li>
</ul>
<h2>Citation</h2>
<pre><code class="language-bibtex">@article{kim2025kvzip,
        title={KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction},
        author={Kim, Jang-Hyun and Kim, Jinuk and Kwon, Sangwoo and Lee, Jae W and Yun, Sangdoo and Song, Hyun Oh},
        journal={arXiv preprint arXiv:2505.23416},
        year={2025}
}
</code></pre>
<h2>Licence</h2>
<p>Licence MIT</p>
<hr />
<p>Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx">Open Ai Tx</a> | Last indexed: 2025-06-11</p>
<hr />

        </div>
    </div>
    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Default Statcounter code for openaitx
https://openaitx.github.io/ -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>
    <!-- End of Statcounter Code -->
</body>
</html>