<!DOCTYPE html>
<html lang="fa">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>KVzip - snu-mllab/KVzip fa</title>
    <meta name="title" content="KVzip - snu-mllab/KVzip fa | KVzip: فشرده‌سازی کش KV مستقل از پرس‌وجو با بازسازی زمینه [مقاله] [بلاگ] چه چیز جدیدی ارائه شده است؟ KVzip کش KV را فشرده می‌کند تا از پرس‌وجوهای متنوع آینده پش...">
    <meta name="description" content="snu-mllab/KVzip - GitHub repository fa documentation and information | KVzip: فشرده‌سازی کش KV مستقل از پرس‌وجو با بازسازی زمینه [مقاله] [بلاگ] چه چیز جدیدی ارائه شده است؟ KVzip کش KV را فشرده می‌کند تا از پرس‌وجوهای متنوع آینده پش...">
    <meta name="keywords" content="snu-mllab, KVzip, GitHub, repository, fa documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/snu-mllab/KVzip/README-fa.html">
    <meta property="og:title" content="KVzip - snu-mllab/KVzip fa | KVzip: فشرده‌سازی کش KV مستقل از پرس‌وجو با بازسازی زمینه [مقاله] [بلاگ] چه چیز جدیدی ارائه شده است؟ KVzip کش KV را فشرده می‌کند تا از پرس‌وجوهای متنوع آینده پش...">
    <meta property="og:description" content="snu-mllab/KVzip - GitHub repository fa documentation and information | KVzip: فشرده‌سازی کش KV مستقل از پرس‌وجو با بازسازی زمینه [مقاله] [بلاگ] چه چیز جدیدی ارائه شده است؟ KVzip کش KV را فشرده می‌کند تا از پرس‌وجوهای متنوع آینده پش...">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">
    <script src="/js/marked.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        body { display: flex; flex-direction: column; min-height: 100vh; }
        .main-container { margin: 0 auto; width: 100%; max-width: 980px; padding: 0 20px; }
        @media (max-width: 768px) { .main-container { padding: 0 15px; } }
        .markdown-body img { max-width: 100%; height: auto; }
        .header { text-align: center; margin-bottom: 30px; padding: 20px; background-color: #f6f8fa; border-bottom: 1px solid #e1e4e8; position: relative; }
        .header .links { margin-top: 10px; font-size: 16px; }
        .header .links a { color: #0366d6; text-decoration: none; margin-left: 5px; }
        .header .links a:hover { text-decoration: underline; }
        .language-badges { margin-top: 15px; text-align: center; }
        .language-badges a { display: inline-block; margin: 2px; text-decoration: none; }
        .language-badges img { height: 20px; border-radius: 3px; }
        .language-badges a:hover img { opacity: 0.8; }
    </style>
</head>
<body>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/snu-mllab/KVzip" id="githubRepoLink" target="_blank">snu-mllab/KVzip</a>
<h1 style="display: none;">KVzip: فشرده‌سازی کش KV مستقل از پرس‌وجو با بازسازی زمینه [مقاله] [بلاگ] چه چیز جدیدی ارائه شده است؟ KVzip کش KV را فشرده می‌کند تا از پرس‌وجوهای متنوع آینده پش...</h1>
        </div>
        <div class="language-badges" id="languageBadges">
            <!-- You can generate language badges here if needed -->
        </div>
    </div>
    <div class="main-container">
        <div class="markdown-body" id="content">
            <h1>KVzip: فشرده‌سازی کش KV مستقل از پرس‌وجو با بازسازی زمینه</h1>
<p>[<a href="https://arxiv.org/abs/2505.23416">مقاله</a>] [<a href="https://janghyun1230.github.io/kvzip/">بلاگ</a>]</p>
<img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/method.png" width="800">
<h2>چه چیز جدیدی ارائه شده است؟</h2>
<ul>
<li>KVzip کش KV را فشرده می‌کند تا از <strong>پرس‌وجوهای متنوع آینده</strong> پشتیبانی کند.</li>
<li>[وابسته به زمینه] دستیابی به <strong>کاهش ۳ تا ۴ برابری در اندازه کش KV</strong> و <strong>کاهش دو برابری تاخیر رمزگشایی</strong> با حداقل افت عملکرد.</li>
<li>[غیر وابسته به زمینه] بهبود فشرده‌سازی KV در سطح سر (head) به سبک <a href="https://github.com/mit-han-lab/duo-attention">DuoAttention</a>، تنها با <strong>چند پاس رو به جلو در کمتر از یک دقیقه</strong> برای بهینه‌سازی امتیاز اهمیت سر (۱۰۰ برابر سریع‌تر).</li>
<li>اجرای demo.py:
<img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/demo.png" width="800"></li>
</ul>
<h3>بنچمارک در حالت مستقل از پرس‌وجو</h3>
<ul>
<li>تسک‌ها: <a href="https://huggingface.co/datasets/rajpurkar/squad">SQuAD</a>، <a href="https://github.com/gkamradt/LLMTest_NeedleInAHaystack">NIAH</a>، <a href="https://github.com/microsoft/MInference/tree/main/scbench">SCBench</a>، <a href="https://huggingface.co/datasets/openai/gsm8k/viewer/main/train?row=7294">GSM8K</a>.</li>
<li>مدل: <a href="https://huggingface.co/Qwen/Qwen2.5-7B-Instruct">Qwen2.5-7B-Instruct-1M</a></li>
</ul>
<img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/benchmark.png" width="800">
<h2>نصب</h2>
<p>ما از CUDA 12.1 و Python 3.10 استفاده کردیم</p>
<pre><code class="language-bash">cd KVzip
pip install -r requirements.txt
pip install flash-attn==2.7.4.post1 --no-build-isolation
make i
</code></pre>
<ul>
<li>برای استفاده از کمیت‌بندی <a href="https://github.com/mit-han-lab/omniserve">QServe</a> لطفاً به <a href="https://github.com/snu-mllab/KVzip/tree/main/model/quant_model"><code>./model/quant_model</code></a> مراجعه کنید.</li>
</ul>
<h3>مجموعه داده</h3>
<ul>
<li>لطفاً مجموعه داده پیش‌پردازش‌شده SCBench را از <a href="https://drive.google.com/file/d/1cqoR6pxxFcjFqvPZkuAmF-fBSAlAbjbN/view?usp=share_link">Google Drive</a> دانلود کنید.</li>
<li>اگر فایل‌ها را از حالت فشرده خارج کردید، کافی است پوشه scbench را جابجا کنید.</li>
</ul>
<pre><code class="language-bash">mv scbench.zip kvzip/data/
cd kvzip/data
unzip scbench.zip  
</code></pre>
<h2>شروع سریع</h2>
<pre><code class="language-python">from model import ModelKVzip

model = ModelKVzip(&quot;Qwen/Qwen2.5-7B-Instruct-1M&quot;)
context = &quot;This is my basic profile. My name is Kim living in Seoul. My major is computer science.&quot;
queries = [&quot;What is my name?&quot;, &quot;Do I live in Seoul?&quot;]

kv = model.prefill(context, load_score=False)  # پر کردن کش KV + امتیازدهی اهمیت
kv.prune(ratio=0.3)  # نسبت فشرده‌سازی، حذف ۷۰٪ KV

for q in queries:
    query_ids = model.apply_template(q)
    output = model.generate(query_ids, kv=kv, update_cache=False)  # استنتاج کارآمد
    print(q, output)
</code></pre>
<ul>
<li>مدل‌های پشتیبانی شده در <a href="https://github.com/snu-mllab/KVzip/blob/main/model/load.py"><code>model/load.py</code></a> فهرست شده‌اند، شامل <strong>LLaMA3، Qwen2.5/3، Gemma3</strong>.</li>
<li>برای حذف سربار فشرده‌سازی، <code>load_score=True</code> تنظیم کنید. این امکان حذف KV مستقل از زمینه را فراهم می‌کند، با تبادل نسبت فشرده‌سازی <code>ratio=0.6</code>.</li>
<li>پس از تولید، جفت‌های KV مربوط به پرس‌وجوها و توکن‌های تولید شده به صورت انتخابی از کش حذف می‌شوند. برای فعال کردن استنتاج چندمرحله‌ای، <code>update_cache=True</code> را تنظیم کنید تا کل تاریخچه تعامل حفظ شود.</li>
</ul>
<h2>پروفایل‌گیری حافظه و زمان محاسبات</h2>
<h3>حذف وابسته به زمینه</h3>
<pre><code class="language-bash">python -B test.py -m [model_name] -d [data_name] --kv_type evict --ratio 0.3
</code></pre>
<ul>
<li>کد بالا همچنین خروجی‌های تولید شده با کش KV کامل و فشرده شده را مقایسه می‌کند.</li>
<li>برای تست سریع، از <code>-d squad</code> استفاده کنید. برای تست زمینه بلند، از <code>-d scbench_kv</code> استفاده کنید.
<ul>
<li>نام‌های داده موجود: <a href="https://github.com/snu-mllab/KVzip/blob/main/data/load.py"><code>data/load.py</code></a>.</li>
<li>نام‌های مدل موجود: <a href="https://github.com/snu-mllab/KVzip/blob/main/model/load.py"><code>model/load.py</code></a>، مانند llama3.1-8b، qwen2.5-7b (یا Qwen/Qwen2.5-7B-Instruct-1M).</li>
</ul>
</li>
<li>ما کرنل CUDA را از <a href="https://github.com/FFY0/AdaKV/tree/main">AdaKV</a> اقتباس کرده‌ایم، که تخصیص بودجه سر غیر یکنواخت را پشتیبانی می‌کند.
<ul>
<li>در حال حاضر، کد ما کرنل بهینه شده برای Gemma3 که از کش KV استاتیک استفاده می‌کند ندارد، بنابراین کد بهبود واقعی کارایی ارائه نمی‌دهد. با این حال، عملکرد مدل می‌تواند با کاهش توجه و نمونه‌برداری KV (<code>--kv_type retain</code>) ارزیابی شود.</li>
</ul>
</li>
</ul>
<h3>حذف مستقل از زمینه (بدون سربار فشرده‌سازی در زمان اجرا)</h3>
<ul>
<li>از پرچم <code>--level head</code> با <code>--ratio 0.6</code> (توصیه شده) استفاده کنید.
<ul>
<li>تمام جفت‌های KV مربوط به یک سر خاص حذف می‌شوند در حالی که جفت‌های KV مربوط به سیستم پرامپت و پرس‌وجو حفظ می‌شوند.</li>
<li>امتیازهای سر پیش‌محاسبه شده برای LLaMA3.1-8B و Qwen2.5-7/14B در <code>./utils/head_score</code> موجود است.</li>
</ul>
</li>
<li>برای محاسبه امتیازهای سر برای مدل‌های دیگر:
<pre><code class="language-bash">python -B test.py -m [model_name] -d scbench_qa_eng --save_head_score
</code></pre>
<ul>
<li>نتایج در <code>./utils/head_score</code> ذخیره خواهد شد.</li>
<li>اگر هدف تسک برنامه‌نویسی است، توصیه می‌کنیم فرمان را با <code>-d scbench_repoqa</code> نیز اجرا کنید. این امکان استفاده مدل از بیشینه امتیازهای سر از هر دو زبان طبیعی و برنامه‌نویسی را فراهم می‌کند که عملکرد را بهبود می‌بخشد.</li>
</ul>
</li>
<li>این امتیازها می‌توانند به صورت یکپارچه با موتور بهینه استنتاج <a href="https://github.com/mit-han-lab/duo-attention">DuoAttention</a> با جایگزینی داده‌های امتیاز سر آنها با امتیازهای ما ادغام شوند.</li>
</ul>
<h2>ارزیابی</h2>
<ul>
<li>برای تولید پاسخ‌های مدل با نسبت‌های فشرده‌سازی KV از ۰.۱ تا ۱.۰:
<pre><code class="language-bash">python -B eval.py -m [model_name] -d [data_name] --kv_type retain --num 100
</code></pre>
<ul>
<li>نتایج در <code>./results/[data_name]</code> ذخیره می‌شود.</li>
<li>مجموعه داده‌های پشتیبانی شده در <code>data/load.py</code> فهرست شده‌اند.</li>
</ul>
</li>
<li>برای محاسبه معیارهای ارزیابی از نتایج تولید شده:
<pre><code class="language-bash">python -B -m results.parse -m [model_name] -d [data_name]
</code></pre>
</li>
</ul>
<h2>اعمال بر روی مدل‌های جدید</h2>
<p>برای ادغام KVzip با یک مدل جدید، باید فایل‌های زیر را به‌روزرسانی کنید:</p>
<ul>
<li><code>attention/attn.py</code><br />
منطق پاس رو به جلو توجه را در صورت نیاز اصلاح کنید. در برخی موارد، به‌روزرسانی‌های kvcache.py و score.py نیز لازم است.</li>
<li><code>model/monkeypatch.py</code><br />
پیاده‌سازی پچ‌های اختصاصی مدل برای ادغام.</li>
<li><code>model/template.py</code><br />
تعریف پرامپت سیستم مدل و قالب‌بندی چت.</li>
</ul>
<h2>ارجاع</h2>
<pre><code class="language-bibtex">@article{kim2025kvzip,
        title={KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction},
        author={Kim, Jang-Hyun and Kim, Jinuk and Kwon, Sangwoo and Lee, Jae W and Yun, Sangdoo and Song, Hyun Oh},
        journal={arXiv preprint arXiv:2505.23416},
        year={2025}
}
</code></pre>
<h2>مجوز</h2>
<p>مجوز MIT</p>
<hr />
<p>Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx">Open Ai Tx</a> | Last indexed: 2025-06-11</p>
<hr />

        </div>
    </div>
    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Default Statcounter code for openaitx
https://openaitx.github.io/ -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>
    <!-- End of Statcounter Code -->
</body>
</html>