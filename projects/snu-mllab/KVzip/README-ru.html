<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>KVzip - snu-mllab/KVzip</title>
    <meta name="title" content="KVzip - snu-mllab/KVzip">
    <meta name="description" content="snu-mllab/KVzip - GitHub repository ru documentation and informationKVzip: Сжатие KV-кэша без учета запросов с восстановлением контекста [Статья] [Блог] Что нового? KVzip сжимает KV-кэш для поддержки разнообразных будущих запрос...">
    <meta name="keywords" content="snu-mllab, KVzip, GitHub, repository, ru documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/snu-mllab/KVzip/README-ru.html">
    <meta property="og:title" content="KVzip - snu-mllab/KVzip">
    <meta property="og:description" content="snu-mllab/KVzip - GitHub repository ru documentation and information">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">
    <script src="/js/marked.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        body { display: flex; flex-direction: column; min-height: 100vh; }
        .main-container { margin: 0 auto; width: 100%; max-width: 980px; padding: 0 20px; }
        @media (max-width: 768px) { .main-container { padding: 0 15px; } }
        .markdown-body img { max-width: 100%; height: auto; }
        .header { text-align: center; margin-bottom: 30px; padding: 20px; background-color: #f6f8fa; border-bottom: 1px solid #e1e4e8; position: relative; }
        .header .links { margin-top: 10px; font-size: 16px; }
        .header .links a { color: #0366d6; text-decoration: none; margin-left: 5px; }
        .header .links a:hover { text-decoration: underline; }
        .language-badges { margin-top: 15px; text-align: center; }
        .language-badges a { display: inline-block; margin: 2px; text-decoration: none; }
        .language-badges img { height: 20px; border-radius: 3px; }
        .language-badges a:hover img { opacity: 0.8; }
    </style>
</head>
<body>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/snu-mllab/KVzip" id="githubRepoLink" target="_blank">snu-mllab/KVzip</a>
<h1 style="display: none;">KVzip: Сжатие KV-кэша без учета запросов с восстановлением контекста [Статья] [Блог] Что нового? KVzip сжимает KV-кэш для поддержки разнообразных будущих запрос...</h1>
        </div>
        <div class="language-badges" id="languageBadges">
            <!-- You can generate language badges here if needed -->
        </div>
    </div>
    <div class="main-container">
        <div class="markdown-body" id="content">
            <h1>KVzip: Сжатие KV-кэша без учета запросов с восстановлением контекста</h1>
<p>[<a href="https://arxiv.org/abs/2505.23416">Статья</a>] [<a href="https://janghyun1230.github.io/kvzip/">Блог</a>]</p>
<img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/method.png" width="800">
<h2>Что нового?</h2>
<ul>
<li>KVzip сжимает KV-кэш для поддержки <strong>разнообразных будущих запросов</strong>.</li>
<li>[Зависимое от контекста] Достигается <strong>сокращение размера KV-кэша в 3–4 раза</strong> и <strong>уменьшение задержки декодирования в 2 раза</strong> при минимальном ухудшении производительности.</li>
<li>[Независимое от контекста] Улучшение сжатия KV на уровне голов, как в <a href="https://github.com/mit-han-lab/duo-attention">DuoAttention</a>, с использованием всего <strong>нескольких проходов вперед в течение одной минуты</strong> для оптимизации важности голов (в 100 раз быстрее).</li>
<li>Запуск demo.py:
<img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/demo.png" width="800"></li>
</ul>
<h3>Тестирование в условиях отсутствия зависимости от запросов</h3>
<ul>
<li>Задачи: <a href="https://huggingface.co/datasets/rajpurkar/squad">SQuAD</a>, <a href="https://github.com/gkamradt/LLMTest_NeedleInAHaystack">NIAH</a>, <a href="https://github.com/microsoft/MInference/tree/main/scbench">SCBench</a>, <a href="https://huggingface.co/datasets/openai/gsm8k/viewer/main/train?row=7294">GSM8K</a>.</li>
<li>Модель: <a href="https://huggingface.co/Qwen/Qwen2.5-7B-Instruct">Qwen2.5-7B-Instruct-1M</a></li>
</ul>
<img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/benchmark.png" width="800">
<h2>Установка</h2>
<p>Использовались CUDA 12.1 и Python 3.10</p>
<pre><code class="language-bash">cd KVzip
pip install -r requirements.txt
pip install flash-attn==2.7.4.post1 --no-build-isolation
make i
</code></pre>
<ul>
<li>Для использования квантизации <a href="https://github.com/mit-han-lab/omniserve">QServe</a>, пожалуйста, следуйте инструкции в <a href="https://github.com/snu-mllab/KVzip/tree/main/model/quant_model"><code>./model/quant_model</code></a>.</li>
</ul>
<h3>Датасет</h3>
<ul>
<li>Пожалуйста, скачайте предобработанный датасет SCBench с <a href="https://drive.google.com/file/d/1cqoR6pxxFcjFqvPZkuAmF-fBSAlAbjbN/view?usp=share_link">Google Drive</a>.</li>
<li>Если скачаны распакованные файлы, просто переместите папку scbench.</li>
</ul>
<pre><code class="language-bash">mv scbench.zip kvzip/data/
cd kvzip/data
unzip scbench.zip  
</code></pre>
<h2>Быстрый старт</h2>
<pre><code class="language-python">from model import ModelKVzip

model = ModelKVzip(&quot;Qwen/Qwen2.5-7B-Instruct-1M&quot;)
context = &quot;This is my basic profile. My name is Kim living in Seoul. My major is computer science.&quot;
queries = [&quot;What is my name?&quot;, &quot;Do I live in Seoul?&quot;]

kv = model.prefill(context, load_score=False)  # заполнение KV-кэша + оценка важности
kv.prune(ratio=0.3)  # коэффициент сжатия, удаление 70% KV

for q in queries:
    query_ids = model.apply_template(q)
    output = model.generate(query_ids, kv=kv, update_cache=False)  # эффективный вывод
    print(q, output)
</code></pre>
<ul>
<li>Поддерживаемые модели перечислены в <a href="https://github.com/snu-mllab/KVzip/blob/main/model/load.py"><code>model/load.py</code></a>, включая <strong>LLaMA3, Qwen2.5/3, Gemma3</strong>.</li>
<li>Установите <code>load_score=True</code> для устранения накладных расходов сжатия. Это включает независимое от контекста удаление KV с компромиссом в коэффициенте сжатия <code>ratio=0.6</code>.</li>
<li>После генерации пары KV, соответствующие запросам и сгенерированным токенам, выборочно удаляются из кэша для дальнейшей обработки. Установите <code>update_cache=True</code> для включения многошагового вывода с сохранением полной истории взаимодействий.</li>
</ul>
<h2>Профилирование памяти и времени вычислений</h2>
<h3>Удаление зависимое от контекста</h3>
<pre><code class="language-bash">python -B test.py -m [model_name] -d [data_name] --kv_type evict --ratio 0.3
</code></pre>
<ul>
<li>Приведённый код также сравнивает результаты генерации с полным и усечённым KV-кэшем.</li>
<li>Для быстрого теста используйте <code>-d squad</code>. Для тестирования с длинным контекстом — <code>-d scbench_kv</code>.
<ul>
<li>Доступные имена датасетов: <a href="https://github.com/snu-mllab/KVzip/blob/main/data/load.py"><code>data/load.py</code></a>.</li>
<li>Доступные имена моделей: <a href="https://github.com/snu-mllab/KVzip/blob/main/model/load.py"><code>model/load.py</code></a>, например llama3.1-8b, qwen2.5-7b (или Qwen/Qwen2.5-7B-Instruct-1M).</li>
</ul>
</li>
<li>Мы адаптировали CUDA-ядро из <a href="https://github.com/FFY0/AdaKV/tree/main">AdaKV</a>, поддерживающее неравномерное распределение бюджета на головы.
<ul>
<li>В настоящее время в нашем коде отсутствует оптимизированное ядро для Gemma3, использующего статический KV-кэш, поэтому код не даёт реальных приростов эффективности. Тем не менее, производительность модели можно оценивать с помощью уменьшенного внимания с подвыборкой KV (<code>--kv_type retain</code>).</li>
</ul>
</li>
</ul>
<h3>Удаление независимое от контекста (без накладных расходов во время работы)</h3>
<ul>
<li>Используйте флаг <code>--level head</code> с <code>--ratio 0.6</code> (рекомендуется).
<ul>
<li>Мы удаляем все KV пары контекста, связанные с определённой головой, при сохранении системного промпта и пар KV запросов.</li>
<li>Предрасчитанные оценки голов доступны для LLaMA3.1-8B и Qwen2.5-7/14B в <code>./utils/head_score</code>.</li>
</ul>
</li>
<li>Чтобы вычислить оценки голов для других моделей:
<pre><code class="language-bash">python -B test.py -m [model_name] -d scbench_qa_eng --save_head_score
</code></pre>
<ul>
<li>Результаты сохраняются в <code>./utils/head_score</code>.</li>
<li>Если задача связана с кодированием, рекомендуем дополнительно запустить команду с <code>-d scbench_repoqa</code>. Это позволит модели использовать максимальные оценки голов из естественного и кода, что улучшит производительность.</li>
</ul>
</li>
<li>Эти оценки могут быть легко интегрированы с оптимизированным движком вывода <a href="https://github.com/mit-han-lab/duo-attention">DuoAttention</a>, заменив их данные об оценке голов нашими.</li>
</ul>
<h2>Оценка</h2>
<ul>
<li>Для генерации ответов модели с коэффициентами сжатия KV от 0.1 до 1.0:
<pre><code class="language-bash">python -B eval.py -m [model_name] -d [data_name] --kv_type retain --num 100
</code></pre>
<ul>
<li>Результаты сохраняются в <code>./results/[data_name]</code>.</li>
<li>Поддерживаемые датасеты перечислены в <code>data/load.py</code>.</li>
</ul>
</li>
<li>Для вычисления метрик оценки по сгенерированным результатам:
<pre><code class="language-bash">python -B -m results.parse -m [model_name] -d [data_name]
</code></pre>
</li>
</ul>
<h2>Применение к новым моделям</h2>
<p>Для интеграции KVzip с новой моделью необходимо обновить следующие файлы:</p>
<ul>
<li><code>attention/attn.py</code><br />
При необходимости изменить логику прямого прохода внимания. В некоторых случаях также потребуется обновление kvcache.py и score.py.</li>
<li><code>model/monkeypatch.py</code><br />
Реализовать специфичные для модели патчи для интеграции.</li>
<li><code>model/template.py</code><br />
Определить системный промпт и шаблоны форматирования чата для модели.</li>
</ul>
<h2>Цитирование</h2>
<pre><code class="language-bibtex">@article{kim2025kvzip,
        title={KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction},
        author={Kim, Jang-Hyun and Kim, Jinuk and Kwon, Sangwoo and Lee, Jae W and Yun, Sangdoo and Song, Hyun Oh},
        journal={arXiv preprint arXiv:2505.23416},
        year={2025}
}
</code></pre>
<h2>Лицензия</h2>
<p>MIT License</p>
<hr />
<p>Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx">Open Ai Tx</a> | Last indexed: 2025-06-11</p>
<hr />

        </div>
    </div>
    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Default Statcounter code for openaitx
https://openaitx.github.io/ -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>
    <!-- End of Statcounter Code -->
</body>
</html>