<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>KVzip - snu-mllab/KVzip ko</title>
    <meta name="title" content="KVzip - snu-mllab/KVzip ko | KVzip: 컨텍스트 재구성을 통한 쿼리-무관 KV 캐시 압축 [논문] [블로그] 주요 내용 KVzip은 다양한 미래 쿼리를 지원하기 위해 KV 캐시를 압축합니다. [컨텍스트 의존적] 최소한의 성능 저하로 KV 캐시 크기 3–4배 축소 및 디코딩 지연 2배 감소를 달성합니다. [컨텍스트...">
    <meta name="description" content="snu-mllab/KVzip - GitHub repository ko documentation and information | KVzip: 컨텍스트 재구성을 통한 쿼리-무관 KV 캐시 압축 [논문] [블로그] 주요 내용 KVzip은 다양한 미래 쿼리를 지원하기 위해 KV 캐시를 압축합니다. [컨텍스트 의존적] 최소한의 성능 저하로 KV 캐시 크기 3–4배 축소 및 디코딩 지연 2배 감소를 달성합니다. [컨텍스트...">
    <meta name="keywords" content="snu-mllab, KVzip, GitHub, repository, ko documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/snu-mllab/KVzip/README-ko.html">
    <meta property="og:title" content="KVzip - snu-mllab/KVzip ko | KVzip: 컨텍스트 재구성을 통한 쿼리-무관 KV 캐시 압축 [논문] [블로그] 주요 내용 KVzip은 다양한 미래 쿼리를 지원하기 위해 KV 캐시를 압축합니다. [컨텍스트 의존적] 최소한의 성능 저하로 KV 캐시 크기 3–4배 축소 및 디코딩 지연 2배 감소를 달성합니다. [컨텍스트...">
    <meta property="og:description" content="snu-mllab/KVzip - GitHub repository ko documentation and information | KVzip: 컨텍스트 재구성을 통한 쿼리-무관 KV 캐시 압축 [논문] [블로그] 주요 내용 KVzip은 다양한 미래 쿼리를 지원하기 위해 KV 캐시를 압축합니다. [컨텍스트 의존적] 최소한의 성능 저하로 KV 캐시 크기 3–4배 축소 및 디코딩 지연 2배 감소를 달성합니다. [컨텍스트...">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">
    <script src="/js/marked.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        body { display: flex; flex-direction: column; min-height: 100vh; }
        .main-container { margin: 0 auto; width: 100%; max-width: 980px; padding: 0 20px; }
        @media (max-width: 768px) { .main-container { padding: 0 15px; } }
        .markdown-body img { max-width: 100%; height: auto; }
        .header { text-align: center; margin-bottom: 30px; padding: 20px; background-color: #f6f8fa; border-bottom: 1px solid #e1e4e8; position: relative; }
        .header .links { margin-top: 10px; font-size: 16px; }
        .header .links a { color: #0366d6; text-decoration: none; margin-left: 5px; }
        .header .links a:hover { text-decoration: underline; }
        .language-badges { margin-top: 15px; text-align: center; }
        .language-badges a { display: inline-block; margin: 2px; text-decoration: none; }
        .language-badges img { height: 20px; border-radius: 3px; }
        .language-badges a:hover img { opacity: 0.8; }
    </style>
</head>
<body>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/snu-mllab/KVzip" id="githubRepoLink" target="_blank">snu-mllab/KVzip</a>
<h1 style="display: none;">KVzip: 컨텍스트 재구성을 통한 쿼리-무관 KV 캐시 압축 [논문] [블로그] 주요 내용 KVzip은 다양한 미래 쿼리를 지원하기 위해 KV 캐시를 압축합니다. [컨텍스트 의존적] 최소한의 성능 저하로 KV 캐시 크기 3–4배 축소 및 디코딩 지연 2배 감소를 달성합니다. [컨텍스트...</h1>
        </div>
        <div class="language-badges" id="languageBadges">
            <!-- You can generate language badges here if needed -->
        </div>
    </div>
    <div class="main-container">
        <div class="markdown-body" id="content">
            <h1>KVzip: 컨텍스트 재구성을 통한 쿼리-무관 KV 캐시 압축</h1>
<p>[<a href="https://arxiv.org/abs/2505.23416">논문</a>] [<a href="https://janghyun1230.github.io/kvzip/">블로그</a>]</p>
<img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/method.png" width="800">
<h2>주요 내용</h2>
<ul>
<li>KVzip은 <strong>다양한 미래 쿼리</strong>를 지원하기 위해 KV 캐시를 압축합니다.</li>
<li>[컨텍스트 의존적] 최소한의 성능 저하로 <strong>KV 캐시 크기 3–4배 축소</strong> 및 <strong>디코딩 지연 2배 감소</strong>를 달성합니다.</li>
<li>[컨텍스트 독립적] 단 몇 분 내에 수행되는 헤드 중요도 점수 최적화(100배 빠름)를 통해 <a href="https://github.com/mit-han-lab/duo-attention">DuoAttention</a> 스타일의 헤드 수준 KV 압축을 향상시킵니다.</li>
<li>demo.py 실행:
<img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/demo.png" width="800"></li>
</ul>
<h3>쿼리-무관 설정 벤치마크</h3>
<ul>
<li>작업: <a href="https://huggingface.co/datasets/rajpurkar/squad">SQuAD</a>, <a href="https://github.com/gkamradt/LLMTest_NeedleInAHaystack">NIAH</a>, <a href="https://github.com/microsoft/MInference/tree/main/scbench">SCBench</a>, <a href="https://huggingface.co/datasets/openai/gsm8k/viewer/main/train?row=7294">GSM8K</a></li>
<li>모델: <a href="https://huggingface.co/Qwen/Qwen2.5-7B-Instruct">Qwen2.5-7B-Instruct-1M</a></li>
</ul>
<img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/benchmark.png" width="800">
<h2>설치</h2>
<p>CUDA 12.1과 Python 3.10을 사용했습니다.</p>
<pre><code class="language-bash">cd KVzip
pip install -r requirements.txt
pip install flash-attn==2.7.4.post1 --no-build-isolation
make i
</code></pre>
<ul>
<li><a href="https://github.com/mit-han-lab/omniserve">QServe</a> 양자화를 사용하려면 <a href="https://github.com/snu-mllab/KVzip/tree/main/model/quant_model"><code>./model/quant_model</code></a>을 참고하세요.</li>
</ul>
<h3>데이터셋</h3>
<ul>
<li>전처리된 SCBench 데이터셋을 <a href="https://drive.google.com/file/d/1cqoR6pxxFcjFqvPZkuAmF-fBSAlAbjbN/view?usp=share_link">Google Drive</a>에서 다운로드하세요.</li>
<li>압축을 푼 파일을 받았다면 scbench 폴더를 옮기면 됩니다.</li>
</ul>
<pre><code class="language-bash">mv scbench.zip kvzip/data/
cd kvzip/data
unzip scbench.zip  
</code></pre>
<h2>빠른 시작</h2>
<pre><code class="language-python">from model import ModelKVzip

model = ModelKVzip(&quot;Qwen/Qwen2.5-7B-Instruct-1M&quot;)
context = &quot;This is my basic profile. My name is Kim living in Seoul. My major is computer science.&quot;
queries = [&quot;What is my name?&quot;, &quot;Do I live in Seoul?&quot;]

kv = model.prefill(context, load_score=False)  # KV 캐시 미리 채우기 + 중요도 점수 산출
kv.prune(ratio=0.3)  # 압축 비율, 70% KV 제거

for q in queries:
    query_ids = model.apply_template(q)
    output = model.generate(query_ids, kv=kv, update_cache=False)  # 효율적 추론
    print(q, output)
</code></pre>
<ul>
<li>지원 모델은 <a href="https://github.com/snu-mllab/KVzip/blob/main/model/load.py"><code>model/load.py</code></a>에서 확인할 수 있으며 <strong>LLaMA3, Qwen2.5/3, Gemma3</strong> 등이 포함됩니다.</li>
<li>압축 오버헤드를 제거하려면 <code>load_score=True</code>로 설정하세요. 이는 <code>ratio=0.6</code>의 압축 비율 트레이드오프와 함께 컨텍스트 독립적 KV 제거를 가능하게 합니다.</li>
<li>생성 후 쿼리 및 생성된 토큰과 관련된 KV 쌍은 추가 처리를 위해 선택적으로 캐시에서 제거됩니다. 다중 턴 추론을 위해 <code>update_cache=True</code>로 설정하면 전체 상호작용 기록을 유지합니다.</li>
</ul>
<h2>메모리 및 계산 시간 프로파일링</h2>
<h3>컨텍스트 의존적 제거</h3>
<pre><code class="language-bash">python -B test.py -m [model_name] -d [data_name] --kv_type evict --ratio 0.3
</code></pre>
<ul>
<li>위 코드는 전체 KV 캐시와 가지치기된 KV 캐시를 사용해 생성한 결과를 비교합니다.</li>
<li>빠른 테스트는 <code>-d squad</code>를, 긴 컨텍스트 테스트는 <code>-d scbench_kv</code>를 사용하세요.
<ul>
<li>사용 가능한 데이터 이름: <a href="https://github.com/snu-mllab/KVzip/blob/main/data/load.py"><code>data/load.py</code></a>.</li>
<li>사용 가능한 모델 이름: <a href="https://github.com/snu-mllab/KVzip/blob/main/model/load.py"><code>model/load.py</code></a>, 예: llama3.1-8b, qwen2.5-7b (또는 Qwen/Qwen2.5-7B-Instruct-1M).</li>
</ul>
</li>
<li>CUDA 커널은 <a href="https://github.com/FFY0/AdaKV/tree/main">AdaKV</a>에서 가져와 비균등 헤드 예산 할당을 지원합니다.
<ul>
<li>현재 Gemma3용 최적화된 커널은 없으며, 따라서 실제 효율성 향상은 없습니다. 하지만 KV 서브샘플링(<code>--kv_type retain</code>)을 통한 감소된 어텐션으로 모델 성능 평가가 가능합니다.</li>
</ul>
</li>
</ul>
<h3>컨텍스트 독립적 제거 (런타임 압축 오버헤드 없음)</h3>
<ul>
<li><code>--level head</code> 플래그와 <code>--ratio 0.6</code> (권장)을 사용하세요.
<ul>
<li>시스템 프롬프트 및 쿼리 KV 쌍은 유지하면서 특정 헤드와 관련된 모든 컨텍스트 KV 쌍을 제거합니다.</li>
<li>사전 계산된 헤드 점수는 LLaMA3.1-8B 및 Qwen2.5-7/14B용으로 <code>./utils/head_score</code>에 제공됩니다.</li>
</ul>
</li>
<li>다른 모델의 헤드 점수를 계산하려면:
<pre><code class="language-bash">python -B test.py -m [model_name] -d scbench_qa_eng --save_head_score
</code></pre>
<ul>
<li>결과는 <code>./utils/head_score</code>에 저장됩니다.</li>
<li>코딩 작업을 목표로 한다면 <code>-d scbench_repoqa</code>를 추가로 실행하는 것을 권장합니다. 이는 자연어 및 코딩 언어 모두에서 최대 헤드 점수를 사용하게 하여 성능을 향상시킵니다.</li>
</ul>
</li>
<li>이 점수들은 <a href="https://github.com/mit-han-lab/duo-attention">DuoAttention</a>의 최적화된 추론 엔진에서 헤드 점수 데이터를 교체하여 원활하게 통합할 수 있습니다.</li>
</ul>
<h2>평가</h2>
<ul>
<li>KV 압축 비율 0.1부터 1.0까지 모델 응답 생성:
<pre><code class="language-bash">python -B eval.py -m [model_name] -d [data_name] --kv_type retain --num 100
</code></pre>
<ul>
<li>결과는 <code>./results/[data_name]</code>에 저장됩니다.</li>
<li>지원 데이터셋은 <code>data/load.py</code>에서 확인하세요.</li>
</ul>
</li>
<li>생성 결과로부터 평가 지표 계산:
<pre><code class="language-bash">python -B -m results.parse -m [model_name] -d [data_name]
</code></pre>
</li>
</ul>
<h2>신규 모델 적용</h2>
<p>KVzip을 새 모델에 통합하려면 다음 파일들을 수정해야 합니다:</p>
<ul>
<li><code>attention/attn.py</code><br />
필요에 따라 어텐션 포워드 패스 로직 수정. 경우에 따라 kvcache.py와 score.py 수정도 필요할 수 있습니다.</li>
<li><code>model/monkeypatch.py</code><br />
모델별 monkey patch 구현.</li>
<li><code>model/template.py</code><br />
모델의 시스템 프롬프트 및 채팅 포맷 템플릿 정의.</li>
</ul>
<h2>인용</h2>
<pre><code class="language-bibtex">@article{kim2025kvzip,
        title={KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction},
        author={Kim, Jang-Hyun and Kim, Jinuk and Kwon, Sangwoo and Lee, Jae W and Yun, Sangdoo and Song, Hyun Oh},
        journal={arXiv preprint arXiv:2505.23416},
        year={2025}
}
</code></pre>
<h2>라이선스</h2>
<p>MIT 라이선스</p>
<hr />
<p>Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx">Open Ai Tx</a> | Last indexed: 2025-06-11</p>
<hr />

        </div>
    </div>
    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Default Statcounter code for openaitx
https://openaitx.github.io/ -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>
    <!-- End of Statcounter Code -->
</body>
</html>