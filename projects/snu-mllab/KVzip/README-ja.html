<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>KVzip - snu-mllab/KVzip</title>
    <meta name="title" content="KVzip - snu-mllab/KVzip">
    <meta name="description" content="snu-mllab/KVzip - GitHub repository ja documentation and informationKVzip: コンテキスト再構築によるクエリ非依存型KVキャッシュ圧縮 [論文] [ブログ] 新機能 KVzipはKVキャッシュを圧縮し、多様な将来のクエリに対応します。 [コンテキスト依存] 3〜4倍のKVキャッシュサイズ削減および2倍のデコードレイテンシ削減を達成し、性能劣化は最小限です。 [コンテキスト非依存] DuoAttentionスタイルのヘッド単位KV圧縮を強化し、ヘッド単位の重要度スコア最適化を1分以内に数回のフォワードパスのみで行います（100倍高速化）。 demo.pyを実行: クエリ非依存設定でのベンチマーク タスク: SQuAD, NIAH, SCBench, GSM8K。 モデル: Qwen2.5-7B-Instruct-1M インストール CUDA 12.1とPython 3.10を使用しています。 cd KVzip pip install -r requirements.txt pip install flash-attn==2.7.4.post1 --no-build-isolation make i QServe量子化を使用する場合は、./model/quant_modelを参照してください。 データセット 前処理済みSCBenchデータセットはGoogle Driveからダウンロードしてください。 ファイルを解凍した場合は、単にscbenchフォルダを移動します。 mv scbench.zip kvzip/data/ cd kvzip/data unzip scbench.zip クイックスタート from model import ModelKVzip model = ModelKVzip(&quot;Qwen/Qwen2.5-7B-Instruct-1M&quot;) context = &quot;これは私の基本プロフィールです。私の名前はキムで、ソウルに住んでいます。専攻はコンピュータサイエンスです。&quot; queries = [&quot;私の名前は何ですか？&quot;, &quot;私はソウルに住んでいますか？&quot;] kv = model.prefill(context, load_score=False) # KVキャッシュの事前生成＋重要度スコアリング kv.prune(ratio=0.3) # 圧縮率、70%のKVを削除 for q in queries: query_ids = model.apply_template(q) output = model.generate(query_ids, kv=kv, update_cache=False) # 効率的な推論 print(q, output) 対応モデルはmodel/load.pyに記載されており、LLaMA3, Qwen2.5/3, Gemma3を含みます。 load_score=Trueを設定すると圧縮オーバーヘッドがなくなり、コンテキスト非依存のKV削除が可能になります。圧縮率はratio=0.6でトレードオフがあります。 生成後、クエリと生成トークンに対応するKVペアはさらに処理のために選択的にキャッシュから削除されます。update_cache=Trueを設定するとマルチターン推論が可能になり、推論中の全ての対話履歴が保持されます。 メモリと計算時間のプロファイリング コンテキスト依存削除 python -B test.py -m [model_name] -d [data_name] --kv_type evict --ratio 0.3 上記コードは、フルKVキャッシュと削減されたKVキャッシュの生成結果も比較します。 簡易テストには-d squadを使用。長文コンテキストテストには-d scbench_kvを使用。 利用可能なデータ名はdata/load.pyを参照。 利用可能なモデル名はmodel/load.pyを参照。例: llama3.1-8b, qwen2.5-7b（またはQwen/Qwen2.5-7B-Instruct-1M）。 CUDAカーネルはAdaKVから適応し、非一様なヘッド予算割り当てをサポート。 現状、Gemma3用の静的KVキャッシュに最適化されたカーネルがなく、実際の効率化はありませんが、KVサブサンプリングによる削減注意でモデル性能評価は可能（--kv_type retain）。 コンテキスト非依存削除（ランタイム圧縮オーバーヘッドなし） --level headフラグと--ratio 0.6（推奨）を使用。 システムプロンプトとクエリのKVペアは保持しつつ、特定ヘッドに紐づく全てのコンテキストKVペアを削除します。 LLaMA3.1-8BおよびQwen2.5-7/14B用の事前計算済みヘッドスコアは./utils/head_scoreにあります。 他モデルのヘッドスコア計算方法: python -B test.py -m [model_name] -d scbench_qa_eng --save_head_score 結果は./utils/head_scoreに保存されます。 コーディングタスクを対象にする場合は、-d scbench_repoqaでも同様に実行することを推奨します。自然言語とコーディング言語の両方から最大のヘッドスコアを使用でき、性能が向上します。 これらのスコアはDuoAttentionの最適化推論エンジンにシームレスに統合可能で、ヘッドスコアデータを置換するだけで利用できます。 評価 KV圧縮率0.1〜1.0の範囲でモデル応答を生成するには: python -B eval.py -m [model_name] -d [data_name] --kv_type retain --num 100 結果は./results/[data_name]に保存されます。 対応データセットはdata/load.pyに記載。...">
    <meta name="keywords" content="snu-mllab, KVzip, GitHub, repository, ja documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/snu-mllab/KVzip/README-ja.html">
    <meta property="og:title" content="KVzip - snu-mllab/KVzip">
    <meta property="og:description" content="snu-mllab/KVzip - GitHub repository ja documentation and information">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">
    <script src="/js/marked.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        body { display: flex; flex-direction: column; min-height: 100vh; }
        .main-container { margin: 0 auto; width: 100%; max-width: 980px; padding: 0 20px; }
        @media (max-width: 768px) { .main-container { padding: 0 15px; } }
        .markdown-body img { max-width: 100%; height: auto; }
        .header { text-align: center; margin-bottom: 30px; padding: 20px; background-color: #f6f8fa; border-bottom: 1px solid #e1e4e8; position: relative; }
        .header .links { margin-top: 10px; font-size: 16px; }
        .header .links a { color: #0366d6; text-decoration: none; margin-left: 5px; }
        .header .links a:hover { text-decoration: underline; }
        .language-badges { margin-top: 15px; text-align: center; }
        .language-badges a { display: inline-block; margin: 2px; text-decoration: none; }
        .language-badges img { height: 20px; border-radius: 3px; }
        .language-badges a:hover img { opacity: 0.8; }
    </style>
</head>
<body>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/snu-mllab/KVzip" id="githubRepoLink" target="_blank">snu-mllab/KVzip</a>
<br>
<h1 style="display: none;">KVzip: コンテキスト再構築によるクエリ非依存型KVキャッシュ圧縮 [論文] [ブログ] 新機能 KVzipはKVキャッシュを圧縮し、多様な将来のクエリに対応します。 [コンテキスト依存] 3〜4倍のKVキャッシュサイズ削減および2倍のデコードレイテンシ削減を達成し、性能劣化は最小限です。 [コンテキスト非依存] DuoAttentionスタイルのヘッド単位KV圧縮を強化し、ヘッド単位の重要度スコア最適化を1分以内に数回のフォワードパスのみで行います（100倍高速化）。 demo.pyを実行: クエリ非依存設定でのベンチマーク タスク: SQuAD, NIAH, SCBench, GSM8K。 モデル: Qwen2.5-7B-Instruct-1M インストール CUDA 12.1とPython 3.10を使用しています。 cd KVzip pip install -r requirements.txt pip install flash-attn==2.7.4.post1 --no-build-isolation make i QServe量子化を使用する場合は、./model/quant_modelを参照してください。 データセット 前処理済みSCBenchデータセットはGoogle Driveからダウンロードしてください。 ファイルを解凍した場合は、単にscbenchフォルダを移動します。 mv scbench.zip kvzip/data/ cd kvzip/data unzip scbench.zip クイックスタート from model import ModelKVzip model = ModelKVzip(&quot;Qwen/Qwen2.5-7B-Instruct-1M&quot;) context = &quot;これは私の基本プロフィールです。私の名前はキムで、ソウルに住んでいます。専攻はコンピュータサイエンスです。&quot; queries = [&quot;私の名前は何ですか？&quot;, &quot;私はソウルに住んでいますか？&quot;] kv = model.prefill(context, load_score=False) # KVキャッシュの事前生成＋重要度スコアリング kv.prune(ratio=0.3) # 圧縮率、70%のKVを削除 for q in queries: query_ids = model.apply_template(q) output = model.generate(query_ids, kv=kv, update_cache=False) # 効率的な推論 print(q, output) 対応モデルはmodel/load.pyに記載されており、LLaMA3, Qwen2.5/3, Gemma3を含みます。 load_score=Trueを設定すると圧縮オーバーヘッドがなくなり、コンテキスト非依存のKV削除が可能になります。圧縮率はratio=0.6でトレードオフがあります。 生成後、クエリと生成トークンに対応するKVペアはさらに処理のために選択的にキャッシュから削除されます。update_cache=Trueを設定するとマルチターン推論が可能になり、推論中の全ての対話履歴が保持されます。 メモリと計算時間のプロファイリング コンテキスト依存削除 python -B test.py -m [model_name] -d [data_name] --kv_type evict --ratio 0.3 上記コードは、フルKVキャッシュと削減されたKVキャッシュの生成結果も比較します。 簡易テストには-d squadを使用。長文コンテキストテストには-d scbench_kvを使用。 利用可能なデータ名はdata/load.pyを参照。 利用可能なモデル名はmodel/load.pyを参照。例: llama3.1-8b, qwen2.5-7b（またはQwen/Qwen2.5-7B-Instruct-1M）。 CUDAカーネルはAdaKVから適応し、非一様なヘッド予算割り当てをサポート。 現状、Gemma3用の静的KVキャッシュに最適化されたカーネルがなく、実際の効率化はありませんが、KVサブサンプリングによる削減注意でモデル性能評価は可能（--kv_type retain）。 コンテキスト非依存削除（ランタイム圧縮オーバーヘッドなし） --level headフラグと--ratio 0.6（推奨）を使用。 システムプロンプトとクエリのKVペアは保持しつつ、特定ヘッドに紐づく全てのコンテキストKVペアを削除します。 LLaMA3.1-8BおよびQwen2.5-7/14B用の事前計算済みヘッドスコアは./utils/head_scoreにあります。 他モデルのヘッドスコア計算方法: python -B test.py -m [model_name] -d scbench_qa_eng --save_head_score 結果は./utils/head_scoreに保存されます。 コーディングタスクを対象にする場合は、-d scbench_repoqaでも同様に実行することを推奨します。自然言語とコーディング言語の両方から最大のヘッドスコアを使用でき、性能が向上します。 これらのスコアはDuoAttentionの最適化推論エンジンにシームレスに統合可能で、ヘッドスコアデータを置換するだけで利用できます。 評価 KV圧縮率0.1〜1.0の範囲でモデル応答を生成するには: python -B eval.py -m [model_name] -d [data_name] --kv_type retain --num 100 結果は./results/[data_name]に保存されます。 対応データセットはdata/load.pyに記載。...</h1>
        </div>
        <div class="language-badges" id="languageBadges">
            <!-- You can generate language badges here if needed -->
        </div>
    </div>
    <div class="main-container">
        <div class="markdown-body" id="content">
            <h1>KVzip: コンテキスト再構築によるクエリ非依存型KVキャッシュ圧縮</h1>
<p>[<a href="https://arxiv.org/abs/2505.23416">論文</a>] [<a href="https://janghyun1230.github.io/kvzip/">ブログ</a>]</p>
<img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/method.png" width="800">
<h2>新機能</h2>
<ul>
<li>KVzipはKVキャッシュを圧縮し、<strong>多様な将来のクエリ</strong>に対応します。</li>
<li>[コンテキスト依存] <strong>3〜4倍のKVキャッシュサイズ削減</strong>および<strong>2倍のデコードレイテンシ削減</strong>を達成し、性能劣化は最小限です。</li>
<li>[コンテキスト非依存] <a href="https://github.com/mit-han-lab/duo-attention">DuoAttention</a>スタイルのヘッド単位KV圧縮を強化し、ヘッド単位の重要度スコア最適化を<strong>1分以内に数回のフォワードパスのみで</strong>行います（100倍高速化）。</li>
<li>demo.pyを実行:
<img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/demo.png" width="800"></li>
</ul>
<h3>クエリ非依存設定でのベンチマーク</h3>
<ul>
<li>タスク: <a href="https://huggingface.co/datasets/rajpurkar/squad">SQuAD</a>, <a href="https://github.com/gkamradt/LLMTest_NeedleInAHaystack">NIAH</a>, <a href="https://github.com/microsoft/MInference/tree/main/scbench">SCBench</a>, <a href="https://huggingface.co/datasets/openai/gsm8k/viewer/main/train?row=7294">GSM8K</a>。</li>
<li>モデル: <a href="https://huggingface.co/Qwen/Qwen2.5-7B-Instruct">Qwen2.5-7B-Instruct-1M</a></li>
</ul>
<img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/benchmark.png" width="800">
<h2>インストール</h2>
<p>CUDA 12.1とPython 3.10を使用しています。</p>
<pre><code class="language-bash">cd KVzip
pip install -r requirements.txt
pip install flash-attn==2.7.4.post1 --no-build-isolation
make i
</code></pre>
<ul>
<li><a href="https://github.com/mit-han-lab/omniserve">QServe</a>量子化を使用する場合は、<a href="https://github.com/snu-mllab/KVzip/tree/main/model/quant_model"><code>./model/quant_model</code></a>を参照してください。</li>
</ul>
<h3>データセット</h3>
<ul>
<li>前処理済みSCBenchデータセットは<a href="https://drive.google.com/file/d/1cqoR6pxxFcjFqvPZkuAmF-fBSAlAbjbN/view?usp=share_link">Google Drive</a>からダウンロードしてください。</li>
<li>ファイルを解凍した場合は、単にscbenchフォルダを移動します。</li>
</ul>
<pre><code class="language-bash">mv scbench.zip kvzip/data/
cd kvzip/data
unzip scbench.zip  
</code></pre>
<h2>クイックスタート</h2>
<pre><code class="language-python">from model import ModelKVzip

model = ModelKVzip(&quot;Qwen/Qwen2.5-7B-Instruct-1M&quot;)
context = &quot;これは私の基本プロフィールです。私の名前はキムで、ソウルに住んでいます。専攻はコンピュータサイエンスです。&quot;
queries = [&quot;私の名前は何ですか？&quot;, &quot;私はソウルに住んでいますか？&quot;]

kv = model.prefill(context, load_score=False)  # KVキャッシュの事前生成＋重要度スコアリング
kv.prune(ratio=0.3)  # 圧縮率、70%のKVを削除

for q in queries:
    query_ids = model.apply_template(q)
    output = model.generate(query_ids, kv=kv, update_cache=False)  # 効率的な推論
    print(q, output)
</code></pre>
<ul>
<li>対応モデルは<a href="https://github.com/snu-mllab/KVzip/blob/main/model/load.py"><code>model/load.py</code></a>に記載されており、<strong>LLaMA3, Qwen2.5/3, Gemma3</strong>を含みます。</li>
<li><code>load_score=True</code>を設定すると圧縮オーバーヘッドがなくなり、コンテキスト非依存のKV削除が可能になります。圧縮率は<code>ratio=0.6</code>でトレードオフがあります。</li>
<li>生成後、クエリと生成トークンに対応するKVペアはさらに処理のために選択的にキャッシュから削除されます。<code>update_cache=True</code>を設定するとマルチターン推論が可能になり、推論中の全ての対話履歴が保持されます。</li>
</ul>
<h2>メモリと計算時間のプロファイリング</h2>
<h3>コンテキスト依存削除</h3>
<pre><code class="language-bash">python -B test.py -m [model_name] -d [data_name] --kv_type evict --ratio 0.3
</code></pre>
<ul>
<li>上記コードは、フルKVキャッシュと削減されたKVキャッシュの生成結果も比較します。</li>
<li>簡易テストには<code>-d squad</code>を使用。長文コンテキストテストには<code>-d scbench_kv</code>を使用。
<ul>
<li>利用可能なデータ名は<a href="https://github.com/snu-mllab/KVzip/blob/main/data/load.py"><code>data/load.py</code></a>を参照。</li>
<li>利用可能なモデル名は<a href="https://github.com/snu-mllab/KVzip/blob/main/model/load.py"><code>model/load.py</code></a>を参照。例: llama3.1-8b, qwen2.5-7b（またはQwen/Qwen2.5-7B-Instruct-1M）。</li>
</ul>
</li>
<li>CUDAカーネルは<a href="https://github.com/FFY0/AdaKV/tree/main">AdaKV</a>から適応し、非一様なヘッド予算割り当てをサポート。
<ul>
<li>現状、Gemma3用の静的KVキャッシュに最適化されたカーネルがなく、実際の効率化はありませんが、KVサブサンプリングによる削減注意でモデル性能評価は可能（<code>--kv_type retain</code>）。</li>
</ul>
</li>
</ul>
<h3>コンテキスト非依存削除（ランタイム圧縮オーバーヘッドなし）</h3>
<ul>
<li><code>--level head</code>フラグと<code>--ratio 0.6</code>（推奨）を使用。
<ul>
<li>システムプロンプトとクエリのKVペアは保持しつつ、特定ヘッドに紐づく全てのコンテキストKVペアを削除します。</li>
<li>LLaMA3.1-8BおよびQwen2.5-7/14B用の事前計算済みヘッドスコアは<code>./utils/head_score</code>にあります。</li>
</ul>
</li>
<li>他モデルのヘッドスコア計算方法:
<pre><code class="language-bash">python -B test.py -m [model_name] -d scbench_qa_eng --save_head_score
</code></pre>
<ul>
<li>結果は<code>./utils/head_score</code>に保存されます。</li>
<li>コーディングタスクを対象にする場合は、<code>-d scbench_repoqa</code>でも同様に実行することを推奨します。自然言語とコーディング言語の両方から最大のヘッドスコアを使用でき、性能が向上します。</li>
</ul>
</li>
<li>これらのスコアは<a href="https://github.com/mit-han-lab/duo-attention">DuoAttention</a>の最適化推論エンジンにシームレスに統合可能で、ヘッドスコアデータを置換するだけで利用できます。</li>
</ul>
<h2>評価</h2>
<ul>
<li>KV圧縮率0.1〜1.0の範囲でモデル応答を生成するには:
<pre><code class="language-bash">python -B eval.py -m [model_name] -d [data_name] --kv_type retain --num 100
</code></pre>
<ul>
<li>結果は<code>./results/[data_name]</code>に保存されます。</li>
<li>対応データセットは<code>data/load.py</code>に記載。</li>
</ul>
</li>
<li>生成結果から評価指標を計算するには:
<pre><code class="language-bash">python -B -m results.parse -m [model_name] -d [data_name]
</code></pre>
</li>
</ul>
<h2>新規モデルへの適用</h2>
<p>KVzipを新規モデルに統合するには、以下のファイルを更新する必要があります:</p>
<ul>
<li><code>attention/attn.py</code><br />
注意のフォワードパスロジックを必要に応じて修正。場合によっては<code>kvcache.py</code>や<code>score.py</code>の更新も必要。</li>
<li><code>model/monkeypatch.py</code><br />
モデル固有のモンキーパッチを実装。</li>
<li><code>model/template.py</code><br />
モデルのシステムプロンプトやチャットフォーマットテンプレートを定義。</li>
</ul>
<h2>引用</h2>
<pre><code class="language-bibtex">@article{kim2025kvzip,
        title={KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction},
        author={Kim, Jang-Hyun and Kim, Jinuk and Kwon, Sangwoo and Lee, Jae W and Yun, Sangdoo and Song, Hyun Oh},
        journal={arXiv preprint arXiv:2505.23416},
        year={2025}
}
</code></pre>
<h2>ライセンス</h2>
<p>MIT License</p>
<hr />
<p>Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx">Open Ai Tx</a> | Last indexed: 2025-06-11</p>
<hr />

        </div>
    </div>
    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Statcounter and other scripts can be added here -->
</body>
</html>