<!DOCTYPE html>
<html lang="it">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>pytorch - pytorch/pytorch</title>
    <meta name="title" content="pytorch - pytorch/pytorch">
    <meta name="description" content="pytorch/pytorch - GitHub repository it documentation and informationPyTorch è un pacchetto Python che offre due funzionalità di alto livello: Calcolo con tensori (simile a NumPy) con potente accelerazione GPU Reti neurali profon...">
    <meta name="keywords" content="pytorch, pytorch, GitHub, repository, it documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/pytorch/pytorch/README-it.html">
    <meta property="og:title" content="pytorch - pytorch/pytorch">
    <meta property="og:description" content="pytorch/pytorch - GitHub repository it documentation and information">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">
    <script src="/js/marked.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        body { display: flex; flex-direction: column; min-height: 100vh; }
        .main-container { margin: 0 auto; width: 100%; max-width: 980px; padding: 0 20px; }
        @media (max-width: 768px) { .main-container { padding: 0 15px; } }
        .markdown-body img { max-width: 100%; height: auto; }
        .header { text-align: center; margin-bottom: 30px; padding: 20px; background-color: #f6f8fa; border-bottom: 1px solid #e1e4e8; position: relative; }
        .header .links { margin-top: 10px; font-size: 16px; }
        .header .links a { color: #0366d6; text-decoration: none; margin-left: 5px; }
        .header .links a:hover { text-decoration: underline; }
        .language-badges { margin-top: 15px; text-align: center; }
        .language-badges a { display: inline-block; margin: 2px; text-decoration: none; }
        .language-badges img { height: 20px; border-radius: 3px; }
        .language-badges a:hover img { opacity: 0.8; }
    </style>
</head>
<body>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/pytorch/pytorch" id="githubRepoLink" target="_blank">pytorch/pytorch</a>
<h1 style="display: none;">PyTorch è un pacchetto Python che offre due funzionalità di alto livello: Calcolo con tensori (simile a NumPy) con potente accelerazione GPU Reti neurali profon...</h1>
        </div>
        <div class="language-badges" id="languageBadges">
            <!-- You can generate language badges here if needed -->
        </div>
    </div>
    <div class="main-container">
        <div class="markdown-body" id="content">
            <p><img src="https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/pytorch-logo-dark.png" alt="PyTorch Logo" /></p>
<hr />
<p>PyTorch è un pacchetto Python che offre due funzionalità di alto livello:</p>
<ul>
<li>Calcolo con tensori (simile a NumPy) con potente accelerazione GPU</li>
<li>Reti neurali profonde basate su un sistema autograd a nastro</li>
</ul>
<p>È possibile riutilizzare i tuoi pacchetti Python preferiti come NumPy, SciPy e Cython per estendere PyTorch quando necessario.</p>
<p>La salute della nostra trunk (segnali di Continuous Integration) è disponibile su <a href="https://hud.pytorch.org/ci/pytorch/pytorch/main">hud.pytorch.org</a>.</p>
<!-- toc -->
<ul>
<li><a href="#ulteriori-informazioni-su-pytorch">Ulteriori informazioni su PyTorch</a>
<ul>
<li><a href="#una-libreria-di-tensori-pronta-per-gpu">Una libreria di tensori pronta per GPU</a></li>
<li><a href="#reti-neurali-dinamiche-autograd-a-nastro">Reti neurali dinamiche: Autograd a nastro</a></li>
<li><a href="#prima-di-tutto-python">Prima di tutto Python</a></li>
<li><a href="#esperienze-imperative">Esperienze imperative</a></li>
<li><a href="#veloce-e-snello">Veloce e snello</a></li>
<li><a href="#estensioni-senza-complicazioni">Estensioni senza complicazioni</a></li>
</ul>
</li>
<li><a href="#installazione">Installazione</a>
<ul>
<li><a href="#binari">Binari</a>
<ul>
<li><a href="#piattaforme-nvidia-jetson">Piattaforme NVIDIA Jetson</a></li>
</ul>
</li>
<li><a href="#dal-sorgente">Dal sorgente</a>
<ul>
<li><a href="#prerequisiti">Prerequisiti</a>
<ul>
<li><a href="#supporto-nvidia-cuda">Supporto NVIDIA CUDA</a></li>
<li><a href="#supporto-amd-rocm">Supporto AMD ROCm</a></li>
<li><a href="#supporto-gpu-intel">Supporto GPU Intel</a></li>
</ul>
</li>
<li><a href="#ottenere-il-sorgente-pytorch">Ottenere il sorgente PyTorch</a></li>
<li><a href="#installare-le-dipendenze">Installare le dipendenze</a></li>
<li><a href="#installare-pytorch">Installare PyTorch</a>
<ul>
<li><a href="#regolare-le-opzioni-di-compilazione-opzionale">Regolare le opzioni di compilazione (Opzionale)</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#immagine-docker">Immagine Docker</a>
<ul>
<li><a href="#utilizzo-di-immagini-precompilate">Utilizzo di immagini precompilate</a></li>
<li><a href="#costruire-limmagine-manualmente">Costruire l'immagine manualmente</a></li>
</ul>
</li>
<li><a href="#compilare-la-documentazione">Compilare la documentazione</a>
<ul>
<li><a href="#compilazione-di-un-pdf">Compilazione di un PDF</a></li>
</ul>
</li>
<li><a href="#versioni-precedenti">Versioni precedenti</a></li>
</ul>
</li>
<li><a href="#primi-passi">Primi passi</a></li>
<li><a href="#risorse">Risorse</a></li>
<li><a href="#comunicazione">Comunicazione</a></li>
<li><a href="#rilasci-e-contributi">Rilasci e contributi</a></li>
<li><a href="#il-team">Il team</a></li>
<li><a href="#licenza">Licenza</a></li>
</ul>
<!-- tocstop -->
<h2>Ulteriori informazioni su PyTorch</h2>
<p><a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Impara le basi di PyTorch</a></p>
<p>A livello granulare, PyTorch è una libreria che consiste nei seguenti componenti:</p>
<p>| Componente | Descrizione |
| ---- | --- |
| <a href="https://pytorch.org/docs/stable/torch.html"><strong>torch</strong></a> | Una libreria di tensori simile a NumPy, con forte supporto GPU |
| <a href="https://pytorch.org/docs/stable/autograd.html"><strong>torch.autograd</strong></a> | Una libreria di differenziazione automatica basata su nastro che supporta tutte le operazioni sui tensori differenziabili in torch |
| <a href="https://pytorch.org/docs/stable/jit.html"><strong>torch.jit</strong></a> | Uno stack di compilazione (TorchScript) per creare modelli serializzabili e ottimizzabili dal codice PyTorch |
| <a href="https://pytorch.org/docs/stable/nn.html"><strong>torch.nn</strong></a> | Una libreria di reti neurali profondamente integrata con autograd, progettata per la massima flessibilità |
| <a href="https://pytorch.org/docs/stable/multiprocessing.html"><strong>torch.multiprocessing</strong></a> | Multiprocessing Python, ma con condivisione magica della memoria dei tensori torch tra i processi. Utile per il caricamento dati e l’addestramento Hogwild |
| <a href="https://pytorch.org/docs/stable/data.html"><strong>torch.utils</strong></a> | DataLoader e altre funzioni di utilità per comodità |</p>
<p>Di solito, PyTorch è utilizzato come:</p>
<ul>
<li>Sostituto di NumPy per sfruttare la potenza delle GPU.</li>
<li>Piattaforma di ricerca per deep learning che offre la massima flessibilità e velocità.</li>
</ul>
<p>Approfondimento:</p>
<h3>Una libreria di tensori pronta per GPU</h3>
<p>Se usi NumPy, hai già utilizzato tensori (anche noti come ndarray).</p>
<p><img src="./docs/source/_static/img/tensor_illustration.png" alt="Tensor illustration" /></p>
<p>PyTorch fornisce tensori che possono risiedere sia su CPU che su GPU e accelera il calcolo in modo significativo.</p>
<p>Offriamo una vasta gamma di routine sui tensori per accelerare e adattarsi alle tue esigenze di calcolo scientifico, come slicing, indicizzazione, operazioni matematiche, algebra lineare, riduzioni.
E sono veloci!</p>
<h3>Reti neurali dinamiche: Autograd a nastro</h3>
<p>PyTorch ha un modo unico di costruire reti neurali: usando e riproducendo un registratore a nastro.</p>
<p>La maggior parte dei framework come TensorFlow, Theano, Caffe e CNTK ha una visione statica del mondo.
Bisogna costruire una rete neurale e riutilizzare la stessa struttura più e più volte.
Cambiare il comportamento della rete significa dover ripartire da zero.</p>
<p>Con PyTorch, utilizziamo una tecnica chiamata auto-differenziazione in modalità inversa, che ti consente di
cambiare il comportamento della rete in modo arbitrario senza latenza o overhead. La nostra ispirazione deriva
da diversi articoli di ricerca su questo argomento, oltre che da lavori attuali e passati come
<a href="https://github.com/twitter/torch-autograd">torch-autograd</a>,
<a href="https://github.com/HIPS/autograd">autograd</a>,
<a href="https://chainer.org">Chainer</a>, ecc.</p>
<p>Sebbene questa tecnica non sia unica di PyTorch, è una delle implementazioni più veloci disponibili.
Ottieni il meglio in termini di velocità e flessibilità per le tue ricerche più creative.</p>
<p><img src="https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/dynamic_graph.gif" alt="Dynamic graph" /></p>
<h3>Prima di tutto Python</h3>
<p>PyTorch non è un binding Python in un framework C++ monolitico.
È costruito per essere profondamente integrato in Python.
Puoi usarlo in modo naturale come useresti <a href="https://www.numpy.org/">NumPy</a> / <a href="https://www.scipy.org/">SciPy</a> / <a href="https://scikit-learn.org">scikit-learn</a> ecc.
Puoi scrivere i tuoi nuovi layer di reti neurali direttamente in Python, usando le tue librerie preferite
e utilizzare pacchetti come <a href="https://cython.org/">Cython</a> e <a href="http://numba.pydata.org/">Numba</a>.
Il nostro obiettivo è non reinventare la ruota dove non necessario.</p>
<h3>Esperienze imperative</h3>
<p>PyTorch è progettato per essere intuitivo, lineare nel pensiero e facile da usare.
Quando esegui una riga di codice, essa viene eseguita. Non esiste una visione asincrona del mondo.
Quando entri in un debugger o ricevi messaggi di errore e stack trace, capirli è semplice.
Lo stack trace punta esattamente al punto in cui il tuo codice è stato definito.
Speriamo che tu non debba mai passare ore a fare debugging a causa di stack trace errati o motori di esecuzione asincroni e opachi.</p>
<h3>Veloce e snello</h3>
<p>PyTorch ha un overhead minimo. Integria librerie di accelerazione
come <a href="https://software.intel.com/mkl">Intel MKL</a> e NVIDIA (<a href="https://developer.nvidia.com/cudnn">cuDNN</a>, <a href="https://developer.nvidia.com/nccl">NCCL</a>) per massimizzare la velocità.
Al cuore, i backend per tensori e reti neurali su CPU e GPU
sono maturi e testati da anni.</p>
<p>Pertanto, PyTorch è molto veloce — sia che tu esegua reti neurali piccole che grandi.</p>
<p>L’utilizzo di memoria in PyTorch è estremamente efficiente rispetto a Torch o ad alcune alternative.
Abbiamo scritto allocatori di memoria personalizzati per la GPU per assicurarci che
i tuoi modelli di deep learning siano il più possibile efficienti in memoria.
Ciò ti consente di addestrare modelli di deep learning più grandi rispetto a prima.</p>
<h3>Estensioni senza complicazioni</h3>
<p>Scrivere nuovi moduli di reti neurali, o interfacciarsi con l’API dei tensori di PyTorch è stato progettato per essere semplice
e con astrazioni minime.</p>
<p>Puoi scrivere nuovi layer di reti neurali in Python usando l’API torch
<a href="https://pytorch.org/tutorials/advanced/numpy_extensions_tutorial.html">o le tue librerie preferite basate su NumPy come SciPy</a>.</p>
<p>Se desideri scrivere i tuoi layer in C/C++, offriamo una comoda API per le estensioni efficiente e con boilerplate minimo.
Non è necessario scrivere codice wrapper. Puoi vedere <a href="https://pytorch.org/tutorials/advanced/cpp_extension.html">un tutorial qui</a> e <a href="https://github.com/pytorch/extension-cpp">un esempio qui</a>.</p>
<h2>Installazione</h2>
<h3>Binari</h3>
<p>I comandi per installare i binari tramite Conda o pip wheels sono disponibili sul nostro sito: <a href="https://pytorch.org/get-started/locally/">https://pytorch.org/get-started/locally/</a></p>
<h4>Piattaforme NVIDIA Jetson</h4>
<p>I pacchetti wheel Python per NVIDIA Jetson Nano, Jetson TX1/TX2, Jetson Xavier NX/AGX e Jetson AGX Orin sono disponibili <a href="https://forums.developer.nvidia.com/t/pytorch-for-jetson-version-1-10-now-available/72048">qui</a> e il container L4T è pubblicato <a href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/l4t-pytorch">qui</a></p>
<p>Richiedono JetPack 4.2 o superiore, e <a href="https://github.com/dusty-nv">@dusty-nv</a> e <a href="https://github.com/ptrblck">@ptrblck</a> li mantengono.</p>
<h3>Dal sorgente</h3>
<h4>Prerequisiti</h4>
<p>Se installi dal sorgente, avrai bisogno di:</p>
<ul>
<li>Python 3.9 o successivo</li>
<li>Un compilatore che supporti completamente C++17, come clang o gcc (gcc 9.4.0 o successivo è richiesto su Linux)</li>
<li>Visual Studio o Visual Studio Build Tool (solo Windows)</li>
</ul>
<p>* Il CI di PyTorch utilizza Visual C++ BuildTools, che sono inclusi in Visual Studio Enterprise,
Professional o Community Edition. Puoi anche installare i build tools da
https://visualstudio.microsoft.com/visual-cpp-build-tools/. I build tools <em>non sono</em> inclusi in Visual Studio Code di default.</p>
<p>Un esempio di configurazione dell’ambiente è mostrato di seguito:</p>
<ul>
<li>Linux:</li>
</ul>
<pre><code class="language-bash">$ source &lt;CONDA_INSTALL_DIR&gt;/bin/activate
$ conda create -y -n &lt;CONDA_NAME&gt;
$ conda activate &lt;CONDA_NAME&gt;
</code></pre>
<ul>
<li>Windows:</li>
</ul>
<pre><code class="language-bash">$ source &lt;CONDA_INSTALL_DIR&gt;\Scripts\activate.bat
$ conda create -y -n &lt;CONDA_NAME&gt;
$ conda activate &lt;CONDA_NAME&gt;
$ call &quot;C:\Program Files\Microsoft Visual Studio\&lt;VERSION&gt;\Community\VC\Auxiliary\Build\vcvarsall.bat&quot; x64
</code></pre>
<h5>Supporto NVIDIA CUDA</h5>
<p>Se vuoi compilare con supporto CUDA, <a href="https://pytorch.org/get-started/locally/">seleziona una versione supportata di CUDA dalla nostra matrice di supporto</a>, quindi installa:</p>
<ul>
<li><a href="https://developer.nvidia.com/cuda-downloads">NVIDIA CUDA</a></li>
<li><a href="https://developer.nvidia.com/cudnn">NVIDIA cuDNN</a> v8.5 o superiore</li>
<li><a href="https://gist.github.com/ax3l/9489132">Compilatore</a> compatibile con CUDA</li>
</ul>
<p>Nota: Puoi consultare la <a href="https://docs.nvidia.com/deeplearning/cudnn/backend/latest/reference/support-matrix.html">matrice di supporto cuDNN</a> per le versioni cuDNN con le varie versioni di CUDA, driver CUDA e hardware NVIDIA supportati.</p>
<p>Se vuoi disabilitare il supporto CUDA, esporta la variabile d’ambiente <code>USE_CUDA=0</code>.
Altre variabili d’ambiente utili possono essere trovate in <code>setup.py</code>.</p>
<p>Se stai compilando per piattaforme NVIDIA Jetson (Jetson Nano, TX1, TX2, AGX Xavier), le istruzioni per installare PyTorch per Jetson Nano sono <a href="https://devtalk.nvidia.com/default/topic/1049071/jetson-nano/pytorch-for-jetson-nano/">disponibili qui</a></p>
<h5>Supporto AMD ROCm</h5>
<p>Se vuoi compilare con supporto ROCm, installa</p>
<ul>
<li><a href="https://rocm.docs.amd.com/en/latest/deploy/linux/quick_start.html">AMD ROCm</a> 4.0 o superiore</li>
<li>ROCm è attualmente supportato solo su sistemi Linux.</li>
</ul>
<p>Di default il sistema di build si aspetta ROCm installato in <code>/opt/rocm</code>. Se ROCm è installato in una directory diversa, la variabile d’ambiente <code>ROCM_PATH</code> deve essere impostata sulla directory di installazione ROCm. Il sistema di build rileva automaticamente l’architettura GPU AMD. Facoltativamente, l’architettura GPU AMD può essere impostata esplicitamente con la variabile d’ambiente <code>PYTORCH_ROCM_ARCH</code> <a href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html#supported-gpus">Architettura GPU AMD</a></p>
<p>Se vuoi disabilitare il supporto ROCm, esporta la variabile d’ambiente <code>USE_ROCM=0</code>.
Altre variabili d’ambiente utili possono essere trovate in <code>setup.py</code>.</p>
<h5>Supporto GPU Intel</h5>
<p>Se vuoi compilare con supporto GPU Intel, segui queste</p>
<ul>
<li><a href="https://www.intel.com/content/www/us/en/developer/articles/tool/pytorch-prerequisites-for-intel-gpus.html">Istruzioni PyTorch Prerequisites for Intel GPUs</a>.</li>
<li>La GPU Intel è supportata per Linux e Windows.</li>
</ul>
<p>Se vuoi disabilitare il supporto GPU Intel, esporta la variabile d’ambiente <code>USE_XPU=0</code>.
Altre variabili d’ambiente utili possono essere trovate in <code>setup.py</code>.</p>
<h4>Ottenere il sorgente PyTorch</h4>
<pre><code class="language-bash">git clone https://github.com/pytorch/pytorch
cd pytorch
# se stai aggiornando una copia locale esistente
git submodule sync
git submodule update --init --recursive
</code></pre>
<h4>Installare le dipendenze</h4>
<p><strong>Comuni</strong></p>
<pre><code class="language-bash">conda install cmake ninja
# Esegui questo comando dalla directory PyTorch dopo aver clonato il codice sorgente usando la sezione “Ottenere il sorgente PyTorch” qui sopra
pip install -r requirements.txt
</code></pre>
<p><strong>Su Linux</strong></p>
<pre><code class="language-bash">pip install mkl-static mkl-include
# Solo CUDA: Aggiungi il supporto LAPACK per la GPU se necessario
# installazione magma: esegui con ambiente conda attivo. specifica la versione CUDA da installare
.ci/docker/common/install_magma_conda.sh 12.4

# (opzionale) Se utilizzi torch.compile con inductor/triton, installa la versione corrispondente di triton
# Esegui dalla directory pytorch dopo il clone
# Per il supporto GPU Intel, esporta esplicitamente `USE_XPU=1` prima di eseguire il comando.
make triton
</code></pre>
<p><strong>Su MacOS</strong></p>
<pre><code class="language-bash"># Aggiungi questo pacchetto solo su macchine con processore intel x86
pip install mkl-static mkl-include
# Aggiungi questi pacchetti se torch.distributed è necessario
conda install pkg-config libuv
</code></pre>
<p><strong>Su Windows</strong></p>
<pre><code class="language-bash">pip install mkl-static mkl-include
# Aggiungi questi pacchetti se torch.distributed è necessario.
# Il supporto del pacchetto distributed su Windows è una funzionalità prototipo e soggetta a cambiamenti.
conda install -c conda-forge libuv=1.39
</code></pre>
<h4>Installare PyTorch</h4>
<p><strong>Su Linux</strong></p>
<p>Se stai compilando per AMD ROCm esegui prima questo comando:</p>
<pre><code class="language-bash"># Esegui solo se stai compilando per ROCm
python tools/amd_build/build_amd.py
</code></pre>
<p>Installa PyTorch</p>
<pre><code class="language-bash">export CMAKE_PREFIX_PATH=&quot;${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}&quot;
python setup.py develop
</code></pre>
<p><strong>Su macOS</strong></p>
<pre><code class="language-bash">python3 setup.py develop
</code></pre>
<p><strong>Su Windows</strong></p>
<p>Se desideri compilare codice python legacy, consulta <a href="https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#building-on-legacy-code-and-cuda">Building on legacy code and CUDA</a></p>
<p><strong>Compilazioni solo CPU</strong></p>
<p>In questa modalità i calcoli PyTorch verranno eseguiti sulla CPU, non sulla GPU.</p>
<pre><code class="language-cmd">python setup.py develop
</code></pre>
<p>Nota su OpenMP: L’implementazione OpenMP preferita è Intel OpenMP (iomp). Per collegarti a iomp, devi scaricare manualmente la libreria e configurare l’ambiente di compilazione modificando <code>CMAKE_INCLUDE_PATH</code> e <code>LIB</code>. Le istruzioni <a href="https://github.com/pytorch/pytorch/blob/main/docs/source/notes/windows.rst#building-from-source">qui</a> sono un esempio per configurare sia MKL che Intel OpenMP. Senza queste configurazioni per CMake, verrà utilizzato il runtime OpenMP di Microsoft Visual C (vcomp).</p>
<p><strong>Compilazione basata su CUDA</strong></p>
<p>In questa modalità i calcoli PyTorch sfrutteranno la GPU tramite CUDA per prestazioni più rapide</p>
<p><a href="https://docs.nvidia.com/gameworks/content/gameworkslibrary/nvtx/nvidia_tools_extension_library_nvtx.htm">NVTX</a> è necessario per compilare Pytorch con CUDA.
NVTX è incluso nella distribuzione CUDA, dove è chiamato &quot;Nsight Compute&quot;. Per installarlo su una CUDA già installata, esegui nuovamente l’installazione CUDA e seleziona la relativa casella.
Assicurati che CUDA con Nsight Compute sia installato dopo Visual Studio.</p>
<p>Attualmente, VS 2017 / 2019 e Ninja sono supportati come generatori di CMake. Se <code>ninja.exe</code> è rilevato nel <code>PATH</code>, Ninja verrà usato come generatore predefinito, altrimenti si userà VS 2017 / 2019.
<br/> Se Ninja è selezionato come generatore, verrà selezionato automaticamente l’ultima versione di MSVC come toolchain.</p>
<p>Ulteriori librerie come
<a href="https://developer.nvidia.com/magma">Magma</a>, <a href="https://github.com/oneapi-src/oneDNN">oneDNN, noto anche come MKLDNN o DNNL</a>, e <a href="https://github.com/mozilla/sccache">Sccache</a> sono spesso necessarie. Consulta <a href="https://github.com/pytorch/pytorch/tree/main/.ci/pytorch/win-test-helpers/installation-helpers">installation-helper</a> per installarle.</p>
<p>Puoi consultare lo script <a href="https://github.com/pytorch/pytorch/blob/main/.ci/pytorch/win-test-helpers/build_pytorch.bat">build_pytorch.bat</a> per altre configurazioni di variabili d’ambiente</p>
<pre><code class="language-cmd">cmd

:: Imposta le variabili di ambiente dopo aver scaricato e decompresso il pacchetto mkl,
:: altrimenti CMake restituirà un errore come `Could NOT find OpenMP`.
set CMAKE_INCLUDE_PATH={Your directory}\mkl\include
set LIB={Your directory}\mkl\lib;%LIB%

:: Leggi attentamente la sezione precedente prima di procedere.
:: [Opzionale] Se vuoi sovrascrivere il toolset usato da Ninja e Visual Studio con CUDA, esegui il seguente blocco di script.
:: &quot;Visual Studio 2019 Developer Command Prompt&quot; verrà avviato automaticamente.
:: Assicurati di avere CMake &gt;= 3.12 prima di farlo quando usi il generatore Visual Studio.
set CMAKE_GENERATOR_TOOLSET_VERSION=14.27
set DISTUTILS_USE_SDK=1
for /f &quot;usebackq tokens=*&quot; %i in (`&quot;%ProgramFiles(x86)%\Microsoft Visual Studio\Installer\vswhere.exe&quot; -version [15^,17^) -products * -latest -property installationPath`) do call &quot;%i\VC\Auxiliary\Build\vcvarsall.bat&quot; x64 -vcvars_ver=%CMAKE_GENERATOR_TOOLSET_VERSION%

:: [Opzionale] Se vuoi sovrascrivere il compilatore host CUDA
set CUDAHOSTCXX=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.27.29110\bin\HostX64\x64\cl.exe

python setup.py develop

</code></pre>
<p><strong>Compilazioni GPU Intel</strong></p>
<p>In questa modalità verrà compilato PyTorch con supporto GPU Intel.</p>
<p>Assicurati che <a href="#prerequisiti">i prerequisiti comuni</a> e <a href="#supporto-gpu-intel">i prerequisiti per GPU Intel</a> siano installati correttamente e che le variabili d’ambiente siano configurate prima di iniziare la build. Per il supporto agli strumenti di build è richiesto <code>Visual Studio 2022</code>.</p>
<p>Quindi PyTorch può essere compilato con il comando:</p>
<pre><code class="language-cmd">:: Comandi CMD:
:: Imposta CMAKE_PREFIX_PATH per trovare i pacchetti corrispondenti
:: %CONDA_PREFIX% funziona solo dopo `conda activate custom_env`

if defined CMAKE_PREFIX_PATH (
    set &quot;CMAKE_PREFIX_PATH=%CONDA_PREFIX%\Library;%CMAKE_PREFIX_PATH%&quot;
) else (
    set &quot;CMAKE_PREFIX_PATH=%CONDA_PREFIX%\Library&quot;
)

python setup.py develop
</code></pre>
<h5>Regolare le opzioni di compilazione (Opzionale)</h5>
<p>Puoi regolare la configurazione delle variabili cmake opzionalmente (senza compilare prima), facendo
quanto segue. Ad esempio, regolare le directory pre-rilevate per CuDNN o BLAS può essere fatto
con questo passaggio.</p>
<p>Su Linux</p>
<pre><code class="language-bash">export CMAKE_PREFIX_PATH=&quot;${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}&quot;
python setup.py build --cmake-only
ccmake build  # o cmake-gui build
</code></pre>
<p>Su macOS</p>
<pre><code class="language-bash">export CMAKE_PREFIX_PATH=&quot;${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}&quot;
MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py build --cmake-only
ccmake build  # o cmake-gui build
</code></pre>
<h3>Immagine Docker</h3>
<h4>Utilizzo di immagini precompilate</h4>
<p>Puoi anche scaricare un’immagine docker precompilata da Docker Hub ed eseguire con docker v19.03+</p>
<pre><code class="language-bash">docker run --gpus all --rm -ti --ipc=host pytorch/pytorch:latest
</code></pre>
<p>Nota che PyTorch utilizza la memoria condivisa per condividere dati tra processi, quindi se viene utilizzato torch multiprocessing (ad esempio
per data loader multithread), la dimensione predefinita del segmento di memoria condivisa con cui il container viene eseguito non è sufficiente, e devi
aumentare la dimensione della memoria condivisa usando le opzioni da linea di comando <code>--ipc=host</code> o <code>--shm-size</code> in <code>nvidia-docker run</code>.</p>
<h4>Costruire l'immagine manualmente</h4>
<p><strong>NOTA:</strong> Deve essere costruita con una versione di docker &gt; 18.06</p>
<p>Il <code>Dockerfile</code> è fornito per costruire immagini con supporto CUDA 11.1 e cuDNN v8.
Puoi passare la variabile make <code>PYTHON_VERSION=x.y</code> per specificare la versione di Python che verrà usata da Miniconda, oppure lasciarla
non impostata per usare quella predefinita.</p>
<pre><code class="language-bash">make -f docker.Makefile
# le immagini sono taggate come docker.io/${your_docker_username}/pytorch
</code></pre>
<p>Puoi anche passare la variabile d’ambiente <code>CMAKE_VARS=&quot;...&quot;</code> per specificare ulteriori variabili CMake da passare durante la build.
Consulta <a href="./setup.py">setup.py</a> per la lista delle variabili disponibili.</p>
<pre><code class="language-bash">make -f docker.Makefile
</code></pre>
<h3>Compilare la documentazione</h3>
<p>Per compilare la documentazione in vari formati, avrai bisogno di <a href="http://www.sphinx-doc.org">Sphinx</a>
e del tema pytorch_sphinx_theme2.</p>
<p>Prima di compilare la documentazione in locale, assicurati che <code>torch</code> sia
installato nel tuo ambiente. Per piccole modifiche, puoi installare la
versione nightly come descritto in <a href="https://pytorch.org/get-started/locally/">Primi passi</a>.</p>
<p>Per modifiche più complesse, come aggiungere un nuovo modulo e docstring per
il nuovo modulo, potresti dover installare torch <a href="#dal-sorgente">dal sorgente</a>.
Consulta le <a href="https://github.com/pytorch/pytorch/wiki/Docstring-Guidelines">Linee guida Docstring</a>
per le convenzioni sui docstring.</p>
<pre><code class="language-bash">cd docs/
pip install -r requirements.txt
make html
make serve
</code></pre>
<p>Esegui <code>make</code> per ottenere una lista di tutti i formati di output disponibili.</p>
<p>Se ottieni un errore katex esegui <code>npm install katex</code>. Se persiste, prova
<code>npm install -g katex</code></p>
<blockquote>
<p>[!NOTA]
Se hai installato <code>nodejs</code> con un gestore di pacchetti diverso (ad es.
<code>conda</code>) allora <code>npm</code> probabilmente installerà una versione di <code>katex</code> non
compatibile con la tua versione di <code>nodejs</code> e la compilazione della documentazione fallirà.
Una combinazione di versioni nota per funzionare è <code>node@6.13.1</code> e
<code>katex@0.13.18</code>. Per installare quest’ultimo con <code>npm</code> puoi eseguire
<code>npm install -g katex@0.13.18</code></p>
</blockquote>
<blockquote>
<p>[!NOTA]
Se vedi un errore di incompatibilità numpy, esegui:</p>
<pre><code>pip install 'numpy&lt;2'
</code></pre>
</blockquote>
<p>Quando apporti modifiche alle dipendenze eseguite dal CI, modifica il file
<code>.ci/docker/requirements-docs.txt</code>.</p>
<h4>Compilazione di un PDF</h4>
<p>Per compilare un PDF di tutta la documentazione PyTorch, assicurati di avere
<code>texlive</code> e LaTeX installati. Su macOS, puoi installarli usando:</p>
<pre><code>brew install --cask mactex
</code></pre>
<p>Per creare il PDF:</p>
<ol>
<li><p>Esegui:</p>
<pre><code>make latexpdf
</code></pre>
<p>Questo genererà i file necessari nella directory <code>build/latex</code>.</p>
</li>
<li><p>Naviga in questa directory ed esegui:</p>
<pre><code>make LATEXOPTS=&quot;-interaction=nonstopmode&quot;
</code></pre>
<p>Questo produrrà un file <code>pytorch.pdf</code> con il contenuto desiderato. Esegui questo
comando ancora una volta per generare l’indice e la tabella dei contenuti corretti.</p>
</li>
</ol>
<blockquote>
<p>[!NOTA]
Per visualizzare la Tabella dei Contenuti, passa alla vista <strong>Table of Contents</strong>
nel tuo visualizzatore PDF.</p>
</blockquote>
<h3>Versioni precedenti</h3>
<p>Le istruzioni di installazione e i binari delle versioni precedenti di PyTorch sono disponibili
sul <a href="https://pytorch.org/get-started/previous-versions">nostro sito</a>.</p>
<h2>Primi passi</h2>
<p>Tre suggerimenti per iniziare:</p>
<ul>
<li><a href="https://pytorch.org/tutorials/">Tutorial: per iniziare a comprendere e utilizzare PyTorch</a></li>
<li><a href="https://github.com/pytorch/examples">Esempi: codice PyTorch di facile comprensione in tutti i domini</a></li>
<li><a href="https://pytorch.org/docs/">La documentazione API</a></li>
<li><a href="https://github.com/pytorch/pytorch/blob/main/GLOSSARY.md">Glossario</a></li>
</ul>
<h2>Risorse</h2>
<ul>
<li><a href="https://pytorch.org/">PyTorch.org</a></li>
<li><a href="https://pytorch.org/tutorials/">Tutorial PyTorch</a></li>
<li><a href="https://github.com/pytorch/examples">Esempi PyTorch</a></li>
<li><a href="https://pytorch.org/hub/">Modelli PyTorch</a></li>
<li><a href="https://www.udacity.com/course/deep-learning-pytorch--ud188">Intro to Deep Learning with PyTorch da Udacity</a></li>
<li><a href="https://www.udacity.com/course/intro-to-machine-learning-nanodegree--nd229">Intro to Machine Learning with PyTorch da Udacity</a></li>
<li><a href="https://www.coursera.org/learn/deep-neural-networks-with-pytorch">Deep Neural Networks with PyTorch da Coursera</a></li>
<li><a href="https://twitter.com/PyTorch">PyTorch Twitter</a></li>
<li><a href="https://pytorch.org/blog/">PyTorch Blog</a></li>
<li><a href="https://www.youtube.com/channel/UCWXI5YeOsh03QvJ59PMaXFw">PyTorch YouTube</a></li>
</ul>
<h2>Comunicazione</h2>
<ul>
<li>Forum: Discuti di implementazioni, ricerca, ecc. https://discuss.pytorch.org</li>
<li>GitHub Issues: Segnalazioni di bug, richieste di funzionalità, problemi di installazione, RFC, idee, ecc.</li>
<li>Slack: Lo <a href="https://pytorch.slack.com/">Slack PyTorch</a> ospita un pubblico principale di utenti e sviluppatori PyTorch di livello medio-avanzato per chat generali, discussioni online, collaborazione, ecc. Se sei un principiante in cerca di aiuto, il canale principale è <a href="https://discuss.pytorch.org">PyTorch Forums</a>. Se hai bisogno di un invito Slack, compila questo modulo: https://goo.gl/forms/PP1AGvNHpSaJP8to1</li>
<li>Newsletter: Nessun rumore, una newsletter unidirezionale con annunci importanti su PyTorch. Puoi iscriverti qui: https://eepurl.com/cbG0rv</li>
<li>Pagina Facebook: Annunci importanti su PyTorch. https://www.facebook.com/pytorch</li>
<li>Per le linee guida sul brand, consulta il nostro sito <a href="https://pytorch.org/">pytorch.org</a></li>
</ul>
<h2>Rilasci e contributi</h2>
<p>Tipicamente, PyTorch ha tre rilasci minori all’anno. Segnalaci eventuali bug <a href="https://github.com/pytorch/pytorch/issues">aprendo un issue</a>.</p>
<p>Apprezziamo tutti i contributi. Se hai intenzione di contribuire con bug-fix, fallo pure senza ulteriori discussioni.</p>
<p>Se vuoi contribuire con nuove funzionalità, funzioni di utilità o estensioni al core, apri prima un issue e discuti la funzionalità con noi.
Inviare una PR senza discussione potrebbe portare a un rifiuto della PR perché potremmo avere una direzione diversa da quella che immagini.</p>
<p>Per saperne di più su come contribuire a Pytorch, consulta la nostra <a href="CONTRIBUTING.md">pagina dei contributi</a>. Per ulteriori informazioni sui rilasci PyTorch, vedi la <a href="RELEASE.md">pagina dei rilasci</a>.</p>
<h2>Il team</h2>
<p>PyTorch è un progetto guidato dalla comunità con diversi ingegneri e ricercatori di talento che vi contribuiscono.</p>
<p>PyTorch è attualmente mantenuto da <a href="http://soumith.ch">Soumith Chintala</a>, <a href="https://github.com/gchanan">Gregory Chanan</a>, <a href="https://github.com/dzhulgakov">Dmytro Dzhulgakov</a>, <a href="https://github.com/ezyang">Edward Yang</a>, e <a href="https://github.com/malfet">Nikita Shulga</a> con contributi importanti da centinaia di persone di talento in varie forme e modalità.
Un elenco non esaustivo ma in crescita include: <a href="https://github.com/killeent">Trevor Killeen</a>, <a href="https://github.com/chsasank">Sasank Chilamkurthy</a>, <a href="https://github.com/szagoruyko">Sergey Zagoruyko</a>, <a href="https://github.com/adamlerer">Adam Lerer</a>, <a href="https://github.com/fmassa">Francisco Massa</a>, <a href="https://github.com/alykhantejani">Alykhan Tejani</a>, <a href="https://github.com/lantiga">Luca Antiga</a>, <a href="https://github.com/albanD">Alban Desmaison</a>, <a href="https://github.com/andreaskoepf">Andreas Koepf</a>, <a href="https://github.com/jekbradbury">James Bradbury</a>, <a href="https://github.com/ebetica">Zeming Lin</a>, <a href="https://github.com/yuandong-tian">Yuandong Tian</a>, <a href="https://github.com/glample">Guillaume Lample</a>, <a href="https://github.com/Maratyszcza">Marat Dukhan</a>, <a href="https://github.com/ngimel">Natalia Gimelshein</a>, <a href="https://github.com/csarofeen">Christian Sarofeen</a>, <a href="https://github.com/martinraison">Martin Raison</a>, <a href="https://github.com/ezyang">Edward Yang</a>, <a href="https://github.com/zdevito">Zachary Devito</a>.</p>
<p>Nota: Questo progetto non è collegato a <a href="https://github.com/hughperkins/pytorch">hughperkins/pytorch</a> con lo stesso nome. Hugh è un valido contributore della comunità Torch e ha aiutato in molti aspetti Torch e PyTorch.</p>
<h2>Licenza</h2>
<p>PyTorch ha una licenza in stile BSD, come riportato nel file <a href="LICENSE">LICENSE</a>.</p>
<hr />
<p><a href="https://github.com/OpenAiTx/OpenAiTx">Powered By OpenAiTx</a></p>
<hr />

        </div>
    </div>
    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Default Statcounter code for openaitx
https://openaitx.github.io/ -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>
    <!-- End of Statcounter Code -->
</body>
</html>