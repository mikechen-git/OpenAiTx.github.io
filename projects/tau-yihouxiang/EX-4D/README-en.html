<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>EX-4D - tau-yihouxiang/EX-4D en</title>
    <meta name="title" content="EX-4D - tau-yihouxiang/EX-4D en | EX-4D: EXtreme Viewpoint 4D Video Synthesis via Depth Watertight Mesh 📄 Paper | 🎥 Homepage | 💻 Code 🌟 Highlights 🎯 Extreme Viewpoint Synthesis: Generate hi...">
    <meta name="description" content="tau-yihouxiang/EX-4D - GitHub repository en documentation and information | EX-4D: EXtreme Viewpoint 4D Video Synthesis via Depth Watertight Mesh 📄 Paper | 🎥 Homepage | 💻 Code 🌟 Highlights 🎯 Extreme Viewpoint Synthesis: Generate hi...">
    <meta name="keywords" content="tau-yihouxiang, EX-4D, GitHub, repository, en documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/tau-yihouxiang/EX-4D/README-en.html">
    <meta property="og:title" content="EX-4D - tau-yihouxiang/EX-4D en | EX-4D: EXtreme Viewpoint 4D Video Synthesis via Depth Watertight Mesh 📄 Paper | 🎥 Homepage | 💻 Code 🌟 Highlights 🎯 Extreme Viewpoint Synthesis: Generate hi...">
    <meta property="og:description" content="tau-yihouxiang/EX-4D - GitHub repository en documentation and information | EX-4D: EXtreme Viewpoint 4D Video Synthesis via Depth Watertight Mesh 📄 Paper | 🎥 Homepage | 💻 Code 🌟 Highlights 🎯 Extreme Viewpoint Synthesis: Generate hi...">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">
    <script src="/js/marked.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        body { display: flex; flex-direction: column; min-height: 100vh; }
        .main-container { margin: 0 auto; width: 100%; max-width: 980px; padding: 0 20px; }
        @media (max-width: 768px) { .main-container { padding: 0 15px; } }
        .markdown-body img { max-width: 100%; height: auto; }
        .header { text-align: center; margin-bottom: 30px; padding: 20px; background-color: #f6f8fa; border-bottom: 1px solid #e1e4e8; position: relative; }
        .header .links { margin-top: 10px; font-size: 16px; }
        .header .links a { color: #0366d6; text-decoration: none; margin-left: 5px; }
        .header .links a:hover { text-decoration: underline; }
        .language-badges { margin-top: 15px; text-align: center; }
        .language-badges a { display: inline-block; margin: 2px; text-decoration: none; }
        .language-badges img { height: 20px; border-radius: 3px; }
        .language-badges a:hover img { opacity: 0.8; }
    </style>
</head>
<body>
    <div style="position: fixed; top: 2px; left: 2px; z-index: 2000; background: rgba(255,255,255,0.95); border-radius: 6px; box-shadow: 0 2px 8px rgba(0,0,0,0.06); padding: 4px 14px; font-size: 15px; color: #222; font-family: 'Segoe UI', Arial, sans-serif; font-weight: 500; letter-spacing: 0.5px;">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" style="color: #0366d6; text-decoration: none; font-weight: 600;">Open AI Tx</a>
    </div>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/tau-yihouxiang/EX-4D" id="githubRepoLink" target="_blank">tau-yihouxiang/EX-4D</a>
<h1 style="display: none;">EX-4D: EXtreme Viewpoint 4D Video Synthesis via Depth Watertight Mesh 📄 Paper | 🎥 Homepage | 💻 Code 🌟 Highlights 🎯 Extreme Viewpoint Synthesis: Generate hi...</h1>
        </div>
        <div class="language-badges" id="languageBadges">
            <!-- You can generate language badges here if needed -->
        </div>
    </div>
    <div class="main-container">
        <div class="markdown-body" id="content">
            <h1>EX-4D: EXtreme Viewpoint 4D Video Synthesis via Depth Watertight Mesh</h1>
<div align="center">
<img src="https://raw.githubusercontent.com/tau-yihouxiang/EX-4D/main/docs/Logo.png" alt="EX-4D Logo" width="250">
<p><a href="https://arxiv.org/abs/2506.05554">📄 Paper</a>  |  <a href="https://tau-yihouxiang.github.io/projects/EX-4D/EX-4D.html">🎥 Homepage</a>  |  <a href="https://github.com/tau-yihouxiang/EX-4D">💻 Code</a></p>
</div>
<h2>🌟 Highlights</h2>
<ul>
<li><strong>🎯 Extreme Viewpoint Synthesis</strong>: Generate high-quality 4D videos with camera movements ranging from -90° to 90°</li>
<li><strong>🔧 Depth Watertight Mesh</strong>: Novel geometric representation that models both visible and occluded regions</li>
<li><strong>⚡ Lightweight Architecture</strong>: Only 1% trainable parameters (140M) of the 14B video diffusion backbone</li>
<li><strong>🎭 No Multi-view Training</strong>: Innovative masking strategy eliminates the need for expensive multi-view datasets</li>
<li><strong>🏆 State-of-the-art Performance</strong>: Outperforms existing methods, especially on extreme camera angles</li>
</ul>
<h2>🎬 Demo Results</h2>
<div align="center">
<img src="https://raw.githubusercontent.com/tau-yihouxiang/EX-4D/main/docs/teaser.png" alt="EX-4D Demo Results" width="800">
</div>
<p><em>EX-4D transforms monocular videos into camera-controllable 4D experiences with physically consistent results under extreme viewpoints.</em></p>
<h2>🏗️ Framework Overview</h2>
<div align="center">
<img src="https://raw.githubusercontent.com/tau-yihouxiang/EX-4D/main/docs/overview.png" alt="EX-4D Architecture">
</div>
<p>Our framework consists of three key components:</p>
<ol>
<li><strong>🔺 Depth Watertight Mesh Construction</strong>: Creates a robust geometric prior that explicitly models both visible and occluded regions</li>
<li><strong>🎭 Simulated Masking Strategy</strong>: Generates effective training data from monocular videos without multi-view datasets</li>
<li><strong>⚙️ Lightweight LoRA Adapter</strong>: Efficiently integrates geometric information with pre-trained video diffusion models</li>
</ol>
<h2>🚀 Quick Start</h2>
<h3>Installation</h3>
<pre><code class="language-bash"># Clone the repository
git clone https://github.com/tau-yihouxiang/EX-4D.git
cd EX-4D

# Create conda environment
conda create -n ex4d python=3.10
conda activate ex4d
# Install PyTorch (2.x recommended)
pip install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu124
# Install Nvdiffrast
pip install git+https://github.com/NVlabs/nvdiffrast.git
# Install dependencies and diffsynth
pip install -e .
# Install depthcrafter for depth estimation. (Follow DepthCrafter's installing instruction for checkpoints preparation.)
git clone https://github.com/Tencent/DepthCrafter.git
</code></pre>
<h3>Download Pretrained Model</h3>
<pre><code class="language-bash">huggingface-cli download Wan-AI/Wan2.1-I2V-14B-480P --local-dir ./models/Wan-AI
huggingface-cli download yihouxiang/EX-4D --local-dir ./models/EX-4D
</code></pre>
<h3>Example Usage</h3>
<h4>1. DW-Mesh Reconstruction</h4>
<pre><code class="language-bash"># --cam 180 (30 / 60 / 90 / zoom_in / zoom_out )
python recon.py --input_video examples/flower/input.mp4 --cam 180 --output_dir outputs/flower --save_mesh
</code></pre>
<h4>2. EX-4D Generation (48GB VRAM required)</h4>
<pre><code class="language-bash">python generate.py --color_video outputs/flower/color_180.mp4 --mask_video outputs/flower/mask_180.mp4 --output_video outputs/flower/output.mp4
</code></pre>
<table>
<tr>
<td width="45%" align="center">
<img src="https://raw.githubusercontent.com/tau-yihouxiang/EX-4D/main/examples/flower/input.gif" width="100%">
<br><b>Input Video</b>
</td>
<td align="center">
<div style="font-size: 2em; color: #4A90E2; padding: 0 0px;">
  ➜
</div>
</td>
<td width="45%" align="center">
<img src="https://raw.githubusercontent.com/tau-yihouxiang/EX-4D/main/examples/flower/output.gif" width="100%">
<br><b>Output Video</b>
</td>
</tr> 
</table>
<!-- ## 📊 Performance

### Quantitative Results
| Method | FID (Extreme) ↓ | FVD (Extreme) ↓ | VBench Score ↑ |
|--------|-----------------|-----------------|----------------|
| ReCamMaster | 64.68 | 943.45 | 0.434 |
| TrajectoryCrafter | 65.33 | 893.80 | 0.447 |
| TrajectoryAttention | 62.49 | 912.14 | 0.389 |
| **EX-4D (Ours)** | **55.42** | **823.61** | **0.450** | -->
<h3>User Study Results</h3>
<ul>
<li><strong>70.7%</strong> of participants preferred EX-4D over baseline methods</li>
<li>Superior performance in physical consistency and extreme viewpoint quality</li>
<li>Significant improvement as camera angles become more extreme</li>
</ul>
<h2>🎯 Applications</h2>
<ul>
<li><strong>🎮 Gaming</strong>: Create immersive 3D game cinematics from 2D footage</li>
<li><strong>🎬 Film Production</strong>: Generate novel camera angles for post-production</li>
<li><strong>🥽 VR/AR</strong>: Create free-viewpoint video experiences</li>
<li><strong>📱 Social Media</strong>: Generate dynamic camera movements for content creation</li>
<li><strong>🏢 Architecture</strong>: Visualize spaces from multiple viewpoints</li>
</ul>
<!-- ## 📈 Benchmarks -->
<!-- ### Viewpoint Range Evaluation

| Range | Small (0°→30°) | Large (0°→60°) | Extreme (0°→90°) | Full (-90°→90°) |
|-------|----------------|----------------|------------------|-----------------|
| FID Score | 44.19 | 50.30 | 55.42 | - |
| Performance Gap | +9.1% better | +8.9% better | +11.3% better | +15.5% better | -->
<!-- *Performance gap compared to the second-best method in each category.* -->
<h2>⚠️ Limitations</h2>
<ul>
<li><strong>Depth Dependency</strong>: Performance relies on monocular depth estimation quality</li>
<li><strong>Computational Cost</strong>: Requires significant computation for high-resolution videos</li>
<li><strong>Reflective Surfaces</strong>: Challenges with reflective or transparent materials</li>
</ul>
<h2>🔮 Future Work</h2>
<ul>
<li>[ ] Real-time inference optimization (3DGS / 4DGS)</li>
<li>[ ] Support for higher resolutions (1K, 2K)</li>
<li>[ ] Neural mesh refinement techniques</li>
</ul>
<h2>🙏 Acknowledgments</h2>
<p>We would like to thank the <a href="https://github.com/modelscope/DiffSynth-Studio/tree/v1.1.1">DiffSynth-Studio v1.1.1</a> project for providing the foundational diffusion framework.</p>
<h2>📚 Citation</h2>
<p>If you find our work useful, please consider citing:</p>
<pre><code class="language-bibtex">@misc{hu2025ex4dextremeviewpoint4d,
      title={EX-4D: EXtreme Viewpoint 4D Video Synthesis via Depth Watertight Mesh}, 
      author={Tao Hu and Haoyang Peng and Xiao Liu and Yuewen Ma},
      year={2025},
      eprint={2506.05554},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2506.05554}, 
}
</code></pre>
<hr />
<p>Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx">Open Ai Tx</a> | Last indexed: 2025-07-08</p>
<hr />

        </div>
    </div>
    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Default Statcounter code for openaitx
https://openaitx.github.io/ -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>
    <!-- End of Statcounter Code -->
</body>
</html>