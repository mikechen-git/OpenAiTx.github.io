<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>c4-genai-suite - English Documentation</title>
    <meta name="description" content="Read c4-genai-suite documentation in English. This project has 123 stars on GitHub.">
    <meta name="keywords" content="c4-genai-suite, English, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "c4-genai-suite",
  "description": "Documentation for c4-genai-suite in English",
  "author": {
    "@type": "Person",
    "name": "codecentric"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 123
  },
  "url": "https://OpenAiTx.github.io/projects/codecentric/c4-genai-suite/README-en.html",
  "sameAs": "https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/README.md",
  "datePublished": "2025-07-09",
  "dateModified": "2025-07-09"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            color: #24292e;
            margin-bottom: 8px;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">c4-genai-suite</h1>
            <div class="project-meta">
                <span class="stars">⭐ 123 stars</span>
                <span class="language">English</span>
                <span>by codecentric</span>
            </div>
        </div>
        
        <div class="content">
            <h1>c4 GenAI Suite</h1></p><p>An AI chatbot application with Model Context Provider (MCP) integration, powered by Langchain and compatibility for all major Large Language Models (LLMs) and Embedding Models.</p><p>Administrators can create assistants with different capabilities by adding extensions, such as RAG (Retrieval-Augmented Generation) services or MCP servers. The application is built using a modern tech stack, including React, NestJS, and Python FastAPI for the REI-S service.</p><p>Users can interact with assistants through a user-friendly interface. Depending on the assistant's configuration, users may be able to ask questions, upload their own files, or use other features. The assistants interact with various LLM providers to provide responses based on the configured extensions. Contextual information provided by the configured extensions allows the assistants to answer domain-specific questions and provide relevant information.</p><p>The application is designed to be modular and extensible, allowing users to create assistants with different capabilities by adding extensions.</p><p><img src="https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/preview.webp" alt="short demo video of basic usage"></p><h2>Features</h2></p><h3>Large Language Models (LLM) and Multimodal Models</h3></p><p>The c4 GenAI Suite already supports many models directly. And if your preferred model is not supported yet, it should be easy to write an extension to support it.</p><ul><li>OpenAI compatible models</li>
<li>Azure OpenAI models</li>
<li>Bedrock models</li>
<li>Google GenAI models</li>
<li>Ollama compatible models</li>
</ul><h3>Retrieval Augmented Generation (RAG)</h3></p><p>The c4 GenAI Suite includes REI-S, a server to prepare files for consumption by the LLM.</p><ul><li>REI-S, a custom integrated RAG server</li>
  <li>Vector stores</li>
    <li>pgvector</li>
    <li>Azure AI Search</li>
  <li>Embedding models</li>
    <li>OpenAI compatible embeddings</li>
    <li>Azure OpenAI embeddings</li>
    <li>Ollama compatible embeddings</li>
  <li>File formats:</li>
    <li>pdf, docx, pptx, xlsx, ...</li>
    <li>audio file voice transcription (via Whisper)</li></p><p></ul><h3>Extensions</h3></p><p>The c4 GenAI Suite is designed for extensibility. Writing extensions is easy, as is using an already existing MCP server.</p><ul><li>Model Context Protocol (MCP) servers</li>
<li>Custom system prompt</li>
<li>Bing Search</li>
<li>Calculator</li>
</ul><h2>Getting Started</h2></p><h3>Using Docker-Compose</h3></p><ul><li>Run <code>docker compose up</code> in the project root.</li>
<li>Open the <a href="http://localhost:3333" target="_blank" rel="noopener noreferrer">application</a> in a browser. The default login credentials are user <code>admin@example.com</code> and password <code>secret</code>.</li></p><p></ul><img src="https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/assistants.webp" alt="video showing assistant configuration"></p><h3>Using Helm & Kubernetes</h3></p><p>For deployment in Kubernetes environments, please refer to the <a href="https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/./helm-chart/README.md" target="_blank" rel="noopener noreferrer">README of our Helm Chart</a>.</p><h3>Setting up Assistants and Extensions</h3></p><p>The c4 GenAI Suite revolves around <em>assistants</em>.
Each assistant consists of a set of extensions, which determine the LLM model and which tools it can use.</p><ul><li>In the admin area (click the username on the bottom left), go to the <a href="http://localhost:3333/admin/assistants" target="_blank" rel="noopener noreferrer">assistants section</a>.</li>
<li>Add an assistant with the green <code>+</code> button next to the section title. Choose a name and a description.</li>
<li>Select the created assistant and click the green <code>+ Add Extension</code>.</li>
<li>Select the model and fill in the credentials.</li>
<li>Use the <code>Test</code> Button to check that it works and <code>save</code>.</li></p><p></ul>Now you can return to the <a href="http://localhost:3333/chat" target="_blank" rel="noopener noreferrer">chat page</a> (click on <code>c4 GenAI Suite</code> in the top left) and start a new conversation with your new assistant.</p><blockquote>[!TIP]</blockquote>
<blockquote>Our <code>docker-compose</code> includes a local Ollama, which runs on the CPU. You can use this for quick testing. But it will be slow and you probably want to use another model. If you want to use it, just create the following model extension in your Assistant.</blockquote>
<blockquote>* Extension: <code>Dev: Ollama</code></blockquote>
<blockquote>* Endpoint: <code>http://ollama:11434</code></blockquote>
<blockquote>* Model: <code>llama3.2</code></blockquote></p><h3>Model Context Protocol (MCP) [optional]</h3></p><p>Use any MCP server offering an <code>sse</code> interface with the <code>MCP Tools</code> Extension (or use our <code>mcp-tool-as-server</code> as a proxy in front of an <code>stdio</code> MCP server).
Each MCP server can be configured in detail as an extension.</p><h3>Retrieval Augmented Generation (RAG) / File Search [optional]</h3></p><p>Use our RAG server <code>REI-S</code> to search user provided files. Just configure a <code>Search Files</code> extension for the assistant.
This process is described in detail in <a href="services/reis/#example-configuration-in-c4" target="_blank" rel="noopener noreferrer">the <code>services/reis</code> subdirectory</a>.</p><h2>Contributing & Development</h2></p><ul><li>See <a href="https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/CONTRIBUTING.md" target="_blank" rel="noopener noreferrer">CONTRIBUTING.md</a> for guidelines on how to contribute.</li>
<li>For developer onboarding, check <a href="https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/DEVELOPERS.md" target="_blank" rel="noopener noreferrer">DEVELOPERS.md</a>.</li></p><p></ul><h2>Main Building Blocks</h2></p><p>The application consists of a <strong>Frontend</strong> , a <strong>Backend</strong>  and a <strong>REI-S</strong>  service.</p><pre><code class="language-">┌──────────┐
│   User   │
└─────┬────┘
      │ access
      ▼
┌──────────┐
│ Frontend │
└─────┬────┘
      │ access
      ▼
┌──────────┐     ┌─────────────────┐
│ Backend  │────►│      LLM        │
└─────┬────┘     └─────────────────┘
      │ access
      ▼
┌──────────┐     ┌─────────────────┐
│  REI-S   │────►│ Embedding Model │
│          │     └─────────────────┘
│          │
│          │     ┌─────────────────┐
│          │────►│  Vector Store   │
└──────────┘     └─────────────────┘</code></pre>
<h3>Frontend</h3></p><p>The frontend is built with React and TypeScript, providing a user-friendly interface for interacting with the backend and REI-S service. It includes features for managing assistants, extensions, and chat functionalities.</p><blockquote>Sources: <code>/frontend</code></blockquote></p><h3>Backend</h3></p><p>The backend is developed using NestJS and TypeScript, serving as the main API layer for the application. It handles requests from the frontend and interacts with llm providers to facilitate chat functionalities. The backend also manages assistants and their extensions, allowing users to configure and use various AI models for their chats.</p><p>Additionally, the backend manages user authentication, and communicates with the REI-S service for file indexing and retrieval.</p><p>For data persistence, the backend uses a <strong>PostgreSQL</strong> database.</p><blockquote>Sources: <code>/backend</code></blockquote></p><h3>REI-S</h3></p><p>The REI-S (<strong>R</strong>etrieval <strong>E</strong>xtraction <strong>I</strong>ngestion <strong>S</strong>erver) is a Python-based server that provides basic RAG (Retrieval-Augmented Generation) capabilities. It allows for file content extraction, indexing and querying, enabling the application to handle large datasets efficiently. The REI-S service is designed to work seamlessly with the backend, providing necessary data for chat functionalities and file searches.</p><p>The REI-S supports Azure AI Search and pgvector for vector storage, allowing for flexible and scalable data retrieval options. The service can be configured using environment variables to specify the type of vector store and connection details.</p><blockquote>Sources: <code>/services/reis</code></blockquote>

---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-07-09

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-07-09 
    </div>
    
</body>
</html>