<!DOCTYPE html>
<html lang="it">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>c4-genai-suite - Italian Documentation</title>
    <meta name="description" content="Read c4-genai-suite documentation in Italian. This project has 123 stars on GitHub.">
    <meta name="keywords" content="c4-genai-suite, Italian, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "c4-genai-suite",
  "description": "Documentation for c4-genai-suite in Italian",
  "author": {
    "@type": "Person",
    "name": "codecentric"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 123
  },
  "url": "https://OpenAiTx.github.io/projects/codecentric/c4-genai-suite/README-it.html",
  "sameAs": "https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/README.md",
  "datePublished": "2025-07-09",
  "dateModified": "2025-07-09"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            color: #24292e;
            margin-bottom: 8px;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">c4-genai-suite</h1>
            <div class="project-meta">
                <span class="stars">⭐ 123 stars</span>
                <span class="language">Italian</span>
                <span>by codecentric</span>
            </div>
        </div>
        
        <div class="content">
            <h1>c4 GenAI Suite</h1></p><p>Un'applicazione chatbot AI con integrazione Model Context Provider (MCP), alimentata da Langchain e compatibile con tutti i principali Large Language Models (LLM) e modelli di Embedding.</p><p>Gli amministratori possono creare assistenti con diverse capacità aggiungendo estensioni, come servizi RAG (Retrieval-Augmented Generation) o server MCP. L'applicazione è costruita utilizzando uno stack tecnologico moderno, tra cui React, NestJS e Python FastAPI per il servizio REI-S.</p><p>Gli utenti possono interagire con gli assistenti tramite un'interfaccia intuitiva. A seconda della configurazione dell'assistente, gli utenti possono porre domande, caricare i propri file o utilizzare altre funzionalità. Gli assistenti interagiscono con vari provider LLM per fornire risposte basate sulle estensioni configurate. Le informazioni contestuali fornite dalle estensioni configurate consentono agli assistenti di rispondere a domande specifiche del dominio e fornire informazioni rilevanti.</p><p>L'applicazione è progettata per essere modulare ed estensibile, consentendo agli utenti di creare assistenti con diverse capacità aggiungendo estensioni.</p><p><img src="https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/preview.webp" alt="short demo video of basic usage"></p><h2>Funzionalità</h2></p><h3>Large Language Models (LLM) e modelli multimodali</h3></p><p>La c4 GenAI Suite supporta già molti modelli direttamente. E se il modello preferito non è ancora supportato, dovrebbe essere semplice scrivere un'estensione per aggiungere il supporto.</p><ul><li>Modelli compatibili con OpenAI</li>
<li>Modelli Azure OpenAI</li>
<li>Modelli Bedrock</li>
<li>Modelli Google GenAI</li>
<li>Modelli compatibili con Ollama</li>
</ul><h3>Retrieval Augmented Generation (RAG)</h3></p><p>La c4 GenAI Suite include REI-S, un server per preparare i file per il consumo da parte dell’LLM.</p><ul><li>REI-S, un server RAG integrato personalizzato</li>
  <li>Vector stores</li>
    <li>pgvector</li>
    <li>Azure AI Search</li>
  <li>Modelli di embedding</li>
    <li>Embedding compatibili con OpenAI</li>
    <li>Embedding Azure OpenAI</li>
    <li>Embedding compatibili con Ollama</li>
  <li>Formati di file:</li>
    <li>pdf, docx, pptx, xlsx, ...</li>
    <li>trascrizione vocale di file audio (tramite Whisper)</li></p><p></ul><h3>Estensioni</h3></p><p>La c4 GenAI Suite è progettata per l’estensibilità. Scrivere estensioni è semplice, così come utilizzare un server MCP già esistente.</p><ul><li>Server Model Context Protocol (MCP)</li>
<li>Systemprompt personalizzato</li>
<li>Bing Search</li>
<li>Calcolatrice</li>
</ul><h2>Per Iniziare</h2></p><h3>Utilizzo di Docker-Compose</h3></p><ul><li>Esegui <code>docker compose up</code> nella directory principale del progetto.</li>
<li>Apri l'<a href="http://localhost:3333" target="_blank" rel="noopener noreferrer">applicazione</a> in un browser. Le credenziali di accesso predefinite sono utente <code>admin@example.com</code> e password <code>secret</code>.</li></p><p></ul><img src="https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/assistants.webp" alt="video che mostra la configurazione dell'assistente"></p><h3>Utilizzo di Helm & Kubernetes</h3></p><p>Per il deployment in ambienti Kubernetes, fare riferimento al <a href="https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/./helm-chart/README.md" target="_blank" rel="noopener noreferrer">README del nostro Helm Chart</a>.</p><h3>Configurazione di Assistenti ed Estensioni</h3></p><p>La c4 GenAI Suite ruota intorno agli <em>assistenti</em>.
Ogni assistente è composto da un set di estensioni, che determinano il modello LLM e quali strumenti può utilizzare.</p><ul><li>Nell'area amministrativa (clicca sul nome utente in basso a sinistra), vai alla <a href="http://localhost:3333/admin/assistants" target="_blank" rel="noopener noreferrer">sezione assistenti</a>.</li>
<li>Aggiungi un assistente con il pulsante verde <code>+</code> accanto al titolo della sezione. Scegli un nome e una descrizione.</li>
<li>Seleziona l'assistente creato e clicca sul verde <code>+ Aggiungi Estensione</code>.</li>
<li>Seleziona il modello e inserisci le credenziali.</li>
<li>Utilizza il pulsante <code>Test</code> per verificare che funzioni e <code>salva</code>.</li></p><p></ul>Ora puoi tornare alla <a href="http://localhost:3333/chat" target="_blank" rel="noopener noreferrer">pagina chat</a> (clicca su <code>c4 GenAI Suite</code> in alto a sinistra) e iniziare una nuova conversazione con il tuo nuovo assistente.</p><blockquote>[!TIP]</blockquote>
<blockquote>Il nostro <code>docker-compose</code> include un Ollama locale, che gira sulla CPU. Puoi usarlo per test veloci. Tuttavia sarà lento e probabilmente vorrai usare un altro modello. Se vuoi usarlo, crea semplicemente la seguente estensione modello nel tuo Assistente.</blockquote>
<blockquote>* Estensione: <code>Dev: Ollama</code></blockquote>
<blockquote>* Endpoint: <code>http://ollama:11434</code></blockquote>
<blockquote>* Modello: <code>llama3.2</code></blockquote></p><h3>Protocollo Model Context (MCP) [opzionale]</h3></p><p>Utilizza qualsiasi server MCP che offra un'interfaccia <code>sse</code> con l'Estensione <code>MCP Tools</code> (oppure utilizza il nostro <code>mcp-tool-as-server</code> come proxy davanti a un server MCP <code>stdio</code>).
Ogni server MCP può essere configurato in dettaglio come estensione.</p><h3>Retrieval Augmented Generation (RAG) / Ricerca File [opzionale]</h3></p><p>Utilizza il nostro server RAG <code>REI-S</code> per cercare nei file forniti dall’utente. È sufficiente configurare un’estensione <code>Search Files</code> per l’assistente.
Questo processo è descritto in dettaglio nella <a href="services/reis/#example-configuration-in-c4" target="_blank" rel="noopener noreferrer">sottodirectory <code>services/reis</code></a>.</p><h2>Contribuire & Sviluppo</h2></p><ul><li>Consulta <a href="https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/CONTRIBUTING.md" target="_blank" rel="noopener noreferrer">CONTRIBUTING.md</a> per le linee guida su come contribuire.</li>
<li>Per l’onboarding degli sviluppatori, consulta <a href="https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/DEVELOPERS.md" target="_blank" rel="noopener noreferrer">DEVELOPERS.md</a>.</li></p><p></ul><h2>Principali Componenti</h2></p><p>L’applicazione è composta da un <strong>Frontend</strong>, un <strong>Backend</strong> e un servizio <strong>REI-S</strong>.</p><pre><code class="language-">┌──────────┐
│   Utente │
└─────┬────┘
      │ accesso
      ▼
┌──────────┐
│ Frontend │
└─────┬────┘
      │ accesso
      ▼
┌──────────┐     ┌──────────────────┐
│ Backend  │────►│      LLM         │
└─────┬────┘     └──────────────────┘
      │ accesso
      ▼
┌──────────┐     ┌──────────────────┐
│  REI-S   │────►│ Modello di       │
│          │     │ Embedding        │
│          │     └──────────────────┘
│          │
│          │     ┌──────────────────┐
│          │────►│ Vector Store     │
└──────────┘     └──────────────────┘</code></pre>
<h3>Frontend</h3></p><p>Il frontend è sviluppato con React e TypeScript, offrendo un'interfaccia utente intuitiva per interagire con il backend e il servizio REI-S. Include funzionalità per la gestione degli assistenti, delle estensioni e delle funzionalità di chat.</p><blockquote>Fonti: <code>/frontend</code></blockquote></p><h3>Backend</h3></p><p>Il backend è sviluppato utilizzando NestJS e TypeScript, fungendo da principale livello API per l'applicazione. Gestisce le richieste provenienti dal frontend e interagisce con i provider llm per facilitare le funzionalità di chat. Il backend gestisce inoltre assistenti e le loro estensioni, permettendo agli utenti di configurare e utilizzare vari modelli AI per le loro chat.</p><p>Inoltre, il backend gestisce l'autenticazione degli utenti e comunica con il servizio REI-S per l'indicizzazione e il recupero dei file.</p><p>Per la persistenza dei dati, il backend utilizza un database <strong>PostgreSQL</strong>.</p><blockquote>Fonti: <code>/backend</code></blockquote></p><h3>REI-S</h3></p><p>Il REI-S (<strong>R</strong>etrieval <strong>E</strong>xtraction <strong>I</strong>ngestion <strong>S</strong>erver) è un server basato su Python che fornisce funzionalità di base RAG (Retrieval-Augmented Generation). Consente l’estrazione, l’indicizzazione e l’interrogazione dei contenuti dei file, permettendo all’applicazione di gestire grandi dataset in modo efficiente. Il servizio REI-S è progettato per funzionare in modo integrato con il backend, fornendo i dati necessari per le funzionalità di chat e la ricerca dei file.</p><p>Il REI-S supporta Azure AI Search e pgvector per l’archiviazione vettoriale, offrendo opzioni di recupero dati flessibili e scalabili. Il servizio può essere configurato tramite variabili d'ambiente per specificare il tipo di archivio vettoriale e i dettagli di connessione.</p><blockquote>Fonti: <code>/services/reis</code></blockquote>

---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-07-09

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-07-09 
    </div>
    
</body>
</html>