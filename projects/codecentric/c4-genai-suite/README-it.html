<!DOCTYPE html>
<html lang="it">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>c4-genai-suite - codecentric/c4-genai-suite it</title>
    <meta name="title" content="c4-genai-suite - codecentric/c4-genai-suite it | c4 GenAI Suite Un'applicazione chatbot AI con integrazione Model Context Provider (MCP), alimentata da Langchain e compatibile con tutti i principali Large Lang...">
    <meta name="description" content="codecentric/c4-genai-suite - GitHub repository it documentation and information | c4 GenAI Suite Un'applicazione chatbot AI con integrazione Model Context Provider (MCP), alimentata da Langchain e compatibile con tutti i principali Large Lang...">
    <meta name="keywords" content="codecentric, c4-genai-suite, GitHub, repository, it documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/codecentric/c4-genai-suite/README-it.html">
    <meta property="og:title" content="c4-genai-suite - codecentric/c4-genai-suite it | c4 GenAI Suite Un'applicazione chatbot AI con integrazione Model Context Provider (MCP), alimentata da Langchain e compatibile con tutti i principali Large Lang...">
    <meta property="og:description" content="codecentric/c4-genai-suite - GitHub repository it documentation and information | c4 GenAI Suite Un'applicazione chatbot AI con integrazione Model Context Provider (MCP), alimentata da Langchain e compatibile con tutti i principali Large Lang...">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">
    <script src="/js/marked.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        body { display: flex; flex-direction: column; min-height: 100vh; }
        .main-container { margin: 0 auto; width: 100%; max-width: 980px; padding: 0 20px; }
        @media (max-width: 768px) { .main-container { padding: 0 15px; } }
        .markdown-body img { max-width: 100%; height: auto; }
        .header { text-align: center; margin-bottom: 30px; padding: 20px; background-color: #f6f8fa; border-bottom: 1px solid #e1e4e8; position: relative; }
        .header .links { margin-top: 10px; font-size: 16px; }
        .header .links a { color: #0366d6; text-decoration: none; margin-left: 5px; }
        .header .links a:hover { text-decoration: underline; }
        .language-badges { margin-top: 15px; text-align: center; }
        .language-badges a { display: inline-block; margin: 2px; text-decoration: none; }
        .language-badges img { height: 20px; border-radius: 3px; }
        .language-badges a:hover img { opacity: 0.8; }
    </style>
</head>
<body>
    <div style="position: fixed; top: 2px; left: 2px; z-index: 2000; background: rgba(255,255,255,0.95); border-radius: 6px; box-shadow: 0 2px 8px rgba(0,0,0,0.06); padding: 4px 14px; font-size: 15px; color: #222; font-family: 'Segoe UI', Arial, sans-serif; font-weight: 500; letter-spacing: 0.5px;">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" style="color: #0366d6; text-decoration: none; font-weight: 600;">Open AI Tx</a>
    </div>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/codecentric/c4-genai-suite" id="githubRepoLink" target="_blank">codecentric/c4-genai-suite</a>
<h1 style="display: none;">c4 GenAI Suite Un'applicazione chatbot AI con integrazione Model Context Provider (MCP), alimentata da Langchain e compatibile con tutti i principali Large Lang...</h1>
        </div>
        <div class="language-badges" id="languageBadges">
            <!-- You can generate language badges here if needed -->
        </div>
    </div>
    <div class="main-container">
        <div class="markdown-body" id="content">
            <h1>c4 GenAI Suite</h1>
<p>Un'applicazione chatbot AI con integrazione Model Context Provider (MCP), alimentata da Langchain e compatibile con tutti i principali Large Language Models (LLM) e modelli di Embedding.</p>
<p>Gli amministratori possono creare assistenti con diverse capacità aggiungendo estensioni, come servizi RAG (Retrieval-Augmented Generation) o server MCP. L'applicazione è costruita utilizzando uno stack tecnologico moderno, tra cui React, NestJS e Python FastAPI per il servizio REI-S.</p>
<p>Gli utenti possono interagire con gli assistenti tramite un'interfaccia intuitiva. A seconda della configurazione dell'assistente, gli utenti possono porre domande, caricare i propri file o utilizzare altre funzionalità. Gli assistenti interagiscono con vari provider LLM per fornire risposte basate sulle estensioni configurate. Le informazioni contestuali fornite dalle estensioni configurate consentono agli assistenti di rispondere a domande specifiche del dominio e fornire informazioni rilevanti.</p>
<p>L'applicazione è progettata per essere modulare ed estensibile, consentendo agli utenti di creare assistenti con diverse capacità aggiungendo estensioni.</p>
<p><img src="https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/preview.webp" alt="short demo video of basic usage" /></p>
<h2>Funzionalità</h2>
<h3>Large Language Models (LLM) e modelli multimodali</h3>
<p>La c4 GenAI Suite supporta già molti modelli direttamente. E se il modello preferito non è ancora supportato, dovrebbe essere semplice scrivere un'estensione per aggiungere il supporto.</p>
<ul>
<li>Modelli compatibili con OpenAI</li>
<li>Modelli Azure OpenAI</li>
<li>Modelli Bedrock</li>
<li>Modelli Google GenAI</li>
<li>Modelli compatibili con Ollama</li>
</ul>
<h3>Retrieval Augmented Generation (RAG)</h3>
<p>La c4 GenAI Suite include REI-S, un server per preparare i file per il consumo da parte dell’LLM.</p>
<ul>
<li>REI-S, un server RAG integrato personalizzato
<ul>
<li>Vector stores
<ul>
<li>pgvector</li>
<li>Azure AI Search</li>
</ul>
</li>
<li>Modelli di embedding
<ul>
<li>Embedding compatibili con OpenAI</li>
<li>Embedding Azure OpenAI</li>
<li>Embedding compatibili con Ollama</li>
</ul>
</li>
<li>Formati di file:
<ul>
<li>pdf, docx, pptx, xlsx, ...</li>
<li>trascrizione vocale di file audio (tramite Whisper)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3>Estensioni</h3>
<p>La c4 GenAI Suite è progettata per l’estensibilità. Scrivere estensioni è semplice, così come utilizzare un server MCP già esistente.</p>
<ul>
<li>Server Model Context Protocol (MCP)</li>
<li>Systemprompt personalizzato</li>
<li>Bing Search</li>
<li>Calcolatrice</li>
</ul>
<h2>Per Iniziare</h2>
<h3>Utilizzo di Docker-Compose</h3>
<ul>
<li>Esegui <code>docker compose up</code> nella directory principale del progetto.</li>
<li>Apri l'<a href="http://localhost:3333">applicazione</a> in un browser. Le credenziali di accesso predefinite sono utente <code>admin@example.com</code> e password <code>secret</code>.</li>
</ul>
<p><img src="https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/assistants.webp" alt="video che mostra la configurazione dell'assistente" /></p>
<h3>Utilizzo di Helm &amp; Kubernetes</h3>
<p>Per il deployment in ambienti Kubernetes, fare riferimento al <a href="https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/./helm-chart/README.md">README del nostro Helm Chart</a>.</p>
<h3>Configurazione di Assistenti ed Estensioni</h3>
<p>La c4 GenAI Suite ruota intorno agli <em>assistenti</em>.
Ogni assistente è composto da un set di estensioni, che determinano il modello LLM e quali strumenti può utilizzare.</p>
<ul>
<li>Nell'area amministrativa (clicca sul nome utente in basso a sinistra), vai alla <a href="http://localhost:3333/admin/assistants">sezione assistenti</a>.</li>
<li>Aggiungi un assistente con il pulsante verde <code>+</code> accanto al titolo della sezione. Scegli un nome e una descrizione.</li>
<li>Seleziona l'assistente creato e clicca sul verde <code>+ Aggiungi Estensione</code>.</li>
<li>Seleziona il modello e inserisci le credenziali.</li>
<li>Utilizza il pulsante <code>Test</code> per verificare che funzioni e <code>salva</code>.</li>
</ul>
<p>Ora puoi tornare alla <a href="http://localhost:3333/chat">pagina chat</a> (clicca su <code>c4 GenAI Suite</code> in alto a sinistra) e iniziare una nuova conversazione con il tuo nuovo assistente.</p>
<blockquote>
<p>[!TIP]
Il nostro <code>docker-compose</code> include un Ollama locale, che gira sulla CPU. Puoi usarlo per test veloci. Tuttavia sarà lento e probabilmente vorrai usare un altro modello. Se vuoi usarlo, crea semplicemente la seguente estensione modello nel tuo Assistente.</p>
<ul>
<li>Estensione: <code>Dev: Ollama</code></li>
<li>Endpoint: <code>http://ollama:11434</code></li>
<li>Modello: <code>llama3.2</code></li>
</ul>
</blockquote>
<h3>Protocollo Model Context (MCP) [opzionale]</h3>
<p>Utilizza qualsiasi server MCP che offra un'interfaccia <code>sse</code> con l'Estensione <code>MCP Tools</code> (oppure utilizza il nostro <code>mcp-tool-as-server</code> come proxy davanti a un server MCP <code>stdio</code>).
Ogni server MCP può essere configurato in dettaglio come estensione.</p>
<h3>Retrieval Augmented Generation (RAG) / Ricerca File [opzionale]</h3>
<p>Utilizza il nostro server RAG <code>REI-S</code> per cercare nei file forniti dall’utente. È sufficiente configurare un’estensione <code>Search Files</code> per l’assistente.
Questo processo è descritto in dettaglio nella <a href="services/reis/#example-configuration-in-c4">sottodirectory <code>services/reis</code></a>.</p>
<h2>Contribuire &amp; Sviluppo</h2>
<ul>
<li>Consulta <a href="https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/CONTRIBUTING.md">CONTRIBUTING.md</a> per le linee guida su come contribuire.</li>
<li>Per l’onboarding degli sviluppatori, consulta <a href="https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/DEVELOPERS.md">DEVELOPERS.md</a>.</li>
</ul>
<h2>Principali Componenti</h2>
<p>L’applicazione è composta da un <strong>Frontend</strong>, un <strong>Backend</strong> e un servizio <strong>REI-S</strong>.</p>
<pre><code>┌──────────┐
│   Utente │
└─────┬────┘
      │ accesso
      ▼
┌──────────┐
│ Frontend │
└─────┬────┘
      │ accesso
      ▼
┌──────────┐     ┌──────────────────┐
│ Backend  │────►│      LLM         │
└─────┬────┘     └──────────────────┘
      │ accesso
      ▼
┌──────────┐     ┌──────────────────┐
│  REI-S   │────►│ Modello di       │
│          │     │ Embedding        │
│          │     └──────────────────┘
│          │
│          │     ┌──────────────────┐
│          │────►│ Vector Store     │
└──────────┘     └──────────────────┘
</code></pre>
<h3>Frontend</h3>
<p>Il frontend è sviluppato con React e TypeScript, offrendo un'interfaccia utente intuitiva per interagire con il backend e il servizio REI-S. Include funzionalità per la gestione degli assistenti, delle estensioni e delle funzionalità di chat.</p>
<blockquote>
<p>Fonti: <code>/frontend</code></p>
</blockquote>
<h3>Backend</h3>
<p>Il backend è sviluppato utilizzando NestJS e TypeScript, fungendo da principale livello API per l'applicazione. Gestisce le richieste provenienti dal frontend e interagisce con i provider llm per facilitare le funzionalità di chat. Il backend gestisce inoltre assistenti e le loro estensioni, permettendo agli utenti di configurare e utilizzare vari modelli AI per le loro chat.</p>
<p>Inoltre, il backend gestisce l'autenticazione degli utenti e comunica con il servizio REI-S per l'indicizzazione e il recupero dei file.</p>
<p>Per la persistenza dei dati, il backend utilizza un database <strong>PostgreSQL</strong>.</p>
<blockquote>
<p>Fonti: <code>/backend</code></p>
</blockquote>
<h3>REI-S</h3>
<p>Il REI-S (<strong>R</strong>etrieval <strong>E</strong>xtraction <strong>I</strong>ngestion <strong>S</strong>erver) è un server basato su Python che fornisce funzionalità di base RAG (Retrieval-Augmented Generation). Consente l’estrazione, l’indicizzazione e l’interrogazione dei contenuti dei file, permettendo all’applicazione di gestire grandi dataset in modo efficiente. Il servizio REI-S è progettato per funzionare in modo integrato con il backend, fornendo i dati necessari per le funzionalità di chat e la ricerca dei file.</p>
<p>Il REI-S supporta Azure AI Search e pgvector per l’archiviazione vettoriale, offrendo opzioni di recupero dati flessibili e scalabili. Il servizio può essere configurato tramite variabili d'ambiente per specificare il tipo di archivio vettoriale e i dettagli di connessione.</p>
<blockquote>
<p>Fonti: <code>/services/reis</code></p>
</blockquote>
<hr />
<p>Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx">Open Ai Tx</a> | Last indexed: 2025-07-09</p>
<hr />

        </div>
    </div>
    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Default Statcounter code for openaitx
https://openaitx.github.io/ -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>
    <!-- End of Statcounter Code -->
</body>
</html>