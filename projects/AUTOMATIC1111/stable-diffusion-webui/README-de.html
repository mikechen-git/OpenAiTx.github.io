<!DOCTYPE html>
<html lang="de">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>stable-diffusion-webui - AUTOMATIC1111/stable-diffusion-webui</title>
    <meta name="title" content="stable-diffusion-webui - AUTOMATIC1111/stable-diffusion-webui">
    <meta name="description" content="AUTOMATIC1111/stable-diffusion-webui - GitHub repository de documentation and informationStable Diffusion Web-UI Eine Web-Oberfläche für Stable Diffusion, implementiert mit der Gradio-Bibliothek. Funktionen Ausführliche Funktionsübersicht mit Bilder...">
    <meta name="keywords" content="AUTOMATIC1111, stable-diffusion-webui, GitHub, repository, de documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/AUTOMATIC1111/stable-diffusion-webui/README-de.html">
    <meta property="og:title" content="stable-diffusion-webui - AUTOMATIC1111/stable-diffusion-webui">
    <meta property="og:description" content="AUTOMATIC1111/stable-diffusion-webui - GitHub repository de documentation and information">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">
    <script src="/js/marked.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        body { display: flex; flex-direction: column; min-height: 100vh; }
        .main-container { margin: 0 auto; width: 100%; max-width: 980px; padding: 0 20px; }
        @media (max-width: 768px) { .main-container { padding: 0 15px; } }
        .markdown-body img { max-width: 100%; height: auto; }
        .header { text-align: center; margin-bottom: 30px; padding: 20px; background-color: #f6f8fa; border-bottom: 1px solid #e1e4e8; position: relative; }
        .header .links { margin-top: 10px; font-size: 16px; }
        .header .links a { color: #0366d6; text-decoration: none; margin-left: 5px; }
        .header .links a:hover { text-decoration: underline; }
        .language-badges { margin-top: 15px; text-align: center; }
        .language-badges a { display: inline-block; margin: 2px; text-decoration: none; }
        .language-badges img { height: 20px; border-radius: 3px; }
        .language-badges a:hover img { opacity: 0.8; }
    </style>
</head>
<body>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui" id="githubRepoLink" target="_blank">AUTOMATIC1111/stable-diffusion-webui</a>
<h1 style="display: none;">Stable Diffusion Web-UI Eine Web-Oberfläche für Stable Diffusion, implementiert mit der Gradio-Bibliothek. Funktionen Ausführliche Funktionsübersicht mit Bilder...</h1>
        </div>
        <div class="language-badges" id="languageBadges">
            <!-- You can generate language badges here if needed -->
        </div>
    </div>
    <div class="main-container">
        <div class="markdown-body" id="content">
            <h1>Stable Diffusion Web-UI</h1>
<p>Eine Web-Oberfläche für Stable Diffusion, implementiert mit der Gradio-Bibliothek.</p>
<p><img src="screenshot.png" alt="" /></p>
<h2>Funktionen</h2>
<p><a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features">Ausführliche Funktionsübersicht mit Bildern</a>:</p>
<ul>
<li>Ursprüngliche txt2img- und img2img-Modi</li>
<li>Ein-Klick-Installations- und Ausführungsskript (Python und Git müssen trotzdem installiert sein)</li>
<li>Outpainting</li>
<li>Inpainting</li>
<li>Color Sketch</li>
<li>Prompt Matrix</li>
<li>Stable Diffusion Upscale</li>
<li>Attention, bestimmte Textteile, auf die das Modell besonders achten soll, spezifizieren
<ul>
<li>a man in a <code>((tuxedo))</code> – Fokus liegt mehr auf Tuxedo</li>
<li>a man in a <code>(tuxedo:1.21)</code> – alternative Syntax</li>
<li>Text markieren und <code>Ctrl+Up</code> oder <code>Ctrl+Down</code> (bzw. <code>Command+Up</code> oder <code>Command+Down</code> unter MacOS) drücken, um die Aufmerksamkeit für ausgewählten Text automatisch anzupassen (Code von anonymem Nutzer beigesteuert)</li>
</ul>
</li>
<li>Loopback, mehrfache img2img-Verarbeitung in Serie</li>
<li>X/Y/Z-Plot, eine Möglichkeit, ein 3D-Bild-Plot mit unterschiedlichen Parametern zu erstellen</li>
<li>Textuelle Inversion
<ul>
<li>beliebig viele Embeddings mit frei wählbaren Namen verwenden</li>
<li>mehrere Embeddings mit unterschiedlicher Anzahl von Vektoren pro Token nutzen</li>
<li>funktioniert mit halbpräzisen Fließkommazahlen</li>
<li>Embeddings auf 8GB trainieren (auch Berichte, dass 6GB funktionieren)</li>
</ul>
</li>
<li>Extras-Tab mit:
<ul>
<li>GFPGAN, neuronales Netzwerk, das Gesichter repariert</li>
<li>CodeFormer, Gesichtsrestaurierung als Alternative zu GFPGAN</li>
<li>RealESRGAN, neuronaler Upscaler</li>
<li>ESRGAN, neuronaler Upscaler mit vielen Drittanbieter-Modellen</li>
<li>SwinIR und Swin2SR (<a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/2092">siehe hier</a>), neuronale Upscaler</li>
<li>LDSR, Latent Diffusion Super Resolution Upscaling</li>
</ul>
</li>
<li>Optionen zur Anpassung des Seitenverhältnisses beim Skalieren</li>
<li>Auswahl der Sampling-Methode
<ul>
<li>Anpassung der Sampler-Eta-Werte (Rauschmultiplier)</li>
<li>Erweiterte Einstellmöglichkeiten für das Rauschen</li>
</ul>
</li>
<li>Verarbeitung jederzeit abbrechen möglich</li>
<li>4GB-Grafikkarten-Unterstützung (auch Berichte, dass 2GB funktionieren)</li>
<li>Korrekte Seeds für Batches</li>
<li>Live-Validierung der Prompt-Token-Länge</li>
<li>Generierungsparameter
<ul>
<li>Die zur Bilderzeugung verwendeten Parameter werden mit dem Bild gespeichert</li>
<li>In PNG-Chunks für PNG, in EXIF für JPEG</li>
<li>Bild in den PNG-Info-Tab ziehen, um Generierungsparameter wiederherzustellen und automatisch ins UI zu kopieren</li>
<li>Kann in den Einstellungen deaktiviert werden</li>
<li>Bild/Text-Parameter per Drag &amp; Drop in das Prompt-Feld ziehen</li>
</ul>
</li>
<li>Button &quot;Read Generation Parameters&quot;, lädt Parameter aus Promptbox ins UI</li>
<li>Einstellungsseite</li>
<li>Ausführen beliebigen Python-Codes aus dem UI (muss mit <code>--allow-code</code> gestartet werden)</li>
<li>Mouseover-Hinweise für die meisten UI-Elemente</li>
<li>Standard-/Min-/Max-/Step-Werte für UI-Elemente per Textkonfiguration änderbar</li>
<li>Tiling-Unterstützung, Checkbox für kachelbare Bilder wie Texturen</li>
<li>Fortschrittsbalken und Live-Vorschau der Bildgenerierung
<ul>
<li>Kann ein separates neuronales Netzwerk für Vorschauen mit minimalem VRAM- oder Rechenbedarf nutzen</li>
</ul>
</li>
<li>Negative Prompt, ein zusätzliches Textfeld, um Unerwünschtes im generierten Bild auszuschließen</li>
<li>Styles, Möglichkeit, Teile von Prompts zu speichern und später per Dropdown anzuwenden</li>
<li>Variationen, Möglichkeit, dasselbe Bild mit kleinen Unterschieden zu generieren</li>
<li>Seed-Resizing, Möglichkeit, dasselbe Bild in leicht anderer Auflösung zu generieren</li>
<li>CLIP-Interrogator, Button, der versucht, den Prompt aus einem Bild zu erraten</li>
<li>Prompt Editing, Möglichkeit, den Prompt während der Generierung zu ändern, z.B. mit einer Wassermelone starten und auf Anime-Girl wechseln</li>
<li>Batch Processing, Verarbeitung einer Dateigruppe per img2img</li>
<li>Img2img Alternative, Reverse-Euler-Methode zur Cross-Attention-Steuerung</li>
<li>Highres Fix, Komfort-Option, um hochauflösende Bilder auf Knopfdruck ohne übliche Verzerrungen zu erzeugen</li>
<li>Checkpoints im laufenden Betrieb neu laden</li>
<li>Checkpoint Merger, Tab zum Zusammenführen von bis zu 3 Checkpoints zu einem</li>
<li><a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Custom-Scripts">Custom Scripts</a> mit vielen Erweiterungen aus der Community</li>
<li><a href="https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/">Composable-Diffusion</a>, Möglichkeit, mehrere Prompts gleichzeitig zu nutzen
<ul>
<li>Prompts durch Großbuchstaben <code>AND</code> trennen</li>
<li>Gewichte für Prompts möglich: <code>a cat :1.2 AND a dog AND a penguin :2.2</code></li>
</ul>
</li>
<li>Kein Token-Limit für Prompts (Original Stable Diffusion erlaubt max. 75 Tokens)</li>
<li>DeepDanbooru-Integration, erstellt Danbooru-Style-Tags für Anime-Prompts</li>
<li><a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Xformers">xformers</a>, erhebliche Geschwindigkeitssteigerung für ausgewählte Karten: (fügen Sie <code>--xformers</code> zu den Befehlszeilenargumenten hinzu)</li>
<li>via Erweiterung: <a href="https://github.com/yfszzx/stable-diffusion-webui-images-browser">History Tab</a>: Bilder bequem im UI anzeigen, direkt nutzen und löschen</li>
<li>&quot;Generate Forever&quot;-Option</li>
<li>Trainings-Tab
<ul>
<li>Optionen für Hypernetworks und Embeddings</li>
<li>Vorverarbeitung von Bildern: Zuschneiden, Spiegeln, Autotagging mit BLIP oder DeepDanbooru (für Anime)</li>
</ul>
</li>
<li>Clip Skip</li>
<li>Hypernetworks</li>
<li>Loras (wie Hypernetworks, aber hübscher)</li>
<li>Separate UI zur Auswahl, mit Vorschau, welche Embeddings, Hypernetworks oder Loras zum Prompt hinzugefügt werden sollen</li>
<li>Auswahlmöglichkeit für anderes VAE aus dem Einstellungsbildschirm</li>
<li>Geschätzte verbleibende Zeit im Fortschrittsbalken</li>
<li>API</li>
<li>Unterstützung für dediziertes <a href="https://github.com/runwayml/stable-diffusion#inpainting-with-stable-diffusion">Inpainting-Modell</a> von RunwayML</li>
<li>via Erweiterung: <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui-aesthetic-gradients">Aesthetic Gradients</a>, Bilder mit spezifischer Ästhetik mittels CLIP-Image-Embeds generieren (<a href="https://github.com/vicgalle/stable-diffusion-aesthetic-gradients">Implementierung</a>)</li>
<li><a href="https://github.com/Stability-AI/stablediffusion">Stable Diffusion 2.0</a> Unterstützung – siehe <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features#stable-diffusion-20">Wiki</a> für Anweisungen</li>
<li><a href="https://arxiv.org/abs/2211.06679">Alt-Diffusion</a> Unterstützung – siehe <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features#alt-diffusion">Wiki</a> für Anweisungen</li>
<li>Jetzt ohne böse Buchstaben!</li>
<li>Checkpoints im safetensors-Format laden</li>
<li>Lockerere Auflösungsbeschränkung: Bilddimensionen müssen Vielfache von 8 statt 64 sein</li>
<li>Jetzt mit Lizenz!</li>
<li>Elemente im UI aus dem Einstellungsbildschirm umsortierbar</li>
<li><a href="https://huggingface.co/segmind/SSD-1B">Segmind Stable Diffusion</a> Unterstützung</li>
</ul>
<h2>Installation und Ausführung</h2>
<p>Stellen Sie sicher, dass alle <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Dependencies">Abhängigkeiten</a> erfüllt sind und folgen Sie der Anleitung für:</p>
<ul>
<li><a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-NVidia-GPUs">NVidia</a> (empfohlen)</li>
<li><a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-AMD-GPUs">AMD</a> GPUs.</li>
<li><a href="https://github.com/openvinotoolkit/stable-diffusion-webui/wiki/Installation-on-Intel-Silicon">Intel CPUs, Intel GPUs (sowohl integriert als auch dediziert)</a> (externes Wiki)</li>
<li><a href="https://github.com/wangshuai09/stable-diffusion-webui/wiki/Install-and-run-on-Ascend-NPUs">Ascend NPUs</a> (externes Wiki)</li>
</ul>
<p>Alternativ können Sie Online-Dienste nutzen (wie Google Colab):</p>
<ul>
<li><a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Online-Services">Liste der Online-Dienste</a></li>
</ul>
<h3>Installation unter Windows 10/11 mit NVidia-GPUs mittels Release-Paket</h3>
<ol>
<li>Laden Sie <code>sd.webui.zip</code> von <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/releases/tag/v1.0.0-pre">v1.0.0-pre</a> herunter und entpacken Sie den Inhalt.</li>
<li>Führen Sie <code>update.bat</code> aus.</li>
<li>Führen Sie <code>run.bat</code> aus.</li>
</ol>
<blockquote>
<p>Für weitere Details siehe <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-NVidia-GPUs">Install-and-Run-on-NVidia-GPUs</a></p>
</blockquote>
<h3>Automatische Installation unter Windows</h3>
<ol>
<li>Installieren Sie <a href="https://www.python.org/downloads/release/python-3106/">Python 3.10.6</a> (Neuere Python-Versionen unterstützen torch nicht). Aktivieren Sie &quot;Add Python to PATH&quot;.</li>
<li>Installieren Sie <a href="https://git-scm.com/download/win">git</a>.</li>
<li>Laden Sie das Repository stable-diffusion-webui herunter, z.B. durch Ausführen von <code>git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git</code>.</li>
<li>Führen Sie <code>webui-user.bat</code> im Windows Explorer als normaler, nicht-administrativer Benutzer aus.</li>
</ol>
<h3>Automatische Installation unter Linux</h3>
<ol>
<li>Installieren Sie die Abhängigkeiten:</li>
</ol>
<pre><code class="language-bash"># Debian-basiert:
sudo apt install wget git python3 python3-venv libgl1 libglib2.0-0
# Red Hat-basiert:
sudo dnf install wget git python3 gperftools-libs libglvnd-glx
# openSUSE-basiert:
sudo zypper install wget git python3 libtcmalloc4 libglvnd
# Arch-basiert:
sudo pacman -S wget git python3
</code></pre>
<p>Falls Ihr System sehr neu ist, müssen Sie python3.11 oder python3.10 installieren:</p>
<pre><code class="language-bash"># Ubuntu 24.04
sudo add-apt-repository ppa:deadsnakes/ppa
sudo apt update
sudo apt install python3.11

# Manjaro/Arch
sudo pacman -S yay
yay -S python311 # nicht mit python3.11 verwechseln

# Nur für 3.11
# Dann Umgebungsvariable im Startskript setzen
export python_cmd=&quot;python3.11&quot;
# oder in webui-user.sh
python_cmd=&quot;python3.11&quot;
</code></pre>
<ol start="2">
<li>Navigieren Sie in das gewünschte Installationsverzeichnis und führen Sie folgenden Befehl aus:</li>
</ol>
<pre><code class="language-bash">wget -q https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/webui.sh
</code></pre>
<p>Oder klonen Sie das Repo einfach an einen beliebigen Ort:</p>
<pre><code class="language-bash">git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui
</code></pre>
<ol start="3">
<li>Führen Sie <code>webui.sh</code> aus.</li>
<li>Überprüfen Sie <code>webui-user.sh</code> auf Optionen.</li>
</ol>
<h3>Installation auf Apple Silicon</h3>
<p>Die Anleitung finden Sie <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Installation-on-Apple-Silicon">hier</a>.</p>
<h2>Mitwirken</h2>
<p>So können Sie Code zu diesem Repo beitragen: <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Contributing">Contributing</a></p>
<h2>Dokumentation</h2>
<p>Die Dokumentation wurde aus dieser README in das <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki">Wiki</a> des Projekts verschoben.</p>
<p>Damit Google und andere Suchmaschinen das Wiki crawlen, hier ein Link zum (nicht für Menschen gedachten) <a href="https://github-wiki-see.page/m/AUTOMATIC1111/stable-diffusion-webui/wiki">crawlbaren Wiki</a>.</p>
<h2>Danksagungen</h2>
<p>Lizenzen für genutzten Code finden Sie unter <code>Einstellungen -&gt; Lizenzen</code> und in der Datei <code>html/licenses.html</code>.</p>
<ul>
<li>Stable Diffusion – https://github.com/Stability-AI/stablediffusion, https://github.com/CompVis/taming-transformers, https://github.com/mcmonkey4eva/sd3-ref</li>
<li>k-diffusion – https://github.com/crowsonkb/k-diffusion.git</li>
<li>Spandrel – https://github.com/chaiNNer-org/spandrel, implementiert
<ul>
<li>GFPGAN – https://github.com/TencentARC/GFPGAN.git</li>
<li>CodeFormer – https://github.com/sczhou/CodeFormer</li>
<li>ESRGAN – https://github.com/xinntao/ESRGAN</li>
<li>SwinIR – https://github.com/JingyunLiang/SwinIR</li>
<li>Swin2SR – https://github.com/mv-lab/swin2sr</li>
</ul>
</li>
<li>LDSR – https://github.com/Hafiidz/latent-diffusion</li>
<li>MiDaS – https://github.com/isl-org/MiDaS</li>
<li>Ideen für Optimierungen – https://github.com/basujindal/stable-diffusion</li>
<li>Cross Attention Layer Optimierung – Doggettx – https://github.com/Doggettx/stable-diffusion, Originalidee für Prompt Editing.</li>
<li>Cross Attention Layer Optimierung – InvokeAI, lstein – https://github.com/invoke-ai/InvokeAI (ursprünglich http://github.com/lstein/stable-diffusion)</li>
<li>Sub-quadratische Cross Attention Layer Optimierung – Alex Birch (https://github.com/Birch-san/diffusers/pull/1), Amin Rezaei (https://github.com/AminRezaei0x443/memory-efficient-attention)</li>
<li>Textuelle Inversion – Rinon Gal – https://github.com/rinongal/textual_inversion (wir nutzen nicht seinen Code, aber seine Ideen).</li>
<li>Idee für SD Upscale – https://github.com/jquesnelle/txt2imghd</li>
<li>Rauschgenerierung für Outpainting mk2 – https://github.com/parlance-zz/g-diffuser-bot</li>
<li>CLIP Interrogator-Idee und Code-Übernahme – https://github.com/pharmapsychotic/clip-interrogator</li>
<li>Idee für Composable Diffusion – https://github.com/energy-based-model/Compositional-Visual-Generation-with-Composable-Diffusion-Models-PyTorch</li>
<li>xformers – https://github.com/facebookresearch/xformers</li>
<li>DeepDanbooru – Interrogator für Anime-Diffuser https://github.com/KichangKim/DeepDanbooru</li>
<li>Sampling in float32-Präzision von einem float16 UNet – marunine für die Idee, Birch-san für das Diffusers-Beispiel (https://github.com/Birch-san/diffusers-play/tree/92feee6)</li>
<li>Instruct pix2pix – Tim Brooks (star), Aleksander Holynski (star), Alexei A. Efros (no star) – https://github.com/timothybrooks/instruct-pix2pix</li>
<li>Sicherheitshinweise – RyotaK</li>
<li>UniPC Sampler – Wenliang Zhao – https://github.com/wl-zhao/UniPC</li>
<li>TAESD – Ollin Boer Bohan – https://github.com/madebyollin/taesd</li>
<li>LyCORIS – KohakuBlueleaf</li>
<li>Restart Sampling – lambertae – https://github.com/Newbeeer/diffusion_restart_sampling</li>
<li>Hypertile – tfernd – https://github.com/tfernd/HyperTile</li>
<li>Erstes Gradio-Skript – von einem anonymen Nutzer auf 4chan gepostet. Danke an den anonymen Nutzer.</li>
<li>(Sie)</li>
</ul>
<hr />
<p><a href="https://github.com/OpenAiTx/OpenAiTx">Powered By OpenAiTx</a></p>
<hr />

        </div>
    </div>
    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Default Statcounter code for openaitx
https://openaitx.github.io/ -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>
    <!-- End of Statcounter Code -->
</body>
</html>