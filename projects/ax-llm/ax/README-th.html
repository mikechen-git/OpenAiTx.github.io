<!DOCTYPE html>
<html lang="th">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ax - ax-llm/ax</title>
    <meta name="title" content="ax - ax-llm/ax">
    <meta name="description" content="ax-llm/ax - GitHub repository th documentation and information# Ax, DSPy สำหรับ Typescript การทำงานกับ LLMs มีความซับซ้อนและมักจะไม่เป็นไปตามที่ต้องการ DSPy ช่วยให้การสร้างสิ่งที่ยอดเยี่ยมด้วย LLMs เป็นเรื่องง่าย เพียงแค่ก...">
    <meta name="keywords" content="ax-llm, ax, GitHub, repository, th documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/ax-llm/ax/README-th.html">
    <meta property="og:title" content="ax - ax-llm/ax">
    <meta property="og:description" content="ax-llm/ax - GitHub repository th documentation and information">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">
    <script src="/js/marked.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        body { display: flex; flex-direction: column; min-height: 100vh; }
        .main-container { margin: 0 auto; width: 100%; max-width: 980px; padding: 0 20px; }
        @media (max-width: 768px) { .main-container { padding: 0 15px; } }
        .markdown-body img { max-width: 100%; height: auto; }
        .header { text-align: center; margin-bottom: 30px; padding: 20px; background-color: #f6f8fa; border-bottom: 1px solid #e1e4e8; position: relative; }
        .header .links { margin-top: 10px; font-size: 16px; }
        .header .links a { color: #0366d6; text-decoration: none; margin-left: 5px; }
        .header .links a:hover { text-decoration: underline; }
        .language-badges { margin-top: 15px; text-align: center; }
        .language-badges a { display: inline-block; margin: 2px; text-decoration: none; }
        .language-badges img { height: 20px; border-radius: 3px; }
        .language-badges a:hover img { opacity: 0.8; }
    </style>
</head>
<body>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/ax-llm/ax" id="githubRepoLink" target="_blank">ax-llm/ax</a>
<h1 style="display: none;"># Ax, DSPy สำหรับ Typescript การทำงานกับ LLMs มีความซับซ้อนและมักจะไม่เป็นไปตามที่ต้องการ DSPy ช่วยให้การสร้างสิ่งที่ยอดเยี่ยมด้วย LLMs เป็นเรื่องง่าย เพียงแค่ก...</h1>
        </div>
        <div class="language-badges" id="languageBadges">
            <!-- You can generate language badges here if needed -->
        </div>
    </div>
    <div class="main-container">
        <div class="markdown-body" id="content">
            <pre><code class="language-markdown"># Ax, DSPy สำหรับ Typescript

การทำงานกับ LLMs มีความซับซ้อนและมักจะไม่เป็นไปตามที่ต้องการ DSPy ช่วยให้การสร้างสิ่งที่ยอดเยี่ยมด้วย LLMs เป็นเรื่องง่าย เพียงแค่กำหนดอินพุตและเอาต์พุต (signature) แล้ว prompt ที่มีประสิทธิภาพจะถูกสร้างขึ้นโดยอัตโนมัติและนำไปใช้ เชื่อมต่อ signatures หลายๆ อันเข้าด้วยกันเพื่อสร้างระบบและ workflow ที่ซับซ้อนโดยใช้ LLMs

และเพื่อให้คุณนำไปใช้งานจริงได้ เรายังมีสิ่งที่จำเป็นอื่นๆ เช่น การสังเกตการณ์ (observability), สตรีมมิ่ง, รองรับมัลติโมดัล (ภาพ, เสียง ฯลฯ), การแก้ไขข้อผิดพลาด, การเรียกใช้งานฟังก์ชันหลายขั้นตอน, MCP, RAG และอื่นๆ

[![NPM Package](https://img.shields.io/npm/v/@ax-llm/ax?style=for-the-badge&amp;color=green)](https://www.npmjs.com/package/@ax-llm/ax)
[![Discord Chat](https://dcbadge.vercel.app/api/server/DSHg3dU7dW?style=for-the-badge)](https://discord.gg/DSHg3dU7dW)
[![Twitter](https://img.shields.io/twitter/follow/dosco?style=for-the-badge&amp;color=red)](https://twitter.com/dosco)

&lt;!-- header --&gt;

## ทำไมต้องใช้ Ax?

- อินเทอร์เฟซมาตรฐานสำหรับ LLM ชั้นนำทั้งหมด
- Prompt สร้างจาก signature ที่เรียบง่าย
- สตรีมมิ่งแบบ native ครบวงจร
- รองรับงบประมาณในการคิดและโทเค็นความคิด
- สร้าง Agents ที่เรียกใช้ agent อื่นได้
- มี MCP, รองรับ Model Context Protocol ในตัว
- แปลงเอกสารรูปแบบใดก็ได้เป็นข้อความ
- RAG, การ chunk อย่างชาญฉลาด, embedding, การค้นหา
- ใช้งานร่วมกับ Vercel AI SDK
- ตรวจสอบผลลัพธ์ระหว่างสตรีมมิ่ง
- รองรับ Multi-modal DSPy
- ปรับแต่ง prompt อัตโนมัติด้วย optimizer
- รองรับ OpenTelemetry / observability
- โค้ด Typescript พร้อมใช้งาน production
- น้ำหนักเบา ไม่มี dependencies

## พร้อมสำหรับ Production

- ไม่มี breaking changes (minor versions)
- มีการทดสอบครอบคลุมมาก
- รองรับ Open Telemetry `gen_ai` ในตัว
- ได้รับความนิยมใน startup หลายแห่ง

## Prompt signature คืออะไร?

&lt;img width=&quot;860&quot; alt=&quot;shapes at 24-03-31 00 05 55&quot; src=&quot;https://github.com/dosco/llm-client/assets/832235/0f0306ea-1812-4a0a-9ed5-76cd908cd26b&quot;&gt;

Prompt ที่มี type-safe ถูกสร้างจาก signature อย่างง่ายโดยอัตโนมัติ Prompt signature ประกอบด้วย
`&quot;task description&quot; inputField:type &quot;field description&quot; -&gt; &quot;outputField:type` แนวคิดของ prompt signature นี้อ้างอิงจากงานวิจัยในหัวข้อ &quot;Demonstrate-Search-Predict&quot;

คุณสามารถมี input และ output หลายฟิลด์ โดยแต่ละฟิลด์เป็นชนิด `string`, `number`, `boolean`, `date`, `datetime`, `class &quot;class1, class2&quot;`, `JSON` หรือ array ของชนิดเหล่านี้ เช่น `string[]` ถ้าไม่กำหนดชนิดจะเป็น `string` โดยปริยาย Suffix `?` ทำให้ฟิลด์เป็นทางเลือก (ปกติจำเป็น) และ `!` ทำให้ฟิลด์เป็น internal เหมาะสำหรับ reasoning

## Output Field Types

| Type                      | คำอธิบาย                            | การใช้งาน                 | ตัวอย่างผลลัพธ์                                      |
| ------------------------- | ------------------------------------ | ------------------------- | ----------------------------------------------------- |
| `string`                  | ลำดับอักขระ                         | `fullName:string`         | `&quot;example&quot;`                                           |
| `number`                  | ค่าตัวเลข                            | `price:number`            | `42`                                                  |
| `boolean`                 | ค่าจริงหรือเท็จ                      | `isEvent:boolean`         | `true`, `false`                                       |
| `date`                    | ค่าวันที่                            | `startDate:date`          | `&quot;2023-10-01&quot;`                                        |
| `datetime`                | วันและเวลารวมกัน                     | `createdAt:datetime`      | `&quot;2023-10-01T12:00:00Z&quot;`                              |
| `class &quot;class1,class2&quot;`   | การจัดประเภท                         | `category:class`          | `[&quot;class1&quot;, &quot;class2&quot;, &quot;class3&quot;]`                      |
| `string[]`                | อาร์เรย์ของ string                   | `tags:string[]`           | `[&quot;example1&quot;, &quot;example2&quot;]`                            |
| `number[]`                | อาร์เรย์ของ number                   | `scores:number[]`         | `[1, 2, 3]`                                           |
| `boolean[]`               | อาร์เรย์ของ boolean                  | `permissions:boolean[]`   | `[true, false, true]`                                 |
| `date[]`                  | อาร์เรย์ของวันที่                    | `holidayDates:date[]`     | `[&quot;2023-10-01&quot;, &quot;2023-10-02&quot;]`                        |
| `datetime[]`              | อาร์เรย์ของวันและเวลา                | `logTimestamps:datetime[]`| `[&quot;2023-10-01T12:00:00Z&quot;, &quot;2023-10-02T12:00:00Z&quot;]`    |
| `class[] &quot;class1,class2&quot;` | หลายคลาส                             | `categories:class[]`      | `[&quot;class1&quot;, &quot;class2&quot;, &quot;class3&quot;]`                      |
| `code &quot;language&quot;`         | โค้ดบล็อกในภาษาที่ระบุ              | `code:code &quot;python&quot;`      | `print('Hello, world!')`                              |

## LLMs ที่รองรับ

`Google Gemini`, `OpenAI`, `Azure OpenAI`, `Anthropic`, `X Grok`, `TogetherAI`, `Cohere`, `Mistral`, `Groq`, `DeepSeek`, `Ollama`, `Reka`, `Hugging Face`

## ติดตั้ง

```bash
npm install @ax-llm/ax
# หรือ
yarn add @ax-llm/ax
</code></pre>
<h2>ตัวอย่าง: ใช้ chain-of-thought เพื่อสรุปข้อความ</h2>
<pre><code class="language-typescript">import { AxAI, AxChainOfThought } from '@ax-llm/ax'

const textToSummarize = `
The technological singularity—or simply the singularity[1]—is a hypothetical future point in time at which technological growth becomes uncontrollable and irreversible, resulting in unforeseeable changes to human civilization.[2][3] ...`

const ai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY as string,
})

const gen = new AxChainOfThought(
  `textToSummarize -&gt; textType:class &quot;note, email, reminder&quot;, shortSummary &quot;summarize in 5 to 10 words&quot;`
)

const res = await gen.forward(ai, { textToSummarize })

console.log('&gt;', res)
</code></pre>
<h2>ตัวอย่าง: สร้าง agent</h2>
<p>ใช้ agent prompt (framework) เพื่อสร้าง agent ที่สามารถประสานงานกับ agent อื่นในการทำงาน Agents สร้างได้ง่ายจาก prompt signature ลองดูตัวอย่าง agent</p>
<pre><code class="language-typescript"># npm run tsx ./src/examples/agent.ts

const researcher = new AxAgent({
  name: 'researcher',
  description: 'Researcher agent',
  signature: `physicsQuestion &quot;physics questions&quot; -&gt; answer &quot;reply in bullet points&quot;`
});

const summarizer = new AxAgent({
  name: 'summarizer',
  description: 'Summarizer agent',
  signature: `text &quot;text so summarize&quot; -&gt; shortSummary &quot;summarize in 5 to 10 words&quot;`
});

const agent = new AxAgent({
  name: 'agent',
  description: 'A an agent to research complex topics',
  signature: `question -&gt; answer`,
  agents: [researcher, summarizer]
});

agent.forward(ai, { questions: &quot;How many atoms are there in the universe&quot; })
</code></pre>
<h2>รองรับ Thinking Models</h2>
<p>Ax รองรับโมเดลที่มีความสามารถในการคิดโดยตรง สามารถควบคุมงบโทเค็นความคิดและเข้าถึง reasoning ของโมเดล ช่วยเข้าใจ reasoning และปรับใช้โทเค็นอย่างเหมาะสม</p>
<pre><code class="language-typescript">const ai = new AxAI({
  name: 'google-gemini',
  apiKey: process.env.GOOGLE_APIKEY as string,
  config: {
    model: AxAIGoogleGeminiModel.Gemini25Flash,
    thinking: { includeThoughts: true },
  },
})

// หรือควบคุม budget ต่อ request
const gen = new AxChainOfThought(`question -&gt; answer`)
const res = await gen.forward(
  ai,
  { question: 'What is quantum entanglement?' },
  { thinkingTokenBudget: 'medium' } // 'minimal', 'low', 'medium', หรือ 'high'
)

// เข้าถึง thoughts ใน response
console.log(res.thoughts) // แสดง reasoning ของโมเดล
</code></pre>
<h2>Vector DBs ที่รองรับ</h2>
<p>Vector database สำคัญในการสร้าง workflow ของ LLM เรามี abstraction สำหรับฐานข้อมูลเวกเตอร์ยอดนิยม และฐานข้อมูลเวกเตอร์ในหน่วยความจำ</p>
<p>| ผู้ให้บริการ | ทดสอบแล้ว  |
| ------------ | ---------- |
| In Memory    | 🟢 100%    |
| Weaviate     | 🟢 100%    |
| Cloudflare   | 🟡 50%     |
| Pinecone     | 🟡 50%     |</p>
<pre><code class="language-typescript">// สร้าง embedding จากข้อความโดยใช้ LLM
const ret = await this.ai.embed({ texts: 'hello world' })

// สร้าง vector db ในหน่วยความจำ
const db = new axDB('memory')

// เพิ่มข้อมูลลง vector db
await this.db.upsert({
  id: 'abc',
  table: 'products',
  values: ret.embeddings[0],
})

// ค้นหาข้อมูลที่คล้ายกันด้วย embeddings
const matches = await this.db.query({
  table: 'products',
  values: embeddings[0],
})
</code></pre>
<p>หรือใช้ <code>AxDBManager</code> ที่จัดการ chunk, embed, query ให้อัตโนมัติ</p>
<pre><code class="language-typescript">const manager = new AxDBManager({ ai, db })
await manager.insert(text)

const matches = await manager.query(
  'John von Neumann on human intelligence and singularity.'
)
console.log(matches)
</code></pre>
<h2>RAG Documents</h2>
<p>ใช้งานเอกสารอย่าง PDF, DOCX, PPT, XLS ฯลฯ กับ LLM เป็นเรื่องยุ่งยาก เราทำให้ง่ายขึ้นด้วย Apache Tika ซึ่งเป็นเอนจินโอเพ่นซอร์สสำหรับประมวลผลเอกสาร</p>
<p>รัน Apache Tika</p>
<pre><code class="language-shell">docker run -p 9998:9998 apache/tika
</code></pre>
<p>แปลงเอกสารเป็นข้อความและสร้าง embedding เพื่อค้นหาด้วย <code>AxDBManager</code> ที่รองรับ reranker และ query rewriter มี implementation เริ่มต้น 2 ตัว ได้แก่ <code>AxDefaultResultReranker</code> และ <code>AxDefaultQueryRewriter</code></p>
<pre><code class="language-typescript">const tika = new AxApacheTika()
const text = await tika.convert('/path/to/document.pdf')

const manager = new AxDBManager({ ai, db })
await manager.insert(text)

const matches = await manager.query('Find some text')
console.log(matches)
</code></pre>
<h2>Multi-modal DSPy</h2>
<p>เมื่อใช้โมเดลอย่าง <code>GPT-4o</code> และ <code>Gemini</code> ที่รองรับ multi-modal prompt สามารถใช้ image field ร่วมใน pipeline ได้</p>
<pre><code class="language-typescript">const image = fs
  .readFileSync('./src/examples/assets/kitten.jpeg')
  .toString('base64')

const gen = new AxChainOfThought(`question, animalImage:image -&gt; answer`)

const res = await gen.forward(ai, {
  question: 'What family does this animal belong to?',
  animalImage: { mimeType: 'image/jpeg', data: image },
})
</code></pre>
<p>สำหรับโมเดลอย่าง <code>gpt-4o-audio-preview</code> ที่รองรับ audio สามารถใช้ audio field ได้</p>
<pre><code class="language-typescript">const audio = fs
  .readFileSync('./src/examples/assets/comment.wav')
  .toString('base64')

const gen = new AxGen(`question, commentAudio:audio -&gt; answer`)

const res = await gen.forward(ai, {
  question: 'What family does this animal belong to?',
  commentAudio: { format: 'wav', data: audio },
})
</code></pre>
<h2>DSPy Chat API</h2>
<p>ได้รับแรงบันดาลใจจาก demonstration weaving ของ DSPy, Ax มี <code>AxMessage</code> สำหรับการจัดการประวัติการสนทนาอย่างไร้รอยต่อ สามารถสร้าง chatbot ที่รักษาบริบทการสนทนาได้เต็มรูปแบบด้วย prompt signature ดูตัวอย่างเพิ่มเติม</p>
<pre><code class="language-shell">GOOGLE_APIKEY=api-key npm run tsx ./src/examples/chat.ts
</code></pre>
<pre><code class="language-typescript">const chatBot = new AxGen&lt;
  { message: string } | ReadonlyArray&lt;ChatMessage&gt;,
  { reply: string }
&gt;(
  `message:string &quot;A casual message from the user&quot; -&gt; reply:string &quot;A friendly, casual response&quot;`
)

await chatBot.forward(ai, [
  {
    role: 'user',
    values: { message: 'Hi! How are you doing today?' },
  },
  {
    role: 'assistant',
    values: { reply: 'I am doing great! How about you?' },
  },
  {
    role: 'user',
    values: { message: 'Thats great!' },
  },
])
</code></pre>
<p>ประวัติการสนทนาจะถูก weave เข้าไปใน prompt อัตโนมัติ ทำให้โมเดลสามารถรักษาบริบทและตอบสนองได้อย่างต่อเนื่อง รองรับทุกฟีเจอร์ของ Ax เช่น สตรีมมิ่ง, การเรียกฟังก์ชัน, chain-of-thought reasoning</p>
<h2>Streaming</h2>
<h3>Assertions</h3>
<p>เรารองรับการ parse output fields และเรียกฟังก์ชันระหว่าง streaming ช่วยให้ fail-fast และแก้ไขข้อผิดพลาดได้ทันทีโดยไม่ต้องรอผลลัพธ์ทั้งหมด ประหยัดโทเค็นและลด latency Assertion ช่วยให้มั่นใจว่า output ตรงตามข้อกำหนด ใช้ได้กับ streaming</p>
<pre><code class="language-typescript">// setup the prompt program
const gen = new AxChainOfThought(
  ai,
  `startNumber:number -&gt; next10Numbers:number[]`
)

// add a assertion ให้แน่ใจว่าไม่มีเลข 5 ใน output field
gen.addAssert(({ next10Numbers }: Readonly&lt;{ next10Numbers: number[] }&gt;) =&gt; {
  return next10Numbers ? !next10Numbers.includes(5) : undefined
}, 'Numbers 5 is not allowed')

// run the program พร้อม streaming
const res = await gen.forward({ startNumber: 1 }, { stream: true })

// หรือ run แบบ end-to-end streaming
const generator = await gen.streamingForward(
  { startNumber: 1 },
  {
    stream: true,
  }
)
for await (const res of generator) {
}
</code></pre>
<p>ตัวอย่างข้างต้นช่วย validate output field แบบเต็มขณะ streaming สามารถใช้ทั้งแบบ streaming และไม่ streaming โดยจะ trigger เมื่อได้รับค่าครบ หากต้องการ validate ระหว่าง streaming จริงๆ ดูตัวอย่างด้านล่าง จะช่วยเพิ่ม performance และประหยัดโทเค็น</p>
<pre><code class="language-typescript">// assertion ตรวจแต่ละบรรทัดต้องขึ้นต้นด้วยตัวเลขและจุด
gen.addStreamingAssert(
  'answerInPoints',
  (value: string) =&gt; {
    const re = /^\d+\./

    // แยกบรรทัด, trim, กรองบรรทัดว่าง, ตรวจ regex
    return value
      .split('\n')
      .map((x) =&gt; x.trim())
      .filter((x) =&gt; x.length &gt; 0)
      .every((x) =&gt; re.test(x))
  },
  'Lines must start with a number and a dot. Eg: 1. This is a line.'
)

// run แบบ streaming
const res = await gen.forward(
  {
    question: 'Provide a list of optimizations to speedup LLM inference.',
  },
  { stream: true, debug: true }
)
</code></pre>
<h3>Field Processors</h3>
<p>Field processor ใช้ประมวลผล field ใน prompt ก่อนส่งเข้า LLM</p>
<pre><code class="language-typescript">const gen = new AxChainOfThought(
  ai,
  `startNumber:number -&gt; next10Numbers:number[]`
)

const streamValue = false

const processorFunction = (value) =&gt; {
  return value.map((x) =&gt; x + 1)
}

// เพิ่ม field processor เข้าโปรแกรม
const processor = new AxFieldProcessor(
  gen,
  'next10Numbers',
  processorFunction,
  streamValue
)

const res = await gen.forward({ startNumber: 1 })
</code></pre>
<h2>Model Context Protocol (MCP)</h2>
<p>Ax รองรับ Model Context Protocol (MCP) อย่างไร้รอยต่อ ให้ agent ของคุณเข้าถึง tools ภายนอกผ่านอินเทอร์เฟซมาตรฐาน</p>
<h3>การใช้ AxMCPClient</h3>
<p><code>AxMCPClient</code> ช่วยเชื่อมต่อกับ MCP server ใดก็ได้และใช้งานผ่าน agent ของคุณ:</p>
<pre><code class="language-typescript">import { AxMCPClient, AxMCPStdioTransport } from '@ax-llm/ax'

// สร้าง MCP client พร้อม transport
const transport = new AxMCPStdioTransport({
  command: 'npx',
  args: ['-y', '@modelcontextprotocol/server-memory'],
})

// สร้าง client พร้อมโหมด debug
const client = new AxMCPClient(transport, { debug: true })

// เริ่มเชื่อมต่อ
await client.init()

// ใช้ฟังก์ชัน client ใน agent
const memoryAgent = new AxAgent({
  name: 'MemoryAssistant',
  description: 'An assistant with persistent memory',
  signature: 'input, userId -&gt; response',
  functions: [client], // ใส่ client เป็น function provider
})

// หรือใช้ client กับ AxGen
const memoryGen = new AxGen('input, userId -&gt; response', {
  functions: [client],
})
</code></pre>
<h3>การใช้ AxMCPClient กับ Remote Server</h3>
<p>เรียก MCP server แบบ remote ได้ง่าย ตัวอย่าง การใช้ DeepWiki MCP server ถามข้อมูล repo สาธารณะบน GitHub DeepWiki MCP server อยู่ที่ <code>https://mcp.deepwiki.com/mcp</code></p>
<pre><code class="language-typescript">import {
  AxAgent,
  AxAI,
  AxAIOpenAIModel,
  AxMCPClient,
  AxMCPStreambleHTTPTransport,
} from '@ax-llm/ax'

// 1. เตรียม MCP transport ไป DeepWiki server
const transport = new AxMCPStreambleHTTPTransport(
  'https://mcp.deepwiki.com/mcp'
)

// 2. สร้าง MCP client
const mcpClient = new AxMCPClient(transport, { debug: false })
await mcpClient.init() // เริ่มเชื่อมต่อ

// 3. เตรียม AI model (เช่น OpenAI)
const ai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY as string,
})

// 4. สร้าง AxAgent ที่ใช้ MCP client
const deepwikiAgent = new AxAgent&lt;
  {
    questionAboutRepo: string
    githubRepositoryUrl: string
  },
  {
    answer: string
  }
&gt;({
  name: 'DeepWikiQueryAgent',
  description: 'Agent to query public GitHub repositories via DeepWiki MCP.',
  signature: 'questionAboutRepo, githubRepositoryUrl -&gt; answer',
  functions: [mcpClient], // ใส่ MCP client ใน agent
})

// 5. ถามคำถามและเรียก agent
const result = await deepwikiAgent.forward(ai, {
  questionAboutRepo: 'What is the main purpose of this library?',
  githubRepositoryUrl: 'https://github.com/dosco/ax',
})
console.log('DeepWiki Answer:', result.answer)
</code></pre>
<p>ตัวอย่างนี้แสดงการเชื่อมต่อ MCP server สาธารณะและใช้งานกับ agent ของ Ax signature (<code>questionAboutRepo, githubRepositoryUrl -&gt; answer</code>) เป็นตัวอย่างเท่านั้น โดยปกติจะดูฟังก์ชันและ signature จาก MCP server โดยตรง (เช่น จาก <code>mcp.getFunctions</code> หรือเอกสาร)</p>
<p>ดูตัวอย่างที่ซับซ้อนกว่านี้เช่น authentication และ custom header ใน remote MCP server ได้ที่ <code>src/examples/mcp-client-pipedream.ts</code></p>
<h2>AI Routing และ Load Balancing</h2>
<p>Ax มี 2 วิธีหลักในการทำงานกับ AI หลายบริการ: load balancer เพื่อความเสถียร และ router สำหรับ routing ตามโมเดล</p>
<h3>Load Balancer</h3>
<p>load balancer แจกจ่าย request ไปยัง AI หลายๆ บริการตามประสิทธิภาพและสถานะ หากบริการใดล้มเหลว จะสลับอัตโนมัติ</p>
<pre><code class="language-typescript">import { AxAI, AxBalancer } from '@ax-llm/ax'

// ตั้งค่าหลาย AI service
const openai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY,
})

const ollama = new AxAI({
  name: 'ollama',
  config: { model: 'nous-hermes2' },
})

const gemini = new AxAI({
  name: 'google-gemini',
  apiKey: process.env.GOOGLE_APIKEY,
})

// สร้าง load balancer
const balancer = new AxBalancer([openai, ollama, gemini])

// ใช้งานเหมือน AI ปกติ - เลือกบริการที่ดีที่สุดให้อัตโนมัติ
const response = await balancer.chat({
  chatPrompt: [{ role: 'user', content: 'Hello!' }],
})

// หรือใช้กับ AxGen
const gen = new AxGen(`question -&gt; answer`)
const res = await gen.forward(balancer, { question: 'Hello!' })
</code></pre>
<h3>Multi-Service Router</h3>
<p>router ช่วยให้ใช้ AI หลายบริการผ่านอินเทอร์เฟซเดียว โดย route ตามโมเดลที่ระบุ</p>
<pre><code class="language-typescript">import { AxAI, AxAIOpenAIModel, AxMultiServiceRouter } from '@ax-llm/ax'

// ตั้งค่า OpenAI กับรายชื่อโมเดล
const openai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY,
  models: [
    {
      key: 'basic',
      model: AxAIOpenAIModel.GPT4OMini,
      description:
        'Model for very simple tasks such as answering quick short questions',
    },
    {
      key: 'medium',
      model: AxAIOpenAIModel.GPT4O,
      description:
        'Model for semi-complex tasks such as summarizing text, writing code, and more',
    },
  ],
})

// ตั้งค่า Gemini
const gemini = new AxAI({
  name: 'google-gemini',
  apiKey: process.env.GOOGLE_APIKEY,
  models: [
    {
      key: 'deep-thinker',
      model: 'gemini-2.0-flash-thinking',
      description:
        'Model that can think deeply about a task, best for tasks that require planning',
    },
    {
      key: 'expert',
      model: 'gemini-2.0-pro',
      description:
        'Model that is the best for very complex tasks such as writing large essays, complex coding, and more',
    },
  ],
})

const ollama = new AxAI({
  name: 'ollama',
  config: { model: 'nous-hermes2' },
})

const secretService = {
  key: 'sensitive-secret',
  service: ollama,
  description: 'Model for sensitive secrets tasks',
}

// สร้าง router
const router = new AxMultiServiceRouter([openai, gemini, secretService])

// route ไปที่ expert model ของ OpenAI
const openaiResponse = await router.chat({
  chatPrompt: [{ role: 'user', content: 'Hello!' }],
  model: 'expert',
})

// หรือใช้กับ AxGen
const gen = new AxGen(`question -&gt; answer`)
const res = await gen.forward(router, { question: 'Hello!' })
</code></pre>
<p>load balancer เหมาะกับ high availability ส่วน router เหมาะกับงานที่ต้องเลือกโมเดลเฉพาะ ทั้งสองใช้ร่วมกับฟีเจอร์ Ax เช่น streaming, function calling, chain-of-thought ได้</p>
<p>สามารถใช้ balancer กับ router ร่วมกันได้</p>
<h2>รองรับ OpenTelemetry</h2>
<p>การ trace และสังเกตการณ์ workflow LLM สำคัญสำหรับ production OpenTelemetry เป็นมาตรฐานอุตสาหกรรม และเรารองรับ namespace <code>gen_ai</code> ดูเพิ่มที่ <code>src/examples/telemetry.ts</code></p>
<pre><code class="language-typescript">import { trace } from '@opentelemetry/api'
import {
  BasicTracerProvider,
  ConsoleSpanExporter,
  SimpleSpanProcessor,
} from '@opentelemetry/sdk-trace-base'

const provider = new BasicTracerProvider()
provider.addSpanProcessor(new SimpleSpanProcessor(new ConsoleSpanExporter()))
trace.setGlobalTracerProvider(provider)

const tracer = trace.getTracer('test')

const ai = new AxAI({
  name: 'ollama',
  config: { model: 'nous-hermes2' },
  options: { tracer },
})

const gen = new AxChainOfThought(
  ai,
  `text -&gt; shortSummary &quot;summarize in 5 to 10 words&quot;`
)

const res = await gen.forward({ text })
</code></pre>
<pre><code class="language-json">{
  &quot;traceId&quot;: &quot;ddc7405e9848c8c884e53b823e120845&quot;,
  &quot;name&quot;: &quot;Chat Request&quot;,
  &quot;id&quot;: &quot;d376daad21da7a3c&quot;,
  &quot;kind&quot;: &quot;SERVER&quot;,
  &quot;timestamp&quot;: 1716622997025000,
  &quot;duration&quot;: 14190456.542,
  &quot;attributes&quot;: {
    &quot;gen_ai.system&quot;: &quot;Ollama&quot;,
    &quot;gen_ai.request.model&quot;: &quot;nous-hermes2&quot;,
    &quot;gen_ai.request.max_tokens&quot;: 500,
    &quot;gen_ai.request.temperature&quot;: 0.1,
    &quot;gen_ai.request.top_p&quot;: 0.9,
    &quot;gen_ai.request.frequency_penalty&quot;: 0.5,
    &quot;gen_ai.request.llm_is_streaming&quot;: false,
    &quot;http.request.method&quot;: &quot;POST&quot;,
    &quot;url.full&quot;: &quot;http://localhost:11434/v1/chat/completions&quot;,
    &quot;gen_ai.usage.completion_tokens&quot;: 160,
    &quot;gen_ai.usage.prompt_tokens&quot;: 290
  }
}
</code></pre>
<h2>การปรับแต่ง prompt (พื้นฐาน)</h2>
<p>คุณสามารถปรับแต่ง prompt โดยใช้โมเดลขนาดใหญ่เพื่อเพิ่มประสิทธิภาพและผลลัพธ์ที่ดีขึ้น ด้วย optimizer เช่น <code>AxBootstrapFewShot</code> และตัวอย่างจากชุดข้อมูล <code>HotPotQA</code> optimizer จะสร้าง demo ซึ่งเมื่อใช้กับ prompt จะช่วยเพิ่มประสิทธิภาพ</p>
<pre><code class="language-typescript">// ดาวน์โหลด HotPotQA dataset จาก huggingface
const hf = new AxHFDataLoader({
  dataset: 'hotpot_qa',
  split: 'train',
})

const examples = await hf.getData&lt;{ question: string; answer: string }&gt;({
  count: 100,
  fields: ['question', 'answer'],
})

const ai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY as string,
})

// ตั้งค่าโปรแกรมสำหรับ tuning
const program = new AxChainOfThought&lt;{ question: string }, { answer: string }&gt;(
  ai,
  `question -&gt; answer &quot;in short 2 or 3 words&quot;`
)

// ตั้งค่า Bootstrap Few Shot optimizer
const optimize = new AxBootstrapFewShot&lt;
  { question: string },
  { answer: string }
&gt;({
  program,
  examples,
})

// ตั้งค่า evaluation metric เช่น em, f1 score
const metricFn: AxMetricFn = ({ prediction, example }) =&gt;
  emScore(prediction.answer as string, example.answer as string)

// run optimizer และอย่าลืมบันทึกผลลัพธ์
const result = await optimize.compile(metricFn);

// บันทึก demo ที่สร้างไว้
fs.writeFileSync('bootstrap-demos.json', JSON.stringify(result.demos, null, 2));
console.log('Demos saved to bootstrap-demos.json');
</code></pre>
<img width="853" alt="tune-prompt" src="https://github.com/dosco/llm-client/assets/832235/f924baa7-8922-424c-9c2c-f8b2018d8d74">
```
<h2>การปรับแต่ง prompt (ขั้นสูง, Mipro v2)</h2>
<p>MiPRO v2 เป็น framework ขั้นสูงสำหรับ optimization prompt ที่ใช้ Bayesian optimization เพื่อค้นหา instruction, demonstration, และ example ที่ดีที่สุดสำหรับโปรแกรม LLM ของคุณโดยอัตโนมัติ</p>
<h3>จุดเด่น</h3>
<ul>
<li><strong>Instruction optimization</strong>: สร้างและทดสอบ instruction อัตโนมัติ</li>
<li><strong>Few-shot example selection</strong>: เลือก demonstration ที่เหมาะสมจาก dataset</li>
<li><strong>Smart Bayesian optimization</strong>: ใช้ UCB strategy สำรวจ configuration อย่างมีประสิทธิภาพ</li>
<li><strong>Early stopping</strong>: หยุดเมื่อไม่มี improvement เพื่อประหยัดคอมพิวต์</li>
<li><strong>Program and data-aware</strong>: คำนึงถึงโครงสร้างโปรแกรมและ dataset</li>
</ul>
<h3>วิธีการทำงาน</h3>
<ol>
<li>สร้าง instruction candidate ต่างๆ</li>
<li>Bootstrap ตัวอย่าง few-shot จากข้อมูล</li>
<li>เลือก example จาก dataset</li>
<li>ใช้ Bayesian optimization หาค่าเหมาะสม</li>
<li>ใช้ configuration ที่ดีที่สุดกับโปรแกรม</li>
</ol>
<h3>การใช้งานเบื้องต้น</h3>
<pre><code class="language-typescript">import { AxAI, AxChainOfThought, AxMiPRO } from '@ax-llm/ax'

// 1. ตั้งค่า AI service
const ai = new AxAI({
  name: 'google-gemini',
  apiKey: process.env.GOOGLE_APIKEY,
})

// 2. สร้างโปรแกรม
const program = new AxChainOfThought(`input -&gt; output`)

// 3. ตั้งค่า optimizer
const optimizer = new AxMiPRO({
  ai,
  program,
  examples: trainingData,
  options: {
    numTrials: 20,
    auto: 'medium',
  },
})

// 4. กำหนด evaluation metric
const metricFn = ({ prediction, example }) =&gt; {
  return prediction.output === example.output
}

// 5. เริ่ม optimization
const optimizedProgram = await optimizer.compile(metricFn, {
  valset: validationData,
})

// 6. ใช้โปรแกรมที่ปรับแต่งแล้ว
const result = await optimizedProgram.forward(ai, { input: 'test input' })
</code></pre>
<h3>ตัวเลือกการตั้งค่า</h3>
<p>| Option                    | คำอธิบาย                                      | ค่าเริ่มต้น |
| ------------------------- | ---------------------------------------------- | ----------- |
| <code>numCandidates</code>           | จำนวน instruction candidate ที่จะสร้าง        | 5           |
| <code>numTrials</code>               | จำนวนรอบ optimization                        | 30          |
| <code>maxBootstrappedDemos</code>    | จำนวน bootstrapped demo สูงสุด                | 3           |
| <code>maxLabeledDemos</code>         | จำนวน labeled example สูงสุด                  | 4           |
| <code>minibatch</code>               | ใช้ minibatch เพื่อประเมินเร็วขึ้น            | true        |
| <code>minibatchSize</code>           | ขนาด minibatch สำหรับประเมิน                  | 25          |
| <code>earlyStoppingTrials</code>     | หยุดเมื่อไม่มี improvement หลัง N trial      | 5           |
| <code>minImprovementThreshold</code> | threshold การพัฒนาคะแนนขั้นต่ำ                | 0.01        |
| <code>programAwareProposer</code>    | ใช้โครงสร้างโปรแกรมเพื่อ proposal ที่ดีกว่า | true        |
| <code>dataAwareProposer</code>       | คำนึงถึง dataset                              | true        |
| <code>verbose</code>                 | แสดงความคืบหน้าอย่างละเอียด                  | false       |
| abort-patterns.ts | ตัวอย่างการ abort request |</p>
<h3>ระดับการ optimization</h3>
<p>ตั้งค่าความเข้มข้นการ optimize ด้วย <code>auto</code> parameter:</p>
<pre><code class="language-typescript">// light (เร็ว แต่ไม่ thorough)
const optimizedProgram = await optimizer.compile(metricFn, { auto: 'light' })

// medium (สมดุล)
const optimizedProgram = await optimizer.compile(metricFn, { auto: 'medium' })

// heavy ( thorough แต่ช้า)
const optimizedProgram = await optimizer.compile(metricFn, { auto: 'heavy' })
</code></pre>
<h3>ตัวอย่างขั้นสูง: วิเคราะห์ความรู้สึก</h3>
<pre><code class="language-typescript">// สร้างโปรแกรม sentiment analysis
const classifyProgram = new AxChainOfThought&lt;
  { productReview: string },
  { label: string }
&gt;(`productReview -&gt; label:string &quot;positive&quot; or &quot;negative&quot;`)

// ตั้งค่า optimizer ขั้นสูง
const optimizer = new AxMiPRO({
  ai,
  program: classifyProgram,
  examples: trainingData,
  options: {
    numCandidates: 3,
    numTrials: 10,
    maxBootstrappedDemos: 2,
    maxLabeledDemos: 3,
    earlyStoppingTrials: 3,
    programAwareProposer: true,
    dataAwareProposer: true,
    verbose: true,
  },
})

// run optimization และบันทึกผลลัพธ์
const optimizedProgram = await optimizer.compile(metricFn, {
  valset: validationData,
})

// บันทึก config สำหรับใช้ต่อ
const programConfig = JSON.stringify(optimizedProgram, null, 2);
await fs.promises.writeFile(&quot;./optimized-config.json&quot;, programConfig);
console.log('&gt; Done. Optimized program config saved to optimized-config.json');
</code></pre>
<h2>การใช้ Tuned Prompts</h2>
<p>ทั้ง Bootstrap Few Shot optimizer และ MiPRO v2 optimizer จะสร้าง <strong>demos</strong> (demonstration) ที่ช่วยปรับปรุงประสิทธิภาพโปรแกรมของคุณอย่างมาก</p>
<h3>Demos คืออะไร?</h3>
<p>Demos คือ input-output example ที่ถูกเพิ่มเข้า prompt อัตโนมัติ เพื่อแนะนำ LLM ให้เห็นตัวอย่างการแก้ปัญหาแบบ few-shot</p>
<h3>โหลดและใช้ demo</h3>
<p>ไม่ว่าจะใช้ Bootstrap หรือ MiPRO การใช้ demo ที่สร้างเหมือนกัน:</p>
<pre><code class="language-typescript">import fs from 'fs'
import { AxAI, AxGen, AxChainOfThought } from '@ax-llm/ax'

// 1. ตั้งค่า AI service
const ai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY,
})

// 2. สร้างโปรแกรม (signature เดียวกับตอน tuning)
const program = new AxChainOfThought(`question -&gt; answer &quot;in short 2 or 3 words&quot;`)

// 3. โหลด demo ที่บันทึกไว้
const demos = JSON.parse(fs.readFileSync('bootstrap-demos.json', 'utf8'))

// 4. ใส่ demo เข้าโปรแกรม
program.setDemos(demos)

// 5. ใช้โปรแกรมที่ปรับปรุงแล้ว
const result = await program.forward(ai, {
  question: 'What castle did David Gregory inherit?'
})

console.log(result) // แม่นยำขึ้นจากตัวอย่างที่เรียนรู้
</code></pre>
<h3>ตัวอย่างง่าย: Text Classification</h3>
<p>ตัวอย่างการใช้ demo เพื่อเพิ่มประสิทธิภาพงาน classification</p>
<pre><code class="language-typescript">// สร้างโปรแกรม classifier
const classifier = new AxGen(`text -&gt; category:class &quot;positive, negative, neutral&quot;`)

// โหลด demo ที่ได้จาก tuning
const savedDemos = JSON.parse(fs.readFileSync('classification-demos.json', 'utf8'))
classifier.setDemos(savedDemos)

// classifier จะเรียนรู้จากตัวอย่างและแม่นยำขึ้น
const result = await classifier.forward(ai, {
  text: &quot;This product exceeded my expectations!&quot;
})

console.log(result.category) // classification แม่นยำขึ้น
</code></pre>
<h3>ข้อดีของการใช้ demo</h3>
<ul>
<li><strong>แม่นยำขึ้น</strong>: โปรแกรมทำงานดีขึ้นมากด้วยตัวอย่างที่เกี่ยวข้อง</li>
<li><strong>ผลลัพธ์สม่ำเสมอ</strong>: demo ช่วยให้รูปแบบ output คงที่</li>
<li><strong>ลด Hallucination</strong>: ตัวอย่างช่วยชี้นำโมเดลให้ตอบตามที่ควร</li>
<li><strong>คุ้มค่า</strong>: ได้ผลลัพธ์ดีขึ้นโดยไม่ต้องใช้โมเดลใหญ่หรือแพงกว่า</li>
</ul>
<h3>Best Practices</h3>
<ol>
<li><strong>บันทึก demo</strong>: บันทึก demo ลงไฟล์ไว้ใช้ซ้ำ</li>
<li><strong>ใช้ signature เดิม</strong>: โหลด demo ด้วย signature เดียวกับตอน tuning</li>
<li><strong>ควบคุมเวอร์ชัน</strong>: เก็บไฟล์ demo ไว้ใน version control</li>
<li><strong>อัปเดตสม่ำเสมอ</strong>: tune ใหม่เป็นระยะด้วยข้อมูลใหม่</li>
</ol>
<p>ทั้ง Bootstrap และ MiPRO v2 สร้าง demo ในรูปแบบเดียวกัน ใช้ pattern นี้ได้กับทั้งสองแบบ</p>
<h2>ฟังก์ชันในตัว</h2>
<p>| Function           | Name               | คำอธิบาย                                            |
| ------------------ | ------------------ | --------------------------------------------------- |
| JS Interpreter     | AxJSInterpreter    | รัน JS ใน sandbox                                    |
| Docker Sandbox     | AxDockerSession    | รันคำสั่งในสภาพแวดล้อม docker                      |
| Embeddings Adapter | AxEmbeddingAdapter | รับ embedding และส่งต่อไปยังฟังก์ชันของคุณ          |</p>
<h2>ดูตัวอย่างทั้งหมด</h2>
<p>ใช้ <code>tsx</code> รันตัวอย่างต่างๆ ช่วยให้รัน typescript ได้บน node และรองรับ <code>.env</code> สำหรับ API Key</p>
<pre><code class="language-shell">OPENAI_APIKEY=api-key npm run tsx ./src/examples/marketing.ts
</code></pre>
<p>| Example                 | คำอธิบาย                                             |
| ----------------------- | ---------------------------------------------------- |
| customer-support.ts     | ดึงข้อมูลสำคัญจากการสื่อสารกับลูกค้า               |
| function.ts             | ตัวอย่าง function calling อย่างง่าย                  |
| food-search.ts          | ตัวอย่าง multi-step, multi-function calling         |
| marketing.ts            | สร้างข้อความ sms การตลาดสั้นๆ ที่มีประสิทธิภาพ      |
| vectordb.ts             | chunk, embed และค้นหาข้อความ                        |
| fibonacci.ts            | ใช้ interpreter JS คำนวณ fibonacci                  |
| summarize.ts            | สรุปข้อความขนาดใหญ่                                 |
| chain-of-thought.ts     | ใช้ chain-of-thought ในการตอบคำถาม                  |
| rag.ts                  | ใช้ multi-hop retrieval เพื่อตอบคำถาม               |
| rag-docs.ts             | แปลง PDF เป็นข้อความและฝังสำหรับ rag search        |
| react.ts                | ใช้ function calling และ reasoning ตอบคำถาม         |
| agent.ts                | agent framework, agent ใช้ agent อื่น/เครื่องมือได้ |
| streaming1.ts           | ตรวจสอบ output field ขณะ streaming                  |
| streaming2.ts           | ตรวจสอบ output field ทีละ field ขณะ streaming       |
| streaming3.ts           | ตัวอย่าง end-to-end streaming <code>streamingForward()</code>  |
| smart-hone.ts           | agent มองหาสุนัขในบ้านอัจฉริยะ                     |
| multi-modal.ts          | ใช้รูปภาพพร้อม input อื่น                           |
| balancer.ts             | balance หลาย llm ตาม cost ฯลฯ                       |
| docker.ts               | ใช้ docker sandbox หาไฟล์ตามคำอธิบาย                |
| prime.ts                | ใช้ field processor ประมวลผล field ใน prompt         |
| simple-classify.ts      | classifier อย่างง่าย                                 |
| mcp-client-memory.ts    | ใช้ MCP server สำหรับ memory กับ Ax                 |
| mcp-client-blender.ts   | ใช้ MCP server สำหรับ Blender กับ Ax                |
| mcp-client-pipedream.ts | ตัวอย่างเชื่อมต่อ MCP remote                        |
| tune-bootstrap.ts       | ใช้ bootstrap optimizer เพื่อเพิ่มประสิทธิภาพ prompt|
| tune-mipro.ts           | ใช้ mipro v2 optimizer เพื่อเพิ่มประสิทธิภาพ prompt|
| tune-usage.ts           | ใช้ tuned prompts ที่ปรับปรุงแล้ว                    |
| telemetry.ts            | trace และส่ง trace ไปยัง Jaeger service              |
| openai-responses.ts     | ตัวอย่างใช้ OpenAI Responses API ใหม่                |
| use-examples.ts | ตัวอย่างใช้ 'examples' เพื่อกำกับ llm |</p>
<h2>เป้าหมายของเรา</h2>
<p>LLM (Large language models) ทรงพลังมากขึ้นเรื่อยๆ และสามารถเป็น backend ให้ผลิตภัณฑ์ของคุณได้ทั้งระบบ แต่ยังมีความซับซ้อนทั้งเรื่อง prompt, model, streaming, function call, error correction ฯลฯ เราต้องการรวมความซับซ้อนเหล่านี้ไว้ในไลบรารีที่บำรุงรักษาและใช้งานง่าย รองรับ LLM ที่ล้ำหน้าทุกตัว และอัปเดตความสามารถใหม่ๆ ตามงานวิจัยล่าสุด เช่น DSPy</p>
<h2>ใช้ไลบรารีนี้อย่างไร</h2>
<h3>1. เลือก AI ที่ต้องการใช้</h3>
<pre><code class="language-ts">// เลือก LLM
const ai = new AxOpenAI({ apiKey: process.env.OPENAI_APIKEY } as AxOpenAIArgs)
</code></pre>
<h3>2. สร้าง prompt signature ตาม usecase</h3>
<pre><code class="language-ts">// signature กำหนด input/output ของ prompt program
const cot = new ChainOfThought(ai, `question:string -&gt; answer:string`, { mem })
</code></pre>
<h3>3. รัน prompt program ที่สร้างขึ้น</h3>
<pre><code class="language-ts">// ส่ง input ที่กำหนดใน signature
const res = await cot.forward({ question: 'Are we in a simulation?' })
</code></pre>
<h3>4. หรือใช้ LLM โดยตรง</h3>
<pre><code class="language-ts">const res = await ai.chat([
  { role: &quot;system&quot;, content: &quot;Help the customer with his questions&quot; }
  { role: &quot;user&quot;, content: &quot;I'm looking for a Macbook Pro M2 With 96GB RAM?&quot; }
]);
</code></pre>
<h2>ใช้ function calling อย่างไร</h2>
<h3>1. กำหนดฟังก์ชัน</h3>
<pre><code class="language-ts">// กำหนดฟังก์ชันและ handler
const functions = [
  {
    name: 'getCurrentWeather',
    description: 'get the current weather for a location',
    parameters: {
      type: 'object',
      properties: {
        location: {
          type: 'string',
          description: 'location to get weather for',
        },
        units: {
          type: 'string',
          enum: ['imperial', 'metric'],
          default: 'imperial',
          description: 'units to use',
        },
      },
      required: ['location'],
    },
    func: async (args: Readonly&lt;{ location: string; units: string }&gt;) =&gt; {
      return `The weather in ${args.location} is 72 degrees`
    },
  },
]
</code></pre>
<h3>2. ส่งฟังก์ชันเข้า prompt</h3>
<pre><code class="language-ts">const cot = new AxGen(ai, `question:string -&gt; answer:string`, { functions })
</code></pre>
<h2>เปิด debug log</h2>
<pre><code class="language-ts">const ai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY,
} as AxOpenAIArgs)
ai.setOptions({ debug: true })
</code></pre>
<h2>ติดต่อเรา</h2>
<p>ยินดีช่วยเหลือ ติดต่อสอบถามหรือเข้าร่วม Discord <a href="https://twitter.com/dosco">twitter/dosco</a></p>
<h2>FAQ</h2>
<h3>1. LLM เลือกฟังก์ชันไม่ถูกต้อง</h3>
<p>ตั้งชื่อและอธิบายฟังก์ชันให้ชัดเจน รวมถึงอธิบาย parameter อย่างกระชับและแม่นยำ</p>
<h3>2. เปลี่ยนการตั้งค่า LLM ได้อย่างไร</h3>
<p>ส่ง object configuration เป็นพารามิเตอร์ที่สองขณะสร้าง LLM object</p>
<pre><code class="language-ts">const apiKey = process.env.OPENAI_APIKEY
const conf = AxOpenAIBestConfig()
const ai = new AxOpenAI({ apiKey, conf } as AxOpenAIArgs)
</code></pre>
<h2>3. Prompt ยาวเกินไป / จะเปลี่ยน max tokens ได้ไหม</h2>
<pre><code class="language-ts">const conf = axOpenAIDefaultConfig() // หรือ OpenAIBestOptions()
conf.maxTokens = 2000
</code></pre>
<h2>4. เปลี่ยนโมเดลอย่างไร (เช่น GPT4)</h2>
<pre><code class="language-ts">const conf = axOpenAIDefaultConfig() // หรือ OpenAIBestOptions()
conf.model = OpenAIModel.GPT4Turbo
</code></pre>
<h2>Monorepo tips &amp; tricks</h2>
<p>ควรติดตั้ง <code>npm install</code> เฉพาะที่ root directory เท่านั้นเพื่อป้องกันการสร้าง <code>package-lock.json</code> ซ้อนซ้อนและ node_modules ซ้ำซ้อน</p>
<p>ติดตั้ง dependencies ใหม่ใน package ให้ใช้ <code>npm install lodash --workspace=ax</code> (หรือแก้ไข <code>package.json</code> แล้วรัน <code>npm install</code> ที่ root)</p>
<pre><code></code></pre>
<hr />
<p>Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx">Open Ai Tx</a> | Last indexed: 2025-06-07</p>
<hr />

        </div>
    </div>
    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Default Statcounter code for openaitx
https://openaitx.github.io/ -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>
    <!-- End of Statcounter Code -->
</body>
</html>