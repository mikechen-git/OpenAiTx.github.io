<!DOCTYPE html>
<html lang="pt">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ax - ax-llm/ax</title>
    <meta name="title" content="ax - ax-llm/ax">
    <meta name="description" content="ax-llm/ax - GitHub repository pt documentation and informationAx, DSPy para Typescript Trabalhar com LLMs é complexo — eles nem sempre fazem o que você deseja. O DSPy facilita a construção de soluções incríveis com LLMs. B...">
    <meta name="keywords" content="ax-llm, ax, GitHub, repository, pt documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/ax-llm/ax/README-pt.html">
    <meta property="og:title" content="ax - ax-llm/ax">
    <meta property="og:description" content="ax-llm/ax - GitHub repository pt documentation and information">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">
    <script src="/js/marked.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        body { display: flex; flex-direction: column; min-height: 100vh; }
        .main-container { margin: 0 auto; width: 100%; max-width: 980px; padding: 0 20px; }
        @media (max-width: 768px) { .main-container { padding: 0 15px; } }
        .markdown-body img { max-width: 100%; height: auto; }
        .header { text-align: center; margin-bottom: 30px; padding: 20px; background-color: #f6f8fa; border-bottom: 1px solid #e1e4e8; position: relative; }
        .header .links { margin-top: 10px; font-size: 16px; }
        .header .links a { color: #0366d6; text-decoration: none; margin-left: 5px; }
        .header .links a:hover { text-decoration: underline; }
        .language-badges { margin-top: 15px; text-align: center; }
        .language-badges a { display: inline-block; margin: 2px; text-decoration: none; }
        .language-badges img { height: 20px; border-radius: 3px; }
        .language-badges a:hover img { opacity: 0.8; }
    </style>
</head>
<body>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/ax-llm/ax" id="githubRepoLink" target="_blank">ax-llm/ax</a>
<h1 style="display: none;">Ax, DSPy para Typescript Trabalhar com LLMs é complexo — eles nem sempre fazem o que você deseja. O DSPy facilita a construção de soluções incríveis com LLMs. B...</h1>
        </div>
        <div class="language-badges" id="languageBadges">
            <!-- You can generate language badges here if needed -->
        </div>
    </div>
    <div class="main-container">
        <div class="markdown-body" id="content">
            <h1>Ax, DSPy para Typescript</h1>
<p>Trabalhar com LLMs é complexo — eles nem sempre fazem o que você deseja. O DSPy facilita a construção de soluções incríveis com LLMs. Basta definir suas entradas e saídas (assinatura) e um prompt eficiente é gerado automaticamente e utilizado. Conecte várias assinaturas para construir sistemas e fluxos de trabalho complexos usando LLMs.</p>
<p>E para te ajudar realmente a usar isso em produção, temos tudo o que você precisa, como observabilidade, streaming, suporte a outras modalidades (imagens, áudio, etc.), correção de erros, chamadas de função multi-etapas, MCP, RAG, etc.</p>
<p><a href="https://www.npmjs.com/package/@ax-llm/ax"><img src="https://img.shields.io/npm/v/@ax-llm/ax?style=for-the-badge&amp;color=green" alt="NPM Package" /></a>
<a href="https://discord.gg/DSHg3dU7dW"><img src="https://dcbadge.vercel.app/api/server/DSHg3dU7dW?style=for-the-badge" alt="Discord Chat" /></a>
<a href="https://twitter.com/dosco"><img src="https://img.shields.io/twitter/follow/dosco?style=for-the-badge&amp;color=red" alt="Twitter" /></a></p>
<!-- header -->
<h2>Por que usar Ax?</h2>
<ul>
<li>Interface padrão para todos os principais LLMs</li>
<li>Prompts compilados a partir de assinaturas simples</li>
<li>Streaming nativo de ponta a ponta</li>
<li>Suporte para orçamento de pensamento e tokens de raciocínio</li>
<li>Construa agentes que podem chamar outros agentes</li>
<li>Suporte nativo ao MCP, Model Context Protocol</li>
<li>Converta documentos de qualquer formato para texto</li>
<li>RAG, chunking inteligente, embedding, consulta</li>
<li>Funciona com Vercel AI SDK</li>
<li>Validação de saída durante o streaming</li>
<li>DSPy multimodal suportado</li>
<li>Ajuste automático de prompts usando otimizadores</li>
<li>Rastreamento / observabilidade OpenTelemetry</li>
<li>Código Typescript pronto para produção</li>
<li>Leve, sem dependências</li>
</ul>
<h2>Pronto para Produção</h2>
<ul>
<li>Sem mudanças incompatíveis (versões menores)</li>
<li>Grande cobertura de testes</li>
<li>Suporte nativo Open Telemetry <code>gen_ai</code></li>
<li>Amplamente usado por startups em produção</li>
</ul>
<h2>O que é uma assinatura de prompt?</h2>
<img width="860" alt="shapes at 24-03-31 00 05 55" src="https://raw.githubusercontent.com/ax-llm/ax/main/githubusercontent.com/dosco/llm-client/assets/832235/0f0306ea-1812-4a0a-9ed5-76cd908cd26b">
<p>Prompts eficientes e type-safe são gerados automaticamente a partir de uma assinatura simples. Uma assinatura de prompt é composta por uma
<code>&quot;descrição da tarefa&quot; campoEntrada:tipo &quot;descrição do campo&quot; -&gt; campoSaída:tipo</code>.
A ideia por trás das assinaturas de prompt é baseada no trabalho feito no artigo
&quot;Demonstrate-Search-Predict&quot;.</p>
<p>Você pode ter vários campos de entrada e saída, e cada campo pode ser dos tipos <code>string</code>, <code>number</code>, <code>boolean</code>, <code>date</code>, <code>datetime</code>,
<code>class &quot;class1, class2&quot;</code>, <code>JSON</code>, ou um array de qualquer um destes, por exemplo, <code>string[]</code>.
Quando um tipo não é definido, o padrão é <code>string</code>. O sufixo <code>?</code> torna o campo opcional (obrigatório por padrão) e <code>!</code> torna o campo interno, útil para coisas como raciocínio.</p>
<h2>Tipos de Campo de Saída</h2>
<p>| Tipo                     | Descrição                             | Uso                        | Exemplo de Saída                                     |
| ------------------------ | ------------------------------------- | -------------------------- | ---------------------------------------------------- |
| <code>string</code>                 | Uma sequência de caracteres.          | <code>fullName:string</code>          | <code>&quot;exemplo&quot;</code>                                          |
| <code>number</code>                 | Um valor numérico.                    | <code>price:number</code>             | <code>42</code>                                                 |
| <code>boolean</code>                | Um valor verdadeiro ou falso.         | <code>isEvent:boolean</code>          | <code>true</code>, <code>false</code>                                      |
| <code>date</code>                   | Um valor de data.                     | <code>startDate:date</code>           | <code>&quot;2023-10-01&quot;</code>                                       |
| <code>datetime</code>               | Valor de data e hora.                 | <code>createdAt:datetime</code>       | <code>&quot;2023-10-01T12:00:00Z&quot;</code>                             |
| <code>class &quot;class1,class2&quot;</code>  | Classificação de itens.               | <code>category:class</code>           | <code>[&quot;class1&quot;, &quot;class2&quot;, &quot;class3&quot;]</code>                     |
| <code>string[]</code>               | Array de strings.                     | <code>tags:string[]</code>            | <code>[&quot;exemplo1&quot;, &quot;exemplo2&quot;]</code>                           |
| <code>number[]</code>               | Array de números.                     | <code>scores:number[]</code>          | <code>[1, 2, 3]</code>                                          |
| <code>boolean[]</code>              | Array de valores booleanos.           | <code>permissions:boolean[]</code>    | <code>[true, false, true]</code>                                |
| <code>date[]</code>                 | Array de datas.                       | <code>holidayDates:date[]</code>      | <code>[&quot;2023-10-01&quot;, &quot;2023-10-02&quot;]</code>                       |
| <code>datetime[]</code>             | Array de datas e horários.            | <code>logTimestamps:datetime[]</code> | <code>[&quot;2023-10-01T12:00:00Z&quot;, &quot;2023-10-02T12:00:00Z&quot;]</code>   |
| <code>class[] &quot;class1,class2&quot;</code>| Várias classes                        | <code>categories:class[]</code>       | <code>[&quot;class1&quot;, &quot;class2&quot;, &quot;class3&quot;]</code>                     |
| <code>code &quot;language&quot;</code>        | Bloco de código em linguagem específica| <code>code:code &quot;python&quot;</code>      | <code>print('Hello, world!')</code>                             |</p>
<h2>LLMs Suportados</h2>
<p><code>Google Gemini</code>, <code>OpenAI</code>, <code>Azure OpenAI</code>, <code>Anthropic</code>, <code>X Grok</code>, <code>TogetherAI</code>, <code>Cohere</code>, <code>Mistral</code>, <code>Groq</code>, <code>DeepSeek</code>, <code>Ollama</code>, <code>Reka</code>,
<code>Hugging Face</code></p>
<h2>Instalação</h2>
<pre><code class="language-bash">npm install @ax-llm/ax
# ou
yarn add @ax-llm/ax
</code></pre>
<h2>Exemplo: Usando chain-of-thought para resumir texto</h2>
<pre><code class="language-typescript">import { AxAI, AxChainOfThought } from '@ax-llm/ax'

const textToSummarize = `
The technological singularity—or simply the singularity[1]—is a hypothetical future point in time at which technological growth becomes uncontrollable and irreversible, resulting in unforeseeable changes to human civilization.[2][3] ...`

const ai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY as string,
})

const gen = new AxChainOfThought(
  `textToSummarize -&gt; textType:class &quot;note, email, reminder&quot;, shortSummary &quot;summarize in 5 to 10 words&quot;`
)

const res = await gen.forward(ai, { textToSummarize })

console.log('&gt;', res)
</code></pre>
<h2>Exemplo: Construindo um agente</h2>
<p>Use o prompt de agente (framework) para construir agentes que trabalham com outros agentes para completar tarefas. Agentes são fáceis de criar com assinaturas de prompt. Experimente o exemplo de agente.</p>
<pre><code class="language-typescript"># npm run tsx ./src/examples/agent.ts

const researcher = new AxAgent({
  name: 'researcher',
  description: 'Researcher agent',
  signature: `physicsQuestion &quot;physics questions&quot; -&gt; answer &quot;reply in bullet points&quot;`
});

const summarizer = new AxAgent({
  name: 'summarizer',
  description: 'Summarizer agent',
  signature: `text &quot;text so summarize&quot; -&gt; shortSummary &quot;summarize in 5 to 10 words&quot;`
});

const agent = new AxAgent({
  name: 'agent',
  description: 'A an agent to research complex topics',
  signature: `question -&gt; answer`,
  agents: [researcher, summarizer]
});

agent.forward(ai, { questions: &quot;How many atoms are there in the universe&quot; })
</code></pre>
<h2>Suporte a Modelos de Pensamento</h2>
<p>Ax oferece suporte nativo para modelos com capacidades de pensamento, permitindo controlar o orçamento de tokens de pensamento e acessar o raciocínio do modelo. Esse recurso ajuda a entender o processo de raciocínio do modelo e otimizar o uso de tokens.</p>
<pre><code class="language-typescript">const ai = new AxAI({
  name: 'google-gemini',
  apiKey: process.env.GOOGLE_APIKEY as string,
  config: {
    model: AxAIGoogleGeminiModel.Gemini25Flash,
    thinking: { includeThoughts: true },
  },
})

// Ou controle o orçamento de pensamento por requisição
const gen = new AxChainOfThought(`question -&gt; answer`)
const res = await gen.forward(
  ai,
  { question: 'What is quantum entanglement?' },
  { thinkingTokenBudget: 'medium' } // 'minimal', 'low', 'medium' ou 'high'
)

// Acesse pensamentos na resposta
console.log(res.thoughts) // Mostra o processo de raciocínio do modelo
</code></pre>
<h2>Bancos de Dados Vetoriais Suportados</h2>
<p>Bancos de dados vetoriais são essenciais para construir fluxos de trabalho com LLM. Temos abstrações limpas sobre bancos de dados vetoriais populares e nosso próprio banco de dados vetorial em memória.</p>
<p>| Provedor    | Testado |
| ----------- | ------- |
| Em Memória  | 🟢 100% |
| Weaviate    | 🟢 100% |
| Cloudflare  | 🟡 50%  |
| Pinecone    | 🟡 50%  |</p>
<pre><code class="language-typescript">// Crie embeddings de texto usando um LLM
const ret = await this.ai.embed({ texts: 'hello world' })

// Crie um banco vetorial em memória
const db = new axDB('memory')

// Insira no banco vetorial
await this.db.upsert({
  id: 'abc',
  table: 'products',
  values: ret.embeddings[0],
})

// Consulte por entradas similares usando embeddings
const matches = await this.db.query({
  table: 'products',
  values: embeddings[0],
})
</code></pre>
<p>Alternativamente, você pode usar o <code>AxDBManager</code> que gerencia chunking, embedding e consultas automaticamente, facilitando tudo.</p>
<pre><code class="language-typescript">const manager = new AxDBManager({ ai, db })
await manager.insert(text)

const matches = await manager.query(
  'John von Neumann on human intelligence and singularity.'
)
console.log(matches)
</code></pre>
<h2>Documentos RAG</h2>
<p>Usar documentos como PDF, DOCX, PPT, XLS, etc., com LLMs é complicado. Facilitamos isso com o Apache Tika, um mecanismo open-source de processamento de documentos.</p>
<p>Inicie o Apache Tika</p>
<pre><code class="language-shell">docker run -p 9998:9998 apache/tika
</code></pre>
<p>Converta documentos para texto e faça o embedding para recuperação usando o <code>AxDBManager</code>, que também suporta reranker e rewriter de consulta. Duas implementações padrão, <code>AxDefaultResultReranker</code> e <code>AxDefaultQueryRewriter</code>, estão disponíveis.</p>
<pre><code class="language-typescript">const tika = new AxApacheTika()
const text = await tika.convert('/path/to/document.pdf')

const manager = new AxDBManager({ ai, db })
await manager.insert(text)

const matches = await manager.query('Find some text')
console.log(matches)
</code></pre>
<h2>DSPy Multimodal</h2>
<p>Ao usar modelos como <code>GPT-4o</code> e <code>Gemini</code> que suportam prompts multimodais, é possível usar campos de imagem, funcionando com todo o pipeline DSP.</p>
<pre><code class="language-typescript">const image = fs
  .readFileSync('./src/examples/assets/kitten.jpeg')
  .toString('base64')

const gen = new AxChainOfThought(`question, animalImage:image -&gt; answer`)

const res = await gen.forward(ai, {
  question: 'What family does this animal belong to?',
  animalImage: { mimeType: 'image/jpeg', data: image },
})
</code></pre>
<p>Ao usar modelos como <code>gpt-4o-audio-preview</code> que suportam prompts multimodais com áudio, também é possível usar campos de áudio, funcionando em todo o pipeline DSP.</p>
<pre><code class="language-typescript">const audio = fs
  .readFileSync('./src/examples/assets/comment.wav')
  .toString('base64')

const gen = new AxGen(`question, commentAudio:audio -&gt; answer`)

const res = await gen.forward(ai, {
  question: 'What family does this animal belong to?',
  commentAudio: { format: 'wav', data: audio },
})
</code></pre>
<h2>API de Chat DSPy</h2>
<p>Inspirado pelo demonstration weaving do DSPy, Ax fornece o <code>AxMessage</code> para gerenciamento de histórico de conversas. Isso permite criar chatbots e agentes conversacionais que mantêm contexto em múltiplas interações, aproveitando todo o poder das assinaturas de prompt. Veja o exemplo para mais detalhes.</p>
<pre><code class="language-shell">GOOGLE_APIKEY=api-key npm run tsx ./src/examples/chat.ts
</code></pre>
<pre><code class="language-typescript">const chatBot = new AxGen&lt;
  { message: string } | ReadonlyArray&lt;ChatMessage&gt;,
  { reply: string }
&gt;(
  `message:string &quot;A casual message from the user&quot; -&gt; reply:string &quot;A friendly, casual response&quot;`
)

await chatBot.forward(ai, [
  {
    role: 'user',
    values: { message: 'Hi! How are you doing today?' },
  },
  {
    role: 'assistant',
    values: { reply: 'I am doing great! How about you?' },
  },
  {
    role: 'user',
    values: { message: 'Thats great!' },
  },
])
</code></pre>
<p>O histórico da conversa é automaticamente incorporado ao prompt, permitindo que o modelo mantenha o contexto e forneça respostas coerentes. Isso funciona perfeitamente com todos os recursos do Ax, incluindo streaming, chamadas de função e chain-of-thought.</p>
<h2>Streaming</h2>
<h3>Asserções</h3>
<p>Suportamos parsing de campos de saída e execução de funções durante o streaming. Isso permite falhas rápidas e correção de erros sem esperar pela saída completa, economizando tokens e reduzindo latência. Asserções são uma forma poderosa de garantir que a saída atenda seus requisitos; elas também funcionam com streaming.</p>
<pre><code class="language-typescript">// configurar o programa de prompt
const gen = new AxChainOfThought(
  ai,
  `startNumber:number -&gt; next10Numbers:number[]`
)

// adicionar uma asserção para garantir que o número 5 não está em um campo de saída
gen.addAssert(({ next10Numbers }: Readonly&lt;{ next10Numbers: number[] }&gt;) =&gt; {
  return next10Numbers ? !next10Numbers.includes(5) : undefined
}, 'Numbers 5 is not allowed')

// executar o programa com streaming habilitado
const res = await gen.forward({ startNumber: 1 }, { stream: true })

// ou execute o programa com streaming end-to-end
const generator = await gen.streamingForward(
  { startNumber: 1 },
  {
    stream: true,
  }
)
for await (const res of generator) {
}
</code></pre>
<p>O exemplo acima permite validar campos inteiros de saída à medida que são transmitidos. Essa validação funciona tanto com streaming quanto sem, e é acionada quando o valor do campo inteiro está disponível. Para validação real durante o streaming, veja o exemplo abaixo. Isso melhora muito o desempenho e economiza tokens em escala de produção.</p>
<pre><code class="language-typescript">// adicionar uma asserção para garantir que todas as linhas começam com um número e um ponto
gen.addStreamingAssert(
  'answerInPoints',
  (value: string) =&gt; {
    const re = /^\d+\./

    // divide o valor em linhas, remove espaços, filtra linhas vazias e verifica se todas começam com o regex
    return value
      .split('\n')
      .map((x) =&gt; x.trim())
      .filter((x) =&gt; x.length &gt; 0)
      .every((x) =&gt; re.test(x))
  },
  'Lines must start with a number and a dot. Eg: 1. This is a line.'
)

// execute o programa com streaming habilitado
const res = await gen.forward(
  {
    question: 'Provide a list of optimizations to speedup LLM inference.',
  },
  { stream: true, debug: true }
)
</code></pre>
<h3>Processadores de Campo</h3>
<p>Processadores de campo são uma forma poderosa de processar campos em um prompt antes de enviá-lo ao LLM.</p>
<pre><code class="language-typescript">const gen = new AxChainOfThought(
  ai,
  `startNumber:number -&gt; next10Numbers:number[]`
)

const streamValue = false

const processorFunction = (value) =&gt; {
  return value.map((x) =&gt; x + 1)
}

// Adicione um processador de campo ao programa
const processor = new AxFieldProcessor(
  gen,
  'next10Numbers',
  processorFunction,
  streamValue
)

const res = await gen.forward({ startNumber: 1 })
</code></pre>
<h2>Model Context Protocol (MCP)</h2>
<p>Ax oferece integração perfeita com o Model Context Protocol (MCP), permitindo que seus agentes acessem ferramentas externas e recursos através de uma interface padronizada.</p>
<h3>Usando AxMCPClient</h3>
<p>O <code>AxMCPClient</code> permite conectar-se a qualquer servidor compatível com MCP e usar suas capacidades dentro dos agentes Ax:</p>
<pre><code class="language-typescript">import { AxMCPClient, AxMCPStdioTransport } from '@ax-llm/ax'

// Inicialize um cliente MCP com um transporte
const transport = new AxMCPStdioTransport({
  command: 'npx',
  args: ['-y', '@modelcontextprotocol/server-memory'],
})

// Crie o cliente com modo debug opcional
const client = new AxMCPClient(transport, { debug: true })

// Inicialize a conexão
await client.init()

// Use as funções do cliente em um agente
const memoryAgent = new AxAgent({
  name: 'MemoryAssistant',
  description: 'An assistant with persistent memory',
  signature: 'input, userId -&gt; response',
  functions: [client], // Passe o cliente como provedor de função
})

// Ou use o cliente com AxGen
const memoryGen = new AxGen('input, userId -&gt; response', {
  functions: [client],
})
</code></pre>
<h3>Usando AxMCPClient com um Servidor Remoto</h3>
<p>Chamar um servidor MCP remoto com Ax é simples. Por exemplo, veja como usar o servidor DeepWiki MCP para perguntar sobre qualquer repositório público do GitHub. O servidor DeepWiki MCP está disponível em <code>https://mcp.deepwiki.com/mcp</code>.</p>
<pre><code class="language-typescript">import {
  AxAgent,
  AxAI,
  AxAIOpenAIModel,
  AxMCPClient,
  AxMCPStreambleHTTPTransport,
} from '@ax-llm/ax'

// 1. Inicialize o transporte MCP para o servidor DeepWiki
const transport = new AxMCPStreambleHTTPTransport(
  'https://mcp.deepwiki.com/mcp'
)

// 2. Crie o cliente MCP
const mcpClient = new AxMCPClient(transport, { debug: false })
await mcpClient.init() // Inicialize a conexão

// 3. Inicialize seu modelo AI (ex: OpenAI)
// Garanta que a variável de ambiente OPENAI_APIKEY esteja definida
const ai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY as string,
})

// 4. Crie um AxAgent que usa o cliente MCP
const deepwikiAgent = new AxAgent&lt;
  {
    // Defina tipos de entrada para clareza, combinando com uma função possível do DeepWiki
    questionAboutRepo: string
    githubRepositoryUrl: string
  },
  {
    answer: string
  }
&gt;({
  name: 'DeepWikiQueryAgent',
  description: 'Agent to query public GitHub repositories via DeepWiki MCP.',
  signature: 'questionAboutRepo, githubRepositoryUrl -&gt; answer',
  functions: [mcpClient], // Forneça o cliente MCP ao agente
})

// 5. Formule uma pergunta e chame o agente
const result = await deepwikiAgent.forward(ai, {
  questionAboutRepo: 'What is the main purpose of this library?',
  githubRepositoryUrl: 'https://github.com/dosco/ax', // Exemplo: biblioteca Ax
})
console.log('DeepWiki Answer:', result.answer)
</code></pre>
<p>Este exemplo mostra como conectar-se a um servidor MCP público e usá-lo dentro de um agente Ax. A assinatura do agente (<code>questionAboutRepo, githubRepositoryUrl -&gt; answer</code>) é uma suposição de como interagir com o serviço DeepWiki; normalmente, você descobriria as funções disponíveis e suas assinaturas diretamente do servidor MCP (ex: via chamada <code>mcp.getFunctions</code>, se suportada, ou documentação).</p>
<p>Para um exemplo mais complexo envolvendo autenticação e cabeçalhos customizados com um servidor MCP remoto, consulte o arquivo <code>src/examples/mcp-client-pipedream.ts</code> neste repositório.</p>
<h2>Roteamento de IA e Balanceamento de Carga</h2>
<p>Ax oferece duas formas poderosas de trabalhar com múltiplos serviços de IA: um balanceador de carga para alta disponibilidade e um roteador para roteamento específico por modelo.</p>
<h3>Balanceador de Carga</h3>
<p>O balanceador de carga distribui automaticamente requisições entre vários serviços de IA com base em desempenho e disponibilidade. Se um serviço falhar, ele faz failover automaticamente para o próximo disponível.</p>
<pre><code class="language-typescript">import { AxAI, AxBalancer } from '@ax-llm/ax'

// Configure múltiplos serviços de IA
const openai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY,
})

const ollama = new AxAI({
  name: 'ollama',
  config: { model: 'nous-hermes2' },
})

const gemini = new AxAI({
  name: 'google-gemini',
  apiKey: process.env.GOOGLE_APIKEY,
})

// Crie um balanceador com todos os serviços
const balancer = new AxBalancer([openai, ollama, gemini])

// Use como um serviço IA regular - utiliza automaticamente o melhor serviço disponível
const response = await balancer.chat({
  chatPrompt: [{ role: 'user', content: 'Hello!' }],
})

// Ou use o balanceador com AxGen
const gen = new AxGen(`question -&gt; answer`)
const res = await gen.forward(balancer, { question: 'Hello!' })
</code></pre>
<h3>Roteador Multi-Serviço</h3>
<p>O roteador permite usar múltiplos serviços de IA por uma única interface, roteando automaticamente as requisições para o serviço correto baseado no modelo especificado.</p>
<pre><code class="language-typescript">import { AxAI, AxAIOpenAIModel, AxMultiServiceRouter } from '@ax-llm/ax'

// Configure OpenAI com lista de modelos
const openai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY,
  models: [
    {
      key: 'basic',
      model: AxAIOpenAIModel.GPT4OMini,
      description:
        'Modelo para tarefas muito simples como responder perguntas rápidas e curtas',
    },
    {
      key: 'medium',
      model: AxAIOpenAIModel.GPT4O,
      description:
        'Modelo para tarefas semi-complexas como resumir textos, escrever código e mais',
    },
  ],
})

// Configure Gemini com lista de modelos
const gemini = new AxAI({
  name: 'google-gemini',
  apiKey: process.env.GOOGLE_APIKEY,
  models: [
    {
      key: 'deep-thinker',
      model: 'gemini-2.0-flash-thinking',
      description:
        'Modelo que pode pensar profundamente sobre uma tarefa, ideal para tarefas que exigem planejamento',
    },
    {
      key: 'expert',
      model: 'gemini-2.0-pro',
      description:
        'Modelo ideal para tarefas muito complexas como escrever ensaios longos, códigos complexos e mais',
    },
  ],
})

const ollama = new AxAI({
  name: 'ollama',
  config: { model: 'nous-hermes2' },
})

const secretService = {
  key: 'sensitive-secret',
  service: ollama,
  description: 'Modelo para tarefas sensíveis',
}

// Crie um roteador com todos os serviços
const router = new AxMultiServiceRouter([openai, gemini, secretService])

// Roteie para o modelo expert do OpenAI
const openaiResponse = await router.chat({
  chatPrompt: [{ role: 'user', content: 'Hello!' }],
  model: 'expert',
})

// Ou use o roteador com AxGen
const gen = new AxGen(`question -&gt; answer`)
const res = await gen.forward(router, { question: 'Hello!' })
</code></pre>
<p>O balanceador é ideal para alta disponibilidade, enquanto o roteador é perfeito quando você precisa de modelos específicos para tarefas específicas. Ambos podem ser usados com qualquer recurso do Ax, como streaming, chamadas de função e prompting chain-of-thought.</p>
<p>Você também pode usar balanceador e roteador juntos — múltiplos balanceadores podem ser usados com o roteador ou vice-versa.</p>
<h2>Suporte OpenTelemetry</h2>
<p>A habilidade de rastrear e observar seu fluxo de trabalho LLM é fundamental para construir fluxos de produção. OpenTelemetry é o padrão da indústria, e suportamos o novo namespace de atributos <code>gen_ai</code>. Confira <code>src/examples/telemetry.ts</code> para mais informações.</p>
<pre><code class="language-typescript">import { trace } from '@opentelemetry/api'
import {
  BasicTracerProvider,
  ConsoleSpanExporter,
  SimpleSpanProcessor,
} from '@opentelemetry/sdk-trace-base'

const provider = new BasicTracerProvider()
provider.addSpanProcessor(new SimpleSpanProcessor(new ConsoleSpanExporter()))
trace.setGlobalTracerProvider(provider)

const tracer = trace.getTracer('test')

const ai = new AxAI({
  name: 'ollama',
  config: { model: 'nous-hermes2' },
  options: { tracer },
})

const gen = new AxChainOfThought(
  ai,
  `text -&gt; shortSummary &quot;summarize in 5 to 10 words&quot;`
)

const res = await gen.forward({ text })
</code></pre>
<pre><code class="language-json">{
  &quot;traceId&quot;: &quot;ddc7405e9848c8c884e53b823e120845&quot;,
  &quot;name&quot;: &quot;Chat Request&quot;,
  &quot;id&quot;: &quot;d376daad21da7a3c&quot;,
  &quot;kind&quot;: &quot;SERVER&quot;,
  &quot;timestamp&quot;: 1716622997025000,
  &quot;duration&quot;: 14190456.542,
  &quot;attributes&quot;: {
    &quot;gen_ai.system&quot;: &quot;Ollama&quot;,
    &quot;gen_ai.request.model&quot;: &quot;nous-hermes2&quot;,
    &quot;gen_ai.request.max_tokens&quot;: 500,
    &quot;gen_ai.request.temperature&quot;: 0.1,
    &quot;gen_ai.request.top_p&quot;: 0.9,
    &quot;gen_ai.request.frequency_penalty&quot;: 0.5,
    &quot;gen_ai.request.llm_is_streaming&quot;: false,
    &quot;http.request.method&quot;: &quot;POST&quot;,
    &quot;url.full&quot;: &quot;http://localhost:11434/v1/chat/completions&quot;,
    &quot;gen_ai.usage.completion_tokens&quot;: 160,
    &quot;gen_ai.usage.prompt_tokens&quot;: 290
  }
}
</code></pre>
<h2>Ajustando os Prompts (Básico)</h2>
<p>Você pode ajustar seus prompts usando um modelo maior para ajudá-los a funcionar de forma mais eficiente e gerar melhores resultados. Isso é feito usando um otimizador como <code>AxBootstrapFewShot</code> com exemplos do popular dataset <code>HotPotQA</code>. O otimizador gera demonstrações (<code>demos</code>) que, quando usadas com o prompt, ajudam a melhorar sua eficiência.</p>
<pre><code class="language-typescript">// Baixe o dataset HotPotQA do huggingface
const hf = new AxHFDataLoader({
  dataset: 'hotpot_qa',
  split: 'train',
})

const examples = await hf.getData&lt;{ question: string; answer: string }&gt;({
  count: 100,
  fields: ['question', 'answer'],
})

const ai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY as string,
})

// Configure o programa para ajustar
const program = new AxChainOfThought&lt;{ question: string }, { answer: string }&gt;(
  ai,
  `question -&gt; answer &quot;in short 2 or 3 words&quot;`
)

// Configure um otimizador Bootstrap Few Shot para ajustar o programa acima
const optimize = new AxBootstrapFewShot&lt;
  { question: string },
  { answer: string }
&gt;({
  program,
  examples,
})

// Configure uma métrica de avaliação em, f1 scores são formas populares de medir performance de recuperação.
const metricFn: AxMetricFn = ({ prediction, example }) =&gt;
  emScore(prediction.answer as string, example.answer as string)

// Execute o otimizador e lembre-se de salvar o resultado para uso posterior
const result = await optimize.compile(metricFn);

// Salve as demos geradas em um arquivo
// import fs from 'fs'; // Certifique-se de importar fs em seu script
fs.writeFileSync('bootstrap-demos.json', JSON.stringify(result.demos, null, 2));
console.log('Demos saved to bootstrap-demos.json');
</code></pre>
<img width="853" alt="tune-prompt" src="https://raw.githubusercontent.com/ax-llm/ax/main/githubusercontent.com/dosco/llm-client/assets/832235/f924baa7-8922-424c-9c2c-f8b2018d8d74">
<h2>Ajustando os Prompts (Avançado, Mipro v2)</h2>
<p>MiPRO v2 é um framework avançado de otimização de prompts que usa otimização Bayesiana para encontrar automaticamente as melhores instruções, demonstrações e exemplos para seus programas LLM. Ao explorar sistematicamente diferentes configurações de prompt, o MiPRO v2 ajuda a maximizar a performance do modelo sem ajustes manuais.</p>
<h3>Principais Recursos</h3>
<ul>
<li><strong>Otimização de instruções</strong>: Gera e testa automaticamente múltiplas instruções</li>
<li><strong>Seleção de exemplos few-shot</strong>: Encontra demonstrações ideais no seu dataset</li>
<li><strong>Otimização Bayesiana inteligente</strong>: Usa UCB para explorar configurações eficientemente</li>
<li><strong>Parada antecipada</strong>: Interrompe a otimização quando não há melhorias</li>
<li><strong>Ciente do programa e dos dados</strong>: Considera estrutura do programa e características do dataset</li>
</ul>
<h3>Como Funciona</h3>
<ol>
<li>Gera várias instruções candidatas</li>
<li>Bootstrapa exemplos few-shot a partir dos seus dados</li>
<li>Seleciona exemplos rotulados diretamente do seu dataset</li>
<li>Usa otimização Bayesiana para encontrar a combinação ideal</li>
<li>Aplica a melhor configuração ao seu programa</li>
</ol>
<h3>Uso Básico</h3>
<pre><code class="language-typescript">import { AxAI, AxChainOfThought, AxMiPRO } from '@ax-llm/ax'

// 1. Configure seu serviço AI
const ai = new AxAI({
  name: 'google-gemini',
  apiKey: process.env.GOOGLE_APIKEY,
})

// 2. Crie seu programa
const program = new AxChainOfThought(`input -&gt; output`)

// 3. Configure o otimizador
const optimizer = new AxMiPRO({
  ai,
  program,
  examples: trainingData, // Seus exemplos de treinamento
  options: {
    numTrials: 20, // Número de configurações a tentar
    auto: 'medium', // Nível de otimização
  },
})

// 4. Defina sua métrica de avaliação
const metricFn = ({ prediction, example }) =&gt; {
  return prediction.output === example.output
}

// 5. Execute a otimização
const optimizedProgram = await optimizer.compile(metricFn, {
  valset: validationData, // Conjunto de validação opcional
})

// 6. Use o programa otimizado
const result = await optimizedProgram.forward(ai, { input: 'test input' })
</code></pre>
<h3>Opções de Configuração</h3>
<p>MiPRO v2 oferece opções de configuração extensivas:</p>
<p>| Opção                    | Descrição                                     | Padrão  |
| ------------------------ | --------------------------------------------- | ------- |
| <code>numCandidates</code>          | Número de instruções candidatas a gerar       | 5       |
| <code>numTrials</code>              | Número de tentativas de otimização            | 30      |
| <code>maxBootstrappedDemos</code>   | Máximo de demonstrações bootstrapped          | 3       |
| <code>maxLabeledDemos</code>        | Máximo de exemplos rotulados                  | 4       |
| <code>minibatch</code>              | Usa minibatches para avaliação mais rápida    | true    |
| <code>minibatchSize</code>          | Tamanho dos minibatches de avaliação          | 25      |
| <code>earlyStoppingTrials</code>    | Para se não houver melhoria após N tentativas | 5       |
| <code>minImprovementThreshold</code>| Limite mínimo de melhoria                     | 0.01    |
| <code>programAwareProposer</code>   | Usa estrutura do programa para propostas      | true    |
| <code>dataAwareProposer</code>      | Considera características do dataset          | true    |
| <code>verbose</code>                | Mostra progresso detalhado da otimização      | false   |
| abort-patterns.ts | Exemplo de como abortar requisições |</p>
<h3>Níveis de Otimização</h3>
<p>Você pode configurar rapidamente a intensidade da otimização com o parâmetro <code>auto</code>:</p>
<pre><code class="language-typescript">// Otimização leve (rápida, menos minuciosa)
const optimizedProgram = await optimizer.compile(metricFn, { auto: 'light' })

// Otimização média (balanceada)
const optimizedProgram = await optimizer.compile(metricFn, { auto: 'medium' })

// Otimização pesada (mais lenta, mais minuciosa)
const optimizedProgram = await optimizer.compile(metricFn, { auto: 'heavy' })
</code></pre>
<h3>Exemplo Avançado: Análise de Sentimento</h3>
<pre><code class="language-typescript">// Crie um programa de análise de sentimento
const classifyProgram = new AxChainOfThought&lt;
  { productReview: string },
  { label: string }
&gt;(`productReview -&gt; label:string &quot;positive&quot; or &quot;negative&quot;`)

// Configure o otimizador com opções avançadas
const optimizer = new AxMiPRO({
  ai,
  program: classifyProgram,
  examples: trainingData,
  options: {
    numCandidates: 3,
    numTrials: 10,
    maxBootstrappedDemos: 2,
    maxLabeledDemos: 3,
    earlyStoppingTrials: 3,
    programAwareProposer: true,
    dataAwareProposer: true,
    verbose: true,
  },
})

// Execute a otimização e salve o resultado
const optimizedProgram = await optimizer.compile(metricFn, {
  valset: validationData,
})

// Salve a configuração para uso futuro
const programConfig = JSON.stringify(optimizedProgram, null, 2);
await fs.promises.writeFile(&quot;./optimized-config.json&quot;, programConfig);
console.log('&gt; Done. Optimized program config saved to optimized-config.json');
</code></pre>
<h2>Usando os Prompts Ajustados</h2>
<p>Tanto o otimizador Bootstrap Few Shot quanto o avançado MiPRO v2 geram <strong>demos</strong> (demonstrações) que melhoram significativamente a performance do seu programa. Essas demos são exemplos que mostram ao LLM como lidar corretamente com tarefas similares.</p>
<h3>O que são Demos?</h3>
<p>Demos são exemplos de entrada-saída que são automaticamente incluídos nos seus prompts para guiar o LLM. Eles atuam como exemplos de few-shot learning, mostrando ao modelo o comportamento esperado para sua tarefa específica.</p>
<h3>Carregando e Usando Demos</h3>
<p>Seja usando Bootstrap Few Shot ou MiPRO v2, o processo de uso das demos geradas é o mesmo:</p>
<pre><code class="language-typescript">import fs from 'fs'
import { AxAI, AxGen, AxChainOfThought } from '@ax-llm/ax'

// 1. Configure seu serviço AI
const ai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY,
})

// 2. Crie seu programa (mesma assinatura usada no ajuste)
const program = new AxChainOfThought(`question -&gt; answer &quot;in short 2 or 3 words&quot;`)

// 3. Carregue as demos do arquivo salvo
const demos = JSON.parse(fs.readFileSync('bootstrap-demos.json', 'utf8'))

// 4. Aplique as demos ao seu programa
program.setDemos(demos)

// 5. Use seu programa aprimorado
const result = await program.forward(ai, {
  question: 'What castle did David Gregory inherit?'
})

console.log(result) // Agora com melhor desempenho usando exemplos aprendidos
</code></pre>
<h3>Exemplo Simples: Classificação de Texto</h3>
<p>Veja um exemplo completo mostrando como demos melhoram uma tarefa de classificação:</p>
<pre><code class="language-typescript">// Crie um programa de classificação
const classifier = new AxGen(`text -&gt; category:class &quot;positive, negative, neutral&quot;`)

// Carregue demos geradas de Bootstrap ou MiPRO tuning
const savedDemos = JSON.parse(fs.readFileSync('classification-demos.json', 'utf8'))
classifier.setDemos(savedDemos)

// Agora o classificador aprendeu com exemplos e tem melhor desempenho
const result = await classifier.forward(ai, {
  text: &quot;This product exceeded my expectations!&quot;
})

console.log(result.category) // Classificação mais precisa
</code></pre>
<h3>Principais Benefícios do Uso de Demos</h3>
<ul>
<li><strong>Maior Precisão</strong>: Programas performam muito melhor com exemplos relevantes</li>
<li><strong>Saída Consistente</strong>: Demos ajudam a manter formatos de resposta consistentes</li>
<li><strong>Reduz Alucinações</strong>: Exemplos direcionam o modelo para comportamentos esperados</li>
<li><strong>Custo-efetivo</strong>: Melhores resultados sem precisar de modelos maiores/caros</li>
</ul>
<h3>Boas Práticas</h3>
<ol>
<li><strong>Salve suas Demos</strong>: Sempre salve demos geradas para reutilização</li>
<li><strong>Combine Assinaturas</strong>: Use exatamente a mesma assinatura ao carregar demos</li>
<li><strong>Controle de Versão</strong>: Mantenha arquivos de demos sob controle de versão para reprodutibilidade</li>
<li><strong>Atualize Regularmente</strong>: Reajuste periodicamente com novos dados para melhorar as demos</li>
</ol>
<p>Tanto Bootstrap Few Shot quanto MiPRO v2 geram demos no mesmo formato, então você pode usar o mesmo padrão de carregamento independentemente do otimizador.</p>
<h2>Funções Embutidas</h2>
<p>| Função              | Nome                | Descrição                                     |
| ------------------- | ------------------- | --------------------------------------------- |
| Interpretador JS    | AxJSInterpreter     | Executa código JS em ambiente isolado         |
| Docker Sandbox      | AxDockerSession     | Executa comandos em ambiente Docker           |
| Adaptador de Embeddings | AxEmbeddingAdapter | Busca e passa embedding para sua função    |</p>
<h2>Veja todos os exemplos</h2>
<p>Use o comando <code>tsx</code> para rodar os exemplos. Ele permite rodar código Typescript no Node. Também suporta uso de arquivo <code>.env</code> para passar as chaves da API de IA ao invés de colocar na linha de comando.</p>
<pre><code class="language-shell">OPENAI_APIKEY=api-key npm run tsx ./src/examples/marketing.ts
</code></pre>
<p>| Exemplo                  | Descrição                                               |
| ------------------------ | ------------------------------------------------------ |
| customer-support.ts      | Extrai detalhes de comunicações com clientes           |
| function.ts              | Exemplo simples de chamada de função                   |
| food-search.ts           | Exemplo multi-etapas e multi-funções                   |
| marketing.ts             | Gera mensagens SMS de marketing curtas e eficazes      |
| vectordb.ts              | Chunk, embed e busca de texto                          |
| fibonacci.ts             | Usa interpretador JS para calcular fibonacci           |
| summarize.ts             | Gera resumo curto de um texto grande                   |
| chain-of-thought.ts      | Usa prompting chain-of-thought para responder questões |
| rag.ts                   | Usa multi-hop retrieval para responder questões        |
| rag-docs.ts              | Converte PDF em texto e faz embedding para RAG search  |
| react.ts                 | Usa chamadas de função e raciocínio para responder     |
| agent.ts                 | Framework de agentes; agentes podem usar outros agentes|
| streaming1.ts            | Validação de campos de saída durante streaming         |
| streaming2.ts            | Validação por campo durante streaming                  |
| streaming3.ts            | Exemplo de streaming end-to-end <code>streamingForward()</code>   |
| smart-hone.ts            | Agente procura cachorro em casa inteligente            |
| multi-modal.ts           | Usa imagem como entrada junto com texto                |
| balancer.ts              | Balanceia entre vários LLMs por custo, etc             |
| docker.ts                | Usa sandbox Docker para encontrar arquivos por descrição|
| prime.ts                 | Usa processadores de campo em prompt                   |
| simple-classify.ts       | Usa classificador simples para classificar itens       |
| mcp-client-memory.ts     | Exemplo de MCP server para memória com Ax              |
| mcp-client-blender.ts    | Exemplo de MCP server para Blender com Ax              |
| mcp-client-pipedream.ts  | Exemplo de integração com MCP remoto                   |
| tune-bootstrap.ts        | Usa otimizador bootstrap para eficiência de prompt     |
| tune-mipro.ts            | Usa otimizador mipro v2 para eficiência de prompt      |
| tune-usage.ts            | Usa prompts otimizados                                 |
| telemetry.ts             | Trace e envia traces para serviço Jaeger               |
| openai-responses.ts      | Exemplo usando OpenAI Responses API                    |
| use-examples.ts | Exemplo de uso de 'examples' para direcionar o llm              |</p>
<h2>Nosso Objetivo</h2>
<p>Grandes modelos de linguagem (LLMs) estão cada vez mais poderosos e já podem funcionar como backend de um produto inteiro. Porém, ainda há muita complexidade a ser gerenciada, como prompts corretos, modelos, streaming, chamadas de função, correção de erros e muito mais. Nosso objetivo é empacotar toda essa complexidade em uma biblioteca bem mantida e fácil de usar, que funcione com todos os LLMs de ponta. Além disso, utilizamos as pesquisas mais recentes para adicionar novas capacidades como DSPy à biblioteca.</p>
<h2>Como usar esta biblioteca?</h2>
<h3>1. Escolha uma IA para trabalhar</h3>
<pre><code class="language-ts">// Escolha um LLM
const ai = new AxOpenAI({ apiKey: process.env.OPENAI_APIKEY } as AxOpenAIArgs)
</code></pre>
<h3>2. Crie uma assinatura de prompt baseada no seu caso de uso</h3>
<pre><code class="language-ts">// A assinatura define as entradas e saídas do seu programa de prompt
const cot = new ChainOfThought(ai, `question:string -&gt; answer:string`, { mem })
</code></pre>
<h3>3. Execute este novo programa de prompt</h3>
<pre><code class="language-ts">// Passe os campos de entrada definidos na assinatura acima
const res = await cot.forward({ question: 'Are we in a simulation?' })
</code></pre>
<h3>4. Ou se quiser usar o LLM diretamente</h3>
<pre><code class="language-ts">const res = await ai.chat([
  { role: &quot;system&quot;, content: &quot;Help the customer with his questions&quot; }
  { role: &quot;user&quot;, content: &quot;I'm looking for a Macbook Pro M2 With 96GB RAM?&quot; }
]);
</code></pre>
<h2>Como usar chamadas de função</h2>
<h3>1. Defina as funções</h3>
<pre><code class="language-ts">// defina uma ou mais funções e um handler de função
const functions = [
  {
    name: 'getCurrentWeather',
    description: 'get the current weather for a location',
    parameters: {
      type: 'object',
      properties: {
        location: {
          type: 'string',
          description: 'location to get weather for',
        },
        units: {
          type: 'string',
          enum: ['imperial', 'metric'],
          default: 'imperial',
          description: 'units to use',
        },
      },
      required: ['location'],
    },
    func: async (args: Readonly&lt;{ location: string; units: string }&gt;) =&gt; {
      return `The weather in ${args.location} is 72 degrees`
    },
  },
]
</code></pre>
<h3>2. Passe as funções para um prompt</h3>
<pre><code class="language-ts">const cot = new AxGen(ai, `question:string -&gt; answer:string`, { functions })
</code></pre>
<h2>Ativar logs de debug</h2>
<pre><code class="language-ts">const ai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY,
} as AxOpenAIArgs)
ai.setOptions({ debug: true })
</code></pre>
<h2>Entre em contato</h2>
<p>Estamos felizes em ajudar! Entre em contato se tiver dúvidas ou junte-se ao Discord
<a href="https://twitter.com/dosco">twitter/dosco</a></p>
<h2>FAQ</h2>
<h3>1. O LLM não encontra a função correta</h3>
<p>Melhore o nome e a descrição da função. Seja claro sobre o que a função faz. Além disso, garanta que os parâmetros tenham boas descrições. As descrições podem ser curtas, mas precisam ser precisas.</p>
<h3>2. Como altero a configuração do LLM que estou usando?</h3>
<p>Você pode passar um objeto de configuração como segundo parâmetro ao criar um novo objeto LLM.</p>
<pre><code class="language-ts">const apiKey = process.env.OPENAI_APIKEY
const conf = AxOpenAIBestConfig()
const ai = new AxOpenAI({ apiKey, conf } as AxOpenAIArgs)
</code></pre>
<h2>3. Meu prompt está muito longo / posso alterar o max tokens?</h2>
<pre><code class="language-ts">const conf = axOpenAIDefaultConfig() // ou OpenAIBestOptions()
conf.maxTokens = 2000
</code></pre>
<h2>4. Como altero o modelo? (ex: quero usar o GPT4)</h2>
<pre><code class="language-ts">const conf = axOpenAIDefaultConfig() // ou OpenAIBestOptions()
conf.model = OpenAIModel.GPT4Turbo
</code></pre>
<h2>Dicas &amp; truques para Monorepo</h2>
<p>É essencial lembrar que devemos rodar <code>npm install</code> apenas no diretório raiz. Isso previne a criação de arquivos <code>package-lock.json</code> aninhados e evita <code>node_modules</code> não deduplicados.</p>
<p>Adicionar novas dependências em pacotes deve ser feito com, por exemplo,
<code>npm install lodash --workspace=ax</code> (ou apenas modificar o <code>package.json</code> apropriado e rodar <code>npm install</code> do root).</p>
<hr />
<p>Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx">Open Ai Tx</a> | Last indexed: 2025-06-07</p>
<hr />

        </div>
    </div>
    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Default Statcounter code for openaitx
https://openaitx.github.io/ -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>
    <!-- End of Statcounter Code -->
</body>
</html>