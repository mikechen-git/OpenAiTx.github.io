<!DOCTYPE html>
<html lang="de">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>StyTR-2 - diyiiyiii/StyTR-2</title>
    <meta name="title" content="StyTR-2 - diyiiyiii/StyTR-2">
    <meta name="description" content="diyiiyiii/StyTR-2 - GitHub repository de documentation and informationStyTr^2 : Bildstiltransfer mit Transformern (CVPR2022) Autoren: Yingying Deng, Fan Tang, XingjiaPan, Weiming Dong, Chongyang Ma, Changsheng Xu In dieser Arbeit wird ein Ansatz vorgestellt, um einen unbeeinflussten Bildstiltransfer auf Basis des Transformer-Modells zu erreichen. Im Vergleich zu aktuellen Methoden können wir den Stilisierungseffekt verbessern. Dieses Repository ist die offizielle Implementierung von SyTr^2 : Image Style Transfer with Transformers. Ergebnispräsentation Im Vergleich zu einigen modernen Algorithmen besitzt unsere Methode eine starke Fähigkeit zur Vermeidung von Inhaltsverlust und eine bessere Fähigkeit zur Merkmalsrepräsentation. Framework Die Gesamtpipeline unseres StyTr^2-Frameworks. Wir teilen die Inhalts- und Stilbilder in Patches auf und verwenden eine lineare Projektion, um Bildsequenzen zu erhalten. Dann werden die mit CAPE angereicherten Inhaltssequenzen dem Content-Transformer-Encoder zugeführt, während die Stilsequenzen dem Style-Transformer-Encoder zugeführt werden. Nach den beiden Transformer-Encodern wird ein mehrschichtiger Transformer-Decoder verwendet, um die Inhaltssequenzen gemäß den Stilsequenzen zu stilisieren. Schließlich verwenden wir einen progressiven Upsampling-Decoder, um die stilisierten Bilder mit...">
    <meta name="keywords" content="diyiiyiii, StyTR-2, GitHub, repository, de documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/diyiiyiii/StyTR-2/README-de.html">
    <meta property="og:title" content="StyTR-2 - diyiiyiii/StyTR-2">
    <meta property="og:description" content="diyiiyiii/StyTR-2 - GitHub repository de documentation and information">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">
    <script src="/js/marked.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        body { display: flex; flex-direction: column; min-height: 100vh; }
        .main-container { margin: 0 auto; width: 100%; max-width: 980px; padding: 0 20px; }
        @media (max-width: 768px) { .main-container { padding: 0 15px; } }
        .markdown-body img { max-width: 100%; height: auto; }
        .header { text-align: center; margin-bottom: 30px; padding: 20px; background-color: #f6f8fa; border-bottom: 1px solid #e1e4e8; position: relative; }
        .header .links { margin-top: 10px; font-size: 16px; }
        .header .links a { color: #0366d6; text-decoration: none; margin-left: 5px; }
        .header .links a:hover { text-decoration: underline; }
        .language-badges { margin-top: 15px; text-align: center; }
        .language-badges a { display: inline-block; margin: 2px; text-decoration: none; }
        .language-badges img { height: 20px; border-radius: 3px; }
        .language-badges a:hover img { opacity: 0.8; }
    </style>
</head>
<body>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/diyiiyiii/StyTR-2" id="githubRepoLink" target="_blank">diyiiyiii/StyTR-2</a>
<br>
<h1 style="display: none;">StyTr^2 : Bildstiltransfer mit Transformern (CVPR2022) Autoren: Yingying Deng, Fan Tang, XingjiaPan, Weiming Dong, Chongyang Ma, Changsheng Xu In dieser Arbeit wird ein Ansatz vorgestellt, um einen unbeeinflussten Bildstiltransfer auf Basis des Transformer-Modells zu erreichen. Im Vergleich zu aktuellen Methoden können wir den Stilisierungseffekt verbessern. Dieses Repository ist die offizielle Implementierung von SyTr^2 : Image Style Transfer with Transformers. Ergebnispräsentation Im Vergleich zu einigen modernen Algorithmen besitzt unsere Methode eine starke Fähigkeit zur Vermeidung von Inhaltsverlust und eine bessere Fähigkeit zur Merkmalsrepräsentation. Framework Die Gesamtpipeline unseres StyTr^2-Frameworks. Wir teilen die Inhalts- und Stilbilder in Patches auf und verwenden eine lineare Projektion, um Bildsequenzen zu erhalten. Dann werden die mit CAPE angereicherten Inhaltssequenzen dem Content-Transformer-Encoder zugeführt, während die Stilsequenzen dem Style-Transformer-Encoder zugeführt werden. Nach den beiden Transformer-Encodern wird ein mehrschichtiger Transformer-Decoder verwendet, um die Inhaltssequenzen gemäß den Stilsequenzen zu stilisieren. Schließlich verwenden wir einen progressiven Upsampling-Decoder, um die stilisierten Bilder mit...</h1>
        </div>
        <div class="language-badges" id="languageBadges">
            <!-- You can generate language badges here if needed -->
        </div>
    </div>
    <div class="main-container">
        <div class="markdown-body" id="content">
            <h1>StyTr^2 : Bildstiltransfer mit Transformern (CVPR2022)</h1>
<p><em>Autoren: <a href="https://diyiiyiii.github.io/">Yingying Deng</a>, Fan Tang, XingjiaPan, Weiming Dong, Chongyang Ma, Changsheng Xu</em></p>
<p>In dieser Arbeit wird ein Ansatz vorgestellt, um einen unbeeinflussten Bildstiltransfer auf Basis des Transformer-Modells zu erreichen. Im Vergleich zu aktuellen Methoden können wir den Stilisierungseffekt verbessern.
Dieses Repository ist die offizielle Implementierung von <a href="https://arxiv.org/abs/2105.14576">SyTr^2 : Image Style Transfer with Transformers</a>.</p>
<h2>Ergebnispräsentation</h2>
<p align="center">
<img src="https://github.com/diyiiyiii/StyTR-2/blob/main/Figure/Unbiased.png" width="90%" height="90%">
</p>
Im Vergleich zu einigen modernen Algorithmen besitzt unsere Methode eine starke Fähigkeit zur Vermeidung von Inhaltsverlust und eine bessere Fähigkeit zur Merkmalsrepräsentation.  <br>
<h2>Framework</h2>
<p align="center">
<img src="https://github.com/diyiiyiii/StyTR-2/blob/main/Figure/network.png" width="100%" height="100%">
</p> 
Die Gesamtpipeline unseres StyTr^2-Frameworks. Wir teilen die Inhalts- und Stilbilder in Patches auf und verwenden eine lineare Projektion, um Bildsequenzen zu erhalten. Dann werden die mit CAPE angereicherten Inhaltssequenzen dem Content-Transformer-Encoder zugeführt, während die Stilsequenzen dem Style-Transformer-Encoder zugeführt werden. Nach den beiden Transformer-Encodern wird ein mehrschichtiger Transformer-Decoder verwendet, um die Inhaltssequenzen gemäß den Stilsequenzen zu stilisieren. Schließlich verwenden wir einen progressiven Upsampling-Decoder, um die stilisierten Bilder mit hoher Auflösung zu erhalten.
<h2>Experiment</h2>
<h3>Anforderungen</h3>
<ul>
<li>python 3.6</li>
<li>pytorch 1.4.0</li>
<li>PIL, numpy, scipy</li>
<li>tqdm  <br></li>
</ul>
<h3>Testen</h3>
<p>Vorgefertigte Modelle: <a href="https://drive.google.com/file/d/1BinnwM5AmIcVubr16tPTqxMjUCE8iu5M/view?usp=sharing">vgg-model</a>,  <a href="https://drive.google.com/file/d/1C3xzTOWx8dUXXybxZwmjijZN8SrC3e4B/view?usp=sharing">vit_embedding</a>, <a href="https://drive.google.com/file/d/1fIIVMTA_tPuaAAFtqizr6sd1XV7CX6F9/view?usp=sharing">decoder</a>, <a href="https://drive.google.com/file/d/1dnobsaLeE889T_LncCkAA2RkqzwsfHYy/view?usp=sharing">Transformer_module</a>   <br>
Bitte laden Sie diese herunter und legen Sie sie in den Ordner ./experiments/ ab.  <br></p>
<pre><code>python test.py  --content_dir input/content/ --style_dir input/style/    --output out
</code></pre>
<h3>Training</h3>
<p>Das Stil-Datenset ist WikiArt, gesammelt von <a href="https://www.wikiart.org/">WIKIART</a>  <br><br />
Das Inhalts-Datenset ist COCO2014  <br></p>
<pre><code>python train.py --style_dir ../../datasets/Images/ --content_dir ../../datasets/train2014 --save_dir models/ --batch_size 8
</code></pre>
<h3>Referenz</h3>
<p>Wenn Sie unsere Arbeit in Ihrer Forschung verwenden, zitieren Sie bitte unser Paper mit dem folgenden BibTeX-Eintrag ~ Vielen Dank ^ . ^. Paper Link <a href="https://arxiv.org/abs/2105.14576">pdf</a><br></p>
<pre><code>@inproceedings{deng2021stytr2,
      title={StyTr^2: Image Style Transfer with Transformers}, 
      author={Yingying Deng and Fan Tang and Weiming Dong and Chongyang Ma and Xingjia Pan and Lei Wang and Changsheng Xu},
      booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
      year={2022},
}
</code></pre>
<hr />
<p>Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx">Open Ai Tx</a> | Last indexed: 2025-06-09</p>
<hr />

        </div>
    </div>
    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Statcounter and other scripts can be added here -->
</body>
</html>