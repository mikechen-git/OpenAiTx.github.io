<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>StyTR-2 - diyiiyiii/StyTR-2</title>
    <meta name="title" content="StyTR-2 - diyiiyiii/StyTR-2">
    <meta name="description" content="diyiiyiii/StyTR-2 - GitHub repository en documentation and informationStyTr^2 : Image Style Transfer with Transformers (CVPR2022) Authors: Yingying Deng, Fan Tang, Xingjia Pan, Weiming Dong, Chongyang Ma, Changsheng Xu This paper is proposed to achieve unbiased image style transfer based on the transformer model. We can promote the stylization effect compared with state-of-the-art methods. This repository is the official implementation of SyTr^2 : Image Style Transfer with Transformers. Results presentation Compared with some state-of-the-art algorithms, our method has a strong ability to avoid content leakage and has better feature representation ability. Framework The overall pipeline of our StyTr^2 framework. We split the content and style images into patches, and use a linear projection to obtain image sequences. Then the content sequences added with CAPE are fed into the content transformer encoder, while the style sequences are fed into the style transformer encoder. Following the two transformer encoders, a multi-layer transformer decoder is adopted to stylize the content sequences...">
    <meta name="keywords" content="diyiiyiii, StyTR-2, GitHub, repository, en documentation">
    <meta name="author" content="Open AI Tx">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://openaitx.github.io/projects/diyiiyiii/StyTR-2/README-en.html">
    <meta property="og:title" content="StyTR-2 - diyiiyiii/StyTR-2">
    <meta property="og:description" content="diyiiyiii/StyTR-2 - GitHub repository en documentation and information">
    <meta property="og:image" content="https://openaitx.github.io/logo_crop.png">
    <link rel="icon" type="image/jpeg" href="/icon.jpg">
    <link rel="apple-touch-icon" href="/icon.jpg">
    <script src="/js/marked.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/css/github.min.css?v=20250613">
    <script src="/js/highlight.min.js?v=20250613"></script>
    <link rel="stylesheet" href="/view.css?v=20250613">
    <style>
        body { display: flex; flex-direction: column; min-height: 100vh; }
        .main-container { margin: 0 auto; width: 100%; max-width: 980px; padding: 0 20px; }
        @media (max-width: 768px) { .main-container { padding: 0 15px; } }
        .markdown-body img { max-width: 100%; height: auto; }
        .header { text-align: center; margin-bottom: 30px; padding: 20px; background-color: #f6f8fa; border-bottom: 1px solid #e1e4e8; position: relative; }
        .header .links { margin-top: 10px; font-size: 16px; }
        .header .links a { color: #0366d6; text-decoration: none; margin-left: 5px; }
        .header .links a:hover { text-decoration: underline; }
        .language-badges { margin-top: 15px; text-align: center; }
        .language-badges a { display: inline-block; margin: 2px; text-decoration: none; }
        .language-badges img { height: 20px; border-radius: 3px; }
        .language-badges a:hover img { opacity: 0.8; }
    </style>
</head>
<body>
    <div class="header">
        <div class="links">
            GitHub Repository: <a href="https://github.com/diyiiyiii/StyTR-2" id="githubRepoLink" target="_blank">diyiiyiii/StyTR-2</a>
<br>
<h1 style="display: none;">StyTr^2 : Image Style Transfer with Transformers (CVPR2022) Authors: Yingying Deng, Fan Tang, Xingjia Pan, Weiming Dong, Chongyang Ma, Changsheng Xu This paper is proposed to achieve unbiased image style transfer based on the transformer model. We can promote the stylization effect compared with state-of-the-art methods. This repository is the official implementation of SyTr^2 : Image Style Transfer with Transformers. Results presentation Compared with some state-of-the-art algorithms, our method has a strong ability to avoid content leakage and has better feature representation ability. Framework The overall pipeline of our StyTr^2 framework. We split the content and style images into patches, and use a linear projection to obtain image sequences. Then the content sequences added with CAPE are fed into the content transformer encoder, while the style sequences are fed into the style transformer encoder. Following the two transformer encoders, a multi-layer transformer decoder is adopted to stylize the content sequences...</h1>
        </div>
        <div class="language-badges" id="languageBadges">
            <!-- You can generate language badges here if needed -->
        </div>
    </div>
    <div class="main-container">
        <div class="markdown-body" id="content">
            <h1>StyTr^2 : Image Style Transfer with Transformers (CVPR2022)</h1>
<p><em>Authors: <a href="https://diyiiyiii.github.io/">Yingying Deng</a>, Fan Tang, Xingjia Pan, Weiming Dong, Chongyang Ma, Changsheng Xu</em></p>
<p>This paper is proposed to achieve unbiased image style transfer based on the transformer model. We can promote the stylization effect compared with state-of-the-art methods.
This repository is the official implementation of <a href="https://arxiv.org/abs/2105.14576">SyTr^2 : Image Style Transfer with Transformers</a>.</p>
<h2>Results presentation</h2>
<p align="center">
<img src="https://raw.githubusercontent.com/diyiiyiii/StyTR-2/main/Figure/Unbiased.png" width="90%" height="90%">
</p>
Compared with some state-of-the-art algorithms, our method has a strong ability to avoid content leakage and has better feature representation ability.  <br>
<h2>Framework</h2>
<p align="center">
<img src="https://raw.githubusercontent.com/diyiiyiii/StyTR-2/main/Figure/network.png" width="100%" height="100%">
</p> 
The overall pipeline of our StyTr^2 framework. We split the content and style images into patches, and use a linear projection to obtain image sequences. Then the content sequences added with CAPE are fed into the content transformer encoder, while the style sequences are fed into the style transformer encoder. Following the two transformer encoders, a multi-layer transformer decoder is adopted to stylize the content sequences according to the style sequences. Finally, we use a progressive upsampling decoder to obtain the stylized images with high-resolution.
<h2>Experiment</h2>
<h3>Requirements</h3>
<ul>
<li>python 3.6</li>
<li>pytorch 1.4.0</li>
<li>PIL, numpy, scipy</li>
<li>tqdm  <br></li>
</ul>
<h3>Testing</h3>
<p>Pretrained models: <a href="https://drive.google.com/file/d/1BinnwM5AmIcVubr16tPTqxMjUCE8iu5M/view?usp=sharing">vgg-model</a>,  <a href="https://drive.google.com/file/d/1C3xzTOWx8dUXXybxZwmjijZN8SrC3e4B/view?usp=sharing">vit_embedding</a>, <a href="https://drive.google.com/file/d/1fIIVMTA_tPuaAAFtqizr6sd1XV7CX6F9/view?usp=sharing">decoder</a>, <a href="https://drive.google.com/file/d/1dnobsaLeE889T_LncCkAA2RkqzwsfHYy/view?usp=sharing">Transformer_module</a>   <br>
Please download them and put them into the folder ./experiments/  <br></p>
<pre><code>python test.py  --content_dir input/content/ --style_dir input/style/    --output out
</code></pre>
<h3>Training</h3>
<p>Style dataset is WikiArt collected from <a href="https://www.wikiart.org/">WIKIART</a>  <br><br />
content dataset is COCO2014  <br></p>
<pre><code>python train.py --style_dir ../../datasets/Images/ --content_dir ../../datasets/train2014 --save_dir models/ --batch_size 8
</code></pre>
<h3>Reference</h3>
<p>If you find our work useful in your research, please cite our paper using the following BibTeX entry ~ Thank you ^ . ^. Paper Link <a href="https://arxiv.org/abs/2105.14576">pdf</a><br></p>
<pre><code>@inproceedings{deng2021stytr2,
      title={StyTr^2: Image Style Transfer with Transformers}, 
      author={Yingying Deng and Fan Tang and Weiming Dong and Chongyang Ma and Xingjia Pan and Lei Wang and Changsheng Xu},
      booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
      year={2022},
}
</code></pre>
<hr />
<p>Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx">Open Ai Tx</a> | Last indexed: 2025-06-09</p>
<hr />

        </div>
    </div>
    <footer class="footer">
        Powered by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank">Open AI Tx</a>
    </footer>
    <!-- Statcounter and other scripts can be added here -->
</body>
</html>